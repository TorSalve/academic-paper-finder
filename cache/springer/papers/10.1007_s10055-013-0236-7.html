<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Creation of a new set of dynamic virtual reality faces for the assessm"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability"/>

    <meta name="dc.source" content="Virtual Reality 2013 18:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-10-22"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (n&#160;=&#160;98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants&#8217; accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant&#8217;s gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-10-22"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="61"/>

    <meta name="prism.endingPage" content="71"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0236-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0236-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0236-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0236-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2014/03"/>

    <meta name="citation_online_date" content="2013/10/22"/>

    <meta name="citation_firstpage" content="61"/>

    <meta name="citation_lastpage" content="71"/>

    <meta name="citation_article_type" content="SI: Cyberinterventions"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0236-7"/>

    <meta name="DOI" content="10.1007/s10055-013-0236-7"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0236-7"/>

    <meta name="description" content="The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used"/>

    <meta name="dc.creator" content="Jos&#233; Guti&#233;rrez-Maldonado"/>

    <meta name="dc.creator" content="Mar Rus-Calafell"/>

    <meta name="dc.creator" content="Joan Gonz&#225;lez-Conde"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Schizophr Res; citation_title=Facial affect recognition and information processing in schizophrenia and bipolar disorder; citation_author=J Addington, D Addington; citation_volume=32; citation_publication_date=1998; citation_pages=171-181; citation_doi=10.1016/S0920-9964(98)00042-5; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Br J Clin Psychol; citation_title=Movement, face processing and schizophrenia: evidence of a differential deficit in expression analysis; citation_author=J Archer, DC Hay, AW Young; citation_volume=33; citation_publication_date=1994; citation_pages=517-528; citation_doi=10.1111/j.2044-8260.1994.tb01148.x; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_title=Social skills training for schizophrenia; citation_publication_date=2004; citation_id=CR3; citation_author=AS Bellack; citation_author=KT Mueser; citation_author=S Gingerich; citation_author=J Agresta; citation_publisher=Guilford Press"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Commun Res; citation_title=Avatar-mediated networking: increasing social presence and interpersonal trust in net-based collaborations; citation_author=G Bente, S R&#252;ggenberg, NC Kr&#228;mer, F Eschenburg; citation_volume=34; citation_publication_date=2008; citation_pages=287-318; citation_doi=10.1111/j.1468-2958.2008.00322.x; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Eur Psychiat; citation_title=Increasing self-esteem: efficacy of a group intervention for individuals with severe mental disorders; citation_author=L Borras, M Boucherie, S Mohr, T Lecomte, N Perroud, P Huguelet; citation_volume=24; citation_publication_date=2009; citation_pages=307-316; citation_doi=10.1016/j.eurpsy.2009.01.003; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=Caricaturing facial expressions; citation_author=AJ Calder, D Rowland, AW Young, I Nimmo-Smith, J Keane, DI Perrett; citation_volume=76; citation_publication_date=2000; citation_pages=105-146; citation_doi=10.1016/S0010-0277(00)00074-3; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Neuropsychologia; citation_title=Facial expression recognition across the adult life span; citation_author=AJ Calder, J Keane, T Manly, R Sprengelmeyer, S Scott, I Nimmo-Smith; citation_volume=41; citation_publication_date=2003; citation_pages=195-202; citation_doi=10.1016/S0028-3932(02)00149-5; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Brain Res; citation_title=Audio-visual integration of emotion expression; citation_author=O Collignon, S Girarda, F Gosselina, S Roya, D Saint-Amoura, M Lassondea, F Leporea; citation_volume=25; citation_publication_date=2008; citation_pages=126-135; citation_doi=10.1016/j.brainres.2008.04.023; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Schizophr Bull; citation_title=The functional significance of social cognition in schizophrenia: a review; citation_author=SM Couture, DL Penn, DL Roberts; citation_volume=32; citation_publication_date=2006; citation_pages=s44-s63; citation_doi=10.1093/schbul/sbl029; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=J Abnorm Psychol; citation_title=Recognition of posed and genuine facial expression of emotion in paranoid and nonparanoid schizophrenia; citation_author=PJ Davis, MG Gibson; citation_volume=109; citation_publication_date=2000; citation_pages=445-450; citation_doi=10.1037/0021-843X.109.3.445; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Disord; citation_title=Emotion recognition in borderline personality disorder-a review of the literature; citation_author=G Domes, L Schulze, SC Herpertz; citation_volume=23; citation_publication_date=2009; citation_pages=6-19; citation_doi=10.1521/pedi.2009.23.1.6; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=Recognition profile of emotions in natural and virtual faces; citation_author=M Dyck, M Winbeck, S Leiberg, Y Chen, RC Gur, K Mathiak; citation_volume=3; citation_publication_date=2008; citation_pages=e3628; citation_doi=10.1371/journal.pone.0003628; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Psychiatr Res; citation_title=Virtual faces as a tool to study emotion recognition deficits in schizophrenia; citation_author=M Dyck, M Winbeck, S Leiberg, Y Chen, K Mathiak; citation_volume=179; citation_publication_date=2010; citation_pages=247-252; citation_doi=10.1016/j.psychres.2009.11.004; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Clin Psychol Rev; citation_title=Emotion recognition via facial expression and affective prosody in schizophrenia: a methodological review; citation_author=J Edwards, H Jackson, P Pattison; citation_volume=22; citation_publication_date=2002; citation_pages=789-832; citation_doi=10.1016/S0272-7358(02)00130-7; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Rev; citation_title=Are there basic emotions?; citation_author=P Ekman; citation_volume=99; citation_publication_date=1992; citation_pages=550-553; citation_doi=10.1037/0033-295X.99.3.550; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_title=Pictures of facial affect CD-Rom; citation_publication_date=1975; citation_id=CR16; citation_author=P Ekman; citation_author=W Friesen; citation_publisher=University of California"/>

    <meta name="citation_reference" content="citation_title=Facial action coding system; citation_publication_date=1978; citation_id=CR17; citation_author=P Ekman; citation_author=W Friesen; citation_publisher=Consulting Psychologists Press"/>

    <meta name="citation_reference" content="Fabri M, Moore D, Hobbs D (2002) Expressive agents: non-verbal communication in collaborative virtual environments. In: Proceedings of autonomous agents and multi-agent systems, Bologna, Italy"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Mediating the expression of emotion in educational collaborative virtual environments: an experimental study; citation_author=M Fabri, D Moore, D Hobbs; citation_volume=7; citation_publication_date=2004; citation_pages=66-81; citation_doi=10.1007/s10055-003-0116-7; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=Avatars in clinical psychology: a framework for the clinical use of virtual humans; citation_author=A Gaggioli, F Mantovani, G Castelnuovo, B Wiederhold, G Riva; citation_volume=6; citation_publication_date=2003; citation_pages=117-125; citation_doi=10.1089/109493103321640301; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Schizophr Bull; citation_title=Social cognition in Schizophrenia: recommendation from measurement and treatment to improve cognition in schizophrenia new approaches conferences; citation_author=M Green, B Olivier, J Crowley, DL Penn, S Silverstein; citation_volume=31; citation_publication_date=2005; citation_pages=882-887; citation_doi=10.1093/schbul/sbi049; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=J Neurosci Methods; citation_title=A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies; citation_author=RC Gur, R Sara, M Hagendoorn, O Marom, P Hughett, L Macy, T Turner, R Bajcsy, A Posner, RE Gur; citation_volume=115; citation_publication_date=2002; citation_pages=137-143; citation_doi=10.1016/S0165-0270(02)00006-7; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=Brain activation during facial emotion processing; citation_author=RC Gur, L Schroeder, T Turner, C McGrath, RM Chan, BI Turetsky, D Alsop, J Maldjian, RE Gur; citation_volume=16; citation_publication_date=2002; citation_pages=651-662; citation_doi=10.1006/nimg.2002.1097; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=Stud Health Technol Inform; citation_title=Association between facial emotion recognition, cognition and alexithymia in patients with schizophrenia: comparison of photographic and virtual reality presentations; citation_author=J Guti&#233;rrez-Maldonado, M Rus-Calafell, S M&#225;rquez-Rej&#243;n, J Ribas-Sabat&#233;; citation_volume=181; citation_publication_date=2012; citation_pages=88-92; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Biol Med; citation_title=Virtual reality prototype for measurement of expression characteristics in emotional situations; citation_author=K Han, J Ku, K Kim, HJ Jang, J Park, JJ Kim, CH Kim, MH Choi, IY Kim, SI Kim; citation_volume=39; citation_publication_date=2009; citation_pages=173-179; citation_doi=10.1016/j.compbiomed.2008.12.002; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Am J Ment Retard; citation_title=Recognition of facial emotional expressions from moving and static displays by individuals with mental retardation; citation_author=NK Harwood, LJ Hall, AJ Shinkfield; citation_volume=104; citation_publication_date=1999; citation_pages=270-278; citation_doi=10.1352/0895-8017(1999)104&lt;0270:ROFEEF&gt;2.0.CO;2; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Schizophr Res; citation_title=A meta-analysis of emotion perception and functional outcomes in schizophrenia; citation_author=F Irani, S Seligman, V Kamath, C Kohler, RC Gur; citation_volume=137; citation_publication_date=2012; citation_pages=203-211; citation_doi=10.1016/j.schres.2012.01.023; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Cognit; citation_title=The role of movement in face recognition; citation_author=B Knight, A Johnston; citation_volume=4; citation_publication_date=1997; citation_pages=265-273; citation_doi=10.1080/713756764; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Am J Psychiatry; citation_title=Facial emotion recognition in schizophrenia: intensity effects and error pattern; citation_author=CG Kohler, TH Turner, WB Bilker, CM Brensinger, SJ Siegel, SJ Kanes, RE Gur, RC Gur; citation_volume=160; citation_publication_date=2003; citation_pages=1768-1774; citation_doi=10.1176/appi.ajp.160.10.1768; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Psychiatr Res; citation_title=Differences in facial expressions of four universal emotions; citation_author=CG Kohler, T Turner, NM Stolar, WB Bilker, CM Brensinger, RE Gur, RC Gur; citation_volume=128; citation_publication_date=2004; citation_pages=235-244; citation_doi=10.1016/j.psychres.2004.07.003; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=CNS Spectr; citation_title=Recognition of facial emotions in neuropsychiatric disorders; citation_author=CG Kohler, TH Turner, RE Gur, RC Gur; citation_volume=9; citation_publication_date=2004; citation_pages=267-274; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=J Nonverbal Behav; citation_title=Moving smiles: the role of dynamic components for the perception of the genuineness of smiles; citation_author=E Krumhuber, A Kappas; citation_volume=29; citation_publication_date=2005; citation_pages=3-24; citation_doi=10.1007/s10919-004-0887-x; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=Pilot study for assessing the behaviors of patients with schizophrenia towards a virtual avatar; citation_author=J Ku, HJ Jang, KU Kim, SH Park, JJ Kim, CH Kim, SW Nam, IY Kim, SI Kim; citation_volume=9; citation_publication_date=2006; citation_pages=531-539; citation_doi=10.1089/cpb.2006.9.531; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=VR-based conversation training program for patients with schizophrenia: a preliminary clinical trial; citation_author=J Ku, K Han, HR Lee, HJ Jang, KU Kim, SH Park, JJ Kim, CH Kim, IY Kim, SI Kim; citation_volume=10; citation_publication_date=2007; citation_pages=567-574; citation_doi=10.1089/cpb.2007.9989; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Curr Opin Psychiatry; citation_title=Emotional information processing in mood disorders: a review of behavioral and neuroimaging findings; citation_author=JM Lepp&#228;nen; citation_volume=19; citation_publication_date=2006; citation_pages=34-39; citation_doi=10.1097/01.yco.0000191500.46411.00; citation_id=CR35"/>

    <meta name="citation_reference" content="Lisetti CL, Nasoz F (2002) MAUI: a multimodal affective user interface. In: Proceedings of Multimedia&#8217;02, Juan-les-Pins, France"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Res Methods; citation_title=Immersive virtual environment technology as a basic research tool in psychology; citation_author=JM Loomis, JJ Blascovich, AC Beall; citation_volume=31; citation_publication_date=1999; citation_pages=557-564; citation_doi=10.3758/BF03200735; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=J Neurosci Methods; citation_title=Facial affect processing in social anxiety: tasks and stimuli; citation_author=JP Machado-de-Sousa, KC Arrais, NT Alves, MH Chagas, C Meneses-Gaya, JA Crippa, JE Hallak; citation_volume=193; citation_publication_date=2010; citation_pages=1-6; citation_doi=10.1016/j.jneumeth.2010.08.013; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_journal_title=Emotion; citation_title=Age-related differences in emotion recognition ability: a cross-sectional study; citation_author=A Mill, J Allik, A Realo, R Valk; citation_volume=9; citation_publication_date=2009; citation_pages=619-630; citation_doi=10.1037/a0016562; citation_id=CR39"/>

    <meta name="citation_reference" content="citation_journal_title=J Abnor Psychol; citation_title=Emotion recognition and social competence in chronic schizophrenia; citation_author=KT Mueser, R Doonan, DL Penn, JJ Blanchard, AS Bellack, P Nishith, J DeLeon; citation_volume=105; citation_publication_date=1996; citation_pages=271-275; citation_doi=10.1037/0021-843X.105.2.271; citation_id=CR40"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Psychopharmacol; citation_title=Improvement in social competence in patients with schizophrenia: a pilot study using a performance-based measure using virtual reality; citation_author=KM Park, J Ku, IH Park, JY Park, SI Kim, JJ Kim; citation_volume=24; citation_publication_date=2009; citation_pages=619-627; citation_doi=10.1002/hup.1071; citation_id=CR41"/>

    <meta name="citation_reference" content="citation_journal_title=Psychiatr Res; citation_title=A virtual reality application in role-plays of social skills training for schizophrenia: a randomized, controlled trial; citation_author=KM Park, J Ku, J Soo-Hee, JY Park, S Kim, JJ Kim; citation_volume=189; citation_publication_date=2011; citation_pages=166-172; citation_doi=10.1016/j.psychres.2011.04.003; citation_id=CR42"/>

    <meta name="citation_reference" content="Parsons TC (2011) Neuropsychological assessment using virtual environments: enhanced assessment technology for improved ecological validity. In: Brahnam S, Jain LC (eds) Advance computer intelligence paradigms in healthcare 6, SCI 337, pp 271&#8211;289"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=Affective interactions using virtual reality: the link between presence and emotions; citation_author=G Riva, F Mantovani, CS Capideville, A Preziosa, F Morganti, D Villani, A Gaggioli, C Botella, M Alca&#241;iz; citation_volume=10; citation_publication_date=2007; citation_pages=45-56; citation_doi=10.1089/cpb.2006.9993; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=Stud Health Technol Inform; citation_title=Improving social behaviour in schizophrenia patients using an integrated virtual reality programme: a case study; citation_author=M Rus-Calafell, J Guit&#233;rrez-Maldonado, J Ribas-Sabat&#233;; citation_volume=181; citation_publication_date=2012; citation_pages=283-286; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Bull; citation_title=Is the universal recognition of emotion from facial expression? A review of the cross-cultural studies; citation_author=JA Russell; citation_volume=115; citation_publication_date=1994; citation_pages=102-141; citation_doi=10.1037/0033-2909.115.1.102; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=Annu Rev Psychol; citation_title=Facial and vocal expression of emotion; citation_author=JA Russell, JA Bachorowski, JM Fernandez-Dols; citation_volume=54; citation_publication_date=2003; citation_pages=329-349; citation_doi=10.1146/annurev.psych.54.101601.145102; citation_id=CR47"/>

    <meta name="citation_reference" content="citation_journal_title=Schizophr Res; citation_title=Facial recognition deficits and cognition in schizophrenia; citation_author=G Sachs, D Steger-Wuchse, I Kryspin-Exner, RC Gur, H Katschnig; citation_volume=68; citation_publication_date=2004; citation_pages=27-35; citation_doi=10.1016/S0920-9964(03)00131-2; citation_id=CR48"/>

    <meta name="citation_reference" content="citation_journal_title=Psicothema; citation_title=Escalas PANAS de afecto positivo y negativo: validaci&#243;n factorial y convergencia transcultural; citation_author=B Sand&#237;n, R Chorot, L Lostao, T Joiner, M Santed, S Valiente; citation_volume=11; citation_publication_date=1999; citation_pages=37-51; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=J Nonverbal Behav; citation_title=Enhanced experienced of emotional arousal in response to dynamic facial expressions; citation_author=W Sato, S Yoshikawa; citation_volume=31; citation_publication_date=2007; citation_pages=119-135; citation_doi=10.1007/s10919-007-0025-7; citation_id=CR50"/>

    <meta name="citation_reference" content="citation_journal_title=Emotion; citation_title=Are facial expressions of emotion produced by categorical affect programs or dynamically driven by appraisal?; citation_author=KR Scherer, H Ellgring; citation_volume=7; citation_publication_date=2007; citation_pages=113-130; citation_doi=10.1037/1528-3542.7.1.113; citation_id=CR51"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Res Methods Instrum; citation_title=Making faces: creating three-dimensional parameterized models of facial expression; citation_author=J Spencer-Smith, H Wild, AH Innes-Ker, J Townsend, C Duffy, C Edwards, K Ervin, N Merritt, PW Paik; citation_volume=33; citation_publication_date=2001; citation_pages=115-123; citation_doi=10.3758/BF03195356; citation_id=CR52"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Brain Sci; citation_title=Understanding and sharing intentions: the origins of cultural cognition; citation_author=M Tomasello, M Carpenter, J Call, T Behne, H Moll; citation_volume=28; citation_publication_date=2005; citation_pages=675-735; citation_id=CR53"/>

    <meta name="citation_reference" content="Vilagrasa S, Susin A (2009) FACe! Facial animation system based on FACS. In: IV Iberoamerican symposium in computers graphics, Isla Margarita, Venezuela"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Netw; citation_title=&#8220;Artificial humans&#8221;: psychology and neuroscience perspective on embodiment and nonverbal communication; citation_author=K Vogeley, G Bente; citation_volume=23; citation_publication_date=2010; citation_pages=1077-1090; citation_doi=10.1016/j.neunet.2010.06.003; citation_id=CR55"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Soc Psychol; citation_title=Development and validation of brief measures of positive and negative affect: the PANAS scales; citation_author=C Watson, LA Clark, A Tellegen; citation_volume=54; citation_publication_date=1988; citation_pages=1063-1070; citation_doi=10.1037/0022-3514.54.6.1063; citation_id=CR56"/>

    <meta name="citation_reference" content="Yee N, Bailenson JN, Rickertsen K (2007) A meta-analysis of the impact of the inclusion and realism of human-like faces on user experiences in interfaces. In: Proceedings of the conference on computer-human interaction (CHI). California, USA"/>

    <meta name="citation_author" content="Jos&#233; Guti&#233;rrez-Maldonado"/>

    <meta name="citation_author_email" content="jgutierrezm@ub.edu"/>

    <meta name="citation_author_institution" content="Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Barcelona, Spain"/>

    <meta name="citation_author" content="Mar Rus-Calafell"/>

    <meta name="citation_author_email" content="m.ruscalafell@gmail.com"/>

    <meta name="citation_author_institution" content="Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Barcelona, Spain"/>

    <meta name="citation_author" content="Joan Gonz&#225;lez-Conde"/>

    <meta name="citation_author_email" content="jgonzalezconde@yahoo.es"/>

    <meta name="citation_author_institution" content="Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Barcelona, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0236-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0236-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability"/>
        <meta property="og:description" content="The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (n&amp;nbsp;&#x3D;&amp;nbsp;98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants’ accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant’s gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0236-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Emotion recognition, Virtual agents, Dynamism, Social skills, Cyberintervention","kwrd":["Emotion_recognition","Virtual_agents","Dynamism","Social_skills","Cyberintervention"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0236-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0236-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=236;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0236-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0236-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0236-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Cyberinterventions</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-10-22" itemprop="datePublished">22 October 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jos_-Guti_rrez_Maldonado" data-author-popup="auth-Jos_-Guti_rrez_Maldonado">José Gutiérrez-Maldonado</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Barcelona" /><meta itemprop="address" content="grid.5841.8, 0000000419370247, Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Passeig de la Vall d’Hebrón, 171, 08035, Barcelona, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mar-Rus_Calafell" data-author-popup="auth-Mar-Rus_Calafell" data-corresp-id="c1">Mar Rus-Calafell<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Barcelona" /><meta itemprop="address" content="grid.5841.8, 0000000419370247, Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Passeig de la Vall d’Hebrón, 171, 08035, Barcelona, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Joan-Gonz_lez_Conde" data-author-popup="auth-Joan-Gonz_lez_Conde">Joan González-Conde</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Barcelona" /><meta itemprop="address" content="grid.5841.8, 0000000419370247, Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Passeig de la Vall d’Hebrón, 171, 08035, Barcelona, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">61</span>–<span itemprop="pageEnd">71</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">858 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0236-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (<i>n</i> = 98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants’ accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant’s gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Research has found impaired facial affect recognition among patients with mood disorders (Leppänen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Leppänen JM (2006) Emotional information processing in mood disorders: a review of behavioral and neuroimaging findings. Curr Opin Psychiatry 19:34–39" href="/article/10.1007/s10055-013-0236-7#ref-CR35" id="ref-link-section-d1415e356">2006</a>), social anxiety (Machado-de-Sousa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Machado-de-Sousa JP, Arrais KC, Alves NT, Chagas MH, de Meneses-Gaya C, Crippa JA, Hallak JE (2010) Facial affect processing in social anxiety: tasks and stimuli. J Neurosci Methods 193:1–6" href="/article/10.1007/s10055-013-0236-7#ref-CR38" id="ref-link-section-d1415e359">2010</a>) and borderline personality (Domes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Domes G, Schulze L, Herpertz SC (2009) Emotion recognition in borderline personality disorder-a review of the literature. J Pers Disord 23:6–19" href="/article/10.1007/s10055-013-0236-7#ref-CR11" id="ref-link-section-d1415e362">2009</a>) although it is in relation to schizophrenia that deficits in this ability have mostly been studied (Addington and Addington <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Addington J, Addington D (1998) Facial affect recognition and information processing in schizophrenia and bipolar disorder. Schizophr Res 32:171–181" href="/article/10.1007/s10055-013-0236-7#ref-CR1" id="ref-link-section-d1415e365">1998</a>; Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774" href="/article/10.1007/s10055-013-0236-7#ref-CR29" id="ref-link-section-d1415e368">2003</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004b" title="Kohler CG, Turner TH, Gur RE, Gur RC (2004b) Recognition of facial emotions in neuropsychiatric disorders. CNS Spectr 9:267–274" href="/article/10.1007/s10055-013-0236-7#ref-CR31" id="ref-link-section-d1415e372">2004b</a>; Mueser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Mueser KT, Doonan R, Penn DL, Blanchard JJ, Bellack AS, Nishith P, DeLeon J (1996) Emotion recognition and social competence in chronic schizophrenia. J Abnor Psychol 105:271–275" href="/article/10.1007/s10055-013-0236-7#ref-CR40" id="ref-link-section-d1415e375">1996</a>; Sachs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sachs G, Steger-Wuchse D, Kryspin-Exner I, Gur RC, Katschnig H (2004) Facial recognition deficits and cognition in schizophrenia. Schizophr Res 68:27–35" href="/article/10.1007/s10055-013-0236-7#ref-CR48" id="ref-link-section-d1415e378">2004</a>). Affect recognition is part of emotion perception, which refers to the ability to infer emotional information from facial expressions, vocal inflections or some combination of these (Couture et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Couture SM, Penn DL, Roberts DL (2006) The functional significance of social cognition in schizophrenia: a review. Schizophr Bull 32:s44–s63" href="/article/10.1007/s10055-013-0236-7#ref-CR9" id="ref-link-section-d1415e381">2006</a>). The authors of a recent meta-analysis (Irani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Irani F, Seligman S, Kamath V, Kohler C, Gur RC (2012) A meta-analysis of emotion perception and functional outcomes in schizophrenia. Schizophr Res 137:203–211" href="/article/10.1007/s10055-013-0236-7#ref-CR27" id="ref-link-section-d1415e384">2012</a>) claimed that there is a strong positive relationship between emotion identification and functional outcome domains involving social problems and social skills. This relationship between accurately identifying emotions in faces and voices and performing efficiently in social environments indicates that this ability could be a useful treatment target for people suffering from schizophrenia spectrum disorders. In fact, virtual reality (VR) is already being used in the treatment of these patients to create avatars and social environments that allow the user to interact with them (Han et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Han K, Ku J, Kim K, Jang HJ, Park J, Kim JJ, Kim CH, Choi MH, Kim IY, Kim SI (2009) Virtual reality prototype for measurement of expression characteristics in emotional situations. Comput Biol Med 39:173–179" href="/article/10.1007/s10055-013-0236-7#ref-CR25" id="ref-link-section-d1415e387">2009</a>; Ku et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ku J, Jang HJ, Kim KU, Park SH, Kim JJ, Kim CH, Nam SW, Kim IY, Kim SI (2006) Pilot study for assessing the behaviors of patients with schizophrenia towards a virtual avatar. Cyberpsychol Behav 9:531–539" href="/article/10.1007/s10055-013-0236-7#ref-CR33" id="ref-link-section-d1415e391">2006</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ku J, Han K, Lee HR, Jang HJ, Kim KU, Park SH, Kim JJ, Kim CH, Kim IY, Kim SI (2007) VR-based conversation training program for patients with schizophrenia: a preliminary clinical trial. Cyberpsychol Behav 10:567–574" href="/article/10.1007/s10055-013-0236-7#ref-CR34" id="ref-link-section-d1415e394">2007</a>; Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Park KM, Ku J, Park IH, Park JY, Kim SI, Kim JJ (2009) Improvement in social competence in patients with schizophrenia: a pilot study using a performance-based measure using virtual reality. Hum Psychopharmacol 24:619–627" href="/article/10.1007/s10055-013-0236-7#ref-CR41" id="ref-link-section-d1415e397">2009</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Park KM, Ku J, Soo-Hee J, Park JY, Kim S, Kim JJ (2011) A virtual reality application in role-plays of social skills training for schizophrenia: a randomized, controlled trial. Psychiatr Res 189:166–172" href="/article/10.1007/s10055-013-0236-7#ref-CR42" id="ref-link-section-d1415e400">2011</a>). Virtual environments can represent social situations and generate realistic virtual humans, which can even respond socially to the person who is interacting with them. This technology offers researchers and clinicians the possibility not only of observing the user’s real-time behaviour when interacting with virtual agents, but also of controlling and modifying the environment and the responses of the avatars. Furthermore, users can become actively engaged in interaction with the virtual world and with other characters (Fabri et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Fabri M, Moore D, Hobbs D (2002) Expressive agents: non-verbal communication in collaborative virtual environments. In: Proceedings of autonomous agents and multi-agent systems, Bologna, Italy" href="/article/10.1007/s10055-013-0236-7#ref-CR18" id="ref-link-section-d1415e403">2002</a>). The ecological validity of this approach derives from the precise presentation and control of dynamic perceptual stimuli. Indeed, virtual environments provide valid assessments and situations that combine the veridical control of laboratory measures with the verisimilitude of everyday experiences (Parsons <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Parsons TC (2011) Neuropsychological assessment using virtual environments: enhanced assessment technology for improved ecological validity. In: Brahnam S, Jain LC (eds) Advance computer intelligence paradigms in healthcare 6, SCI 337, pp 271–289" href="/article/10.1007/s10055-013-0236-7#ref-CR43" id="ref-link-section-d1415e406">2011</a>). Thus, creating convincing artificial social entities, also called “embodied conversational agents”, seems to be one of the most important challenges to successfully achieve human–computer interaction and to simulate the complex reality of the human mind and behaviour through technology (Vogeley and Bente <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Vogeley K, Bente G (2010) “Artificial humans”: psychology and neuroscience perspective on embodiment and nonverbal communication. Neural Netw 23:1077–1090" href="/article/10.1007/s10055-013-0236-7#ref-CR55" id="ref-link-section-d1415e410">2010</a>). Embodiment as a feature of artificial agents can be defined as the presence of human-like physical properties, which enable the transmission of verbal signals (Bente et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bente G, Rüggenberg S, Krämer NC, Eschenburg F (2008) Avatar-mediated networking: increasing social presence and interpersonal trust in net-based collaborations. Hum Commun Res 34:287–318" href="/article/10.1007/s10055-013-0236-7#ref-CR4" id="ref-link-section-d1415e413">2008</a>).</p><p>In studies of emotion recognition, experimental stimuli are most commonly presented to subjects in the form of photographs or static images. However, some authors have suggested that such stimuli may not reflect the liveliness and true form of dynamic human facial expression (Harwood et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Harwood NK, Hall LJ, Shinkfield AJ (1999) Recognition of facial emotional expressions from moving and static displays by individuals with mental retardation. Am J Ment Retard 104:270–278" href="/article/10.1007/s10055-013-0236-7#ref-CR26" id="ref-link-section-d1415e419">1999</a>, Collignon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Collignon O, Girarda S, Gosselina F, Roya S, Saint-Amoura D, Lassondea M, Leporea F (2008) Audio-visual integration of emotion expression. Brain Res 25:126–135" href="/article/10.1007/s10055-013-0236-7#ref-CR8" id="ref-link-section-d1415e422">2008</a>). This is because the receiver is unlikely to simply decode an emotional message in a simple, reflex-like manner (Russell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Russell JA, Bachorowski JA, Fernandez-Dols JM (2003) Facial and vocal expression of emotion. Annu Rev Psychol 54:329–349" href="/article/10.1007/s10055-013-0236-7#ref-CR47" id="ref-link-section-d1415e425">2003</a>), but needs to see how the emotion appears and disappears from the face. Knight and Johnston (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Knight B, Johnston A (1997) The role of movement in face recognition. Vis Cognit 4:265–273" href="/article/10.1007/s10055-013-0236-7#ref-CR28" id="ref-link-section-d1415e428">1997</a>) had previously claimed that the movement of the face may provide information that facilitates recognition, and they argued that the benefits of dynamic information act through a better representation of the face’s three-dimensional structure. More recently, Sato and Yoshikawa (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sato W, Yoshikawa S (2007) Enhanced experienced of emotional arousal in response to dynamic facial expressions. J Nonverbal Behav 31:119–135" href="/article/10.1007/s10055-013-0236-7#ref-CR50" id="ref-link-section-d1415e431">2007</a>) reported very interesting findings suggesting that a dynamic presentation of facial expression induces a more intense emotional experience than does a static presentation. Likewise, Krumhuber and Kappas (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Krumhuber E, Kappas A (2005) Moving smiles: the role of dynamic components for the perception of the genuineness of smiles. J Nonverbal Behav 29:3–24" href="/article/10.1007/s10055-013-0236-7#ref-CR32" id="ref-link-section-d1415e435">2005</a>) demonstrated the important role of dynamism in perceiving the genuineness of smiles when a person perceives the emotion of happiness. Other researchers have used videotapes as presentation stimuli for emotion recognition ability in order to include the “aliveness” and genuineness of expressions (Harwood et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Harwood NK, Hall LJ, Shinkfield AJ (1999) Recognition of facial emotional expressions from moving and static displays by individuals with mental retardation. Am J Ment Retard 104:270–278" href="/article/10.1007/s10055-013-0236-7#ref-CR26" id="ref-link-section-d1415e438">1999</a>; Davis and Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Davis PJ, Gibson MG (2000) Recognition of posed and genuine facial expression of emotion in paranoid and nonparanoid schizophrenia. J Abnorm Psychol 109:445–450" href="/article/10.1007/s10055-013-0236-7#ref-CR10" id="ref-link-section-d1415e441">2000</a>). However, these video tasks have not been well validated and present several limitations in their format and scene duration (Edwards et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Edwards J, Jackson H, Pattison P (2002) Emotion recognition via facial expression and affective prosody in schizophrenia: a methodological review. Clin Psychol Rev 22:789–832" href="/article/10.1007/s10055-013-0236-7#ref-CR14" id="ref-link-section-d1415e444">2002</a>). But, most importantly, both still images and videotapes present several limitations when they have to be integrated into a simulated interaction or face-to-face encounters.</p><p>Virtual reality may offer a way of overcoming this limitation, as it generates realistic and active faces for computer characters that can be included in virtual environments so as to perform face-to-face conversational interactions. Recently, research has also focused on the development of virtual faces to create new facial stimuli and to study different aspects of emotion recognition, such as the role of dynamics, identification or face processing. Some of these studies have used virtual faces from existing software packages that contain a library of pre-rigged humans, such as Poser<sup>®</sup>, Half Life 2<sup>®</sup> or People Putty<sup>®</sup> (Spencer-Smith et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Spencer-Smith J, Wild H, Innes-Ker AH, Townsend J, Duffy C, Edwards C, Ervin K, Merritt N, Paik PW (2001) Making faces: creating three-dimensional parameterized models of facial expression. Behav Res Methods Instrum 33:115–123" href="/article/10.1007/s10055-013-0236-7#ref-CR52" id="ref-link-section-d1415e456">2001</a>; Lisetti and Nasoz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lisetti CL, Nasoz F (2002) MAUI: a multimodal affective user interface. In: Proceedings of Multimedia’02, Juan-les-Pins, France" href="/article/10.1007/s10055-013-0236-7#ref-CR36" id="ref-link-section-d1415e459">2002</a>; Krumhuber and Kappas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Krumhuber E, Kappas A (2005) Moving smiles: the role of dynamic components for the perception of the genuineness of smiles. J Nonverbal Behav 29:3–24" href="/article/10.1007/s10055-013-0236-7#ref-CR32" id="ref-link-section-d1415e463">2005</a>; Dyck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628" href="/article/10.1007/s10055-013-0236-7#ref-CR12" id="ref-link-section-d1415e466">2008</a>), while others have created an interface with which to build their own faces from scratch (Fabri et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Fabri M, Moore D, Hobbs D (2002) Expressive agents: non-verbal communication in collaborative virtual environments. In: Proceedings of autonomous agents and multi-agent systems, Bologna, Italy" href="/article/10.1007/s10055-013-0236-7#ref-CR18" id="ref-link-section-d1415e469">2002</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual environments: an experimental study. Virtual Real 7:66–81" href="/article/10.1007/s10055-013-0236-7#ref-CR19" id="ref-link-section-d1415e472">2004</a>; Vilagrasa and Susin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Vilagrasa S, Susin A (2009) FACe! Facial animation system based on FACS. In: IV Iberoamerican symposium in computers graphics, Isla Margarita, Venezuela" href="/article/10.1007/s10055-013-0236-7#ref-CR54" id="ref-link-section-d1415e475">2009</a>). Whatever the approach, however, the creation of facial stimuli takes as its reference the Facial Action Coding System (FACS) developed by Ekman and Friesen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, Palo Alto" href="/article/10.1007/s10055-013-0236-7#ref-CR17" id="ref-link-section-d1415e478">1978</a>). The main objective of FACS was to create a coding system for precisely cataloguing facial movements in naturally occurring faces without having to resort to imprecise or emotion-related terminology (Spencer-Smith et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Spencer-Smith J, Wild H, Innes-Ker AH, Townsend J, Duffy C, Edwards C, Ervin K, Merritt N, Paik PW (2001) Making faces: creating three-dimensional parameterized models of facial expression. Behav Res Methods Instrum 33:115–123" href="/article/10.1007/s10055-013-0236-7#ref-CR52" id="ref-link-section-d1415e482">2001</a>). The basic element of this system is the action unit (AU), which represents the muscular activity that produces momentary changes in facial appearance. Thus, expressions can be coded by detecting the presence of combinations of AUs in the face. Fabri et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Fabri M, Moore D, Hobbs D (2002) Expressive agents: non-verbal communication in collaborative virtual environments. In: Proceedings of autonomous agents and multi-agent systems, Bologna, Italy" href="/article/10.1007/s10055-013-0236-7#ref-CR18" id="ref-link-section-d1415e485">2002</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual environments: an experimental study. Virtual Real 7:66–81" href="/article/10.1007/s10055-013-0236-7#ref-CR19" id="ref-link-section-d1415e488">2004</a>) directly compared the Pictures of Facial Affect (POFA; Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Ekman P, Friesen W (1975) Pictures of facial affect CD-Rom. University of California, San Francisco" href="/article/10.1007/s10055-013-0236-7#ref-CR16" id="ref-link-section-d1415e491">1975</a>), consisting of 110 black and white images of Caucasian actors portraying six universal emotions plus neutral expressions, with a set of virtual faces created using H-Anim (Virtual Reality Modelling Language Humanoid). However, these authors did not pay attention to the realism of the faces, but rather to the effectiveness of using a reduced feature set to build a successfully recognized core set of avatar facial expressions. Subsequently, Dyck et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628" href="/article/10.1007/s10055-013-0236-7#ref-CR12" id="ref-link-section-d1415e494">2008</a>) validated a set of virtual facial stimuli, comparing them with natural emotions displayed in photographs which had previously been validated (Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774" href="/article/10.1007/s10055-013-0236-7#ref-CR29" id="ref-link-section-d1415e497">2003</a>; Gur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Gur RC, Sara R, Hagendoorn M, Marom O, Hughett P, Macy L, Turner T, Bajcsy R, Posner A, Gur RE (2002a) A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies. J Neurosci Methods 115:137–143" href="/article/10.1007/s10055-013-0236-7#ref-CR22" id="ref-link-section-d1415e501">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Gur RC, Schroeder L, Turner T, McGrath C, Chan RM, Turetsky BI, Alsop D, Maldjian J, Gur RE (2002b) Brain activation during facial emotion processing. Neuroimage 16:651–662" href="/article/10.1007/s10055-013-0236-7#ref-CR23" id="ref-link-section-d1415e504">b</a>). However, they did not include dynamism (e.g. incorporating an animation that simulated movement). Both these studies concluded that virtual faces were as valid as pictures when assessing or training the ability to recognize facial emotions. In addition, Yee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Yee N, Bailenson JN, Rickertsen K (2007) A meta-analysis of the impact of the inclusion and realism of human-like faces on user experiences in interfaces. In: Proceedings of the conference on computer-human interaction (CHI). California, USA" href="/article/10.1007/s10055-013-0236-7#ref-CR57" id="ref-link-section-d1415e507">2007</a>), in their meta-analysis about the impact of the inclusion and realism of human-like faces on user experiences in interfaces, concluded that human-like representations with higher realism produced more positive social interactions than did representations with lower realism.</p><p>Building on this existing literature, the purpose of the present study was to create a new set of dynamic virtual faces with high realism that could subsequently be integrated into a virtual reality cyberintervention to train people with schizophrenia in the full repertoire of social skills. Social skills refer to the constituent behaviours that enable an individual to achieve social competence in daily living (Bellack et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bellack AS, Mueser KT, Gingerich S, Agresta J (2004) Social skills training for schizophrenia. Guilford Press, New York" href="/article/10.1007/s10055-013-0236-7#ref-CR3" id="ref-link-section-d1415e513">2004</a>), and people with schizophrenia present serious impairments in these capabilities. One of the most important of these social impairments is the accurate recognition of facial expressions of emotion. In the schizophrenia sufferer, emotional distortions can interfere with functional achievement as it relates to emotion: for example, if a patient with schizophrenia has a deficit in the ability to recognize facial emotions and does not know how to recognize these expressions, it can impact negatively on relationships and social interactions. Moreover, the importance of improving facial emotion recognition capability, an essential part in social skills training programmes, implies improvement in both the expression of emotions as prosocial behaviour and the patient’s self-esteem (Borras et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Borras L, Boucherie M, Mohr S, Lecomte T, Perroud N, Huguelet P (2009) Increasing self-esteem: efficacy of a group intervention for individuals with severe mental disorders. Eur Psychiat 24:307–316" href="/article/10.1007/s10055-013-0236-7#ref-CR5" id="ref-link-section-d1415e516">2009</a>). In a subsequent study by Dyck et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Mathiak K (2010) Virtual faces as a tool to study emotion recognition deficits in schizophrenia. Psychiatr Res 179:247–252" href="/article/10.1007/s10055-013-0236-7#ref-CR13" id="ref-link-section-d1415e519">2010</a>), the authors used the set of virtual faces previously validated with control participants (Dyck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628" href="/article/10.1007/s10055-013-0236-7#ref-CR12" id="ref-link-section-d1415e522">2008</a>, described above), to compare the facial emotion recognition performance between non-clinical participants and schizophrenia patients. Their virtual faces seemed to be sensitive to the patients’ emotion recognition deficits. The authors concluded that the virtual characters represented a valuable tool that extends research options in the field of schizophrenia because control, animation and change of parameters can be directly achieved. The authors also stated that before virtual faces displaying emotions are included in virtual training programmes, it is necessary to test whether virtual facial expressions are perceived in the same way as natural faces.</p><p>In the present study, having created the new set of virtual faces, they were then compared with an already validated set of photographs used for the assessment of facial emotion recognition ability, in order to test if the virtual faces could be recognized as well as natural emotional expressions. The final step of the study will be to include these virtual agents as graphical representations of humanoid actors within shared virtual environments used in a cyberintervention for people suffering from schizophrenia. Therefore, testing the preliminary materials that would be included in the final cyberintervention was considered a crucial prerequisite for the use of virtual characters in research and clinical settings. Moreover, the Ethics Committee of the Adult Mental Health Unit (AMHU) of Igualada (the mental health unit involved in the whole research project) required the research team to test both virtual environments and virtual characters in a non-clinical population before establishing the virtual intervention for schizophrenia patients in the clinical setting.</p><p>Firstly, it was hypothesized that there would be no significant difference in the accuracy of emotion recognition between natural and virtual faces. Following this equivalence premise, it was secondly hypothesized that reaction times (RT) would be different for each emotion in both types of stimuli presentation. Thirdly, it was hypothesized that there would be no significant effect of the participant’s age and gender on the accuracy of emotion recognition in either presentation condition. Finally, to explore the facilitator effect of the dynamism included in the virtual faces, it was hypothesized that participants would find much easier to recognize dynamic faces (virtual) than static faces (photographs).</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Method</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Participants</h3><p>Ninety-eight healthy volunteers were recruited from different faculties of the University of Barcelona and through the Rius i Taulet Adult Education Centre (Barcelona). The only inclusion criterion was for participants to be between the age of 18 and 65. The mean age of the sample was 32.58 years (SD = 9.23, range 21–53), and 65.7 % of participants were female. The exclusion criteria were having a diagnosis of mental disorder or a personal history of physical illness or organic disease. All the participants signed a prior consent form and completed the entire task as designed for the study.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec4">Measures</h3>
                  <h3 class="c-article__sub-heading">Demographic questionnaire</h3>
                  <p>This consisted of 10 questions regarding age, gender, education level, marital status, profession and health (specifically, if the participants had ever been diagnosed with a mental disorder and/or required psychological treatment).</p>
                
                  <h3 class="c-article__sub-heading">Positive and negative affect schedule</h3>
                  <p>(PANAS; Watson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Watson C, Clark LA, Tellegen A (1988) Development and validation of brief measures of positive and negative affect: the PANAS scales. J Pers Soc Psychol 54:1063–1070" href="/article/10.1007/s10055-013-0236-7#ref-CR56" id="ref-link-section-d1415e560">1988</a>; Spanish adaptation by Sandín et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Sandín B, Chorot R, Lostao L, Joiner T, Santed M, Valiente S (1999) Escalas PANAS de afecto positivo y negativo: validación factorial y convergencia transcultural. Psicothema 11:37–51" href="/article/10.1007/s10055-013-0236-7#ref-CR49" id="ref-link-section-d1415e563">1999</a>): this is a 20-item self-report questionnaire which measures the individual’s positive and negative affect. Each item is rated on a 5-point scale ranging from 1 = very slightly or not at all to 5 = extremely and indicates the extent to which the respondent has felt this way in general. Two factors are derived from the scale: positive affect (PA) and negative affect (NA). Its validity and reliability have been demonstrated in different samples (Sandín et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Sandín B, Chorot R, Lostao L, Joiner T, Santed M, Valiente S (1999) Escalas PANAS de afecto positivo y negativo: validación factorial y convergencia transcultural. Psicothema 11:37–51" href="/article/10.1007/s10055-013-0236-7#ref-CR49" id="ref-link-section-d1415e566">1999</a>). This schedule was included in order to control for the participant’s mood state or non-specific depression symptoms. In the event that a participant presented such symptoms and scored above the lower cut-off for positive affect or the higher for negative affect (PA &lt; 25; NA &gt; 35) on the PANAS, this participant was excluded from the study. Any participant was excluded of the sample by using this criterion.</p>
                
                  <h3 class="c-article__sub-heading">Committed errors</h3>
                  <p>This score comprises the errors made in the facial emotion recognition task. According to the original Penn Emotion Recognition Test—96 Faces (PERT96; Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774" href="/article/10.1007/s10055-013-0236-7#ref-CR29" id="ref-link-section-d1415e577">2003</a>, described below), a forced-choice format was used: participants had to choose the correct option from the following labels: happiness, sadness, fear, disgust, anger and neutral for the displayed face. Correct responses were scored as 1, and incorrect responses were scored as 0 (considering both missing responses and incorrect identifications). The possible highest total score was 44; thus, higher scores indicated better facial emotion recognition</p>
                
                  <h3 class="c-article__sub-heading">Qualitative assessment</h3>
                  <p>This questionnaire consisted of three multiple-choice questions regarding the experience with the facial emotion recognition task. The three questions were as follows: (1) “Do you think that the faces you have seen are representative of human facial expression?” (Yes; No; Don’t know); (2) “Did you find the experience aversive?” (Yes; No; Don’t know) and (3) “Which type of presentation of the faces was easier for you?” (Photographs; Virtual faces; No difference).</p>
                <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec5">Facial stimuli</h3><p>The target stimuli comprised faces representing the five basic emotions: happiness, sadness, fear, disgust and anger (Ekman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Ekman P (1992) Are there basic emotions? Psychol Rev 99:550–553" href="/article/10.1007/s10055-013-0236-7#ref-CR15" id="ref-link-section-d1415e596">1992</a>), plus a neutral emotion (no emotion).</p><ul class="u-list-style-none">
                    <li>
                      <p>Natural Faces: a total of 44 photographs (corresponding to 2 males and 2 females, young and old in both cases, displaying 5 emotions with 2 intensities, as well as neutrality) were selected from the Penn Emotion Recognition Test—96 Faces version (PERT96; Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774" href="/article/10.1007/s10055-013-0236-7#ref-CR29" id="ref-link-section-d1415e605">2003</a>). This is a computer-based test containing 96 colour photographs of facial expressions of emotions, which include both high- and middle-intensity angry, fearful, happy, sad, disgusted and neutral faces. This set of photographs has been standardized and used reliably as a neurobehavioural probe in emotion recognition studies (Gur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Gur RC, Sara R, Hagendoorn M, Marom O, Hughett P, Macy L, Turner T, Bajcsy R, Posner A, Gur RE (2002a) A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies. J Neurosci Methods 115:137–143" href="/article/10.1007/s10055-013-0236-7#ref-CR22" id="ref-link-section-d1415e608">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Gur RC, Schroeder L, Turner T, McGrath C, Chan RM, Turetsky BI, Alsop D, Maldjian J, Gur RE (2002b) Brain activation during facial emotion processing. Neuroimage 16:651–662" href="/article/10.1007/s10055-013-0236-7#ref-CR23" id="ref-link-section-d1415e611">b</a>). Furthermore, this test was also chosen because is one of the most commonly used task of assessing facial emotion recognition deficits in people suffering from schizophrenia (Edwards et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Edwards J, Jackson H, Pattison P (2002) Emotion recognition via facial expression and affective prosody in schizophrenia: a methodological review. Clin Psychol Rev 22:789–832" href="/article/10.1007/s10055-013-0236-7#ref-CR14" id="ref-link-section-d1415e614">2002</a>; Sachs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sachs G, Steger-Wuchse D, Kryspin-Exner I, Gur RC, Katschnig H (2004) Facial recognition deficits and cognition in schizophrenia. Schizophr Res 68:27–35" href="/article/10.1007/s10055-013-0236-7#ref-CR48" id="ref-link-section-d1415e617">2004</a>; Green et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Green M, Olivier B, Crowley J, Penn DL, Silverstein S (2005) Social cognition in Schizophrenia: recommendation from measurement and treatment to improve cognition in schizophrenia new approaches conferences. Schizophr Bull 31:882–887" href="/article/10.1007/s10055-013-0236-7#ref-CR21" id="ref-link-section-d1415e621">2005</a>).</p>
                    </li>
                    <li>
                      <p>
                        <i>Virtual faces</i>: forty-four avatars (as in the natural faces condition, corresponding to 2 males and 2 females, young and old in both cases, displaying 5 emotions with 2 intensities, as well as neutrality) were created to display the above-mentioned basic emotions using the facial surface changes proposed by Ekman and Friesen in their Facial Action Coding System (FACS, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, Palo Alto" href="/article/10.1007/s10055-013-0236-7#ref-CR17" id="ref-link-section-d1415e633">1978</a>). Firstly, the structure of the face was created using a network of bones, which allowed modification of the 66 action units of the FACS. This first stage was developed using 3ds STUDIO Max<sup>®</sup> (Autodesk, Inc., USA). Faces were then morphed according to the action units (AUs) in the FACS (Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, Palo Alto" href="/article/10.1007/s10055-013-0236-7#ref-CR17" id="ref-link-section-d1415e638">1978</a>). All parameters for each emotion were controlled in order to make the faces as realistic as possible, and in order to match them to the intensity of the photographs used. Further modelling and animations were also applied using 3ds STUDIO Max<sup>®</sup>. To make the images more realistic, textures were included with the help of Photoshop 6.0<sup>®</sup>. The dynamism of the human faces was enhanced by adding a skin that fitted closely to the bones; the 3D faces were capable of continuous movement (a transition simulating muscular movement) from the neutral face (the structure of the bones with all the FACS at 0) to the final expression. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0236-7#Tab1">1</a> shows all the specific action units used to create the particular emotions. Virtual faces were matched to the intensity parameters of the PERT96 photographs: half of the virtual faces were designed with AU magnitude settings ranging from 0.1 to 0.5 (middle intensity), while the other half were designed with magnitude settings close to 1.0 (high intensity).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Active action units (FACS) modified in each emotion (except neutral) when creating the virtual faces</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0236-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                      
                    </li>
                    <li>
                      <p>The example in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0236-7#Fig1">1</a> shows how parameter changes can adapt the happiness expression to high- or middle intensity. Finally, 3DVIA Virtools<sup>®</sup> was used to correctly display each emotion during the final presentation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0236-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0236-7/MediaObjects/10055_2013_236_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0236-7/MediaObjects/10055_2013_236_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Creation of a virtual face: an example of neutral, middle-intensity happiness and high-intensity happiness expressions in the young male</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0236-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                      
                    </li>
                  </ul>
                <p>The final task was also designed using 3DVIA Virtools<sup>®</sup>. The task itself was divided into two blocks separated by a 20-s rest interval. The order of the blocks was counterbalanced (mixed-random order), and each image was presented once and randomly. Each face was presented for a maximum of 7 s, and participants had to choose the corresponding emotion from a list that appeared on the left-hand side of the screen (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0236-7#Fig2">2</a>). The number of errors was recorded in a data matrix created by the same program.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0236-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0236-7/MediaObjects/10055_2013_236_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0236-7/MediaObjects/10055_2013_236_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Examples of the recognition screen (VR and photographs)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0236-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec6">Hardware</h3><p>A laptop with a 15.6 inch monitor and stereoscopic view was used for task presentation (Acer Aspire 5738dg, 2.2 GHz Core 2 Duo, 4 GB of RAM, and ATI Radeon HD 4570 graphics). This hardware was selected due to its portability and ergonomic features.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec7">Procedure</h3><p>The entire procedure was carried out in a single session. After interviewing participants and checking the inclusion and exclusion criteria, they were given the demographic questionnaire and the PANAS to complete. Each participant was then seated in front of the 3D laptop. The following instructions were explained orally and also appeared in written form on the screen: “You will now see a series of faces that express EMOTIONS. Please identify each of the emotions that you think the character is expressing by choosing an adjective from the list on the screen. The task is divided into two parts, and you will have 20 s to rest after completing the first part”. The emotion categories were also read out to participants before starting the task. Each participant was instructed to respond as naturally and spontaneously as possible. Participants were required to use 3D glasses when the virtual faces appeared, but they were asked to take them off before the photographs block.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec8">Data analysis</h3><p>Different statistical analyses were performed using SPSS 15.0 for Windows (SPSS Inc., Chicago, USA). The accuracy of emotion recognition in both stimuli presentation conditions (photographs and VR) was analysed using a one-way repeated-measures ANOVA. The factor analysed was <i>type of presentation</i>, with two levels (photographs and VR). For the analysis of the overall rate of correct responses, the participant’s gender and age were considered as covariates ANCOVA. To explore possible age effects, the sample was divided into two groups for further analysis: the “young group” (<i>N</i> = 66; from 18- to 35-year old) and the “older group” (<i>N</i> = 32; from 35- to 65-year old). Post hoc testing with Bonferroni correction was computed to analyse specific difference between age groups in both experimental conditions. Confusion matrices were generated for the error responses, and the percentage of occurrence of each response was described. A Pearson’s correlation was calculated to examine the degree of relationship of both stimuli presentation conditions. Finally, reaction times were also analysed using a one-way repeated-measures ANOVA, comparing the six types of emotion in each type of presentation. Descriptive analyses were computed for the qualitative assessment.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Results</h2><div class="c-article-section__content" id="Sec9-content"><h3 class="c-article__sub-heading" id="Sec10">Demographic characteristics of the sample</h3><p>All 98 participants completed both the assessment and the task, and there were no missing data. The demographic details of the participants are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0236-7#Tab2">2</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Demographic data</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0236-7/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec11">Accuracy in emotion recognition</h3><p>The one-way repeated-measures ANOVA revealed no significant differences in the overall rate of correct responses for the two presentation conditions. However, there were significant differences between photographs and VR faces in relation to disgust (<i>F</i>
                  <sub>1,97</sub> = 38.32, <i>p</i> = 0.00; Partial <i>η</i>
                  <sup>2</sup> = 0.283) and anger (<i>F</i>
                  <sub>1,97</sub> = 49.31, <i>p</i> = 0.00; Partial <i>η</i>
                  <sup>2</sup> = 0.337). Specifically, participants made more recognition errors when looking at disgusted faces in the virtual reality condition, whereas faces expressing anger were often less recognized by participants when this emotion was displayed using photographs. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0236-7#Tab3">3</a> shows the percentages for each successfully identified emotion.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Percentage of successful answers for each emotion, as well as confusions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0236-7/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0236-7#Fig3">3</a> offers a general view of the results obtained and shows that happiness was the emotion with the highest rate of correct identification.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0236-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0236-7/MediaObjects/10055_2013_236_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0236-7/MediaObjects/10055_2013_236_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The rate of correct recognition for each emotion in the two presentation conditions. *<i>p</i> &lt; 0.01</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0236-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0236-7#Tab3">3</a> shows the errors committed by participants when assessing expressions according to categories. This confusion matrix shows that in the photographs condition, sadness was mainly confused with fear and neutrality, while anger was mainly confused with neutral. A tendency to confuse disgust with anger was also found when exploring errors in photographs.</p><p>In the VR condition, disgust was equally confused with anger and fear, whereas sadness was confused with neutral. A tendency to confuse anger with disgust was also found when exploring errors in virtual faces.</p><p>The Pearson’s correlation determined a high-positive correlation between both stimuli presentation conditions (photographs and VR): <i>r</i> = 0.831, <i>p</i> = 0.00.</p><h3 class="c-article__sub-heading" id="Sec12">Age, sex and accuracy</h3><p>There was no interaction effect between <i>gender</i> and <i>type of presentation</i> (<i>F</i>
                  <sub>1,95</sub> = 0.247, <i>p</i> = 0.67; Partial <i>η</i>
                  <sup>2</sup> = 0.002). However, there was a significant interaction between <i>age</i> and <i>type of presentation</i> (<i>F</i>
                  <sub>1,95</sub> = 10.15, <i>p</i> &lt; 0.05). Subsequent analysis revealed that there was a significant difference between facial emotion recognitions in the photographs condition as compared with virtual faces in the “older group” (<i>F</i>
                  <sub>1,31</sub> = 6.21, <i>p</i> = 0.21; Partial <i>η</i>
                  <sup>2</sup> = 0.288): pairwise comparisons showed participants in this group committed more errors in the photographs (<i>M</i> = 16.09, SD = 1.23) than in the virtual faces condition (<i>M</i> = 14.31, SD = 0.93) condition. No differences were found in the “young group”.</p><h3 class="c-article__sub-heading" id="Sec13">Reaction times</h3><p>Happiness was the most rapidly recognized emotion in both conditions (<i>M</i>
                  <sub>photographs</sub> = 1,920.31 ms, SD = 607.18 ms; <i>M</i>
                  <sub>VR</sub> = 2,491.52 ms, SD = 557.74 ms), whereas participants took longest to identify anger in the photographs condition (<i>M</i> = 3,319.67 ms, SD = 733.69 ms) and disgust in the VR condition (<i>M</i> = 3,299.29 ms, SD = 872.98 ms).</p><h3 class="c-article__sub-heading" id="Sec14">Qualitative assessment</h3><p>Regarding the qualitative assessment, 83.5 % of participants stated that the emotions seen in the task were representative of human expression. None of the participants considered the task to be aversive or expressed a reluctance to complete it. Finally, 82 % of the sample stated that VR faces were easier to recognize than were photographs, the main reason being that they “could appreciate the movement”.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Discussion</h2><div class="c-article-section__content" id="Sec15-content"><p>The aim of this study was to investigate whether emotions expressed by virtual agents were recognized as accurately as a set of already validated facial stimuli (natural faces) (PERT96; Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774" href="/article/10.1007/s10055-013-0236-7#ref-CR29" id="ref-link-section-d1415e2221">2003</a>). This was a preliminary step towards a broader objective being addressed by our group, namely to incorporate these virtual agents into a complex virtual reality programme designed to train social skills in people with schizophrenia spectrum disorders in one-to-one sessions. As mentioned above, the Ethics Committee of the AMHU of Igualada required this preliminary step before using the virtual humans and environments on a sample of patients suffering from schizophrenia. This recommendation was made according to the principle issues that the Research Ethics Services concerns itself with: rights, safety, dignity and the well-being of research participants (in this specific case, schizophrenia patients). The main goal of this recommendation was to ensure the reliability and safety of the material and the system before using it in patients that, because of their psychotic symptomatology, suffer from high distress and present severe deficits in emotion recognition. Thus, it was absolutely necessary to ensure that the virtual humans were displaying the correct pattern of emotional states, assessed by non-clinical participants, to prevent confusion and discomfort of patients when delivering the cyberintervention.</p><p>As we firstly hypothesized, the results confirmed that virtual reality faces can be as useful and reliable as natural faces for assessing emotion recognition. The use of posed photographs has sometimes been considered a limitation in previous emotion recognition studies (Gur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Gur RC, Sara R, Hagendoorn M, Marom O, Hughett P, Macy L, Turner T, Bajcsy R, Posner A, Gur RE (2002a) A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies. J Neurosci Methods 115:137–143" href="/article/10.1007/s10055-013-0236-7#ref-CR22" id="ref-link-section-d1415e2227">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Gur RC, Schroeder L, Turner T, McGrath C, Chan RM, Turetsky BI, Alsop D, Maldjian J, Gur RE (2002b) Brain activation during facial emotion processing. Neuroimage 16:651–662" href="/article/10.1007/s10055-013-0236-7#ref-CR23" id="ref-link-section-d1415e2230">b</a>), due to the fact that everyday facial expressions are brief and dynamic (Archer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Archer J, Hay DC, Young AW (1994) Movement, face processing and schizophrenia: evidence of a differential deficit in expression analysis. Br J Clin Psychol 33:517–528" href="/article/10.1007/s10055-013-0236-7#ref-CR2" id="ref-link-section-d1415e2233">1994</a>). In this regard, virtual reality offers several advantages in terms of the liveliness and dynamism of facial stimuli, as well as the possibility of creating simulated situations involving “first person” interactions with human-like characters. Even though the artificiality of virtual agents may cause some controversy regarding their generalization, the advantages they offer over other assessment methods (for instance, the possibility of real-time manipulation of characters and their verbal and non-verbal responses) make this technology a very valuable tool with a wide range of applications in clinical psychology. As mentioned by Russell (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Russell JA (1994) Is the universal recognition of emotion from facial expression? A review of the cross-cultural studies. Psychol Bull 115:102–141" href="/article/10.1007/s10055-013-0236-7#ref-CR46" id="ref-link-section-d1415e2236">1994</a>), observers watching a facial movement automatically infer the situation, internal feelings and the action. Furthermore, as Sato and Yoshikawa (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sato W, Yoshikawa S (2007) Enhanced experienced of emotional arousal in response to dynamic facial expressions. J Nonverbal Behav 31:119–135" href="/article/10.1007/s10055-013-0236-7#ref-CR50" id="ref-link-section-d1415e2239">2007</a>) proposed, the dynamic presentation of emotional facial expressions is better able to evoke subjective emotional experience than are static images.</p><p>Other findings of this study are consistent with previous research (Fabri et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual environments: an experimental study. Virtual Real 7:66–81" href="/article/10.1007/s10055-013-0236-7#ref-CR19" id="ref-link-section-d1415e2245">2004</a>; Dyck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628" href="/article/10.1007/s10055-013-0236-7#ref-CR12" id="ref-link-section-d1415e2248">2008</a>). Dyck et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628" href="/article/10.1007/s10055-013-0236-7#ref-CR12" id="ref-link-section-d1415e2251">2008</a>) found that accuracy was significantly greater for sadness and fear when virtual reality faces were used to display these emotions. Similarly to the present results, Fabri et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual environments: an experimental study. Virtual Real 7:66–81" href="/article/10.1007/s10055-013-0236-7#ref-CR19" id="ref-link-section-d1415e2254">2004</a>) found that anger was associated with significantly improved accuracy when virtual faces were compared with natural faces. Calder et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Calder AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Cognition 76:105–146" href="/article/10.1007/s10055-013-0236-7#ref-CR6" id="ref-link-section-d1415e2257">2000</a>) stated that anger is one of the most easily recognized emotions from the top half of the face. When creating a virtual agent, this part of the face can easily be modulated by lowering the eyebrow; however, this facial characteristic may be more difficult to represent by an actor or in a natural face. Disgust seemed to be the only emotion for which the correct recognition rate was significantly worse in the virtual reality than in the photographs condition. Earlier studies have reported this limitation in the recognition of disgust (Spencer-Smith et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Spencer-Smith J, Wild H, Innes-Ker AH, Townsend J, Duffy C, Edwards C, Ervin K, Merritt N, Paik PW (2001) Making faces: creating three-dimensional parameterized models of facial expression. Behav Res Methods Instrum 33:115–123" href="/article/10.1007/s10055-013-0236-7#ref-CR52" id="ref-link-section-d1415e2261">2001</a>; Fabri et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual environments: an experimental study. Virtual Real 7:66–81" href="/article/10.1007/s10055-013-0236-7#ref-CR19" id="ref-link-section-d1415e2264">2004</a>; Dyck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628" href="/article/10.1007/s10055-013-0236-7#ref-CR12" id="ref-link-section-d1415e2267">2008</a>) when this emotion was presented in VR. This phenomenon may be due to the difficulty of authentically recreating the naso-labial area (Spencer-Smith et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Spencer-Smith J, Wild H, Innes-Ker AH, Townsend J, Duffy C, Edwards C, Ervin K, Merritt N, Paik PW (2001) Making faces: creating three-dimensional parameterized models of facial expression. Behav Res Methods Instrum 33:115–123" href="/article/10.1007/s10055-013-0236-7#ref-CR52" id="ref-link-section-d1415e2270">2001</a>) although special attention was paid to this aspect when creating the virtual avatars (especially with the combination of AU9 + AU10 + AU15 + AU16). Other researchers have considered disgust to be a special case because it may be a combination of other basic emotions (Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004a" title="Kohler CG, Turner T, Stolar NM, Bilker WB, Brensinger CM, Gur RE, Gur RC (2004a) Differences in facial expressions of four universal emotions. Psychiatr Res 128:235–244" href="/article/10.1007/s10055-013-0236-7#ref-CR30" id="ref-link-section-d1415e2273">2004a</a>). At all events, our findings indicate that the FACS is a good method not only for objectively describing emotions, but also for creating dynamic facial expressions with computer-simulating programs. Indeed, we were able to generate high-quality animation, following the muscle movements encoded by the FACS.</p><p>Following our second hypotheses, we found that participants were able to identify happiness faster than the other emotions in both types of presentation. However, and coinciding with the least recognized emotions in each type of presentation, it took longer for participants to recognize anger when it was displayed by photographs and disgust when it was displayed by the virtual face. Although reaction time is not an important target when training people to improve social skills, this pattern of results shows that complex emotions (such as anger, disgust or sadness), with more AUs implied in the facial movement, may require longer time to be recognized and is more likely to be mistaken. Comparisons of reaction times between photographs and VR were not possible due to the fact that virtual faces included the dynamic animation at the beginning of the image and, consequently, participants had to wait a few milliseconds before they could see the apex expression of the animated avatar.</p><p>Regarding the third hypotheses, the results showed that the “older group” found it easier to recognize emotion correctly in the virtual characters than in the natural faces (photographs). Some authors have reported that there is a decline in facial emotion recognition ability as people grow older (Calder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Calder AJ, Keane J, Manly T, Sprengelmeyer R, Scott S, Nimmo-Smith I et al (2003) Facial expression recognition across the adult life span. Neuropsychologia 41:195–202" href="/article/10.1007/s10055-013-0236-7#ref-CR7" id="ref-link-section-d1415e2282">2003</a>; Mill et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Mill A, Allik J, Realo A, Valk R (2009) Age-related differences in emotion recognition ability: a cross-sectional study. Emotion 9:619–630" href="/article/10.1007/s10055-013-0236-7#ref-CR39" id="ref-link-section-d1415e2285">2009</a>). These authors state that a growing proportion of the overall ageing population may be considerably less accurate in processing emotional signs and cues. People suffering from schizophrenia also present cognitive impairment, which likewise aggravates their ability to process social and emotional information. If the dynamism provided by virtual reality technology does indeed facilitate the recognition of human emotions, this could be a new advantage of using this technology as a tool for assessing and training facial emotion recognition and emotion processing in people with schizophrenia, as well as in other individuals with difficulties in this area. In fact, more than the eighty percentage of the sample agreed that virtual faces, which included a dynamic animation, were much easier to recognize than the natural ones (statics).</p><p>Recently, Riva et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Riva G, Mantovani F, Capideville CS, Preziosa A, Morganti F, Villani D, Gaggioli A, Botella C, Alcañiz M (2007) Affective interactions using virtual reality: the link between presence and emotions. Cyberpsychol Behav 10:45–56" href="/article/10.1007/s10055-013-0236-7#ref-CR44" id="ref-link-section-d1415e2291">2007</a>) demonstrated that VR is an effective means of mood induction, and they also suggested that the sense of presence is not only determined by the environment’s graphical realism or display dimension, but also to a great extent by the emotional characteristics of the experience provided by the technology. Following Gaggioli et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Gaggioli A, Mantovani F, Castelnuovo G, Wiederhold B, Riva G (2003) Avatars in clinical psychology: a framework for the clinical use of virtual humans. Cyberpsychol Behav 6:117–125" href="/article/10.1007/s10055-013-0236-7#ref-CR20" id="ref-link-section-d1415e2294">2003</a>), our virtual humans, which will interact with patients when delivering the cyberintervention, will meet the requirements for all three levels of analysis: <i>physical</i> features (appropriate appearance in relation to their role), <i>internal</i> characteristics (facial expression, gesticulation, non-verbal behaviour and prosody) and, finally, <i>interaction</i> characteristics (the system will allow a successful two-way conversation between the virtual human and the participant, as well as offering the therapist a high degree of control over the virtual human’s behaviour). Therefore, the use of virtual characters with high realism and human-like behaviour, as well as emotional expression, could be a highly reliable tool for training people with a deficit in emotion perception, social information processing and even social anxiety.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusions</h2><div class="c-article-section__content" id="Sec16-content"><p>In sum, the results show that VR faces are as valid as standardized natural faces for accurately recreating human-like facial expressions of emotions.</p><p>The present findings demonstrate that VR environments enable the introduction of virtual agents that can be created using feasible faces and other interpersonal communication cues (sounds, laughs, affect prosody), and this has obvious applications in relation to emotional and social training. As the virtual character’s morphology, external appearance and movements in time and space can be changed and controlled externally, virtual agents and environments have become a powerful tool for experimental psychology (Loomis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Loomis JM, Blascovich JJ, Beall AC (1999) Immersive virtual environment technology as a basic research tool in psychology. Behav Res Methods 31:557–564" href="/article/10.1007/s10055-013-0236-7#ref-CR37" id="ref-link-section-d1415e2316">1999</a>). The findings also have several clinical implications, since the advances being made through virtual reality technology can help to overcome some of the limitations associated with the use of static faces. In addition, this technology is able to simulate social encounters: for example, using virtual reality with a correct pattern of emotional characters could generate alternative social environments. Virtual reality also allows therapists to control and manipulate the avatar’s behaviour in order to provide immediate feedback to people suffering from emotional and behavioural disorders, such as schizophrenia or social anxiety disorder. Our research group is currently using the task described here with psychotic patients, and the preliminary results show that virtual faces are a useful tool not only for assessing the ability of patients with schizophrenia to recognize emotions, but also for studying the clinical, cognitive and personality characteristics which underlie this impairment (Gutiérrez-Maldonado et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gutiérrez-Maldonado J, Rus-Calafell M, Márquez-Rejón S, Ribas-Sabaté J (2012) Association between facial emotion recognition, cognition and alexithymia in patients with schizophrenia: comparison of photographic and virtual reality presentations. Stud Health Technol Inform 181:88–92" href="/article/10.1007/s10055-013-0236-7#ref-CR24" id="ref-link-section-d1415e2319">2012</a>).</p><p>Several limitations of the present study need to be acknowledged. Firstly, it has to be stressed in this context that several studies have shown that dynamic and static facial expressions are processed differently neurologically (Collignon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Collignon O, Girarda S, Gosselina F, Roya S, Saint-Amoura D, Lassondea M, Leporea F (2008) Audio-visual integration of emotion expression. Brain Res 25:126–135" href="/article/10.1007/s10055-013-0236-7#ref-CR8" id="ref-link-section-d1415e2325">2008</a>). This has to be considered an important limitation when comparing both facial stimuli. However, to the best of our knowledge, no well-validated set of dynamic video sequences is available to the research community that permits a reliable comparison with the virtual faces. Furthermore, the purpose of the present study was to examine the feasibility of the created virtual faces as well as their evaluation as perceived social agents, rather than the exploration of the neural mechanism involved in the facial emotion capability. Secondly, the predominance of women in the sample may constitute a weakness, even though no significant effect of participants’ gender was found. Thirdly, although culture is known to be an important variable when studying emotion (Russell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Russell JA (1994) Is the universal recognition of emotion from facial expression? A review of the cross-cultural studies. Psychol Bull 115:102–141" href="/article/10.1007/s10055-013-0236-7#ref-CR46" id="ref-link-section-d1415e2328">1994</a>), this aspect was not included in the analysis of the present data. However, some authors in this field consider that social cognition needs to be carefully differentiated in relation to cultural influences and should be seen as a “universal” category enabling humans to develop language, culture or technology (Tomasello et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Tomasello M, Carpenter M, Call J, Behne T, Moll H (2005) Understanding and sharing intentions: the origins of cultural cognition. Behav Brain Sci 28:675–735" href="/article/10.1007/s10055-013-0236-7#ref-CR53" id="ref-link-section-d1415e2331">2005</a>). Finally, only six expressions of emotions were included in the study, and some authors consider that these correspond to only a small number of basic emotions with prototypical expressions produced by neuromotor programs (Scherer and Ellgring <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Scherer KR, Ellgring H (2007) Are facial expressions of emotion produced by categorical affect programs or dynamically driven by appraisal? Emotion 7:113–130" href="/article/10.1007/s10055-013-0236-7#ref-CR51" id="ref-link-section-d1415e2334">2007</a>).</p><p>Despite these limitations, the present study adds to the existing literature on the application of virtual reality in the context of research, assessment and psychotherapy for psychiatric and psychological disorders. As mentioned before, the next stage of our research will be to include the virtual agents as social virtual entities in our VR social skills training programme: the <i>Soskitrain</i> (for a further description of the programme, see Rus-Calafell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Rus-Calafell M, Guitérrez-Maldonado J, Ribas-Sabaté J (2012) Improving social behaviour in schizophrenia patients using an integrated virtual reality programme: a case study. Stud Health Technol Inform 181:283–286" href="/article/10.1007/s10055-013-0236-7#ref-CR45" id="ref-link-section-d1415e2343">2012</a>). Further studies are now needed to establish more fully the potential of embodied conversational agents in the design of new treatment programmes and cyberinterventions for disorders of this kind.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Addington, D. Addington, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Addington J, Addington D (1998) Facial affect recognition and information processing in schizophrenia and bipo" /><p class="c-article-references__text" id="ref-CR1">Addington J, Addington D (1998) Facial affect recognition and information processing in schizophrenia and bipolar disorder. Schizophr Res 32:171–181</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0920-9964%2898%2900042-5" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20affect%20recognition%20and%20information%20processing%20in%20schizophrenia%20and%20bipolar%20disorder&amp;journal=Schizophr%20Res&amp;volume=32&amp;pages=171-181&amp;publication_year=1998&amp;author=Addington%2CJ&amp;author=Addington%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Archer, DC. Hay, AW. Young, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Archer J, Hay DC, Young AW (1994) Movement, face processing and schizophrenia: evidence of a differential defi" /><p class="c-article-references__text" id="ref-CR2">Archer J, Hay DC, Young AW (1994) Movement, face processing and schizophrenia: evidence of a differential deficit in expression analysis. Br J Clin Psychol 33:517–528</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.2044-8260.1994.tb01148.x" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Movement%2C%20face%20processing%20and%20schizophrenia%3A%20evidence%20of%20a%20differential%20deficit%20in%20expression%20analysis&amp;journal=Br%20J%20Clin%20Psychol&amp;volume=33&amp;pages=517-528&amp;publication_year=1994&amp;author=Archer%2CJ&amp;author=Hay%2CDC&amp;author=Young%2CAW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AS. Bellack, KT. Mueser, S. Gingerich, J. Agresta, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Bellack AS, Mueser KT, Gingerich S, Agresta J (2004) Social skills training for schizophrenia. Guilford Press," /><p class="c-article-references__text" id="ref-CR3">Bellack AS, Mueser KT, Gingerich S, Agresta J (2004) Social skills training for schizophrenia. Guilford Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20skills%20training%20for%20schizophrenia&amp;publication_year=2004&amp;author=Bellack%2CAS&amp;author=Mueser%2CKT&amp;author=Gingerich%2CS&amp;author=Agresta%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Bente, S. Rüggenberg, NC. Krämer, F. Eschenburg, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bente G, Rüggenberg S, Krämer NC, Eschenburg F (2008) Avatar-mediated networking: increasing social presence a" /><p class="c-article-references__text" id="ref-CR4">Bente G, Rüggenberg S, Krämer NC, Eschenburg F (2008) Avatar-mediated networking: increasing social presence and interpersonal trust in net-based collaborations. Hum Commun Res 34:287–318</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1468-2958.2008.00322.x" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Avatar-mediated%20networking%3A%20increasing%20social%20presence%20and%20interpersonal%20trust%20in%20net-based%20collaborations&amp;journal=Hum%20Commun%20Res&amp;volume=34&amp;pages=287-318&amp;publication_year=2008&amp;author=Bente%2CG&amp;author=R%C3%BCggenberg%2CS&amp;author=Kr%C3%A4mer%2CNC&amp;author=Eschenburg%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Borras, M. Boucherie, S. Mohr, T. Lecomte, N. Perroud, P. Huguelet, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Borras L, Boucherie M, Mohr S, Lecomte T, Perroud N, Huguelet P (2009) Increasing self-esteem: efficacy of a g" /><p class="c-article-references__text" id="ref-CR5">Borras L, Boucherie M, Mohr S, Lecomte T, Perroud N, Huguelet P (2009) Increasing self-esteem: efficacy of a group intervention for individuals with severe mental disorders. Eur Psychiat 24:307–316</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.eurpsy.2009.01.003" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Increasing%20self-esteem%3A%20efficacy%20of%20a%20group%20intervention%20for%20individuals%20with%20severe%20mental%20disorders&amp;journal=Eur%20Psychiat&amp;volume=24&amp;pages=307-316&amp;publication_year=2009&amp;author=Borras%2CL&amp;author=Boucherie%2CM&amp;author=Mohr%2CS&amp;author=Lecomte%2CT&amp;author=Perroud%2CN&amp;author=Huguelet%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Calder, D. Rowland, AW. Young, I. Nimmo-Smith, J. Keane, DI. Perrett, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Calder AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Cog" /><p class="c-article-references__text" id="ref-CR6">Calder AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Cognition 76:105–146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0010-0277%2800%2900074-3" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Caricaturing%20facial%20expressions&amp;journal=Cognition&amp;volume=76&amp;pages=105-146&amp;publication_year=2000&amp;author=Calder%2CAJ&amp;author=Rowland%2CD&amp;author=Young%2CAW&amp;author=Nimmo-Smith%2CI&amp;author=Keane%2CJ&amp;author=Perrett%2CDI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Calder, J. Keane, T. Manly, R. Sprengelmeyer, S. Scott, I. Nimmo-Smith, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Calder AJ, Keane J, Manly T, Sprengelmeyer R, Scott S, Nimmo-Smith I et al (2003) Facial expression recognitio" /><p class="c-article-references__text" id="ref-CR7">Calder AJ, Keane J, Manly T, Sprengelmeyer R, Scott S, Nimmo-Smith I et al (2003) Facial expression recognition across the adult life span. Neuropsychologia 41:195–202</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0028-3932%2802%2900149-5" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20expression%20recognition%20across%20the%20adult%20life%20span&amp;journal=Neuropsychologia&amp;volume=41&amp;pages=195-202&amp;publication_year=2003&amp;author=Calder%2CAJ&amp;author=Keane%2CJ&amp;author=Manly%2CT&amp;author=Sprengelmeyer%2CR&amp;author=Scott%2CS&amp;author=Nimmo-Smith%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Collignon, S. Girarda, F. Gosselina, S. Roya, D. Saint-Amoura, M. Lassondea, F. Leporea, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Collignon O, Girarda S, Gosselina F, Roya S, Saint-Amoura D, Lassondea M, Leporea F (2008) Audio-visual integr" /><p class="c-article-references__text" id="ref-CR8">Collignon O, Girarda S, Gosselina F, Roya S, Saint-Amoura D, Lassondea M, Leporea F (2008) Audio-visual integration of emotion expression. Brain Res 25:126–135</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.brainres.2008.04.023" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Audio-visual%20integration%20of%20emotion%20expression&amp;journal=Brain%20Res&amp;volume=25&amp;pages=126-135&amp;publication_year=2008&amp;author=Collignon%2CO&amp;author=Girarda%2CS&amp;author=Gosselina%2CF&amp;author=Roya%2CS&amp;author=Saint-Amoura%2CD&amp;author=Lassondea%2CM&amp;author=Leporea%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SM. Couture, DL. Penn, DL. Roberts, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Couture SM, Penn DL, Roberts DL (2006) The functional significance of social cognition in schizophrenia: a rev" /><p class="c-article-references__text" id="ref-CR9">Couture SM, Penn DL, Roberts DL (2006) The functional significance of social cognition in schizophrenia: a review. Schizophr Bull 32:s44–s63</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fschbul%2Fsbl029" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20functional%20significance%20of%20social%20cognition%20in%20schizophrenia%3A%20a%20review&amp;journal=Schizophr%20Bull&amp;volume=32&amp;pages=s44-s63&amp;publication_year=2006&amp;author=Couture%2CSM&amp;author=Penn%2CDL&amp;author=Roberts%2CDL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PJ. Davis, MG. Gibson, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Davis PJ, Gibson MG (2000) Recognition of posed and genuine facial expression of emotion in paranoid and nonpa" /><p class="c-article-references__text" id="ref-CR10">Davis PJ, Gibson MG (2000) Recognition of posed and genuine facial expression of emotion in paranoid and nonparanoid schizophrenia. J Abnorm Psychol 109:445–450</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0021-843X.109.3.445" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20posed%20and%20genuine%20facial%20expression%20of%20emotion%20in%20paranoid%20and%20nonparanoid%20schizophrenia&amp;journal=J%20Abnorm%20Psychol&amp;volume=109&amp;pages=445-450&amp;publication_year=2000&amp;author=Davis%2CPJ&amp;author=Gibson%2CMG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Domes, L. Schulze, SC. Herpertz, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Domes G, Schulze L, Herpertz SC (2009) Emotion recognition in borderline personality disorder-a review of the " /><p class="c-article-references__text" id="ref-CR11">Domes G, Schulze L, Herpertz SC (2009) Emotion recognition in borderline personality disorder-a review of the literature. J Pers Disord 23:6–19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1521%2Fpedi.2009.23.1.6" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20recognition%20in%20borderline%20personality%20disorder-a%20review%20of%20the%20literature&amp;journal=J%20Pers%20Disord&amp;volume=23&amp;pages=6-19&amp;publication_year=2009&amp;author=Domes%2CG&amp;author=Schulze%2CL&amp;author=Herpertz%2CSC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Dyck, M. Winbeck, S. Leiberg, Y. Chen, RC. Gur, K. Mathiak, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and " /><p class="c-article-references__text" id="ref-CR12">Dyck M, Winbeck M, Leiberg S, Chen Y, Gur RC, Mathiak K (2008) Recognition profile of emotions in natural and virtual faces. PLoS ONE 3:e3628</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1371%2Fjournal.pone.0003628" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20profile%20of%20emotions%20in%20natural%20and%20virtual%20faces&amp;journal=PLoS%20ONE&amp;volume=3&amp;publication_year=2008&amp;author=Dyck%2CM&amp;author=Winbeck%2CM&amp;author=Leiberg%2CS&amp;author=Chen%2CY&amp;author=Gur%2CRC&amp;author=Mathiak%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Dyck, M. Winbeck, S. Leiberg, Y. Chen, K. Mathiak, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Dyck M, Winbeck M, Leiberg S, Chen Y, Mathiak K (2010) Virtual faces as a tool to study emotion recognition de" /><p class="c-article-references__text" id="ref-CR13">Dyck M, Winbeck M, Leiberg S, Chen Y, Mathiak K (2010) Virtual faces as a tool to study emotion recognition deficits in schizophrenia. Psychiatr Res 179:247–252</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.psychres.2009.11.004" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20faces%20as%20a%20tool%20to%20study%20emotion%20recognition%20deficits%20in%20schizophrenia&amp;journal=Psychiatr%20Res&amp;volume=179&amp;pages=247-252&amp;publication_year=2010&amp;author=Dyck%2CM&amp;author=Winbeck%2CM&amp;author=Leiberg%2CS&amp;author=Chen%2CY&amp;author=Mathiak%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Edwards, H. Jackson, P. Pattison, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Edwards J, Jackson H, Pattison P (2002) Emotion recognition via facial expression and affective prosody in sch" /><p class="c-article-references__text" id="ref-CR14">Edwards J, Jackson H, Pattison P (2002) Emotion recognition via facial expression and affective prosody in schizophrenia: a methodological review. Clin Psychol Rev 22:789–832</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0272-7358%2802%2900130-7" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20recognition%20via%20facial%20expression%20and%20affective%20prosody%20in%20schizophrenia%3A%20a%20methodological%20review&amp;journal=Clin%20Psychol%20Rev&amp;volume=22&amp;pages=789-832&amp;publication_year=2002&amp;author=Edwards%2CJ&amp;author=Jackson%2CH&amp;author=Pattison%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Ekman P (1992) Are there basic emotions? Psychol Rev 99:550–553" /><p class="c-article-references__text" id="ref-CR15">Ekman P (1992) Are there basic emotions? Psychol Rev 99:550–553</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-295X.99.3.550" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Are%20there%20basic%20emotions%3F&amp;journal=Psychol%20Rev&amp;volume=99&amp;pages=550-553&amp;publication_year=1992&amp;author=Ekman%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Ekman, W. Friesen, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Ekman P, Friesen W (1975) Pictures of facial affect CD-Rom. University of California, San Francisco" /><p class="c-article-references__text" id="ref-CR16">Ekman P, Friesen W (1975) Pictures of facial affect CD-Rom. University of California, San Francisco</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pictures%20of%20facial%20affect%20CD-Rom&amp;publication_year=1975&amp;author=Ekman%2CP&amp;author=Friesen%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Ekman, W. Friesen, " /><meta itemprop="datePublished" content="1978" /><meta itemprop="headline" content="Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, Palo Alto" /><p class="c-article-references__text" id="ref-CR17">Ekman P, Friesen W (1978) Facial action coding system. Consulting Psychologists Press, Palo Alto</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20action%20coding%20system&amp;publication_year=1978&amp;author=Ekman%2CP&amp;author=Friesen%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fabri M, Moore D, Hobbs D (2002) Expressive agents: non-verbal communication in collaborative virtual environm" /><p class="c-article-references__text" id="ref-CR18">Fabri M, Moore D, Hobbs D (2002) Expressive agents: non-verbal communication in collaborative virtual environments. In: Proceedings of autonomous agents and multi-agent systems, Bologna, Italy</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Fabri, D. Moore, D. Hobbs, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual envi" /><p class="c-article-references__text" id="ref-CR19">Fabri M, Moore D, Hobbs D (2004) Mediating the expression of emotion in educational collaborative virtual environments: an experimental study. Virtual Real 7:66–81</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-003-0116-7" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mediating%20the%20expression%20of%20emotion%20in%20educational%20collaborative%20virtual%20environments%3A%20an%20experimental%20study&amp;journal=Virtual%20Real&amp;volume=7&amp;pages=66-81&amp;publication_year=2004&amp;author=Fabri%2CM&amp;author=Moore%2CD&amp;author=Hobbs%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Gaggioli, F. Mantovani, G. Castelnuovo, B. Wiederhold, G. Riva, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Gaggioli A, Mantovani F, Castelnuovo G, Wiederhold B, Riva G (2003) Avatars in clinical psychology: a framewor" /><p class="c-article-references__text" id="ref-CR20">Gaggioli A, Mantovani F, Castelnuovo G, Wiederhold B, Riva G (2003) Avatars in clinical psychology: a framework for the clinical use of virtual humans. Cyberpsychol Behav 6:117–125</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2F109493103321640301" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Avatars%20in%20clinical%20psychology%3A%20a%20framework%20for%20the%20clinical%20use%20of%20virtual%20humans&amp;journal=Cyberpsychol%20Behav&amp;volume=6&amp;pages=117-125&amp;publication_year=2003&amp;author=Gaggioli%2CA&amp;author=Mantovani%2CF&amp;author=Castelnuovo%2CG&amp;author=Wiederhold%2CB&amp;author=Riva%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Green, B. Olivier, J. Crowley, DL. Penn, S. Silverstein, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Green M, Olivier B, Crowley J, Penn DL, Silverstein S (2005) Social cognition in Schizophrenia: recommendation" /><p class="c-article-references__text" id="ref-CR21">Green M, Olivier B, Crowley J, Penn DL, Silverstein S (2005) Social cognition in Schizophrenia: recommendation from measurement and treatment to improve cognition in schizophrenia new approaches conferences. Schizophr Bull 31:882–887</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fschbul%2Fsbi049" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20cognition%20in%20Schizophrenia%3A%20recommendation%20from%20measurement%20and%20treatment%20to%20improve%20cognition%20in%20schizophrenia%20new%20approaches%20conferences&amp;journal=Schizophr%20Bull&amp;volume=31&amp;pages=882-887&amp;publication_year=2005&amp;author=Green%2CM&amp;author=Olivier%2CB&amp;author=Crowley%2CJ&amp;author=Penn%2CDL&amp;author=Silverstein%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RC. Gur, R. Sara, M. Hagendoorn, O. Marom, P. Hughett, L. Macy, T. Turner, R. Bajcsy, A. Posner, RE. Gur, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Gur RC, Sara R, Hagendoorn M, Marom O, Hughett P, Macy L, Turner T, Bajcsy R, Posner A, Gur RE (2002a) A metho" /><p class="c-article-references__text" id="ref-CR22">Gur RC, Sara R, Hagendoorn M, Marom O, Hughett P, Macy L, Turner T, Bajcsy R, Posner A, Gur RE (2002a) A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies. J Neurosci Methods 115:137–143</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0165-0270%2802%2900006-7" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20method%20for%20obtaining%203-dimensional%20facial%20expressions%20and%20its%20standardization%20for%20use%20in%20neurocognitive%20studies&amp;journal=J%20Neurosci%20Methods&amp;volume=115&amp;pages=137-143&amp;publication_year=2002&amp;author=Gur%2CRC&amp;author=Sara%2CR&amp;author=Hagendoorn%2CM&amp;author=Marom%2CO&amp;author=Hughett%2CP&amp;author=Macy%2CL&amp;author=Turner%2CT&amp;author=Bajcsy%2CR&amp;author=Posner%2CA&amp;author=Gur%2CRE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RC. Gur, L. Schroeder, T. Turner, C. McGrath, RM. Chan, BI. Turetsky, D. Alsop, J. Maldjian, RE. Gur, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Gur RC, Schroeder L, Turner T, McGrath C, Chan RM, Turetsky BI, Alsop D, Maldjian J, Gur RE (2002b) Brain acti" /><p class="c-article-references__text" id="ref-CR23">Gur RC, Schroeder L, Turner T, McGrath C, Chan RM, Turetsky BI, Alsop D, Maldjian J, Gur RE (2002b) Brain activation during facial emotion processing. Neuroimage 16:651–662</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fnimg.2002.1097" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain%20activation%20during%20facial%20emotion%20processing&amp;journal=Neuroimage&amp;volume=16&amp;pages=651-662&amp;publication_year=2002&amp;author=Gur%2CRC&amp;author=Schroeder%2CL&amp;author=Turner%2CT&amp;author=McGrath%2CC&amp;author=Chan%2CRM&amp;author=Turetsky%2CBI&amp;author=Alsop%2CD&amp;author=Maldjian%2CJ&amp;author=Gur%2CRE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Gutiérrez-Maldonado, M. Rus-Calafell, S. Márquez-Rejón, J. Ribas-Sabaté, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Gutiérrez-Maldonado J, Rus-Calafell M, Márquez-Rejón S, Ribas-Sabaté J (2012) Association between facial emoti" /><p class="c-article-references__text" id="ref-CR24">Gutiérrez-Maldonado J, Rus-Calafell M, Márquez-Rejón S, Ribas-Sabaté J (2012) Association between facial emotion recognition, cognition and alexithymia in patients with schizophrenia: comparison of photographic and virtual reality presentations. Stud Health Technol Inform 181:88–92</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Association%20between%20facial%20emotion%20recognition%2C%20cognition%20and%20alexithymia%20in%20patients%20with%20schizophrenia%3A%20comparison%20of%20photographic%20and%20virtual%20reality%20presentations&amp;journal=Stud%20Health%20Technol%20Inform&amp;volume=181&amp;pages=88-92&amp;publication_year=2012&amp;author=Guti%C3%A9rrez-Maldonado%2CJ&amp;author=Rus-Calafell%2CM&amp;author=M%C3%A1rquez-Rej%C3%B3n%2CS&amp;author=Ribas-Sabat%C3%A9%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Han, J. Ku, K. Kim, HJ. Jang, J. Park, JJ. Kim, CH. Kim, MH. Choi, IY. Kim, SI. Kim, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Han K, Ku J, Kim K, Jang HJ, Park J, Kim JJ, Kim CH, Choi MH, Kim IY, Kim SI (2009) Virtual reality prototype " /><p class="c-article-references__text" id="ref-CR25">Han K, Ku J, Kim K, Jang HJ, Park J, Kim JJ, Kim CH, Choi MH, Kim IY, Kim SI (2009) Virtual reality prototype for measurement of expression characteristics in emotional situations. Comput Biol Med 39:173–179</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.compbiomed.2008.12.002" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20prototype%20for%20measurement%20of%20expression%20characteristics%20in%20emotional%20situations&amp;journal=Comput%20Biol%20Med&amp;volume=39&amp;pages=173-179&amp;publication_year=2009&amp;author=Han%2CK&amp;author=Ku%2CJ&amp;author=Kim%2CK&amp;author=Jang%2CHJ&amp;author=Park%2CJ&amp;author=Kim%2CJJ&amp;author=Kim%2CCH&amp;author=Choi%2CMH&amp;author=Kim%2CIY&amp;author=Kim%2CSI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NK. Harwood, LJ. Hall, AJ. Shinkfield, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Harwood NK, Hall LJ, Shinkfield AJ (1999) Recognition of facial emotional expressions from moving and static d" /><p class="c-article-references__text" id="ref-CR26">Harwood NK, Hall LJ, Shinkfield AJ (1999) Recognition of facial emotional expressions from moving and static displays by individuals with mental retardation. Am J Ment Retard 104:270–278</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1352%2F0895-8017%281999%29104%3C0270%3AROFEEF%3E2.0.CO%3B2" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20facial%20emotional%20expressions%20from%20moving%20and%20static%20displays%20by%20individuals%20with%20mental%20retardation&amp;journal=Am%20J%20Ment%20Retard&amp;volume=104&amp;pages=270-278&amp;publication_year=1999&amp;author=Harwood%2CNK&amp;author=Hall%2CLJ&amp;author=Shinkfield%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Irani, S. Seligman, V. Kamath, C. Kohler, RC. Gur, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Irani F, Seligman S, Kamath V, Kohler C, Gur RC (2012) A meta-analysis of emotion perception and functional ou" /><p class="c-article-references__text" id="ref-CR27">Irani F, Seligman S, Kamath V, Kohler C, Gur RC (2012) A meta-analysis of emotion perception and functional outcomes in schizophrenia. Schizophr Res 137:203–211</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.schres.2012.01.023" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20meta-analysis%20of%20emotion%20perception%20and%20functional%20outcomes%20in%20schizophrenia&amp;journal=Schizophr%20Res&amp;volume=137&amp;pages=203-211&amp;publication_year=2012&amp;author=Irani%2CF&amp;author=Seligman%2CS&amp;author=Kamath%2CV&amp;author=Kohler%2CC&amp;author=Gur%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Knight, A. Johnston, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Knight B, Johnston A (1997) The role of movement in face recognition. Vis Cognit 4:265–273" /><p class="c-article-references__text" id="ref-CR28">Knight B, Johnston A (1997) The role of movement in face recognition. Vis Cognit 4:265–273</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F713756764" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20movement%20in%20face%20recognition&amp;journal=Vis%20Cognit&amp;volume=4&amp;pages=265-273&amp;publication_year=1997&amp;author=Knight%2CB&amp;author=Johnston%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CG. Kohler, TH. Turner, WB. Bilker, CM. Brensinger, SJ. Siegel, SJ. Kanes, RE. Gur, RC. Gur, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion reco" /><p class="c-article-references__text" id="ref-CR29">Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1176%2Fappi.ajp.160.10.1768" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20emotion%20recognition%20in%20schizophrenia%3A%20intensity%20effects%20and%20error%20pattern&amp;journal=Am%20J%20Psychiatry&amp;volume=160&amp;pages=1768-1774&amp;publication_year=2003&amp;author=Kohler%2CCG&amp;author=Turner%2CTH&amp;author=Bilker%2CWB&amp;author=Brensinger%2CCM&amp;author=Siegel%2CSJ&amp;author=Kanes%2CSJ&amp;author=Gur%2CRE&amp;author=Gur%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CG. Kohler, T. Turner, NM. Stolar, WB. Bilker, CM. Brensinger, RE. Gur, RC. Gur, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kohler CG, Turner T, Stolar NM, Bilker WB, Brensinger CM, Gur RE, Gur RC (2004a) Differences in facial express" /><p class="c-article-references__text" id="ref-CR30">Kohler CG, Turner T, Stolar NM, Bilker WB, Brensinger CM, Gur RE, Gur RC (2004a) Differences in facial expressions of four universal emotions. Psychiatr Res 128:235–244</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.psychres.2004.07.003" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Differences%20in%20facial%20expressions%20of%20four%20universal%20emotions&amp;journal=Psychiatr%20Res&amp;volume=128&amp;pages=235-244&amp;publication_year=2004&amp;author=Kohler%2CCG&amp;author=Turner%2CT&amp;author=Stolar%2CNM&amp;author=Bilker%2CWB&amp;author=Brensinger%2CCM&amp;author=Gur%2CRE&amp;author=Gur%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CG. Kohler, TH. Turner, RE. Gur, RC. Gur, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kohler CG, Turner TH, Gur RE, Gur RC (2004b) Recognition of facial emotions in neuropsychiatric disorders. CNS" /><p class="c-article-references__text" id="ref-CR31">Kohler CG, Turner TH, Gur RE, Gur RC (2004b) Recognition of facial emotions in neuropsychiatric disorders. CNS Spectr 9:267–274</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20facial%20emotions%20in%20neuropsychiatric%20disorders&amp;journal=CNS%20Spectr&amp;volume=9&amp;pages=267-274&amp;publication_year=2004&amp;author=Kohler%2CCG&amp;author=Turner%2CTH&amp;author=Gur%2CRE&amp;author=Gur%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Krumhuber, A. Kappas, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Krumhuber E, Kappas A (2005) Moving smiles: the role of dynamic components for the perception of the genuinene" /><p class="c-article-references__text" id="ref-CR32">Krumhuber E, Kappas A (2005) Moving smiles: the role of dynamic components for the perception of the genuineness of smiles. J Nonverbal Behav 29:3–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10919-004-0887-x" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Moving%20smiles%3A%20the%20role%20of%20dynamic%20components%20for%20the%20perception%20of%20the%20genuineness%20of%20smiles&amp;journal=J%20Nonverbal%20Behav&amp;volume=29&amp;pages=3-24&amp;publication_year=2005&amp;author=Krumhuber%2CE&amp;author=Kappas%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Ku, HJ. Jang, KU. Kim, SH. Park, JJ. Kim, CH. Kim, SW. Nam, IY. Kim, SI. Kim, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Ku J, Jang HJ, Kim KU, Park SH, Kim JJ, Kim CH, Nam SW, Kim IY, Kim SI (2006) Pilot study for assessing the be" /><p class="c-article-references__text" id="ref-CR33">Ku J, Jang HJ, Kim KU, Park SH, Kim JJ, Kim CH, Nam SW, Kim IY, Kim SI (2006) Pilot study for assessing the behaviors of patients with schizophrenia towards a virtual avatar. Cyberpsychol Behav 9:531–539</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fcpb.2006.9.531" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pilot%20study%20for%20assessing%20the%20behaviors%20of%20patients%20with%20schizophrenia%20towards%20a%20virtual%20avatar&amp;journal=Cyberpsychol%20Behav&amp;volume=9&amp;pages=531-539&amp;publication_year=2006&amp;author=Ku%2CJ&amp;author=Jang%2CHJ&amp;author=Kim%2CKU&amp;author=Park%2CSH&amp;author=Kim%2CJJ&amp;author=Kim%2CCH&amp;author=Nam%2CSW&amp;author=Kim%2CIY&amp;author=Kim%2CSI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Ku, K. Han, HR. Lee, HJ. Jang, KU. Kim, SH. Park, JJ. Kim, CH. Kim, IY. Kim, SI. Kim, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Ku J, Han K, Lee HR, Jang HJ, Kim KU, Park SH, Kim JJ, Kim CH, Kim IY, Kim SI (2007) VR-based conversation tra" /><p class="c-article-references__text" id="ref-CR34">Ku J, Han K, Lee HR, Jang HJ, Kim KU, Park SH, Kim JJ, Kim CH, Kim IY, Kim SI (2007) VR-based conversation training program for patients with schizophrenia: a preliminary clinical trial. Cyberpsychol Behav 10:567–574</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fcpb.2007.9989" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=VR-based%20conversation%20training%20program%20for%20patients%20with%20schizophrenia%3A%20a%20preliminary%20clinical%20trial&amp;journal=Cyberpsychol%20Behav&amp;volume=10&amp;pages=567-574&amp;publication_year=2007&amp;author=Ku%2CJ&amp;author=Han%2CK&amp;author=Lee%2CHR&amp;author=Jang%2CHJ&amp;author=Kim%2CKU&amp;author=Park%2CSH&amp;author=Kim%2CJJ&amp;author=Kim%2CCH&amp;author=Kim%2CIY&amp;author=Kim%2CSI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Leppänen, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Leppänen JM (2006) Emotional information processing in mood disorders: a review of behavioral and neuroimaging" /><p class="c-article-references__text" id="ref-CR35">Leppänen JM (2006) Emotional information processing in mood disorders: a review of behavioral and neuroimaging findings. Curr Opin Psychiatry 19:34–39</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2F01.yco.0000191500.46411.00" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20information%20processing%20in%20mood%20disorders%3A%20a%20review%20of%20behavioral%20and%20neuroimaging%20findings&amp;journal=Curr%20Opin%20Psychiatry&amp;volume=19&amp;pages=34-39&amp;publication_year=2006&amp;author=Lepp%C3%A4nen%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lisetti CL, Nasoz F (2002) MAUI: a multimodal affective user interface. In: Proceedings of Multimedia’02, Juan" /><p class="c-article-references__text" id="ref-CR36">Lisetti CL, Nasoz F (2002) MAUI: a multimodal affective user interface. In: Proceedings of Multimedia’02, Juan-les-Pins, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Loomis, JJ. Blascovich, AC. Beall, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Loomis JM, Blascovich JJ, Beall AC (1999) Immersive virtual environment technology as a basic research tool in" /><p class="c-article-references__text" id="ref-CR37">Loomis JM, Blascovich JJ, Beall AC (1999) Immersive virtual environment technology as a basic research tool in psychology. Behav Res Methods 31:557–564</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03200735" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20virtual%20environment%20technology%20as%20a%20basic%20research%20tool%20in%20psychology&amp;journal=Behav%20Res%20Methods&amp;volume=31&amp;pages=557-564&amp;publication_year=1999&amp;author=Loomis%2CJM&amp;author=Blascovich%2CJJ&amp;author=Beall%2CAC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JP. Machado-de-Sousa, KC. Arrais, NT. Alves, MH. Chagas, C. Meneses-Gaya, JA. Crippa, JE. Hallak, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Machado-de-Sousa JP, Arrais KC, Alves NT, Chagas MH, de Meneses-Gaya C, Crippa JA, Hallak JE (2010) Facial aff" /><p class="c-article-references__text" id="ref-CR38">Machado-de-Sousa JP, Arrais KC, Alves NT, Chagas MH, de Meneses-Gaya C, Crippa JA, Hallak JE (2010) Facial affect processing in social anxiety: tasks and stimuli. J Neurosci Methods 193:1–6</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jneumeth.2010.08.013" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20affect%20processing%20in%20social%20anxiety%3A%20tasks%20and%20stimuli&amp;journal=J%20Neurosci%20Methods&amp;volume=193&amp;pages=1-6&amp;publication_year=2010&amp;author=Machado-de-Sousa%2CJP&amp;author=Arrais%2CKC&amp;author=Alves%2CNT&amp;author=Chagas%2CMH&amp;author=Meneses-Gaya%2CC&amp;author=Crippa%2CJA&amp;author=Hallak%2CJE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Mill, J. Allik, A. Realo, R. Valk, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Mill A, Allik J, Realo A, Valk R (2009) Age-related differences in emotion recognition ability: a cross-sectio" /><p class="c-article-references__text" id="ref-CR39">Mill A, Allik J, Realo A, Valk R (2009) Age-related differences in emotion recognition ability: a cross-sectional study. Emotion 9:619–630</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fa0016562" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Age-related%20differences%20in%20emotion%20recognition%20ability%3A%20a%20cross-sectional%20study&amp;journal=Emotion&amp;volume=9&amp;pages=619-630&amp;publication_year=2009&amp;author=Mill%2CA&amp;author=Allik%2CJ&amp;author=Realo%2CA&amp;author=Valk%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KT. Mueser, R. Doonan, DL. Penn, JJ. Blanchard, AS. Bellack, P. Nishith, J. DeLeon, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Mueser KT, Doonan R, Penn DL, Blanchard JJ, Bellack AS, Nishith P, DeLeon J (1996) Emotion recognition and soc" /><p class="c-article-references__text" id="ref-CR40">Mueser KT, Doonan R, Penn DL, Blanchard JJ, Bellack AS, Nishith P, DeLeon J (1996) Emotion recognition and social competence in chronic schizophrenia. J Abnor Psychol 105:271–275</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0021-843X.105.2.271" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20recognition%20and%20social%20competence%20in%20chronic%20schizophrenia&amp;journal=J%20Abnor%20Psychol&amp;volume=105&amp;pages=271-275&amp;publication_year=1996&amp;author=Mueser%2CKT&amp;author=Doonan%2CR&amp;author=Penn%2CDL&amp;author=Blanchard%2CJJ&amp;author=Bellack%2CAS&amp;author=Nishith%2CP&amp;author=DeLeon%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Park, J. Ku, IH. Park, JY. Park, SI. Kim, JJ. Kim, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Park KM, Ku J, Park IH, Park JY, Kim SI, Kim JJ (2009) Improvement in social competence in patients with schiz" /><p class="c-article-references__text" id="ref-CR41">Park KM, Ku J, Park IH, Park JY, Kim SI, Kim JJ (2009) Improvement in social competence in patients with schizophrenia: a pilot study using a performance-based measure using virtual reality. Hum Psychopharmacol 24:619–627</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fhup.1071" aria-label="View reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Improvement%20in%20social%20competence%20in%20patients%20with%20schizophrenia%3A%20a%20pilot%20study%20using%20a%20performance-based%20measure%20using%20virtual%20reality&amp;journal=Hum%20Psychopharmacol&amp;volume=24&amp;pages=619-627&amp;publication_year=2009&amp;author=Park%2CKM&amp;author=Ku%2CJ&amp;author=Park%2CIH&amp;author=Park%2CJY&amp;author=Kim%2CSI&amp;author=Kim%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Park, J. Ku, J. Soo-Hee, JY. Park, S. Kim, JJ. Kim, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Park KM, Ku J, Soo-Hee J, Park JY, Kim S, Kim JJ (2011) A virtual reality application in role-plays of social " /><p class="c-article-references__text" id="ref-CR42">Park KM, Ku J, Soo-Hee J, Park JY, Kim S, Kim JJ (2011) A virtual reality application in role-plays of social skills training for schizophrenia: a randomized, controlled trial. Psychiatr Res 189:166–172</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.psychres.2011.04.003" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20virtual%20reality%20application%20in%20role-plays%20of%20social%20skills%20training%20for%20schizophrenia%3A%20a%20randomized%2C%20controlled%20trial&amp;journal=Psychiatr%20Res&amp;volume=189&amp;pages=166-172&amp;publication_year=2011&amp;author=Park%2CKM&amp;author=Ku%2CJ&amp;author=Soo-Hee%2CJ&amp;author=Park%2CJY&amp;author=Kim%2CS&amp;author=Kim%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parsons TC (2011) Neuropsychological assessment using virtual environments: enhanced assessment technology for" /><p class="c-article-references__text" id="ref-CR43">Parsons TC (2011) Neuropsychological assessment using virtual environments: enhanced assessment technology for improved ecological validity. In: Brahnam S, Jain LC (eds) Advance computer intelligence paradigms in healthcare 6, SCI 337, pp 271–289</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Riva, F. Mantovani, CS. Capideville, A. Preziosa, F. Morganti, D. Villani, A. Gaggioli, C. Botella, M. Alcañiz, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Riva G, Mantovani F, Capideville CS, Preziosa A, Morganti F, Villani D, Gaggioli A, Botella C, Alcañiz M (2007" /><p class="c-article-references__text" id="ref-CR44">Riva G, Mantovani F, Capideville CS, Preziosa A, Morganti F, Villani D, Gaggioli A, Botella C, Alcañiz M (2007) Affective interactions using virtual reality: the link between presence and emotions. Cyberpsychol Behav 10:45–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fcpb.2006.9993" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affective%20interactions%20using%20virtual%20reality%3A%20the%20link%20between%20presence%20and%20emotions&amp;journal=Cyberpsychol%20Behav&amp;volume=10&amp;pages=45-56&amp;publication_year=2007&amp;author=Riva%2CG&amp;author=Mantovani%2CF&amp;author=Capideville%2CCS&amp;author=Preziosa%2CA&amp;author=Morganti%2CF&amp;author=Villani%2CD&amp;author=Gaggioli%2CA&amp;author=Botella%2CC&amp;author=Alca%C3%B1iz%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Rus-Calafell, J. Guitérrez-Maldonado, J. Ribas-Sabaté, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Rus-Calafell M, Guitérrez-Maldonado J, Ribas-Sabaté J (2012) Improving social behaviour in schizophrenia patie" /><p class="c-article-references__text" id="ref-CR45">Rus-Calafell M, Guitérrez-Maldonado J, Ribas-Sabaté J (2012) Improving social behaviour in schizophrenia patients using an integrated virtual reality programme: a case study. Stud Health Technol Inform 181:283–286</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Improving%20social%20behaviour%20in%20schizophrenia%20patients%20using%20an%20integrated%20virtual%20reality%20programme%3A%20a%20case%20study&amp;journal=Stud%20Health%20Technol%20Inform&amp;volume=181&amp;pages=283-286&amp;publication_year=2012&amp;author=Rus-Calafell%2CM&amp;author=Guit%C3%A9rrez-Maldonado%2CJ&amp;author=Ribas-Sabat%C3%A9%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JA. Russell, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Russell JA (1994) Is the universal recognition of emotion from facial expression? A review of the cross-cultur" /><p class="c-article-references__text" id="ref-CR46">Russell JA (1994) Is the universal recognition of emotion from facial expression? A review of the cross-cultural studies. Psychol Bull 115:102–141</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-2909.115.1.102" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20the%20universal%20recognition%20of%20emotion%20from%20facial%20expression%3F%20A%20review%20of%20the%20cross-cultural%20studies&amp;journal=Psychol%20Bull&amp;volume=115&amp;pages=102-141&amp;publication_year=1994&amp;author=Russell%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JA. Russell, JA. Bachorowski, JM. Fernandez-Dols, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Russell JA, Bachorowski JA, Fernandez-Dols JM (2003) Facial and vocal expression of emotion. Annu Rev Psychol " /><p class="c-article-references__text" id="ref-CR47">Russell JA, Bachorowski JA, Fernandez-Dols JM (2003) Facial and vocal expression of emotion. Annu Rev Psychol 54:329–349</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1146%2Fannurev.psych.54.101601.145102" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20and%20vocal%20expression%20of%20emotion&amp;journal=Annu%20Rev%20Psychol&amp;volume=54&amp;pages=329-349&amp;publication_year=2003&amp;author=Russell%2CJA&amp;author=Bachorowski%2CJA&amp;author=Fernandez-Dols%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Sachs, D. Steger-Wuchse, I. Kryspin-Exner, RC. Gur, H. Katschnig, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Sachs G, Steger-Wuchse D, Kryspin-Exner I, Gur RC, Katschnig H (2004) Facial recognition deficits and cognitio" /><p class="c-article-references__text" id="ref-CR48">Sachs G, Steger-Wuchse D, Kryspin-Exner I, Gur RC, Katschnig H (2004) Facial recognition deficits and cognition in schizophrenia. Schizophr Res 68:27–35</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0920-9964%2803%2900131-2" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20recognition%20deficits%20and%20cognition%20in%20schizophrenia&amp;journal=Schizophr%20Res&amp;volume=68&amp;pages=27-35&amp;publication_year=2004&amp;author=Sachs%2CG&amp;author=Steger-Wuchse%2CD&amp;author=Kryspin-Exner%2CI&amp;author=Gur%2CRC&amp;author=Katschnig%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Sandín, R. Chorot, L. Lostao, T. Joiner, M. Santed, S. Valiente, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Sandín B, Chorot R, Lostao L, Joiner T, Santed M, Valiente S (1999) Escalas PANAS de afecto positivo y negativ" /><p class="c-article-references__text" id="ref-CR49">Sandín B, Chorot R, Lostao L, Joiner T, Santed M, Valiente S (1999) Escalas PANAS de afecto positivo y negativo: validación factorial y convergencia transcultural. Psicothema 11:37–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Escalas%20PANAS%20de%20afecto%20positivo%20y%20negativo%3A%20validaci%C3%B3n%20factorial%20y%20convergencia%20transcultural&amp;journal=Psicothema&amp;volume=11&amp;pages=37-51&amp;publication_year=1999&amp;author=Sand%C3%ADn%2CB&amp;author=Chorot%2CR&amp;author=Lostao%2CL&amp;author=Joiner%2CT&amp;author=Santed%2CM&amp;author=Valiente%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Sato, S. Yoshikawa, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Sato W, Yoshikawa S (2007) Enhanced experienced of emotional arousal in response to dynamic facial expressions" /><p class="c-article-references__text" id="ref-CR50">Sato W, Yoshikawa S (2007) Enhanced experienced of emotional arousal in response to dynamic facial expressions. J Nonverbal Behav 31:119–135</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10919-007-0025-7" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Enhanced%20experienced%20of%20emotional%20arousal%20in%20response%20to%20dynamic%20facial%20expressions&amp;journal=J%20Nonverbal%20Behav&amp;volume=31&amp;pages=119-135&amp;publication_year=2007&amp;author=Sato%2CW&amp;author=Yoshikawa%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KR. Scherer, H. Ellgring, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Scherer KR, Ellgring H (2007) Are facial expressions of emotion produced by categorical affect programs or dyn" /><p class="c-article-references__text" id="ref-CR51">Scherer KR, Ellgring H (2007) Are facial expressions of emotion produced by categorical affect programs or dynamically driven by appraisal? Emotion 7:113–130</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F1528-3542.7.1.113" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Are%20facial%20expressions%20of%20emotion%20produced%20by%20categorical%20affect%20programs%20or%20dynamically%20driven%20by%20appraisal%3F&amp;journal=Emotion&amp;volume=7&amp;pages=113-130&amp;publication_year=2007&amp;author=Scherer%2CKR&amp;author=Ellgring%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Spencer-Smith, H. Wild, AH. Innes-Ker, J. Townsend, C. Duffy, C. Edwards, K. Ervin, N. Merritt, PW. Paik, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Spencer-Smith J, Wild H, Innes-Ker AH, Townsend J, Duffy C, Edwards C, Ervin K, Merritt N, Paik PW (2001) Maki" /><p class="c-article-references__text" id="ref-CR52">Spencer-Smith J, Wild H, Innes-Ker AH, Townsend J, Duffy C, Edwards C, Ervin K, Merritt N, Paik PW (2001) Making faces: creating three-dimensional parameterized models of facial expression. Behav Res Methods Instrum 33:115–123</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03195356" aria-label="View reference 52">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 52 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Making%20faces%3A%20creating%20three-dimensional%20parameterized%20models%20of%20facial%20expression&amp;journal=Behav%20Res%20Methods%20Instrum&amp;volume=33&amp;pages=115-123&amp;publication_year=2001&amp;author=Spencer-Smith%2CJ&amp;author=Wild%2CH&amp;author=Innes-Ker%2CAH&amp;author=Townsend%2CJ&amp;author=Duffy%2CC&amp;author=Edwards%2CC&amp;author=Ervin%2CK&amp;author=Merritt%2CN&amp;author=Paik%2CPW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Tomasello, M. Carpenter, J. Call, T. Behne, H. Moll, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Tomasello M, Carpenter M, Call J, Behne T, Moll H (2005) Understanding and sharing intentions: the origins of " /><p class="c-article-references__text" id="ref-CR53">Tomasello M, Carpenter M, Call J, Behne T, Moll H (2005) Understanding and sharing intentions: the origins of cultural cognition. Behav Brain Sci 28:675–735</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20and%20sharing%20intentions%3A%20the%20origins%20of%20cultural%20cognition&amp;journal=Behav%20Brain%20Sci&amp;volume=28&amp;pages=675-735&amp;publication_year=2005&amp;author=Tomasello%2CM&amp;author=Carpenter%2CM&amp;author=Call%2CJ&amp;author=Behne%2CT&amp;author=Moll%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vilagrasa S, Susin A (2009) FACe! Facial animation system based on FACS. In: IV Iberoamerican symposium in com" /><p class="c-article-references__text" id="ref-CR54">Vilagrasa S, Susin A (2009) FACe! Facial animation system based on FACS. In: IV Iberoamerican symposium in computers graphics, Isla Margarita, Venezuela</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Vogeley, G. Bente, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Vogeley K, Bente G (2010) “Artificial humans”: psychology and neuroscience perspective on embodiment and nonve" /><p class="c-article-references__text" id="ref-CR55">Vogeley K, Bente G (2010) “Artificial humans”: psychology and neuroscience perspective on embodiment and nonverbal communication. Neural Netw 23:1077–1090</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neunet.2010.06.003" aria-label="View reference 55">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%9CArtificial%20humans%E2%80%9D%3A%20psychology%20and%20neuroscience%20perspective%20on%20embodiment%20and%20nonverbal%20communication&amp;journal=Neural%20Netw&amp;volume=23&amp;pages=1077-1090&amp;publication_year=2010&amp;author=Vogeley%2CK&amp;author=Bente%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Watson, LA. Clark, A. Tellegen, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Watson C, Clark LA, Tellegen A (1988) Development and validation of brief measures of positive and negative af" /><p class="c-article-references__text" id="ref-CR56">Watson C, Clark LA, Tellegen A (1988) Development and validation of brief measures of positive and negative affect: the PANAS scales. J Pers Soc Psychol 54:1063–1070</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0022-3514.54.6.1063" aria-label="View reference 56">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20and%20validation%20of%20brief%20measures%20of%20positive%20and%20negative%20affect%3A%20the%20PANAS%20scales&amp;journal=J%20Pers%20Soc%20Psychol&amp;volume=54&amp;pages=1063-1070&amp;publication_year=1988&amp;author=Watson%2CC&amp;author=Clark%2CLA&amp;author=Tellegen%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yee N, Bailenson JN, Rickertsen K (2007) A meta-analysis of the impact of the inclusion and realism of human-l" /><p class="c-article-references__text" id="ref-CR57">Yee N, Bailenson JN, Rickertsen K (2007) A meta-analysis of the impact of the inclusion and realism of human-like faces on user experiences in interfaces. In: Proceedings of the conference on computer-human interaction (CHI). California, USA</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0236-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We are grateful to the University of Pennsylvania’s Brain Behaviour Laboratory for allowing us to use their facial emotional stimuli (Gur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Gur RC, Sara R, Hagendoorn M, Marom O, Hughett P, Macy L, Turner T, Bajcsy R, Posner A, Gur RE (2002a) A method for obtaining 3-dimensional facial expressions and its standardization for use in neurocognitive studies. J Neurosci Methods 115:137–143" href="/article/10.1007/s10055-013-0236-7#ref-CR22" id="ref-link-section-d1415e2359">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Gur RC, Schroeder L, Turner T, McGrath C, Chan RM, Turetsky BI, Alsop D, Maldjian J, Gur RE (2002b) Brain activation during facial emotion processing. Neuroimage 16:651–662" href="/article/10.1007/s10055-013-0236-7#ref-CR23" id="ref-link-section-d1415e2362">b</a>; Kohler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kohler CG, Turner TH, Bilker WB, Brensinger CM, Siegel SJ, Kanes SJ, Gur RE, Gur RC (2003) Facial emotion recognition in schizophrenia: intensity effects and error pattern. Am J Psychiatry 160:1768–1774" href="/article/10.1007/s10055-013-0236-7#ref-CR29" id="ref-link-section-d1415e2365">2003</a>). We wish to acknowledge the Computer Scientists Nicolás Toledan and Víctor Sánchez of the VR-PSY Lab for their invaluable work. This study was supported by the Institute for the Brain, Cognition, and Behaviour (IR3C), University of Barcelona, and by a research grant awarded to Mar Rus-Calafell by the government of Catalonia’s Agency for the Management of University and Research Grants (AGAUR) (FI-DGR/2010).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Personality, Assessment and Psychological Treatments, University of Barcelona, Passeig de la Vall d’Hebrón, 171, 08035, Barcelona, Spain</p><p class="c-article-author-affiliation__authors-list">José Gutiérrez-Maldonado, Mar Rus-Calafell &amp; Joan González-Conde</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jos_-Guti_rrez_Maldonado"><span class="c-article-authors-search__title u-h3 js-search-name">José Gutiérrez-Maldonado</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jos%C3%A9+Guti%C3%A9rrez-Maldonado&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jos%C3%A9+Guti%C3%A9rrez-Maldonado" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jos%C3%A9+Guti%C3%A9rrez-Maldonado%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mar-Rus_Calafell"><span class="c-article-authors-search__title u-h3 js-search-name">Mar Rus-Calafell</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mar+Rus-Calafell&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mar+Rus-Calafell" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mar+Rus-Calafell%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Joan-Gonz_lez_Conde"><span class="c-article-authors-search__title u-h3 js-search-name">Joan González-Conde</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Joan+Gonz%C3%A1lez-Conde&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Joan+Gonz%C3%A1lez-Conde" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Joan+Gonz%C3%A1lez-Conde%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0236-7/email/correspondent/c1/new">Mar Rus-Calafell</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Creation%20of%20a%20new%20set%20of%20dynamic%20virtual%20reality%20faces%20for%20the%20assessment%20and%20training%20of%20facial%20emotion%20recognition%20ability&amp;author=Jos%C3%A9%20Guti%C3%A9rrez-Maldonado%20et%20al&amp;contentID=10.1007%2Fs10055-013-0236-7&amp;publication=1359-4338&amp;publicationDate=2013-10-22&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Gutiérrez-Maldonado, J., Rus-Calafell, M. &amp; González-Conde, J. Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability.
                    <i>Virtual Reality</i> <b>18, </b>61–71 (2014). https://doi.org/10.1007/s10055-013-0236-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0236-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-03-21">21 March 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-10-09">09 October 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-10-22">22 October 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-03">March 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0236-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0236-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Emotion recognition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual agents</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Dynamism</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Social skills</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cyberintervention</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0236-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=236;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

