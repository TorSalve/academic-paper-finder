<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A brush device with visual and haptic feedback for virtual painting of"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We have previously developed a mixed reality (MR) painting system with which a user could take a physical object in the real world and apply virtual paint to it. However, this system could not..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A brush device with visual and haptic feedback for virtual painting of 3D virtual objects"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-06-06"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We have previously developed a mixed reality (MR) painting system with which a user could take a physical object in the real world and apply virtual paint to it. However, this system could not provide the sensation of painting on virtual objects in MR space. Therefore, we subsequently proposed and developed mechanisms that simulated the effect of touch and movement when a brush device was used to paint on a virtual canvas. In this paper, we use visual and haptic feedback to provide the sensation of painting on virtual three-dimensional objects using a new brush device called the MAI Painting Brush++. We evaluate and confirm its effectiveness through several user studies."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-06-06"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="167"/>

    <meta name="prism.endingPage" content="181"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0317-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0317-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0317-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0317-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A brush device with visual and haptic feedback for virtual painting of 3D virtual objects"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2018/06"/>

    <meta name="citation_online_date" content="2017/06/06"/>

    <meta name="citation_firstpage" content="167"/>

    <meta name="citation_lastpage" content="181"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0317-0"/>

    <meta name="DOI" content="10.1007/s10055-017-0317-0"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0317-0"/>

    <meta name="description" content="We have previously developed a mixed reality (MR) painting system with which a user could take a physical object in the real world and apply virtual paint "/>

    <meta name="dc.creator" content="Mai Otsuki"/>

    <meta name="dc.creator" content="Kenji Sugihara"/>

    <meta name="dc.creator" content="Azusa Toda"/>

    <meta name="dc.creator" content="Fumihisa Shibata"/>

    <meta name="dc.creator" content="Asako Kimura"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bau O, Poupyrev I, Le Goc M, Galliot L, Glisson M (2012) REVEL: tactile feedback technology for augmented reality. In: ACM SIGGRAPH 2012 emerging technologies (SIGGRAPH &#8216;12). ACM, New York, NY, USA, Article 17. doi: 
                    10.1145/2343456.2343473
                    
                  
                "/>

    <meta name="citation_reference" content="Baxter B, Scheib V, Lin MC, Manocha D (2001) DAB: interactive haptic painting with 3D virtual brushes. In: Proceedings of the 28th annual conference on Computer graphics and interactive techniques (SIGGRAPH &#8216;01). ACM, New York, NY, USA, pp 461&#8211;468. doi: 
                    10.1145/383259.383313
                    
                  
                "/>

    <meta name="citation_reference" content="Baxter W, Wendt J, Lin MC (2004) IMPaSTo: a realistic, interactive model for paint. In: Spencer SN (Ed.) Proceedings of the 3rd international symposium on non-photorealistic animation and rendering (NPAR &#8216;04). ACM, New York, NY, USA, pp 45&#8211;148. doi: 
                    10.1145/987657.987665
                    
                  
                "/>

    <meta name="citation_reference" content="Chu NS-H, Tai C-L (2005) MoXi: real-time ink dispersion in absorbent paper. In: J Buhler (Ed.) ACM SIGGRAPH 2005 sketches (SIGGRAPH &#8216;05). ACM, New York, NY, USA, Article 62. doi: 
                    10.1145/1187112.1187186
                    
                  
                "/>

    <meta name="citation_reference" content="Curtis CJ, Anderson SE, Seims JE, Fleischer KW, Salesin DH (1997) Computer-generated watercolor. In: Proceedings of the 24th annual conference on Computer graphics and interactive techniques (SIGGRAPH &#8216;97). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, pp 421&#8211;430. doi: 
                    10.1145/258734.258896
                    
                  
                "/>

    <meta name="citation_reference" content="Foskey M, Otaduy MA, Lin MC (2005) ArtNova: Touch-enabled 3D model design. In: J Fujii (Ed.) ACM SIGGRAPH 2005 courses (SIGGRAPH &#8216;05). ACM, New York, NY, USA, Article 188. doi: 
                    10.1145/1198555.1198619
                    
                  
                "/>

    <meta name="citation_reference" content="citation_title=Non-photorealistic rendering; citation_publication_date=2001; citation_id=CR7; citation_author=B Gooch; citation_author=A Gooch; citation_publisher=A. K. Peters Ltd."/>

    <meta name="citation_reference" content="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of the 2005 ACM SIGCHI international conference on advances in computer entertainment technology (ACE &#8216;05). ACM, New York, NY, USA, pp 213&#8211;216. doi: 
                    10.1145/1178477.1178510
                    
                  
                "/>

    <meta name="citation_reference" content="Kamuro S, Minamizawa K, Kawakami N, Tachi S (2009) Ungrounded kinesthetic pen for haptic interaction with virtual environments. In: Proceedings of robot and human interactive communication (RO-MAN 2009), pp 436&#8211;441, Sept. 27 2009&#8211;Oct. 2. doi: 
                    10.1109/ROMAN.2009.5326217
                    
                  
                "/>

    <meta name="citation_reference" content="Kamuro S, Minamizawa K, Tachi S (2011) 3D Haptic modeling system using ungrounded pen-shaped kinesthetic display. In: Proceedings of the 2011 IEEE virtual reality conference (VR &#8216;11). IEEE Computer Society, Washington, DC, USA, pp 217&#8211;218. doi: 
                    10.1109/VR.2011.5759476
                    
                  
                "/>

    <meta name="citation_reference" content="Kato G, Kuroda Y, Nisky I, Kiyokawa K, Takemura H (2015) HapSticks: tool-mediated interaction with grounding-free haptic interface. In: SIGGRAPH Asia 2015 haptic media and contents design (SA &#8216;15). ACM, New York, NY, USA, Article 7. doi: 
                    10.1145/2818384.2818387
                    
                  
                "/>

    <meta name="citation_reference" content="Kim H-J, Kim H, Chae S, Seo J, Han T-D (2013) AR pen and hand gestures: a new tool for pen drawings. In CHI &#8216;13 extended abstracts on human factors in computing systems (CHI EA &#8216;13). ACM, New York, NY, USA, pp 943&#8211;948. doi: 
                    10.1145/2468356.2468525
                    
                  
                "/>

    <meta name="citation_reference" content="Liu X, Gu J (2014) FlexStroke: a flexible, deformable brush-tip with dynamic stiffness for digital input. In: Proceedings of the 8th international conference on tangible, embedded and embodied interaction (TEI &#8216;14). ACM, New York, NY, USA, pp. 39&#8211;40. doi: 
                    10.1145/2540930.2540982
                    
                  
                "/>

    <meta name="citation_reference" content="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST &#8216;10). ACM, New York, NY, USA, pp 97&#8211;100. doi: 
                    10.1145/1866029.1866045
                    
                  
                "/>

    <meta name="citation_reference" content="Ryokai K, Marti S, Ishii H (2004) I/O brush: drawing with everyday objects as ink. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI &#8216;04). ACM, New York, NY, USA, pp 303&#8211;310. doi: 
                    10.1145/985692.985731
                    
                  
                "/>

    <meta name="citation_reference" content="Saito S, Nakajima M (1999) 3D physics-based brush model for painting. In: ACM SIGGRAPH 99 conference abstracts and applications (SIGGRAPH &#8216;99). ACM, New York, NY, USA, p 226. doi: 
                    10.1145/311625.312110
                    
                  
                "/>

    <meta name="citation_reference" content="Sandor C, Uchiyama S, Yamamoto H (2007) Visuo-haptic systems: half-mirrors considered harmful. In: Proceedings of the second joint EuroHaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems (WHC &#8216;07). IEEE Computer Society, Washington, DC, USA, pp 292&#8211;297. doi:
                    10.1109/WHC.2007.125
                    
                  
                "/>

    <meta name="citation_reference" content="Vandoren P, Claesen L, Van Laerhoven T, Taelman J, Raymaekers C, Flerackers E, Van Reeth F (2009) FluidPaint: an interactive digital painting system using real wet brushes. In: Proceedings of the ACM international conference on interactive tabletops and surfaces (ITS &#8216;09). ACM, New York, NY, USA, pp 53&#8211;56. doi: 
                    10.1145/1731903.1731914
                    
                  
                "/>

    <meta name="citation_reference" content="Yeom J, Lee G (2012) Designing a user interface for a painting application supporting real watercolor painting processes. In: Proceedings of the 10th Asia pacific conference on computer human interaction (APCHI &#8216;12). ACM, New York, NY, USA, pp 219&#8211;226. doi: 
                    10.1145/2350046.2350091
                    
                  
                "/>

    <meta name="citation_author" content="Mai Otsuki"/>

    <meta name="citation_author_email" content="otsuki@emp.tsukuba.ac.jp"/>

    <meta name="citation_author_institution" content="University of Tsukuba, Tsukuba, Japan"/>

    <meta name="citation_author" content="Kenji Sugihara"/>

    <meta name="citation_author_institution" content="Ritsumeikan University, Kusatsu, Japan"/>

    <meta name="citation_author" content="Azusa Toda"/>

    <meta name="citation_author_institution" content="Ritsumeikan University, Kusatsu, Japan"/>

    <meta name="citation_author" content="Fumihisa Shibata"/>

    <meta name="citation_author_email" content="fshibata@is.ritsumei.ac.jp"/>

    <meta name="citation_author_institution" content="Ritsumeikan University, Kusatsu, Japan"/>

    <meta name="citation_author" content="Asako Kimura"/>

    <meta name="citation_author_email" content="asa@rm.is.ritsumei.ac.jp"/>

    <meta name="citation_author_institution" content="Ritsumeikan University, Kusatsu, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0317-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0317-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A brush device with visual and haptic feedback for virtual painting of 3D virtual objects"/>
        <meta property="og:description" content="We have previously developed a mixed reality (MR) painting system with which a user could take a physical object in the real world and apply virtual paint to it. However, this system could not provide the sensation of painting on virtual objects in MR space. Therefore, we subsequently proposed and developed mechanisms that simulated the effect of touch and movement when a brush device was used to paint on a virtual canvas. In this paper, we use visual and haptic feedback to provide the sensation of painting on virtual three-dimensional objects using a new brush device called the MAI Painting Brush++. We evaluate and confirm its effectiveness through several user studies."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A brush device with visual and haptic feedback for virtual painting of 3D virtual objects | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0317-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Painting system, Mixed reality, Augmented reality, Input device, Paintbrush, Visual and haptic feedback","kwrd":["Painting_system","Mixed_reality","Augmented_reality","Input_device","Paintbrush","Visual_and_haptic_feedback"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0317-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0317-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=317;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0317-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A brush device with visual and haptic feedback for virtual painting of 3D virtual objects
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0317-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0317-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-06-06" itemprop="datePublished">06 June 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A brush device with visual and haptic feedback for virtual painting of 3D virtual objects</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mai-Otsuki" data-author-popup="auth-Mai-Otsuki" data-corresp-id="c1">Mai Otsuki<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tsukuba" /><meta itemprop="address" content="0000 0001 2369 4728, grid.20515.33, University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki, 305-8571, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kenji-Sugihara" data-author-popup="auth-Kenji-Sugihara">Kenji Sugihara</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ritsumeikan University" /><meta itemprop="address" content="0000 0000 8863 9909, grid.262576.2, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga, 525-8577, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Azusa-Toda" data-author-popup="auth-Azusa-Toda">Azusa Toda</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ritsumeikan University" /><meta itemprop="address" content="0000 0000 8863 9909, grid.262576.2, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga, 525-8577, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Fumihisa-Shibata" data-author-popup="auth-Fumihisa-Shibata">Fumihisa Shibata</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ritsumeikan University" /><meta itemprop="address" content="0000 0000 8863 9909, grid.262576.2, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga, 525-8577, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Asako-Kimura" data-author-popup="auth-Asako-Kimura">Asako Kimura</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ritsumeikan University" /><meta itemprop="address" content="0000 0000 8863 9909, grid.262576.2, Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga, 525-8577, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">167</span>–<span itemprop="pageEnd">181</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">596 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0317-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We have previously developed a mixed reality (MR) painting system with which a user could take a physical object in the real world and apply virtual paint to it. However, this system could not provide the sensation of painting on virtual objects in MR space. Therefore, we subsequently proposed and developed mechanisms that simulated the effect of touch and movement when a brush device was used to paint on a virtual canvas. In this paper, we use visual and haptic feedback to provide the sensation of painting on virtual three-dimensional objects using a new brush device called the MAI Painting Brush++. We evaluate and confirm its effectiveness through several user studies.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The field of computer graphics has progressed remarkably over recent decades. Numerous drawing and photo-retouching software packages have been developed as a result of research into non-photorealistic rendering (Gooch and Gooch <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Gooch B, Gooch A (2001) Non-photorealistic rendering. A. K. Peters Ltd., Natick" href="/article/10.1007/s10055-017-0317-0#ref-CR7" id="ref-link-section-d59260e397">2001</a>). Quality digital painting software now allows users to create realistic watercolor and oil paintings and is widely used by professional artists and designers as well as by amateurs.</p><p>Several studies have focused on simulating traditional graphical styles (e.g., oil and watercolor painting) with a graphics tablet (Curtis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Curtis CJ, Anderson SE, Seims JE, Fleischer KW, Salesin DH (1997) Computer-generated watercolor. In: Proceedings of the 24th annual conference on Computer graphics and interactive techniques (SIGGRAPH ‘97). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, pp 421–430. doi: &#xA;                    10.1145/258734.258896&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR5" id="ref-link-section-d59260e403">1997</a>; Chu and Tai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Chu NS-H, Tai C-L (2005) MoXi: real-time ink dispersion in absorbent paper. In: J Buhler (Ed.) ACM SIGGRAPH 2005 sketches (SIGGRAPH ‘05). ACM, New York, NY, USA, Article 62. doi: &#xA;                    10.1145/1187112.1187186&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR4" id="ref-link-section-d59260e406">2005</a>; Baxter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baxter W, Wendt J, Lin MC (2004) IMPaSTo: a realistic, interactive model for paint. In: Spencer SN (Ed.) Proceedings of the 3rd international symposium on non-photorealistic animation and rendering (NPAR ‘04). ACM, New York, NY, USA, pp 45–148. doi: &#xA;                    10.1145/987657.987665&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR3" id="ref-link-section-d59260e409">2004</a>; Saito and Nakajima <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Saito S, Nakajima M (1999) 3D physics-based brush model for painting. In: ACM SIGGRAPH 99 conference abstracts and applications (SIGGRAPH ‘99). ACM, New York, NY, USA, p 226. doi: &#xA;                    10.1145/311625.312110&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR16" id="ref-link-section-d59260e412">1999</a>), and the quality of such simulations is constantly improving. In most digital painting applications, a graphics tablet and a two-dimensional (2D) display are used as the input and output devices, respectively. However, some users prefer direct manipulation that allows them to see the movements of the brush tip as opposed to indirect manipulation of a graphics tablet. Furthermore, the tip of a graphics tablet pen is relatively rigid and can only move by 1–2 mm at most. In contrast, the tip of an actual paintbrush is far more flexible and can move dynamically. To bridge this gap, a brush-shaped device whose touch sensation is similar to that of an actual paintbrush has been commercialized (e.g., sensuBrush<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>). The users of this device can see the brush tip bend and can feel the reaction and friction between the brush and the painting surface on a tablet computer.</p><p>However, actual painting is done not only on flat canvases but also on three-dimensional (3D) objects (e.g., cups, bowls, vases) that can be held, moved, and rotated while being painted. Based on these requirements, we developed a mixed reality (MR) painting system based on a paintbrush device called the MR-based Artistic Interactive (MAI) Painting Brush (MAI Painting Expert Ver. 1.0) (Otsuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST ‘10). ACM, New York, NY, USA, pp 97–100. doi: &#xA;                    10.1145/1866029.1866045&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR14" id="ref-link-section-d59260e431">2010</a>; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig1">1</a>). In this system, users apply virtual paint directly onto real 3D objects using the MAI Painting Brush, observing it from various directions and perspectives. Here, MR is a technology that merges real and virtual worlds in real time so that both real and virtual objects can be manipulated simultaneously.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Using MAI Painting Expert 1.0 (Otsuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST ‘10). ACM, New York, NY, USA, pp 97–100. doi: &#xA;                    10.1145/1866029.1866045&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR14" id="ref-link-section-d59260e447">2010</a>), a user paints with virtual ink directly on real objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The next logical step is to make the painting target also virtual, in which case haptic feedback such as reaction or/and frictional forces no longer arises naturally. In order to facilitate virtual painting onto virtual objects, the creation of visual and haptic feedback is indispensable. Hence, we have constructed haptic feedback mechanisms that provide physical sensations from virtual objects to simulate painting by brush onto real ones, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Concept sketch of a user using the proposed new paintbrush device to paint with virtual ink directly on virtual objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Previous research has aimed at virtual painting with the sensation of actual painting by using brush-like devices such as I/O Brush (Ryokai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ryokai K, Marti S, Ishii H (2004) I/O brush: drawing with everyday objects as ink. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI ‘04). ACM, New York, NY, USA, pp 303–310. doi: &#xA;                    10.1145/985692.985731&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR15" id="ref-link-section-d59260e487">2004</a>), FluidPaint (Vandoren et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Vandoren P, Claesen L, Van Laerhoven T, Taelman J, Raymaekers C, Flerackers E, Van Reeth F (2009) FluidPaint: an interactive digital painting system using real wet brushes. In: Proceedings of the ACM international conference on interactive tabletops and surfaces (ITS ‘09). ACM, New York, NY, USA, pp 53–56. doi: &#xA;                    10.1145/1731903.1731914&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR18" id="ref-link-section-d59260e490">2009</a>), ThermoPainter (Iwai and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of the 2005 ACM SIGCHI international conference on advances in computer entertainment technology (ACE ‘05). ACM, New York, NY, USA, pp 213–216. doi: &#xA;                    10.1145/1178477.1178510&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR8" id="ref-link-section-d59260e493">2005</a>), Yeom’s system (Yeom and Lee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Yeom J, Lee G (2012) Designing a user interface for a painting application supporting real watercolor painting processes. In: Proceedings of the 10th Asia pacific conference on computer human interaction (APCHI ‘12). ACM, New York, NY, USA, pp 219–226. doi: &#xA;                    10.1145/2350046.2350091&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR19" id="ref-link-section-d59260e496">2012</a>), Kim’s system (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kim H-J, Kim H, Chae S, Seo J, Han T-D (2013) AR pen and hand gestures: a new tool for pen drawings. In CHI ‘13 extended abstracts on human factors in computing systems (CHI EA ‘13). ACM, New York, NY, USA, pp 943–948. doi: &#xA;                    10.1145/2468356.2468525&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR12" id="ref-link-section-d59260e499">2013</a>), and FlexStroke (Liu and Gu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Liu X, Gu J (2014) FlexStroke: a flexible, deformable brush-tip with dynamic stiffness for digital input. In: Proceedings of the 8th international conference on tangible, embedded and embodied interaction (TEI ‘14). ACM, New York, NY, USA, pp. 39–40. doi: &#xA;                    10.1145/2540930.2540982&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR13" id="ref-link-section-d59260e503">2014</a>). However, since the painting targets of all these studies were actual 2D displays, they could make use of real haptic feedback from real objects. In terms of virtual haptic feedback, REVEL tactile feedback technology (Bau et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Bau O, Poupyrev I, Le Goc M, Galliot L, Glisson M (2012) REVEL: tactile feedback technology for augmented reality. In: ACM SIGGRAPH 2012 emerging technologies (SIGGRAPH ‘12). ACM, New York, NY, USA, Article 17. doi: &#xA;                    10.1145/2343456.2343473&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR1" id="ref-link-section-d59260e506">2012</a>) is a way to modify the tactile sensation of a real object by using a virtual texture that is created from an electronic signal. There was also the series of PHANTOM which is a grounded haptic device-related studies to create sensations for virtual painting on 2D/3D objects: (Baxter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Baxter B, Scheib V, Lin MC, Manocha D (2001) DAB: interactive haptic painting with 3D virtual brushes. In: Proceedings of the 28th annual conference on Computer graphics and interactive techniques (SIGGRAPH ‘01). ACM, New York, NY, USA, pp 461–468. doi: &#xA;                    10.1145/383259.383313&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR2" id="ref-link-section-d59260e509">2001</a>), ArtNova (Foskey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Foskey M, Otaduy MA, Lin MC (2005) ArtNova: Touch-enabled 3D model design. In: J Fujii (Ed.) ACM SIGGRAPH 2005 courses (SIGGRAPH ‘05). ACM, New York, NY, USA, Article 188. doi: &#xA;                    10.1145/1198555.1198619&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR6" id="ref-link-section-d59260e512">2005</a>), and Visuo-Haptic Systems (Sandor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sandor C, Uchiyama S, Yamamoto H (2007) Visuo-haptic systems: half-mirrors considered harmful. In: Proceedings of the second joint EuroHaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems (WHC ‘07). IEEE Computer Society, Washington, DC, USA, pp 292–297. doi:&#xA;                    10.1109/WHC.2007.125&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR17" id="ref-link-section-d59260e515">2007</a>). However, the PHANTOM system limits the user’s movements to within the range of the mechanical linkages. Pen De Touch (Kamuro et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kamuro S, Minamizawa K, Kawakami N, Tachi S (2009) Ungrounded kinesthetic pen for haptic interaction with virtual environments. In: Proceedings of robot and human interactive communication (RO-MAN 2009), pp 436–441, Sept. 27 2009–Oct. 2. doi: &#xA;                    10.1109/ROMAN.2009.5326217&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR9" id="ref-link-section-d59260e518">2009</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Kamuro S, Minamizawa K, Tachi S (2011) 3D Haptic modeling system using ungrounded pen-shaped kinesthetic display. In: Proceedings of the 2011 IEEE virtual reality conference (VR ‘11). IEEE Computer Society, Washington, DC, USA, pp 217–218. doi: &#xA;                    10.1109/VR.2011.5759476&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR10" id="ref-link-section-d59260e522">2011</a>) and HapSticks (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Kato G, Kuroda Y, Nisky I, Kiyokawa K, Takemura H (2015) HapSticks: tool-mediated interaction with grounding-free haptic interface. In: SIGGRAPH Asia 2015 haptic media and contents design (SA ‘15). ACM, New York, NY, USA, Article 7. doi: &#xA;                    10.1145/2818384.2818387&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR11" id="ref-link-section-d59260e525">2015</a>) proposed haptic devices that do not restrict user movement. However, these devices were not developed for brush painting. Users could just touch the virtual objects by using these haptic devices, but they did not provide the painting sensation neither the difference of material, reaction force during stroking, nor the spring of the brush.</p><p>The aim of the present study is to analyze the sensation of real painting and imitate it on virtual objects without restricting user movement, so that they can paint freely onto virtual objects of various sizes and shapes with the painting sensation beyond the touch. The remainder of this paper is organized as follows. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec2">2</a>, we analyze the sensation of real painting and develop the models. Next, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec11">3</a>, we describe the implementation of this system. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec17">4</a>, we investigate whether the proposed method is actually effective through three user studies. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec23">5</a>, we discuss the result of user studies and the limitations of our approach. Finally, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec24">6</a>, we summarize our results.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Visual and haptic feedback model</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Analysis of the sensation of painting using a paintbrush</h3><p>When painting with a brush, users can predict the stroke’s width and shape as well as the brush tip’s movements. In addition, users can obtain information such as the canvas texture or shape from the “sensation of painting,” i.e., from tip bending (deformation of the brush tip) and the reaction and frictional forces that act on the brush and the user’s hand and fingers.</p><p>To provide this sensation of painting using our device, we analyzed the act of painting in the real world, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig3">3</a>. In general, the painting operation can be categorized into the following two actions: pushing the brush tip onto the canvas and stroking the brush tip across the canvas. We assumed that while a user paints, he/she experiences the tactile sensation of painting via the pushing and stroking pressures arising from these two operations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Analysis of the sensation of painting</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig3">3</a>, when a user pushes the brush onto the canvas, he/she perceives the pushing sensation by varying (i-a) the amount of tip bending and (ii) the reaction forces from the canvas. When a user strokes the canvas with the brush, the frictional forces are generated in the opposite direction to the movement, and the user perceives this sensation by varying (i-b) the direction of tip bending and (iii) the direction of the force felt on the user’s hand. Furthermore, the frictional force changes depending on the canvas material and the dryness of the brush tip. In our system, therefore, a slight pause and dry-brush sensation are also provided to the user.</p><p>From this analysis, while painting, a paintbrush undergoes various changes, including (i) the bending of the brush tip (realized by the (i-a) amount and (i-b) direction of bending), (ii) the reaction force due to the canvas, and (iii) the frictional forces between the brush tip and the canvas. By installing mechanisms that simulate these three changes to our brush device, we can realize a virtual canvas. This mechanism is described in more detail in the following subsections.</p><p>Regarding (i) the deformation of the brush tip, this depends on two factors: the force with which it is pressed onto the canvas and the direction in which it is moved. The amount of tip bending is proportional to the pressure applied to the canvas, and the tip bending direction is the opposite direction to the painting (i.e., movement) one. Similarly, regarding (ii) and (iii), the greater the pressure with which the paintbrush is pressed onto the canvas, the greater the reaction and friction forces that the gripping fingers experience. Furthermore, the direction of the frictional force is opposite to the direction of the movement because of the friction force between the brush tip and canvas. Therefore, the force from the canvas to the brush tip and that from the brush to the user’s hand are generated by the same factors, i.e., the amount of pressure and the painting direction. We use “reaction force vector” here to identify this force. In the next subsection, we describe our model for calculating this reaction force vector that changes according to the amount of brush pressure applied to the canvas and the painting direction. We refer to this model as the “basic model.”</p><p>Additionally, other factors affect the painting sensations experienced by the user, such as the dryness and hardness of the brush tip and the type of canvas material. For example, the frictional force between the brush and the canvas changes depending on the dryness of the brush and affects the stroking sensations. To improve the overall experience of painting, we also improved our basic model by incorporating these factors that affect the “frictional force” between brush tip and canvas. We also considered the brush-tip elasticity; the user can feel the brush springing back after its tip is pressed onto the canvas in a downward motion in the case of harder brush tips. We named the model that incorporates these two factors the “extended model.”</p><h3 class="c-article__sub-heading" id="Sec4">Basic model</h3><p>In this subsection, we describe a model to calculate the reaction force vector <b>X</b> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig4">4</a>a), which changes according to the amount of brush pressure applied to the canvas and the painting direction. This model is the “basic model,” whereas an extended model is presented in the next subsection.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Parameters of the model representing visual and haptic feedback while painting. <b>a</b> Calculating the direction of reaction force vector <b>X</b> using previous frame. <b>b</b> Projecting reaction force vector <b>OO′</b> onto plane<i> S</i> perpendicular to the shaft of the brush device</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Calculating the direction of the reaction force vector</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig4">4</a>a shows the parameters used in our model. The positions of <b>O</b> and <b>P</b>
                        <sub>tip</sub> (the heel and toe of the brush tip, respectively) are calculated from <b>P</b>
                        <sub>sensor</sub> (the position of the magnetic sensor in the device), <i>l</i>
                        <sub>tip</sub> (the length of the bent brush tip), and <i>l</i>
                        <sub>handle</sub> (the length between <b>P</b>
                        <sub>sensor</sub> and <b>O</b>). When vector <b>OP</b>
                        <sub>tip</sub> intersects with a polygon of a virtual canvas, it is assumed that the device touches the polygon of a virtual object.</p><p>Focusing on the relationship between the painting direction and the tip direction, even when the painting direction is kept constant, the 3D direction of the brush will not remain exactly opposite to the painting direction when painting on a curved surface, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig5">5</a>a (This does not hold in case the painting direction is parallel to the canvas.). The tip direction consists of two vectors: the vectors perpendicular and parallel to the brush shaft. The former is affected by the painting direction and is typically in the opposite direction.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Painting on curved surface. <b>a</b> The painting direction and brush tip direction. <b>b</b> The painting direction and direction of the vectors perpendicular to the brush shaft</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Conversely, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig5">5</a>b, the latter is affected by the angle between the canvas and the shaft of the brush as well as the amount of pressure applied to the canvas surface. Thus, the 3D direction of the brush tip is not simply the opposite of the painting direction, but is instead based on the amount of tip bending, which changes depending on the amount of pressure applied. Therefore, for tip bending, we must consider the movement of the toe of the tip and calculate the position of it (<b>P</b>′<sub>tip</sub>). This is obtained from the model developed in our previous study (Otsuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST ‘10). ACM, New York, NY, USA, pp 97–100. doi: &#xA;                    10.1145/1866029.1866045&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR14" id="ref-link-section-d59260e736">2010</a>), which is further detailed in subsection <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec10">2.4</a>.</p><p>The direction of painting can be calculated from the direction of <b>OO</b>′, where <b>O</b>′ is the heel position of the tip in the previous process (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig4">4</a> (a)). First, we set plane <i>S</i> perpendicular to <b>OP</b>
                        <sub>sensor</sub>, the axis of the device. Next, <b>X</b> is the vector that projects <b>OO</b>′ onto <i>S</i>. Direction angle <i>θ</i> is formed by <b>X</b> and reference vector <b>V</b> on plane <i>S</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig4">4</a>b).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Calculating the magnitude of the reaction force vector</h4><p>If <b>OP</b>
                        <sub>tip</sub> (or <b>OP</b>
                        <sub>tip</sub>′ in the case of brush tip bending; see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec9">2.3.2</a>) intersects a polygon of the virtual canvas, intersection point <b>Q</b> (or <b>Q</b>′) and the length of <b>OQ</b> (or <b>OQ</b>′) can be calculated. The shorter the length of <b>OQ</b>, the greater and stronger the bend angle and reaction force on the user’s finger. Therefore, the magnitude of reaction force vector <i>w</i> is described by the following equation (with <i>α</i> being a variable that changes on the basis of the actuator of the device):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$w = \frac{{l_{\text{tip}} - |{\mathbf{OQ}}|}}{{l_{\text{tip}} }} \times \alpha$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
<p>The bend angle of the brush tip <i>ϕ</i> can be calculated by the following equation (with maximum bend angle <i>ϕ</i>
                        <sub>Max</sub>):</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\phi = \frac{w}{\alpha }\phi_{\text{Max}}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
<h3 class="c-article__sub-heading" id="Sec7">Extended model</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Frictional force</h4><p>In actual painting, we perceive the sensation of stroking a brush tip on a canvas; this sensation constantly changes depending on our actions. We can consider this stroking sensation as being the sensation when the brush tip repeatedly pauses in the minute bumps of the canvas. In actual painting, from this stroking sensation, the painter takes cues such as the dryness of the brush or the canvas material. By incorporating changes in this sensation in our system, we can improve the overall sensation of painting.</p><p>
                        <i>Factors contributing to the stroking sensation</i> The condition of the brush and canvas materials change the stroking sensation experienced by the painter. We analyzed actual painting and identified factors of frictional force. In our system, we realized the stroking sensation by using the change in frictional force due to the following four factors.</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>The speed of painting: Users more readily perceive changes in the frictional force when they move the brush slowly.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>The amount of pressure applied: When users paint a thin line, it is difficult to perceive the frictional force; however, when they paint a thick line, they more readily feel the frictional force, because the brush tip is pushed onto the canvas with greater pressure.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>The dryness of the brush tip: When the brush tip has adequate water, users are able to move the brush more smoothly and do not feel the frictional force (unlike when the brush tip is dry).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>The canvas material: When using a rough cloth as a canvas, users can feel the roughness with their hands through the brush. Conversely, when using a smooth ceramic canvas, users can move the brush more smoothly. When the brush tip has adequate water, it is difficult to perceive the difference between canvas materials; however, when the brush tip dries out, it becomes easier to notice the difference. Some materials absorb water quickly, such as cloth; as a result, the brush tip dries faster.</p>
                        
                      </li>
                    </ol>
<p>
                        <i>Modeling of the changes in frictional force</i> Frictional force <i>F</i> can be calculated by the following equation using friction coefficient <i>μ</i> and normal force <i>N</i>:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F = \mu \cdot N$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
<p>In our system, we assume the canvas surface and brush to have minor unevenness, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig6">6</a>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig7">7</a> shows the frictional force acting between the brush and the canvas. When the brush tip is caught between the small bumps of the canvas, the frictional force acts on the brush tip, and when the tip is released, the frictional force no longer acts on the brush. The positions and depths of the bumps of an actual canvas are random, and the friction coefficient barely changes. If <i>F</i> is the maximum frictional force for a given material, minimum frictional force <i>F</i>′ is calculated using the following equation:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F{\prime = }\left( {1.0 - \varepsilon } \right)\mu N\,\left( {0 \le \varepsilon \le 1} \right)$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Contact surface between canvas and brush tip</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Before and after releasing the brush tip from a paused state</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>Here, <i>ε</i> is the percentage of the size of non-contacting area between the brush and the canvas when the frictional force is minimum. Between the before and after stages of releasing the brush tip, a change in force occurs, causing a small vibration sensation. Further, when the frictional force increases, the change in force also increases, and therefore the magnitude of the vibration increases. We assume that the change between <i>F</i> and <i>F</i>′ is the change in the force’s magnitude and provide this sensation as a vibration.</p><p>In addition, the amount of water in the brush tip affects friction coefficient <i>μ</i>. Because the frictional force decreases when the brush tip has enough water, <i>μ</i> decreases. From this, we conclude that the frictional force is maximum when the brush tip is dry. This frictional force, which depends on the amount of water in the brush, can be calculated by multiplying weighted-function g(<i>v</i>) by <i>μ</i> when the frictional force is maximum.</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$g\left( v \right) = 1 - \left( {v/V_{\hbox{max} } } \right)$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
<p>Here, <i>v</i> is the current amount of water and <i>V</i>
                        <sub>max</sub> is the maximum amount of water.</p><p>When there is adequate amount of water, even though the frictional force is not 0, users perceive similar sensation regardless of the material. Thus, there exists minimum friction coefficient <i>α</i> that is common in every material. From this, we calculate friction coefficient <i>μ</i>′ depending on the amount of water in the brush tip as follows:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\mu {\prime } = \left( {\mu - \alpha } \right)g\left( v \right) + \alpha$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
<p>By substituting this into Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0317-0#Equ4">4</a>), we obtain friction force <i>F</i>″ reflecting the water amount as follows:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F{\prime \prime } = \left( {1.0 - \varepsilon } \right)\mu {\prime }N$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
<p>In actual painting, there is a coefficient of water absorption that depends on the canvas material. Although the coefficient of water absorption does not directly affect the frictional force, the speed at which the water in the brush tip is reduced affects the change in the frictional force during painting. By implementing the coefficient of water absorption, we can realize the change in frictional force that mimics actual painting.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Brush tip spring</h4><p>In actual painting, painters make use of a painting technique that utilizes the spring of the brush tip, in which the brush tip is bent with a downward movement of the brush, and then let to spring back free of the canvas. Using this technique, users can make dynamic strokes. From the view of haptic sensation, the user can feel when the brush springs back. Therefore, we further develop our model to include the sensation of springing back of the brush tip.</p><p>Brush tip springing occurs when the user bends a wet brush tip by applying additional force onto the canvas; while doing so, the user will feel a “jumping sensation” in their hands, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig8">8</a>. However, this sensation is not common with all kinds of brushes. It occurs quite easily with hard brush tips. There are various paintbrushes made from a wide variety of materials, and therefore have varying degrees of elasticity.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Brush tip bending and springing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In general, when the brush tip bends, an elastic force tries to recover the brush tip to its original shape; this force is based on the hardness of the brush tip and the amount of tip bending. As the user makes a downward movement, the brush spring occurs from this force. More specifically, the brush spring is controlled by elastic force <i>F</i>
                        <sub>e</sub>, which can be calculated by the brush tip’s hardness <i>k</i> and the angle of bending <i>θ</i>.</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F_{\text{e}} = k\theta$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
<p>However, the brush tip does not jump until it bends beyond a certain threshold, because the frictional force also acts on the brush tip. Therefore, when the tip is bending, frictional force <i>F</i>
                        <sub>f</sub> is equal to <i>F</i>
                        <sub>e</sub>, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig9">9</a>. More specifically, we have the following:</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} F_{\text{f}} &amp;= F_{\text{e}} \hfill \\ F_{\text{f}}&amp; = \mu_{\text{s}} N \hfill \\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Frictional force and elastic force when the brush tip is bending</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Here, <i>μ</i>
                        <sub>s</sub> is a static friction coefficient. By bending the brush tip with increasing force, the elastic force becomes larger than the frictional force, which in turn causes the brush tip to spring off the canvas. We show this relationship as</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F_{\text{f}} \le F_{\text{e}}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div>
<p>After the brush tip springs back, the brush tip slides on the canvas to recover its original shape, and the amount of bending decreases. With the decrease in the amount of bending, the frictional and elastic forces appear to become equal again; however, the brush tip does not stop and instead jumps. Because the brush tip slides on the canvas, the frictional force acting on the tip switches from static friction to kinetic friction, and rapidly decreases. The conditions for which the brush spring will occur are as follows:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} F_{\text{f}} {\prime } &amp;\le F_{\text{e}} \hfill \\ F_{\text{f}} {\prime } &amp;= \mu_{\text{k}} N \hfill \\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div>
<p>Here, <i>F</i>
                        <sub>f</sub>′ is the frictional force while the brush tip is sliding and <i>μ</i>
                        <sub>k</sub> is the kinetic friction coefficient.</p><p>From the above, when the brush tip spring occurs, elastic force <i>F</i>
                        <sub>e</sub> acts on the brush tip and the user perceives the force feedback in the direction of the brush tip spring—i.e., the jumping sensation is generated by this elastic force. By providing this force feedback depending on <i>F</i>
                        <sub>e</sub>, we can implement the jumping sensation caused by the brush spring.</p><h3 class="c-article__sub-heading" id="Sec10">Painting model</h3><p>The painting process is the same as our MAI Painting Expert Ver. 1.0 (Otsuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST ‘10). ACM, New York, NY, USA, pp 97–100. doi: &#xA;                    10.1145/1866029.1866045&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR14" id="ref-link-section-d59260e1808">2010</a>). As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig10">10</a>, the system generates brush strokes by placing footprints (i.e., marks made by touching a brush to a canvas) continuously along the trajectory of the brush device movements. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig11">11</a> shows the basic shape of each footprint. Below is the algorithm for Generating brush strokes:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>MAI Painting Brush++. <b>a</b> Outer appearance. <b>b</b> Inner mechanisms. <b>c</b> Size and position of center of gravity. <b>d</b> Movements to provide feedback</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Generating brush strokes by placing footprints along the trajectory of the brush device</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Modify the size, shape, and direction of the basic footprint shape from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig11">11</a>, depending on inputs from the brush device.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Calculate a painting point based on the position and orientation of the brush device, as well as the amount and direction of tip bending. A painting point is a point of intersection of the brush tip (segment) and the canvas (polygon).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Render the footprint at the painting point.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Go back to step 1.</p>
                      
                    </li>
                  </ol>
<p>Regarding Step 1, from our previous study, the amount and direction of tip bending can be detected by the analog stick controller in the shaft of the device. In this study, tip bending cannot be detected from the sensor because the canvas is virtual object, and it does not bend the tip. Therefore, it is estimated from the reaction force vector that acts on the brush tip. Therefore, we apply the direction and magnitude of the vector to this painting model—i.e., the direction of vector θ is used as the direction of the brush tip, and magnitude <i>w</i> is used to calculate the amount of brush tip bending <i>ϕ</i> (see Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0317-0#Equ2">2</a>)).</p><p>Regarding step 2, to render a footprint, the system needs to calculate the painting point, which is the center of the footprint on the canvas. Using rotation matrix <b>R</b> given by <i>ϕ</i>, we can calculate <b>P</b>′<sub>tip</sub> and detect a collision point of <b>OP</b>′<sub>tip</sub> and a polygon. This collision point <b>Q</b>′ is the painting point.</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\mathbf{P}}_{{\text{tip}}}^{\prime} = {\mathbf{OP}}_{\text{tip}} *{\mathbf{R}} + {\mathbf{O}}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div>
</div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Implementation</h2><div class="c-article-section__content" id="Sec11-content"><h3 class="c-article__sub-heading" id="Sec12">MAI Painting Brush++</h3><p>For this study, we designed and implemented the MAI Painting Brush++ in which the mechanisms to realize factors (i)–(iii) described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec3">2.1</a> are included. These mechanisms imitate visual and haptic changes. To realize (i), the mechanism simulates the bending of the brush tip for visual feedback. To realize (ii) and (iii), the mechanism simulates both reaction and frictional forces for haptic feedback.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig12">12</a>a–c shows the outer appearance of the brush, the inner mechanisms, and the sizes and position of the device’s center of gravity, respectively. The weight (not including the cable) is approximately 170 g. Although it is still heavier than typical painting brush, by making the cable to be along by user’s hands, the user’s load was reduced.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Basic shape of a footprint</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The mechanisms are controlled by three wires that are simultaneously linked to one motor each. We installed a DC motor (Maxon, RE10, 1.5 W, gear ratio 4.7:1) as an actuator that controls the driving distance and the high resolution of the strength and direction of the reaction force. We adopted a cylindrical flap with which the user can sense the reaction force without limiting the way in which the device should be held. By actuating the motor, cylindrical flap that is the body of the device is tilted, thus providing the reaction force to a user’s index fingers and thumb, and the direction of the tip of the MAI Painting Brush++ changes and provides the visual feedback based on the direction of the painting, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig12">12</a>d.</p><p>The amount of actuating a motor <i>d</i>
                    <sub>
                    <i>n</i>
                  </sub> (<i>n</i> = motor id) is calculated by the following equation:</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$d_{n} = \cos \left( {\theta - 2\left( {n - 1} \right)\pi /N} \right)w$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div>
<p>Here, θ is the direction angle, <i>N</i> is the total number of the actuator which are installed to brush device, and <i>w</i> is the magnitude of reaction force vector.</p><h3 class="c-article__sub-heading" id="Sec13">Applying the extended model (frictional force) to MAI Painting Brush++</h3><p>In the case of applying the extended model (frictional force) as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec8">2.3.1</a>, the frictional force barely changes and is similar to the sensation of a vibration. When the pressure on the canvas increases, the change in the frictional force also increases, therefore increasing the magnitude of vibrations. Based on this concept, we changed the magnitude of vibrations of the cylindrical flap depending on the movement of the device. The tilt of the cylindrical flap is controlled by the basic model. At the same time, by using the frictional force model, the system vibrates the device based on the tilt of the cylindrical flap.</p><p>For controlling the vibrations, we consider the amplitude and frequency of the vibrations. The amplitude can be calculated by Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0317-0#Equ4">4</a>), which is based on the amount of tip bending and the canvas material. The frequency can be realized based on the model explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec8">2.3.1</a>. Specifically, the frictional force changes when the brush moves on the canvas because the canvas absorbs the water of the brush tip and the amount of water reduces. Based on this, we change the frequency of the vibrations depending on the length of painting. In our system, the user can re-wet the brush tip using water cup and refill the color by color palette (See Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec24">6</a>).</p><h3 class="c-article__sub-heading" id="Sec14">Applying the extended model (brush tip spring) to MAI Painting Brush++</h3><p>In addition to the extended model, we applied the brush tip spring model. As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec9">2.3.2</a>, the time at which the brush springs back is controlled by the relationship between elastic force <i>F</i>
                    <sub>e</sub> (given by the brush tip’s hardness <i>k</i> and the amount of bending <i>θ</i>) and friction force <i>F</i>
                    <sub>f</sub> (given by the friction coefficient of the material <i>μ</i>, a normal force <i>N</i> and weighted-function g(<i>v</i>) depending on the dryness of the brush tip).</p><p>In addition, when the brush tip springs back, it is necessary to provide a reaction force in the same direction as the brush tip spring. In our device, to provide this sensation, the system quickly moves a cylindrical flap from the tilted state (i.e., the brush tip is bent) to the default state of the brush. The larger the tilt angle of the cylindrical flap when the brush tip spring begins, the larger the force from the cylindrical flap. Therefore, the more the brush tip bends, the stronger the reaction force.</p><h3 class="c-article__sub-heading" id="Sec15">System configuration</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig13">13</a> shows our system configuration. We used a Windows XP PC with an Intel Core i7 Ext 965 CPU and 6144 MB of RAM to manage and display the MR space. Users watched the MR space and painting results through a video see-through head-mounted display (HMD) (Canon VH-2002).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>System configuration</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The HMD, brush device and controller for moving virtual objects (canvas) had magnetic sensors (Polhemus LIBERTY) to detect their positions and orientations. To generate MR space, the two input images (real world) from cameras of the HMD were captured and sent to the PC through a video capture card (ViewCast Osprey-440). Next, the images were generated using the position and orientation of the HMD from the magnetic sensor in real time and were superimposed onto the real-world images. Thereafter, two output images from a graphics card (NVIDIA GTX 280) were displayed to the user by each display of the HMD.</p><p>All code in the system was written in C++/CLI in the .NET framework. We used OpenGL and the OpenGL Utility Toolkit (GLUT) for the graphics API. The brush device was connected to the main PC through an input/output (I/O) box. The I/O box retrieves information from the devices and sends such information to the main PC, which then sends back commands to control the devices using the model described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec2">2</a>.</p><h3 class="c-article__sub-heading" id="Sec16">Mixed reality painting system</h3><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig14">14</a>, we developed MAI Painting Expert 2.0, with which users can enjoy digital painting on virtual 3D objects; this system is based on MAI Painting Expert 1.0 (Otsuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST ‘10). ACM, New York, NY, USA, pp 97–100. doi: &#xA;                    10.1145/1866029.1866045&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-017-0317-0#ref-CR14" id="ref-link-section-d59260e2263">2010</a>). Note that our objective is to provide visual and haptic feedback from the device. Therefore, we implemented the system as a testbed that has simple functions, not as a high-level simulation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Overview of MAI Painting Expert 2.0</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In MAI Painting Expert 2.0, when a user touches a virtual palette or virtual objects with the brush device (i.e., the MAI Painting Brush++), the system provides the sensation of painting to the user depending on the pressure applied by the brush on the canvas and the painting direction, which are calculated by the position of the device. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig15">15</a> shows the brush tip bending along the shape of an example virtual surface during painting. Furthermore, using magnetic sensors, the user can move and rotate the virtual canvas freely.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Painting on a curved surface using the MAI Painting Brush++</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The occlusion problem of virtual objects hiding the brush device and the user’s hand is solved by extracting the area of the brush device and the user’s hand from captured images. By masking the area, virtual objects are not rendered there. For this extraction, we use the position and orientation of the brush device and its brightness contrast with the background. Therefore, we need to make the environment brighter than the user’s hand and the device.</p><p>To confirm whether our device and proposed models are applicable to painting on a curved canvas or other such complex surface, we asked four users to paint on a variety of virtual objects, including a cat, a teapot, a house, and a dish. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig16">16</a> shows several examples of objects painted by users as a preliminary test. All users commented that when they painted on a curved surface, they could feel the smoothness of the curve because the resolution of the reaction force strength and direction were high enough.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig16_HTML.jpg?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig16_HTML.jpg" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Painting examples</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Since there is no actual surface, a user cannot experience the force feedback from a virtual canvas naturally. Therefore, the user often pushed the device onto the virtual canvas too much, especially when painting on a curved surface. This problem occurs because the distance between the brush device and the surface changes frequently. However, when users performed digital painting using our device, they could trace the surface assuredly by the cues from the sensation of painting. Users also commented that the sensation of painting was more suitable for painting on curved surfaces than on flat ones. The reason behind this is that the distance between the device and the surface constantly changes, thereby changing the provided feedback. From this, we confirmed that our device and proposed models are effective not only for a 2D canvas but also 3D objects that include curved surfaces. Regarding processing speed, we observed that it depends on the detail level of the canvas. In the case of the teapot, which consists of ~7500 polygons, the frame rate is &gt;30 fps. Additionally, the latency between the applying paint on the surface and the bending of the brush tip is approx. 500 ms due to actuating the motor. However, users did not comment that they mind the delay.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">User studies</h2><div class="c-article-section__content" id="Sec17-content"><h3 class="c-article__sub-heading" id="Sec18">User study 1: basic model</h3><p>The MAI Painting Brush++ was evaluated in a user study with 10 participants (8 males, 2 females, aged between 21 and 25 years, Mean: 23.3). The participants were required to evaluated the perceived sensations of painting with and without the basic model in the following four tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig17">17</a>): (a) touching a virtual object; (b) painting lines; (c) painting ovals; (d) painting over the edge of a virtual object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Tasks in user study 1. <b>a</b> Touching the virtual object. <b>b</b> Painting lines. <b>c</b> Painting ovals. <b>d</b> Painting over the edge</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In the case without the basic model, MAI Painting Brush++ did not provide any visual or haptic feedback, even if the brush touches the virtual canvas. The brush stroke is shown, and its width depends on the amount of pressure toward the canvas. In the case with the basic model, in addition to the brush stroke, the device provides both of visual and haptic feedback that change according to the amount of brush pressure applied to the canvas and according to the painting direction.</p><p>These trials were repeated until the participant was satisfied; this approach remained unchanged in all user studies. In this user study, each participant evaluated his/her sensation of painting with the basic model using a seven-point Likert scale, with 1 being the most negative score, 7 the most positive and 4 the baseline that corresponded to his/her experience of painting without the basic model. The order of the two conditions (i.e., with or without the basic model) and the four tasks was assigned at random for each participant.</p><p>The results, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig18">18</a> (regarding the numeric details, see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0317-0#Tab1">1</a>), show that our basic model had a scores &gt;4 for all tasks. All participants commented that feedback with regard to the strength and direction of the reaction force greatly improved when the basic model was applied. Four participants commented that when painting without the basic model, it was difficult to trace the surface of the virtual object, and they often put their device inside it. In contrast, when painting with the basic model, they could more easily trace the surface of the virtual object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig18_HTML.gif?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig18_HTML.gif" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Results of user study 1</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Average and standard deviation of user study 1</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0317-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>From these results, the effectiveness of the mechanism in the MAI Painting Brush++ with the basic model has been demonstrated.</p><h3 class="c-article__sub-heading" id="Sec19">User study 2: extended model (frictional force)</h3><p>In this user study, we confirmed whether we can improve the stroking sensation in the case of painting lines by applying the frictional force model.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Measuring the friction and water absorption coefficients</h4><p>In preparing for this user study, we measured the friction and water absorption coefficients of several materials, including canvas, paper, and ceramic. The coefficients of each material were measured using the following procedure.</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(a)</span>
                        
                          <p>Friction coefficient</p>
                        
                      </li>
                    </ol><p>
All materials measured in this preparation step were dry. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig19">19</a>, when an object moves distance <i>s</i> (m) on a slope with a fixed angle in <i>t</i> (sec), the kinetic friction coefficient <i>μ</i>
                        <sub>dry</sub> is calculated by the following equation:</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\mu_{dry} = \tan \psi - \frac{2s}{{g \cdot t^{2} \cdot \cos \psi }}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig19_HTML.gif?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig19_HTML.gif" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Measurement of the kinetic friction-force coefficient</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>Here, <i>g</i> is the gravitational acceleration. We measured <i>t</i> (sec) by sliding each object (canvas, paper, and ceramic) on the slope to calculate the kinetic friction coefficient. To unify the weight of each object, in the case of canvas and paper, we covered the underneath of the ceramic with a piece of canvas or paper.</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(b)</span>
                        
                          <p>Water absorption coefficient</p>
                        
                      </li>
                    </ol><p>
The water absorption coefficient is the rate of reduction of water for each material. We measured this by painting a line on each material. First, we applied a fixed amount of water to a dry brush using a dropper. Next, we painted a line on each material, and then compare the lengths at which the dry-brush effect first appeared. Our measurement results are given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0317-0#Tab2">2</a>. Regarding the water absorption coefficient, the values shown in the table are relative to that of paper, which is set to be 1.0.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Measurement results of coefficients of each material</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0317-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Evaluation of the extended model (frictional force)</h4><p>Participants were required to paint lines on virtual planes and compare their perception of the sensation of painting with and without the extended model (friction-force model). Note that when the friction-force model was not applied, we applied only the basic model. As with the evaluation of the basic model (user study 1), each participant evaluated his/her sensation of painting with the friction-force model using a seven-point Likert scale with the baseline score of 4 corresponding to the case of only the basic model.</p><p>These virtual planes had different properties that imitated the materials described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec20">4.2.1</a> (i.e., canvas, paper, and ceramic). We informed the participants regarding the materials used before each trial so they would know what to expect.</p><p>In this user study, we implemented the following conditions:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">(a)</span>
                        
                          <p>paint lines using only the friction coefficient</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(b)</span>
                        
                          <p>paint lines using both the friction and water absorption coefficients</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(c)</span>
                        
                          <p>perform (b) after stroking real material with a finger</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">(d)</span>
                        
                          <p>perform (b) after stroking real material with an actual brush</p>
                        
                      </li>
                    </ol>
<p>First, participants tried and evaluated three materials under condition (a). Thereafter, they repeated the same procedure under the remaining conditions. The order was (b), (c), then (d) because we wanted their evaluations without any previous knowledge (a and b), and also to compare with the case of having previous knowledge (c and d). For each condition, the user performed the test without the friction-force model for comparison before the case with the friction-force model. The order of the three materials was assigned at random for each participant.</p><p>To confirm whether our model successfully presented differences in materials using only haptic feedback, the textures shown were the same for all objects. Furthermore, the participants were 10 new people (8 males, 2 females, aged between 21 and 25 years, Mean: 22.1) not involved in user study 1.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig20">20</a> and Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0317-0#Tab3">3</a> show our results, which indicate that when the friction-force model was applied, the scores were generally higher, with scores &gt;4 for all materials and conditions. Therefore, we conclude that, for the stroking sensation, both the friction and water absorption coefficients were effective even though the participants had previous knowledge about the materials by stroking them with their fingers or an actual brush.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-20"><figure><figcaption><b id="Fig20" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 20</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/20" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig20_HTML.gif?as=webp"></source><img aria-describedby="figure-20-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig20_HTML.gif" alt="figure20" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-20-desc"><p>Results of user study 2</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/20" data-track-dest="link:Figure20 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Average and standard deviation of user study 2 (upper row: average, bottom row: standard deviation in each material)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0317-0/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec22">User study 3: extended model (brush tip spring)</h3><p>In this user study, 10 participants (9 males, 1 females, aged between 21 and 25 years, Mean: 22.8 not involved either in user study 1 or 2) were required to paint lines on a virtual plane, as in user study 2. They were then asked to evaluate their sensation of painting with and without the brush-tip spring model. Without the brush-tip spring model, we applied only the basic model. We asked the participants to roll their wrist during painting, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig21">21</a>, to bend the brush tip with a downward movement of the brush, and then to let it spring back free of the canvas. The participants evaluated the resulting sensation of painting in the case with the brush-tip spring model using a five-point Likert scale, with the baseline score of 3 corresponding to the case without the brush-tip spring model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-21"><figure><figcaption><b id="Fig21" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 21</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/21" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig21_HTML.gif?as=webp"></source><img aria-describedby="figure-21-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig21_HTML.gif" alt="figure21" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-21-desc"><p>Rolling of the wrist during painting</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/21" data-track-dest="link:Figure21 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0317-0#Fig22">22</a> and Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0317-0#Tab4">4</a> show our results, which indicates that when the brush-tip spring model was applied, the score was generally higher. However, three participants commented that although they could recognize the difference between the cases with and without the model, they were unsure whether it was indeed the brush-spring model, because the difference was so small. To address this problem, we must improve the mechanism rather than the model, for example, by improving the range of movement of the cylindrical flap or improving the response speed of the motors, thus providing a larger feedback force on the user’s hand.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-22"><figure><figcaption><b id="Fig22" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 22</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/22" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig22_HTML.gif?as=webp"></source><img aria-describedby="figure-22-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0317-0/MediaObjects/10055_2017_317_Fig22_HTML.gif" alt="figure22" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-22-desc"><p>Results of user study 3</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0317-0/figures/22" data-track-dest="link:Figure22 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Average and standard deviation of user study 3</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0317-0/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec23"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23">Discussion and future work</h2><div class="c-article-section__content" id="Sec23-content"><p>One limitation of our method is that our device cannot stop the user’s movement if he/she pushes the device into the virtual canvas, because it is an ungrounded haptic feedback device. However, as shown in the preliminary study in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec16">3.5</a> and the user studies in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec17">4</a>, by using visual and haptic feedback, our system could provide the sensation of painting even if the canvas was a virtual object, and make the users to trace its surface.</p><p>Regarding the size of working area, as we mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec15">3.4</a>, the size is in excess of a sphere of radius 90 cm in our system. This is dependent on the range of tracking sensors, and we used Polhemus Liberty as one selection. If we change the tracking method (e.g., optical tracking, tracking by computer vision), the size of working area can be extended for from desktop operation to wall or room-size operation.</p><p>Because our focus was to develop a device that could provide visual and haptic feedback for painting on virtual objects, we developed the associated painting system, MAI Painting Expert 2.0 as a testbed for confirming the effectiveness of our proposed model, and it has only certain simple functions. Nevertheless, the actual painting brushes allow certain interesting and unique expressions, such as blotting and soft and hard edges. These expressions are already available in the painting simulation systems mentioned in the related studies (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec1">1</a>). By collaborating with the authors of related painting systems, it would be possible to improve painting expressions of our system in the future.</p><p>Regarding the user studies, we confirmed via qualitative evaluation that our proposed model and the mechanism of the device were effective. However, we did not conduct a user study that involved a qualitative comparison with a real experience. Therefore, we plan to conduct such qualitative evaluation in future work.</p><p>The target user of our system is regardless of being a professional or an amateur. We simply provided a new input device for digital painting for the user who wants to watch the tip of the brush bend and feel the reaction and friction forces between the brush and the canvas, as we described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0317-0#Sec1">1</a>. In this study, although our system is very primitive, if it could be improved in relation to its tracking accuracy and performance, we believe that it has the potential to satisfy professional users.</p><p>In 2D digital painting, users often print out their work. Therefore, we believe that many users may wish to have a physical 3D rendition of their work here as well. The simplest method would be to use commercial software that generates papercraft from 3D objects with texture. It is also possible that using a high-grade method such as a 3D printer and the industrial hydrographics technique. For future work, we plan to connect our system to such 3D-output ones. We believe that our technology can encourage digital creation.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Conclusion</h2><div class="c-article-section__content" id="Sec24-content"><p>Previously, we developed an MR-based painting system that allows users to enjoy digital painting on visualizations of real objects and to experience a painting sensation similar to that of actual painting. However, because both real and virtual objects can be manipulated in MR space, the purpose of this study has been to develop a system with which users can paint on virtual objects (both 2D and 3D ones) directly in a manner similar to actual painting. When painting on virtual objects, the main problem we faced was how to provide the sensation of painting to the user. To solve it, we designed and developed a brush device, the MAI Painting Brush++, that has visual and haptic feedback mechanisms.</p><p>We first analyzed actual painting techniques, from which we developed a painting model. Next, based on this model, we designed and developed a brush device with mechanisms that provide the sensation of painting. Our device controls the brush tip and a cylindrical flap via three wires and three motors in order to represent tip bending both visually and haptically via the reaction force by moving the cylindrical flap while the user paints on the virtual object.</p><p>We defined the force generated by “pushing back” from the canvas combined with the frictional force between the brush and the canvas as the “reaction force,” and then developed a basic model to represent it. Furthermore, we developed an extended model that consists of the following two sub-models: (1) the friction-force changing model, which provides different sensations based on the canvas material and the amount of water in the brush tip, and (2) the brush-tip spring model, which provides a haptic feedback when the brush springs back after the user presses its tip onto the canvas. Through our various user studies, we confirmed the effectiveness of our device and proposed models.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>“Sensu brush” <a href="http://www.sensubrush.com/">http://www.sensubrush.com/</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bau O, Poupyrev I, Le Goc M, Galliot L, Glisson M (2012) REVEL: tactile feedback technology for augmented real" /><p class="c-article-references__text" id="ref-CR1">Bau O, Poupyrev I, Le Goc M, Galliot L, Glisson M (2012) REVEL: tactile feedback technology for augmented reality. In: ACM SIGGRAPH 2012 emerging technologies (SIGGRAPH ‘12). ACM, New York, NY, USA, Article 17. doi: <a href="https://doi.org/10.1145/2343456.2343473">10.1145/2343456.2343473</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baxter B, Scheib V, Lin MC, Manocha D (2001) DAB: interactive haptic painting with 3D virtual brushes. In: Pro" /><p class="c-article-references__text" id="ref-CR2">Baxter B, Scheib V, Lin MC, Manocha D (2001) DAB: interactive haptic painting with 3D virtual brushes. In: Proceedings of the 28th annual conference on Computer graphics and interactive techniques (SIGGRAPH ‘01). ACM, New York, NY, USA, pp 461–468. doi: <a href="https://doi.org/10.1145/383259.383313">10.1145/383259.383313</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baxter W, Wendt J, Lin MC (2004) IMPaSTo: a realistic, interactive model for paint. In: Spencer SN (Ed.) Proce" /><p class="c-article-references__text" id="ref-CR3">Baxter W, Wendt J, Lin MC (2004) IMPaSTo: a realistic, interactive model for paint. In: Spencer SN (Ed.) Proceedings of the 3rd international symposium on non-photorealistic animation and rendering (NPAR ‘04). ACM, New York, NY, USA, pp 45–148. doi: <a href="https://doi.org/10.1145/987657.987665">10.1145/987657.987665</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chu NS-H, Tai C-L (2005) MoXi: real-time ink dispersion in absorbent paper. In: J Buhler (Ed.) ACM SIGGRAPH 20" /><p class="c-article-references__text" id="ref-CR4">Chu NS-H, Tai C-L (2005) MoXi: real-time ink dispersion in absorbent paper. In: J Buhler (Ed.) ACM SIGGRAPH 2005 sketches (SIGGRAPH ‘05). ACM, New York, NY, USA, Article 62. doi: <a href="https://doi.org/10.1145/1187112.1187186">10.1145/1187112.1187186</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Curtis CJ, Anderson SE, Seims JE, Fleischer KW, Salesin DH (1997) Computer-generated watercolor. In: Proceedin" /><p class="c-article-references__text" id="ref-CR5">Curtis CJ, Anderson SE, Seims JE, Fleischer KW, Salesin DH (1997) Computer-generated watercolor. In: Proceedings of the 24th annual conference on Computer graphics and interactive techniques (SIGGRAPH ‘97). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, pp 421–430. doi: <a href="https://doi.org/10.1145/258734.258896">10.1145/258734.258896</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Foskey M, Otaduy MA, Lin MC (2005) ArtNova: Touch-enabled 3D model design. In: J Fujii (Ed.) ACM SIGGRAPH 2005" /><p class="c-article-references__text" id="ref-CR6">Foskey M, Otaduy MA, Lin MC (2005) ArtNova: Touch-enabled 3D model design. In: J Fujii (Ed.) ACM SIGGRAPH 2005 courses (SIGGRAPH ‘05). ACM, New York, NY, USA, Article 188. doi: <a href="https://doi.org/10.1145/1198555.1198619">10.1145/1198555.1198619</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Gooch, A. Gooch, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Gooch B, Gooch A (2001) Non-photorealistic rendering. A. K. Peters Ltd., Natick" /><p class="c-article-references__text" id="ref-CR7">Gooch B, Gooch A (2001) Non-photorealistic rendering. A. K. Peters Ltd., Natick</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Non-photorealistic%20rendering&amp;publication_year=2001&amp;author=Gooch%2CB&amp;author=Gooch%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of the 2005 ACM SI" /><p class="c-article-references__text" id="ref-CR8">Iwai D, Sato K (2005) Heat sensation in image creation with thermal vision. In: Proceedings of the 2005 ACM SIGCHI international conference on advances in computer entertainment technology (ACE ‘05). ACM, New York, NY, USA, pp 213–216. doi: <a href="https://doi.org/10.1145/1178477.1178510">10.1145/1178477.1178510</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kamuro S, Minamizawa K, Kawakami N, Tachi S (2009) Ungrounded kinesthetic pen for haptic interaction with virt" /><p class="c-article-references__text" id="ref-CR9">Kamuro S, Minamizawa K, Kawakami N, Tachi S (2009) Ungrounded kinesthetic pen for haptic interaction with virtual environments. In: Proceedings of robot and human interactive communication (RO-MAN 2009), pp 436–441, Sept. 27 2009–Oct. 2. doi: <a href="https://doi.org/10.1109/ROMAN.2009.5326217">10.1109/ROMAN.2009.5326217</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kamuro S, Minamizawa K, Tachi S (2011) 3D Haptic modeling system using ungrounded pen-shaped kinesthetic displ" /><p class="c-article-references__text" id="ref-CR10">Kamuro S, Minamizawa K, Tachi S (2011) 3D Haptic modeling system using ungrounded pen-shaped kinesthetic display. In: Proceedings of the 2011 IEEE virtual reality conference (VR ‘11). IEEE Computer Society, Washington, DC, USA, pp 217–218. doi: <a href="https://doi.org/10.1109/VR.2011.5759476">10.1109/VR.2011.5759476</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato G, Kuroda Y, Nisky I, Kiyokawa K, Takemura H (2015) HapSticks: tool-mediated interaction with grounding-f" /><p class="c-article-references__text" id="ref-CR11">Kato G, Kuroda Y, Nisky I, Kiyokawa K, Takemura H (2015) HapSticks: tool-mediated interaction with grounding-free haptic interface. In: SIGGRAPH Asia 2015 haptic media and contents design (SA ‘15). ACM, New York, NY, USA, Article 7. doi: <a href="https://doi.org/10.1145/2818384.2818387">10.1145/2818384.2818387</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim H-J, Kim H, Chae S, Seo J, Han T-D (2013) AR pen and hand gestures: a new tool for pen drawings. In CHI ‘1" /><p class="c-article-references__text" id="ref-CR12">Kim H-J, Kim H, Chae S, Seo J, Han T-D (2013) AR pen and hand gestures: a new tool for pen drawings. In CHI ‘13 extended abstracts on human factors in computing systems (CHI EA ‘13). ACM, New York, NY, USA, pp 943–948. doi: <a href="https://doi.org/10.1145/2468356.2468525">10.1145/2468356.2468525</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liu X, Gu J (2014) FlexStroke: a flexible, deformable brush-tip with dynamic stiffness for digital input. In: " /><p class="c-article-references__text" id="ref-CR13">Liu X, Gu J (2014) FlexStroke: a flexible, deformable brush-tip with dynamic stiffness for digital input. In: Proceedings of the 8th international conference on tangible, embedded and embodied interaction (TEI ‘14). ACM, New York, NY, USA, pp. 39–40. doi: <a href="https://doi.org/10.1145/2540930.2540982">10.1145/2540930.2540982</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that real" /><p class="c-article-references__text" id="ref-CR14">Otsuki M, Sugihara K, Kimura A, Shibata F, Tamura H (2010) MAI painting brush: an interactive device that realizes the feeling of real painting. In: Proceedings of the 23nd annual ACM symposium on user interface software and technology (UIST ‘10). ACM, New York, NY, USA, pp 97–100. doi: <a href="https://doi.org/10.1145/1866029.1866045">10.1145/1866029.1866045</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ryokai K, Marti S, Ishii H (2004) I/O brush: drawing with everyday objects as ink. In: Proceedings of the SIGC" /><p class="c-article-references__text" id="ref-CR15">Ryokai K, Marti S, Ishii H (2004) I/O brush: drawing with everyday objects as ink. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI ‘04). ACM, New York, NY, USA, pp 303–310. doi: <a href="https://doi.org/10.1145/985692.985731">10.1145/985692.985731</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saito S, Nakajima M (1999) 3D physics-based brush model for painting. In: ACM SIGGRAPH 99 conference abstracts" /><p class="c-article-references__text" id="ref-CR16">Saito S, Nakajima M (1999) 3D physics-based brush model for painting. In: ACM SIGGRAPH 99 conference abstracts and applications (SIGGRAPH ‘99). ACM, New York, NY, USA, p 226. doi: <a href="https://doi.org/10.1145/311625.312110">10.1145/311625.312110</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sandor C, Uchiyama S, Yamamoto H (2007) Visuo-haptic systems: half-mirrors considered harmful. In: Proceedings" /><p class="c-article-references__text" id="ref-CR17">Sandor C, Uchiyama S, Yamamoto H (2007) Visuo-haptic systems: half-mirrors considered harmful. In: Proceedings of the second joint EuroHaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems (WHC ‘07). IEEE Computer Society, Washington, DC, USA, pp 292–297. doi:<a href="https://doi.org/10.1109/WHC.2007.125">10.1109/WHC.2007.125</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vandoren P, Claesen L, Van Laerhoven T, Taelman J, Raymaekers C, Flerackers E, Van Reeth F (2009) FluidPaint: " /><p class="c-article-references__text" id="ref-CR18">Vandoren P, Claesen L, Van Laerhoven T, Taelman J, Raymaekers C, Flerackers E, Van Reeth F (2009) FluidPaint: an interactive digital painting system using real wet brushes. In: Proceedings of the ACM international conference on interactive tabletops and surfaces (ITS ‘09). ACM, New York, NY, USA, pp 53–56. doi: <a href="https://doi.org/10.1145/1731903.1731914">10.1145/1731903.1731914</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yeom J, Lee G (2012) Designing a user interface for a painting application supporting real watercolor painting" /><p class="c-article-references__text" id="ref-CR19">Yeom J, Lee G (2012) Designing a user interface for a painting application supporting real watercolor painting processes. In: Proceedings of the 10th Asia pacific conference on computer human interaction (APCHI ‘12). ACM, New York, NY, USA, pp 219–226. doi: <a href="https://doi.org/10.1145/2350046.2350091">10.1145/2350046.2350091</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0317-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki, 305-8571, Japan</p><p class="c-article-author-affiliation__authors-list">Mai Otsuki</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Ritsumeikan University, 1-1-1 Noji-higashi, Kusatsu, Shiga, 525-8577, Japan</p><p class="c-article-author-affiliation__authors-list">Kenji Sugihara, Azusa Toda, Fumihisa Shibata &amp; Asako Kimura</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Mai-Otsuki"><span class="c-article-authors-search__title u-h3 js-search-name">Mai Otsuki</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mai+Otsuki&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mai+Otsuki" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mai+Otsuki%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kenji-Sugihara"><span class="c-article-authors-search__title u-h3 js-search-name">Kenji Sugihara</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kenji+Sugihara&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kenji+Sugihara" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kenji+Sugihara%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Azusa-Toda"><span class="c-article-authors-search__title u-h3 js-search-name">Azusa Toda</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Azusa+Toda&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Azusa+Toda" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Azusa+Toda%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Fumihisa-Shibata"><span class="c-article-authors-search__title u-h3 js-search-name">Fumihisa Shibata</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Fumihisa+Shibata&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Fumihisa+Shibata" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Fumihisa+Shibata%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Asako-Kimura"><span class="c-article-authors-search__title u-h3 js-search-name">Asako Kimura</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Asako+Kimura&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Asako+Kimura" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Asako+Kimura%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0317-0/email/correspondent/c1/new">Mai Otsuki</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20brush%20device%20with%20visual%20and%20haptic%20feedback%20for%20virtual%20painting%20of%203D%20virtual%20objects&amp;author=Mai%20Otsuki%20et%20al&amp;contentID=10.1007%2Fs10055-017-0317-0&amp;publication=1359-4338&amp;publicationDate=2017-06-06&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0317-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0317-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Otsuki, M., Sugihara, K., Toda, A. <i>et al.</i> A brush device with visual and haptic feedback for virtual painting of 3D virtual objects.
                    <i>Virtual Reality</i> <b>22, </b>167–181 (2018). https://doi.org/10.1007/s10055-017-0317-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0317-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-09-25">25 September 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-05-28">28 May 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-06-06">06 June 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-06">June 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0317-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0317-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Painting system</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Input device</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Paintbrush</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual and haptic feedback</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0317-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=317;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

