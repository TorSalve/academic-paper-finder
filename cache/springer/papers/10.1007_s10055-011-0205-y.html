<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Toward the design of transitional interfaces: an exploratory study on "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="A task that can be decomposed into subtasks with different technological demands may be a challenge, since it requires multiple interactive environments as well as transitions between them. Some of..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface"/>

    <meta name="dc.source" content="Virtual Reality 2011 16:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-12-23"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="A task that can be decomposed into subtasks with different technological demands may be a challenge, since it requires multiple interactive environments as well as transitions between them. Some of these transitions may involve changes in hardware devices and interface paradigms at the same time. Some previous works have proposed various setups for hybrid user interfaces, but none of them focused on the design of transition interactions. Our work emphasizes the importance of interaction continuity as a guideline in the design and evaluation of transitional interfaces within a hybrid user interface (HUI). Finally, an exploratory study demonstrates how this design aspect is perceived by users during transitions in an HUI composed by three interactive environments."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-12-23"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="271"/>

    <meta name="prism.endingPage" content="288"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0205-y"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0205-y"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0205-y.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0205-y"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2012/11"/>

    <meta name="citation_online_date" content="2011/12/23"/>

    <meta name="citation_firstpage" content="271"/>

    <meta name="citation_lastpage" content="288"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0205-y"/>

    <meta name="DOI" content="10.1007/s10055-011-0205-y"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0205-y"/>

    <meta name="description" content="A task that can be decomposed into subtasks with different technological demands may be a challenge, since it requires multiple interactive environments as"/>

    <meta name="dc.creator" content="Felipe G. Carvalho"/>

    <meta name="dc.creator" content="Daniela G. Trevisan"/>

    <meta name="dc.creator" content="Alberto Raposo"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Alencar MFC, Raposo AB, Barbosa SDJ (2011) Composition of HCI evaluation methods for hybrid virtual environments. In: proceedings of the 2011 ACM symposium on applied computing, pp 1237&#8211;1244"/>

    <meta name="citation_reference" content="Baumgartner S, Ebert A, Deller M (2007) Dimensional congruence for interactive visual data mining and knowledge discovery. In: EuroVis, Eurographics Association, pp 99&#8211;106"/>

    <meta name="citation_reference" content="Benko H, Ishak E, Feiner S (2005) Cross-dimensional gestural interaction techniques for hybrid immersive environments. In: Virtual Reality 2005, IEEE, pp 209&#8211;216"/>

    <meta name="citation_reference" content="Billinghurst M, Kato H, Poupyrev I (1999) The magicbook&#8212;moving seamlessly between reality and virtuality. In: IWAR 99, pp 35&#8211;44"/>

    <meta name="citation_reference" content="Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of volumetric medical data. In: IEEE symposium on 3D user interfaces, IEEE"/>

    <meta name="citation_reference" content="Bowman D, Kruijff E, LaViola J, Poupyrev I (2005) 3D user interfaces&#8212;theory and practice. Addison-Wesley"/>

    <meta name="citation_reference" content="Butz A, Hollerer T et&#160;al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: IWAR 99, pp 35&#8211;44"/>

    <meta name="citation_reference" content="Carvalho F, Raposo A, Gattass M (2009) A transitional interface between 2D/3D functional spaces in a desktop semi-immersive system. In: VRCAI 2009 The 8th international ACM conference on virtual-reality continuum and its applications in industry, ACM"/>

    <meta name="citation_reference" content="Czerwinski M, Tan DS, Robertson GG (2002) Women take a wider view. In: CHI &#8217;02: proceedings of the SIGCHI conference on human factors in computing systems, ACM, New York, pp 195&#8211;202"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE transactions on visualization and computer graphics; citation_title=Cave and fishtank virtual-reality displays: a qualitative and quantitative comparison; citation_author=C Demiralp, DB Karelitz, S Zhang, DH Laidlaw; citation_volume=12; citation_issue=3; citation_publication_date=2006; citation_pages=323-330; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_title=The semiotic engineering of human-computer interaction; citation_publication_date=2005; citation_id=CR11; citation_author=CS Souza; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=J Univers Access Inf Soc; citation_title=Assessing continuity and compatibility in augmented reality systems; citation_author=E Dubois, L Nigay, J Troccaz; citation_volume=1; citation_issue=4; citation_publication_date=2002; citation_pages=263-273; citation_doi=10.1007/s10209-002-0024-8; citation_id=CR12"/>

    <meta name="citation_reference" content="Feiner S, Shamash A (1991) Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers. In: ACM UIST 91, ACM, pp 9&#8211;17"/>

    <meta name="citation_reference" content="Grasset R, Looser J, Billinghurst M (2006) Transitional interface: concept, issues and framework. In: ISMAR, IEEE, pp 231&#8211;232"/>

    <meta name="citation_reference" content="Grasset R, Duenser A, Billinghurst M (2008) Moving between contexts&#8212;a user evaluation of a transitional interface. In: ICAT 2008: 18th international conference on artificial reality and telexistence, Keio University, Yokohama"/>

    <meta name="citation_reference" content="Hall ET (1969) The hidden dimension. Anchor Books"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Iterative design of seamless collaboration media; citation_author=H Ishii, M Kobayashi, K Arita; citation_volume=37; citation_issue=8; citation_publication_date=1994; citation_pages=83-97; citation_doi=10.1145/179606.179687; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Multimedia IEEE; citation_title=SeamlessDesign for 3D object creation; citation_author=K Kiyokawa, H Takemura, N Yokoya; citation_volume=7; citation_issue=1; citation_publication_date=2000; citation_pages=22-33; citation_doi=10.1109/93.839308; citation_id=CR18"/>

    <meta name="citation_reference" content="Masso JPM (2008) A structured approach to the development of 3D user interfaces. PhD thesis, ALBACETE, ES, adviser-Pascual Gonzalez Lopez"/>

    <meta name="citation_reference" content="Milgram P, Colquhoun HJ (1999) A taxonomy of real and virtual world display integration. Mixed reality: merging real and virtual worlds. Ohmsha and Springer pp 5&#8211;30"/>

    <meta name="citation_reference" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IECE transactions on information and systems, pp 1321&#8211;1329"/>

    <meta name="citation_reference" content="Nakashima K, Machida T, Kiyokawa K, Takamura H (2005) A 2D&#8211;3D integrated environment for cooperative work. In: symposium on virtual reality software and technology, ACM, pp 16&#8211;22"/>

    <meta name="citation_reference" content="Polys NF, Kim S, Bowman DA (2005) Effects of information layout, screen size, and field of view on user performance in information-rich virtual environments. In: VRST &#8217;05: proceedings of the ACM symposium on virtual reality software and technology, ACM, New York, pp 46&#8211;55"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Visual Comput Gr; citation_title=A comparative study of desktop, fishtank, and cave systems for the exploration of volume rendered confocal data sets; citation_author=null Prabhat, A Forsberg, M Katzourin, K Wharton, M Slater; citation_volume=14; citation_issue=3; citation_publication_date=2008; citation_pages=551-563; citation_doi=10.1109/TVCG.2007.70433; citation_id=CR24"/>

    <meta name="citation_reference" content="Raja D, Bowman D, Lucas J, North C (2004) Exploring the benefits of immersion in abstract information visualization. In: 8th international immersive projection technology workshop"/>

    <meta name="citation_reference" content="Raposo A, Santos I, Soares L, Wagner G, Corseuil E, Gattas M (2009) Environ: integrating VR and CAD in engineering projects. In: IEEE computer graphics and applications, IEEE, vol 29, pp 91&#8211;95"/>

    <meta name="citation_reference" content="Rascar R, Welch G, Cutts M, et&#160;al (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: SIGGRAPH, ACM, pp 179&#8211;188"/>

    <meta name="citation_reference" content="Raymaekers C, Boeck JD, Weyer TD, Coninx K (2005) The effect of display size on navigation in a virtual environment. In: proceedings international conference on enactive interfaces&#8212;enactive &#8217;05"/>

    <meta name="citation_reference" content="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: proceedings of the ACM symposium on user interface software and technology, ACM, pp 29&#8211;36"/>

    <meta name="citation_reference" content="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: proceedings of CHI&#8217;99, ACM, pp 378&#8211;385"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Evaluating effectiveness of interaction techniques across immersive virtual environmental systems; citation_author=A Steed, C Parker; citation_volume=14; citation_issue=5; citation_publication_date=2005; citation_pages=511-527; citation_doi=10.1162/105474605774918750; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput-Hum Interact; citation_title=Physically large displays improve performance on spatial tasks; citation_author=DS Tan, D Gergle, P Scupelli, R Pausch; citation_volume=13; citation_issue=1; citation_publication_date=2006; citation_pages=71-99; citation_doi=10.1145/1143518.1143521; citation_id=CR32"/>

    <meta name="citation_reference" content="Trevisan DG (2004) Designing smooth connections between worlds. In: extended abstracts of the 2004 conference on human factors and computing systems CHI2004, Session doctoral consortium, ACM Press, pp 1043&#8211;1044"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real J; citation_title=Conceptualizing mixed spaces of interaction for designing continuous interaction; citation_author=DG Trevisan, J Vanderdonckt, B Macq; citation_volume=8; citation_issue=2; citation_publication_date=2004; citation_pages=83-95; citation_doi=10.1007/s10055-004-0140-2; citation_id=CR34"/>

    <meta name="citation_author" content="Felipe G. Carvalho"/>

    <meta name="citation_author_email" content="kamel@tecgraf.puc-rio.br"/>

    <meta name="citation_author_institution" content="Computer Graphics Technology Group, Tecgraf, Pontifical Catholic University of Rio de Janeiro, PUC-Rio, Rio de Janeiro, Brazil"/>

    <meta name="citation_author" content="Daniela G. Trevisan"/>

    <meta name="citation_author_email" content="daniela@ic.uff.br"/>

    <meta name="citation_author_institution" content="Computer Institute, Federal Fluminense University, Niteroi, Brazil"/>

    <meta name="citation_author" content="Alberto Raposo"/>

    <meta name="citation_author_email" content="abraposo@inf.puc-rio.br"/>

    <meta name="citation_author_institution" content="Department of Informatics, Pontifical Catholic University of Rio de Janeiro, PUC-Rio, Rio de Janeiro, Brazil"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0205-y&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0205-y"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface"/>
        <meta property="og:description" content="A task that can be decomposed into subtasks with different technological demands may be a challenge, since it requires multiple interactive environments as well as transitions between them. Some of these transitions may involve changes in hardware devices and interface paradigms at the same time. Some previous works have proposed various setups for hybrid user interfaces, but none of them focused on the design of transition interactions. Our work emphasizes the importance of interaction continuity as a guideline in the design and evaluation of transitional interfaces within a hybrid user interface (HUI). Finally, an exploratory study demonstrates how this design aspect is perceived by users during transitions in an HUI composed by three interactive environments."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0205-y","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Transitional interfaces, Hybrid user interfaces, Continuity properties","kwrd":["Transitional_interfaces","Hybrid_user_interfaces","Continuity_properties"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0205-y","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0205-y","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=205;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0205-y">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0205-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0205-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-12-23" itemprop="datePublished">23 December 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Felipe_G_-Carvalho" data-author-popup="auth-Felipe_G_-Carvalho">Felipe G. Carvalho</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Pontifical Catholic University of Rio de Janeiro, PUC-Rio" /><meta itemprop="address" content="grid.4839.6, 000000012323852X, Computer Graphics Technology Group, Tecgraf, Pontifical Catholic University of Rio de Janeiro, PUC-Rio, Rio de Janeiro, RJ, Brazil" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniela_G_-Trevisan" data-author-popup="auth-Daniela_G_-Trevisan" data-corresp-id="c1">Daniela G. Trevisan<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Federal Fluminense University" /><meta itemprop="address" content="grid.411173.1, 0000000121846919, Computer Institute, Federal Fluminense University, Niteroi, RJ, Brazil" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Alberto-Raposo" data-author-popup="auth-Alberto-Raposo">Alberto Raposo</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Pontifical Catholic University of Rio de Janeiro, PUC-Rio" /><meta itemprop="address" content="grid.4839.6, 000000012323852X, Department of Informatics, Pontifical Catholic University of Rio de Janeiro, PUC-Rio, Rio de Janeiro, RJ, Brazil" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">271</span>–<span itemprop="pageEnd">288</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">540 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0205-y/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>A task that can be decomposed into subtasks with different technological demands may be a challenge, since it requires multiple interactive environments as well as transitions between them. Some of these transitions may involve changes in hardware devices and interface paradigms at the same time. Some previous works have proposed various setups for hybrid user interfaces, but none of them focused on the design of transition interactions. Our work emphasizes the importance of interaction continuity as a guideline in the design and evaluation of transitional interfaces within a hybrid user interface (HUI). Finally, an exploratory study demonstrates how this design aspect is perceived by users during transitions in an HUI composed by three interactive environments.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The evolution of human–computer interaction has led to the development of different interfaces and interaction mechanisms which are progressively being combined to create richer forms of interactions. The need to support tasks that require the use of different interfaces has become increasingly evident with the emergence of fields of study such as hybrid user interfaces (HUI) (Feiner and Shamash <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Feiner S, Shamash A (1991) Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers. In: ACM UIST 91, ACM, pp 9–17" href="/article/10.1007/s10055-011-0205-y#ref-CR13" id="ref-link-section-d25697e383">1991</a>). In this field, efforts are being joined in order to blend virtual (applications, interaction techniques, graphics, etc.) and physical (input and output) elements from one or more interfaces harmoniously. The design of HUIs may thus result in new interfaces with hybrid resources, aiming to integrate previously separate functions.</p><p>The way in which hardware and software elements are integrated into an HUI may demand certain interface mechanisms or a kind of interface that connects others, e.g., a form of transition interface or a <i>transitional interface</i> (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Billinghurst M, Kato H, Poupyrev I (1999) The magicbook—moving seamlessly between reality and virtuality. In: IWAR 99, pp 35–44" href="/article/10.1007/s10055-011-0205-y#ref-CR4" id="ref-link-section-d25697e392">1999</a>; Grasset et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Grasset R, Looser J, Billinghurst M (2006) Transitional interface: concept, issues and framework. In: ISMAR, IEEE, pp 231–232" href="/article/10.1007/s10055-011-0205-y#ref-CR14" id="ref-link-section-d25697e395">2006</a>).</p><p>The concept of transitional interfaces was first introduced by the MagicBook project (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Billinghurst M, Kato H, Poupyrev I (1999) The magicbook—moving seamlessly between reality and virtuality. In: IWAR 99, pp 35–44" href="/article/10.1007/s10055-011-0205-y#ref-CR4" id="ref-link-section-d25697e401">1999</a>) through an application that allowed the user to move seamlessly along the mixed reality continuum (Milgram and Kishino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IECE transactions on information and systems, pp 1321–1329" href="/article/10.1007/s10055-011-0205-y#ref-CR21" id="ref-link-section-d25697e404">1994</a>). In the MagicBook, the user walks from the real world to virtual reality passing through augmented reality using a hand-held device. The transition between these environments consisted of automated virtual camera movements and image effects, such as fade in and out.</p><p>A few years later, the concept of transitional interface was formalized with the purpose of generalizing this term for multiple contexts (Grasset et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Grasset R, Looser J, Billinghurst M (2006) Transitional interface: concept, issues and framework. In: ISMAR, IEEE, pp 231–232" href="/article/10.1007/s10055-011-0205-y#ref-CR14" id="ref-link-section-d25697e410">2006</a>). The possible contexts encompassed 3D spaces with properties such as scale (macro, micro, nano), representation (photorealistic, cartoon, etc.), and others. For each context, there was a clear definition of a motion function related to the properties of a virtual camera, and, for each transition between these contexts, there was a transition function related to visual factors in order to provide a continuously smooth transitional motion rather than teleportation. The hardware and software setups used by authors during the development of these ideas were based on the contexts of virtual reality and augmented reality, and they relied on a constant set of input devices and displays during the studies, such as a hand-held device (Milgram and Colquhoun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Milgram P, Colquhoun HJ (1999) A taxonomy of real and virtual world display integration. Mixed reality: merging real and virtual worlds. Ohmsha and Springer pp 5–30" href="/article/10.1007/s10055-011-0205-y#ref-CR20" id="ref-link-section-d25697e413">1999</a>) or a head-mounted display combined with a 3D tracking input device (Grasset et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Grasset R, Duenser A, Billinghurst M (2008) Moving between contexts—a user evaluation of a transitional interface. In: ICAT 2008: 18th international conference on artificial reality and telexistence, Keio University, Yokohama" href="/article/10.1007/s10055-011-0205-y#ref-CR15" id="ref-link-section-d25697e416">2008</a>). However, there was no change in interface paradigm during transitions, since the contexts were restricted to 3D environments and interactions during transitions were conceived in an intuitive way.</p><p>In this work, we claim that the range of action of a transitional interface may be actually larger than the mixed reality continuum. Masso (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Masso JPM (2008) A structured approach to the development of 3D user interfaces. PhD thesis, ALBACETE, ES, adviser-Pascual Gonzalez Lopez" href="/article/10.1007/s10055-011-0205-y#ref-CR19" id="ref-link-section-d25697e423">2008</a>) proposed the mixed virtuality continuum as an extension of the continuum proposed by Milgram and Kishino (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IECE transactions on information and systems, pp 1321–1329" href="/article/10.1007/s10055-011-0205-y#ref-CR21" id="ref-link-section-d25697e426">1994</a>). In this continuum, before virtual reality (VR) interfaces there is a series of intermediary interfaces ranging from 1D to pure 3D, passing through 2D graphical interfaces, 2D desktop, 3D desktop, 3D rendering of 2D interfaces, among others. This mixed virtuality places such interfaces in the context of the evolution of the human–computer interaction. Altogether, they comprise a diversity of hardware and software settings that can be combined to create hybrid environments such as HUIs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig1">1</a>). Actually, it makes sense to think of HUIs and transitional interfaces as complementary.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Relationship between mixed continuums, hybrid environment, and transitional interfaces</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Considering this view of HUIs, the concept of transitional interface can also be extended. The transitions may be placed between contexts that include an interface (in the mixed virtuality or mixed reality continuum) and the input devices and displays used (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig1">1</a>). The idea of motion function in this expanded view would be seen as a function describing which input devices, interaction techniques, and displays are used at a given time. On the other hand, a transition function includes aspects such as procedures to change or handle devices as well as the use of graphic and audio effects to prepare for the motion function of the following context. It is important to note that this paper will use the term “interactive environment” with the same meaning as “context” defined hereby.</p><p>The main goal of this work consists in exploring the continuity properties (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec4">3.1</a>) of transition interactions between interactive environments. We discuss the use of such design aspects as a guideline for the evaluation process and seeking to obtain smoother transitions within a hybrid interactive system. To achieve such goal, a semi-immersive system called HybridDesk was developed. It is composed of three interactive environments, and transitions between these environments were created to help accomplish a 3D annotation task in an oil and gas scenario application.</p><p>The remainder of the paper is structured as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec2">2</a> reviews relevant work in the areas of hybrid user interfaces and transitional interfaces. Interaction continuity as a design aspect is discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec3">3</a>. The HybridDesk prototype is introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec6">4</a> as a practical example to analyze the design issues presented. The issues related to the conception of transitional interfaces within HybridDesk are presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec10">5</a>. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec11">6</a> reviews an exploratory evaluation carried out with users in order to obtain content to analyze the interaction. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec14">7</a> presents conclusions and discusses future directions of this work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Interface systems that attempt to combine hardware and software components of different natures are sometimes called hybrid user interfaces, mixed reality systems, or, until recently, hybrid display systems. They are characterized by the use of multiple elements, such as multiple devices or multiple user interfaces. The term HUI was first proposed by Feiner and Shamash (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Feiner S, Shamash A (1991) Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers. In: ACM UIST 91, ACM, pp 9–17" href="/article/10.1007/s10055-011-0205-y#ref-CR13" id="ref-link-section-d25697e492">1991</a>) with reference to a heterogeneous environment, rich in interaction techniques, and with different kinds of devices used in a complementary way.</p><p>A pioneer work in this area was the Office of the Future (Rascar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Rascar R, Welch G, Cutts M, et al (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: SIGGRAPH, ACM, pp 179–188" href="/article/10.1007/s10055-011-0205-y#ref-CR27" id="ref-link-section-d25697e498">1998</a>), which combined several computer vision and computer graphics techniques to analyze surfaces of the real world and then project virtual information on them.</p><p>Rekimoto and Saitoh (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: proceedings of CHI’99, ACM, pp 378–385" href="/article/10.1007/s10055-011-0205-y#ref-CR30" id="ref-link-section-d25697e504">1999</a>) explored the heterogeneity of HUIs using several computers and displays (projections and notebooks) in the same work environment. The EMMIE project (Butz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Butz A, Hollerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: IWAR 99, pp 35–44" href="/article/10.1007/s10055-011-0205-y#ref-CR7" id="ref-link-section-d25697e507">1999</a>) developed a system designed for collaboration that was similar to the work by Rekimoto but included the use of augmented reality (AR) with a see-through HMD (head-mounted display).</p><p>Nakashima et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakashima K, Machida T, Kiyokawa K, Takamura H (2005) A 2D–3D integrated environment for cooperative work. In: symposium on virtual reality software and technology, ACM, pp 16–22" href="/article/10.1007/s10055-011-0205-y#ref-CR22" id="ref-link-section-d25697e513">2005</a>) presented a prototype of a collaborative work environment with 2D and 3D environments for graphic modeling tasks. The environment uses a display called IllusionHole for 3D interactions, while 2D interactions are supported by a projection. The lack of transitions between those environments could be explained by the fact that they are involved in the same 3D task, using different interactions techniques. This same explanation could be used for an HUI for the manipulation of medical data, also using 2D and 3D interactions, which was developed by Bornik et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of volumetric medical data. In: IEEE symposium on 3D user interfaces, IEEE" href="/article/10.1007/s10055-011-0205-y#ref-CR5" id="ref-link-section-d25697e516">2006</a>): a single 3D pointer is used as interaction tool, and two visualization forms are available, one on a tablet and another on a projection.</p><p>Benko et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Benko H, Ishak E, Feiner S (2005) Cross-dimensional gestural interaction techniques for hybrid immersive environments. In: Virtual Reality 2005, IEEE, pp 209–216" href="/article/10.1007/s10055-011-0205-y#ref-CR3" id="ref-link-section-d25697e523">2005</a>) created a hybrid environment composed of an LCD (liquid crystal display) placed vertically, a touchable display placed horizontally, and a see-through HMD for AR. This environment is used for manipulating archeological objects. This work is interesting because it included the development of visual transitions between the visualization modes on each display, i.e., the graphics became 3D from the 2D projection on the screen.</p><p>Baumgartner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Baumgartner S, Ebert A, Deller M (2007) Dimensional congruence for interactive visual data mining and knowledge discovery. In: EuroVis, Eurographics Association, pp 99–106" href="/article/10.1007/s10055-011-0205-y#ref-CR2" id="ref-link-section-d25697e529">2007</a>) developed an HUI to explore the organization of a desktop, disposing documents spatially. The organization of documents is done through gestures, using a glove. The documents in the 3D space are visualized in an autostereoscopic display and edited with the keyboard and a pen on a tablet below the display.</p><p>In general, the above-mentioned works sought to blend different technologies in order to execute certain tasks. However, there is the need for a criterion, or at least a reference to a given methodology or concept, behind this “technological mixture”. Moreover, there seems to be no explicit concern about possible transitions among the integrated technologies.</p><p>Although the concept of transitional interfaces was coined a decade ago in the MagicBook project (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Billinghurst M, Kato H, Poupyrev I (1999) The magicbook—moving seamlessly between reality and virtuality. In: IWAR 99, pp 35–44" href="/article/10.1007/s10055-011-0205-y#ref-CR4" id="ref-link-section-d25697e537">1999</a>), until now few works have been published on this topic. SeamlessDesign (Kiyokawa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kiyokawa K, Takemura H, Yokoya N (2000) SeamlessDesign for 3D object creation. IEEE Multimedia IEEE 7(1):22–33" href="/article/10.1007/s10055-011-0205-y#ref-CR18" id="ref-link-section-d25697e540">2000</a>) was a demonstration contemporary to MagicBook. It was a collaborative virtual workspace for rapid prototyping, which supported both virtual and augmented reality environments using optical see-through head-mounted displays. Augmented reality was used for face-to-face collaboration, and virtual reality to provide multiple perspectives. However, these pioneer works did not investigate the effects of those seamless transitions along the mixed reality continuum. Grasset et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Grasset R, Looser J, Billinghurst M (2006) Transitional interface: concept, issues and framework. In: ISMAR, IEEE, pp 231–232" href="/article/10.1007/s10055-011-0205-y#ref-CR14" id="ref-link-section-d25697e543">2006</a>) proposed an initial attempt to formalize this concept and published the first work (Grasset et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Grasset R, Duenser A, Billinghurst M (2008) Moving between contexts—a user evaluation of a transitional interface. In: ICAT 2008: 18th international conference on artificial reality and telexistence, Keio University, Yokohama" href="/article/10.1007/s10055-011-0205-y#ref-CR15" id="ref-link-section-d25697e546">2008</a>) reporting some evaluation studies in this field. Usability, performance, presence, and awareness were assessed in that work. The evaluation scenario was composed by a series of navigation tasks that forced the subjects to move across two interactive environments: virtual reality and augmented reality. However, most of the results from this work reported failures in the navigation techniques used in each environment, as well as disorientation problems at the exit and entry points of the environments before and after the transitions. The authors said that such problems would be fixed using stereo vision and some sort of visual feedback.</p><p>The design and evaluation of transitional interfaces is still an open problem. There are plenty of issues to be addressed, such as the transition between interactive environments with different hardware and software technologies. While Grasset’s work reported results in a scenario with a homogeneous hardware setup (HMD and 3D input device) across the two environments, our work deals with explicit hardware and software transition, taking some properties into account to guide the design and evaluation of this case.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Transition design based on interaction continuity</h2><div class="c-article-section__content" id="Sec3-content"><p>In this section we briefly define continuous interaction based on the perceptive, cognitive, and functional continuity properties, followed by the proposed design approach to reach smooth transitions in HUIs.</p><h3 class="c-article__sub-heading" id="Sec4">What is interaction continuity?</h3><p>An interactive environment is assumed to be the complete presentation environment required for carrying out a particular interactive task. The interactive environment contains representations of the visual, haptic, and auditory elements a user interface offers its users, as well as their relationships. In more complex environments, the user's task can be distributed over various interactive environments, and discontinuity during transitions can become a problem. The idea of continuity is related to the process of avoiding breaks in the interaction, which could result in disorientation and failure.</p><p>The importance of continuity in the development of real-time collaboration systems was mentioned by Ishii et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8):83–97" href="/article/10.1007/s10055-011-0205-y#ref-CR17" id="ref-link-section-d25697e567">1994</a>). They defined a seam as a spatial, temporal, or functional constraint that forces the user to shift among a variety of spaces or modes of operation. For example, the seam between word processing using a computer and traditional pen and paper makes it difficult to produce digital copies of handwritten documents without a translation step. All authors have agreed that systems that ask users to abandon their acquired skills and to learn a new protocol are likely to encounter strong resistance.</p><p>Dubois et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Univers Access Inf Soc 1(4):263–273" href="/article/10.1007/s10055-011-0205-y#ref-CR12" id="ref-link-section-d25697e573">2002</a>) consider continuity at the perceptual and cognitive levels. Perceptual continuity is present if the user perceives the different representations of a given entity directly and smoothly. Cognitive continuity is present if the cognitive processes involved in the interpretation of the different representations perceived are similar.</p><p>Here we consider continuity as the capability of the system to promote a smooth interaction with the user during task accomplishment considering perceptual, cognitive, and functional aspects (Trevisan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Trevisan DG (2004) Designing smooth connections between worlds. In: extended abstracts of the 2004 conference on human factors and computing systems CHI2004, Session doctoral consortium, ACM Press, pp 1043–1044" href="/article/10.1007/s10055-011-0205-y#ref-CR33" id="ref-link-section-d25697e579">2004</a>; Trevisan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Trevisan DG, Vanderdonckt J, Macq B (2004) Conceptualizing mixed spaces of interaction for designing continuous interaction. Virtual Real J 8(2):83–95" href="/article/10.1007/s10055-011-0205-y#ref-CR34" id="ref-link-section-d25697e582">2004</a>). Perceptual continuity is defined as the ability of the system to provide information to the user in one perceptual environment [e.g., when the user is wearing a see-through head-mounted display (HMD)]. Cognitive continuity is defined as the ability of the system to ensure that the user will interpret the perceived information correctly and that the perceived information is accurate with regard to the internal state of the system (e.g., by using similar representations of the real and virtual objects). Functional continuity is defined as the degree of adaptability of the user in relation to changing and learning new modes of interaction. Consequently, the functional property is related to the interaction technique used.</p><p>All these definitions point out the need for smooth transitions between different operation modes in order to avoid frustration during the accomplishment of complex tasks.</p><h3 class="c-article__sub-heading" id="Sec5">Design approach</h3><p>When there are multiple sources of information and two or more interactive environments, we must make choices about what to attend to and when. At times, we need to focus our attention exclusively on a single item without interference from other items. At other times, we may need to time-share or divide our attention between two or more items of interest, which can be part of the same or different interactive environments.</p><p>For example, assuming real and virtual worlds as two different environments, in the Museum project (Rekimoto and Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: proceedings of the ACM symposium on user interface software and technology, ACM, pp 29–36" href="/article/10.1007/s10055-011-0205-y#ref-CR29" id="ref-link-section-d25697e598">1995</a>) the user wears a see-through HMD in which information about an exhibit is displayed. The user is thus able to perceive real objects with added synthetic information. The object of this task is the painting being shown, therefore the focus of the task is either in the virtual world or in the real world.</p><p>The user could be performing a task in order to manipulate or modify an object in one or more interactive environments. Considering all the possibilities of interaction focus while the user is performing a specific task, we have identified three possible combinations:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Interaction focus in one interactive environment without shared attention. In this type of interaction, the user’s attention is focused on a single object in the environment and there is no other object competing for it.</p>
                    </li>
                    <li>
                      <p>Interaction focus shared in one interactive environment. In this case the interaction focus is shared between two or more objects of the same environment.</p>
                    </li>
                    <li>
                      <p>Interaction focus shared between interactive environments. Here the interaction focus is shared between objects belonging to different environments.</p>
                    </li>
                  </ul>
                <p>Regarding the last item, where the interaction focus can be shared between different environments, we must define what will be the insertion context of the interactive environments. Here we are interested in defining the surrounding space according to the user’s focus while performing a task. An interactive environment can be viewed using any device (screen, HMD, etc.) or any physical object (projected on a table, wall, etc.). Many disciplines can provide appropriate figures for physical properties. For instance, perceptual psychology may inform us to what extent a person considers an object distant or close. Sociology interprets this as four spatial zones depending on the distance from the subject.</p><p>Here we are following the four spatial zones proposed by Hall (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1969" title="Hall ET (1969) The hidden dimension. Anchor Books" href="/article/10.1007/s10055-011-0205-y#ref-CR16" id="ref-link-section-d25697e628">1969</a>) for describing interactive environments, depending on the level of periphery (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig2">2</a>):
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Central zone: corresponds to an insertion distance of 0–45 cm from the user.</p>
                    </li>
                    <li>
                      <p>Personal zone: corresponds to an insertion distance of 46 cm–1.2 m from the user.</p>
                    </li>
                    <li>
                      <p>Social zone: corresponds to an insertion distance of 1.3–3.6 m from the user.</p>
                    </li>
                    <li>
                      <p>Public zone: corresponds to an insertion distance greater than 3.6 m from the user.</p>
                    </li>
                  </ul>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Insertion zones according to level of periphery</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>If the interactive environment is inserted in the central zone of the user’s task, she/he does not need to change her/his attention focus to perform the task. If the user’s attention focus changes frequently, then it is probable that the interactive environment has been inserted outside the central zone, in a peripheral context.</p><p>In our design approach for the proposed HybridDesk System, we considered transitions between two spatial zones of display insertion: personal and central. As will be described in the next section, the navigation environment is placed on the personal insertion zone, while the manipulation and WIMP environments are placed on the central zone.</p><p>In the HUI/transition approach, we suggest, as a guideline, that the design should first consider the needs of the main task that the user intends to address in terms of interface. At this stage, it is worth thinking about the need for interfaces with different paradigms, such as virtual reality, augmented reality, WIMP, etc., and whether it is useful to involve significant cognitive effort in the transitions between the different interfaces.</p><p>Once the interfaces have been defined, the following step focuses on the definition of input devices, displays, and interaction techniques for each interface. This definition involves the analysis of perceptual and functional properties of transitions. The more different the devices and displays of each interface are, the greater the discontinuity between them will be. This decision should take into account a tradeoff between the advantage of having different interactive environments and the effects of discontinuities caused by transitions between them. On the one hand, the use of different interactive environments that demand frequent transitions with any kind of discontinuity may be exhausting, and we may need to consider the use of some features during a transition to smooth any discontinuities. On the other hand, the imposition of a technology to avoid a discontinuity during a transition may result in the poorer performance of a task in an interactive environment.</p><p>The notion of interplay among the three continuity properties is important to handle transition discontinuities. As we will show in this work, the enhancement of a continuity property against a discontinuity of another one may contribute to smooth the transition between interactive environments.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Conception of the HybridDesk system</h2><div class="c-article-section__content" id="Sec6-content"><p>The design of HybridDesk (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig3">3</a>) was based on the requirements determined for a 3D annotation task in an oil and gas application (Carvalho et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Carvalho F, Raposo A, Gattass M (2009) A transitional interface between 2D/3D functional spaces in a desktop semi-immersive system. In: VRCAI 2009 The 8th international ACM conference on virtual-reality continuum and its applications in industry, ACM" href="/article/10.1007/s10055-011-0205-y#ref-CR8" id="ref-link-section-d25697e697">2009</a>). These requirements helped to identify the technological setup used to compose the interactive environments within HybridDesk. This setup should be able to address the tasks performed in a semi-immersive workplace with stereoscopy, head tracking, and a wand, as well as to support a WIMP interface.
<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>
                </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>HybridDesk scheme with the three interactive environments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec7">3D annotation task</h3><p>The 3D annotation task was considered important because it could be decomposed into different subtasks ranging from symbolic input information to spatial input used for 3D interaction techniques. The 3D annotation task is a way to insert information into a virtual environment, and often this information is related to a particular object. To handle the creation and management of symbolic information, we have chosen to take advantage of past experiences of users with WIMP interfaces rather than creating a completely new environment. Basically, we want to use all the resources available in a WIMP interface to create files, and “3D shortcuts” (3D Icons) to these files are created inside the virtual environment by the user. The visual feedback of the shortcuts has the shape of a 3D cube (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Attaching the 3D annotation to a virtual environment.<i> Left</i> selecting the annotation.<i> Right</i> attaching an annotation icon</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Annotations are very important in oil and gas applications (Raposo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Raposo A, Santos I, Soares L, Wagner G, Corseuil E, Gattas M (2009) Environ: integrating VR and CAD in engineering projects. In: IEEE computer graphics and applications, IEEE, vol 29, pp 91–95" href="/article/10.1007/s10055-011-0205-y#ref-CR26" id="ref-link-section-d25697e769">2009</a>). For instance, a virtual oil platform is composed by a huge number of different objects with many professionals working on it, and the need to leave various information (audio data, text data, presentations, movies, etc.) attached to some objects is important to convey intentions or observations.</p><p>Regarding 3D interaction aspects related to the 3D annotation task, we have identified three subtasks: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                          <i>3D navigation and 3D selection</i>: to search and select an object or to explore the 3D virtual environment. Models of oil platforms are extremely dense geometric scenarios, with plenty of things to explore. For these subtasks, virtual reality and some associated 3D UI techniques are appropriated interface paradigms.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                          <i>3D manipulation</i>: to manipulate objects through translations and rotations, and to enable the insertion of annotations using 3D Icons. Virtual reality is also a suitable interface paradigm for this subtask.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>
                          <i>Symbolic input</i>: to create and edit textual annotations. To handle the creation and management of symbolic information, we have chosen to use WIMP interfaces.</p>
                      
                    </li>
                  </ol>
                <h3 class="c-article__sub-heading" id="Sec8">Interactive environments</h3><p>To support the subtask requirements and the appropriate interface paradigms described above, we designed three interactive environments for HybridDesk: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                          <i>VR-Nav</i>: some studies have indicated that navigation tasks tend to benefit from displays with larger FOV (field of view) and FOR (field of regard) (Raymaekers et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Raymaekers C, Boeck JD, Weyer TD, Coninx K (2005) The effect of display size on navigation in a virtual environment. In: proceedings international conference on enactive interfaces—enactive ’05" href="/article/10.1007/s10055-011-0205-y#ref-CR28" id="ref-link-section-d25697e841">2005</a>; Czerwinski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Czerwinski M, Tan DS, Robertson GG (2002) Women take a wider view. In: CHI ’02: proceedings of the SIGCHI conference on human factors in computing systems, ACM, New York, pp 195–202" href="/article/10.1007/s10055-011-0205-y#ref-CR9" id="ref-link-section-d25697e844">2002</a>; Tan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Tan DS, Gergle D, Scupelli P, Pausch R (2006) Physically large displays improve performance on spatial tasks. ACM Trans Comput-Hum Interact 13(1):71–99" href="/article/10.1007/s10055-011-0205-y#ref-CR32" id="ref-link-section-d25697e847">2006</a>). Because of the larger visual space of these displays, their properties also provide better support for searches during selection tasks (Steed and Parker, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Steed A, Parker C (2005) Evaluating effectiveness of interaction techniques across immersive virtual environmental systems. Presence Teleoper Virtual Environ 14(5):511–527" href="/article/10.1007/s10055-011-0205-y#ref-CR31" id="ref-link-section-d25697e850">2005</a>; Raja et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Raja D, Bowman D, Lucas J, North C (2004) Exploring the benefits of immersion in abstract information visualization. In: 8th international immersive projection technology workshop" href="/article/10.1007/s10055-011-0205-y#ref-CR25" id="ref-link-section-d25697e854">2004</a>). The hardware setup of VR-Nav is composed of four projection screens that are used simultaneously to provide a broader FOV and a larger physical visualization area near the user. The display resulting from this set of screens was named MiniCave and was inspired by the idea of CAVE adapted to a desk (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig5">5</a>). A wand (a Wiimote tracked by an optical tracker) is used as input device, and a 3D representation for it was created in the virtual environment. This representation is visually similar to the real device and receives the translations and rotations from the tracker device. The ray-casting method was implemented for the selection and “grabbing in the air” techniques (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bowman D, Kruijff E, LaViola J, Poupyrev I (2005) 3D user interfaces—theory and practice. Addison-Wesley" href="/article/10.1007/s10055-011-0205-y#ref-CR6" id="ref-link-section-d25697e860">2005</a>) during the navigation task. According to the level of periphery of the display insertion introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec5">3.2</a>, this environment is placed on the personal zone of user interaction.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                          <i>VR-Manip</i>: for manipulation tasks, we have understood that it is important to perform a detailed visual inspection of a selected object in order to identify particular points to leave annotations. We therefore chose an isomorphic interaction technique
<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> to handle virtual objects at a short distance, like people do with real objects. Local manipulation tasks have shown evidence of being more prone to visual stimuli in a restricted visualization area, which suggests the need for a narrower FOV (Steed and Parker, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Steed A, Parker C (2005) Evaluating effectiveness of interaction techniques across immersive virtual environmental systems. Presence Teleoper Virtual Environ 14(5):511–527" href="/article/10.1007/s10055-011-0205-y#ref-CR31" id="ref-link-section-d25697e892">2005</a>; Demiralp et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Demiralp C, Karelitz DB, Zhang S, Laidlaw DH (2006) Cave and fishtank virtual-reality displays: a qualitative and quantitative comparison. IEEE transactions on visualization and computer graphics 12(3):323–330, member-Cullen D. Jackson" href="/article/10.1007/s10055-011-0205-y#ref-CR10" id="ref-link-section-d25697e895">2006</a>; Prabhat et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Prabhat, Forsberg A, Katzourin M, Wharton K, Slater M (2008) A comparative study of desktop, fishtank, and cave systems for the exploration of volume rendered confocal data sets. IEEE Trans Visual Comput Gr 14(3):551–563" href="/article/10.1007/s10055-011-0205-y#ref-CR24" id="ref-link-section-d25697e898">2008</a>). However, as the focus of attention is more intense in this restricted area, visual cues are more noticeable. Thus, display features such as brightness, resolution, sharpness, and photorealism become important (Polys et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Polys NF, Kim S, Bowman DA (2005) Effects of information layout, screen size, and field of view on user performance in information-rich virtual environments. In: VRST ’05: proceedings of the ACM symposium on virtual reality software and technology, ACM, New York, pp 46–55" href="/article/10.1007/s10055-011-0205-y#ref-CR23" id="ref-link-section-d25697e902">2005</a>). The inspiration for the technological setup came from the idea of a Reachin display which allows the manipulation of virtual objects within the reachable space of the arm. Most Reachin displays use a mirror or a semi-transparent surface to show an illusion of a virtual content behind it. Such illusion is based on the reflection of an image from a CRT or a projection screen. For all of these reasons, we have chosen an LCD for the manipulation task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig6">6</a>). Although a regular LCD is much thicker than a mirror or glass, we believe it provides reasonable access to the space behind it, which is the area of interaction with the virtual objects selected on the VR-Nav environment. Moreover, a regular LCD has all the display features required to facilitate visual inspections. According to the level of periphery of the display insertion described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec5">3.2</a>, this environment is placed on the central zone of user interaction.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>
                          <i>WIMP</i>: basically, we wish to use all the resources available in a WIMP interface to create files, and “3D shortcuts” (3D Icons) to these files are attached to the associated 3D objects by the user in the VR-Manip environment. The visual feedback of the shortcuts has the shape of a 3D cube. We chose Microsoft Windows XP as WIMP interface and, regarding its associated hardware, we maintained the conventional setup, composed by monitor, keyboard, and mouse. According to the level of periphery of the display insertion described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec5">3.2</a>, this environment is placed on the central zone of user interaction.</p>
                      
                    </li>
                  </ol>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>VR-Nav interactive environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>VR-Manip interactive environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig7">7</a> shows a scheme of the events that trigger transitions in HybridDesk. The VR-Nav &gt; VR-Manip transition is triggered by pointing to an object in the virtual environment (using the ray-casting technique) and pressing the button “A” of the wand. After that, the user has to move the LCD to the front of the MiniCave. Pressing the button “Home” of the wand returns from VR-Manip to VR-Nav. The user then has to move the LCD out of the MiniCave.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Trigger events</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The VR-Nav &gt; WIMP transition is triggered by pointing to a 3D icon attached to a virtual object and pressing the button “A” of the wand. To return to VR-Nav it is necessary to use a context menu and select “Back to HybridDesk VR-Nav”.</p><p>The transition from VR-Manip to WIMP is triggered by pressing the button “A” of the wand, and to return from WIMP to VR-Manip the user needs to use a context menu over the selected file and select “Send to HybridDesk VR-Manip”.</p><h3 class="c-article__sub-heading" id="Sec9">Technology setup</h3><p>Instead of using a cluster of PCs and synchronization mechanisms, the semi-immersive visualization using HybridDesk was managed by a single PC with a 3D accelerated video card with two video outputs. HybridDesk needs a minimum of five video outputs: four to projectors and one to the LCD. To handle this configuration we used two video splitters. These splitters are of the TripleHead2Go (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig8">8</a>a) type and divide one video output into three. Basically each splitter has a driver which increases the resolution of the desktop over a video output, and then the hardware splits it. For example, a video output that would normally have a resolution of 1,024 × 768 pixels can be transformed by this driver into a 3,078 × 768 output and then be divided into three 1,024 × 768 outputs. Thus, an extended desktop of six video outputs was created, three of 800 × 600 (2,400 × 600) and the three of 1,024 × 768 (3,072 × 768) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig8">8</a>c). Together, these outputs formed a large desktop display over the projectors and the LCD (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig8">8</a>b).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>
                          <b>a</b> Two Matrox TripleHead2Go splitters to extend <b>b</b> the desktop through all the screens <b>c</b> using only one graphics board with two video outputs</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The three-dimensional effect was achieved using a stereoscopic anaglyph on simple projection screens. Since the stereoscopic image was generated by software and only one resulting image was displayed for each screen, there was no need to use special screens or advanced projectors. The video card used was NVIDIA GeForce 8800 GT, a conventional accelerator board.</p><p>Using a Wiimote (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig10">10</a>b) as a wand was a reasonable choice, since it is cheap and offers a variety of events without the need for wires, as all data communication is done via Bluetooth. HybridDesk also used an optical tracking system called Bratrack (<a href="http://www.bratrack.com">http://www.bratrack.com</a>) to estimate the translations and rotations applied to the navigation and manipulation tasks, as well as for head tracking (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig9">9</a>). Track points were attached to the Wiimote and to the stereoscopic glasses (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Optical tracker composed by two infrared cameras</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>
                          <b>a</b> Anaglyphic stereo glasses and<b> b</b> Wiimote used as wand</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>To generate and manage 3D graphics, a system was implemented using the OpenSceneGraph library (<a href="http://www.openscenegraph.org">http://www.openscenegraph.org</a>), which is based on the idea of scene graphs. The system rendered and managed six virtual cameras: four for the MiniCave (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig11">11</a>a, b), one for the LCD, and an additional camera for an overview of the position of the observer within the virtual environment (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig11">11</a>c, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig12">12</a>h). In order to create the anaglyph stereoscopic images, 10 images per frame were computed to generate the left and right eyes for each virtual camera (the extra camera did not use stereoscopy).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>
                          <b>a</b>,<b> b</b> Set of virtual cameras used in the MiniCave. <b>c</b> Additional camera to provide an overview of the user’s position inside the virtual world</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Screenshot of the extended desktop showing the six windows for the screens:<b> a</b>
                          <i> left</i>,<b> b</b>
                          <i> front</i>,<b> c</b>
                          <i> right</i>,<b> f</b> LCD,<b> g</b>
                          <i> bottom</i>, and<b> h</b> overview. In<b> d</b> and<b> e</b> there are empty spaces generated by the splitter driver to compose a final<i> rectangular</i> image for the screenshot</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For each virtual camera, the system created a window on the desktop so that each window occupied an area corresponding to a video output. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig12">12</a> there is a screenshot showing the whole desktop.</p><p>The MiniCave screens were built using glass boards for the three vertical screens and an acrylic sheet for the table top. Projection screens were placed inside the MiniCave over the glass and acrylic surfaces. A set of mirrors was used to reduce the distance between each projector and the corresponding screens (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig13">13</a>). For this, wooden frames containing a mirror were constructed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig13">13</a>a). Keystone correction was used on some projectors depending on their position and orientation adjustments. See for instance the inclination of the projector in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig13">13</a>c.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>
                          <b>a</b> Mirrors in the MiniCave. Projector/mirror pairs for the screens:<b> b</b>
                          <i> front</i>,<b> c</b> under the table, and<b> d</b> sides.<b> e</b> Overview of the HybridDesk structure during its development</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Conception of transitional interfaces</h2><div class="c-article-section__content" id="Sec10-content"><p>Once the user’s interaction focus during the execution of the 3D annotation task is shared between objects belonging to three different environments, this section describes our design choices based on the previous interactive environments to analyze the continuity properties (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec3">3</a>) and to implement features to smooth identified discontinuities in transitions.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig14">14</a>, there is a scheme of devices and displays used in each interactive environment as well as the continuity issues during the transitions between them. Care was taken in these transitions to maintain at least one continuity property between the environments. In the VR-Nav to VR-Manip transitions and vice-versa, we maintained cognitive and functional continuity, since the representation of objects is the same as well as the input device. In the transitions both from VR-Nav and VR-Manip to WIMP, there is a functional discontinuity, since the user changes the input devices, from the wand to mouse/keyboard. In these transitions, we maintained cognitive continuity by associating two similar representations (2D Icon in a WIMP file manager and 3D Icon in virtual environment) of the same concept (data file).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Continuity properties for the HybridDesk prototype</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig15">15</a>, we illustrate the devices and displays used in each interactive environment, as well as the changes of input devices (functional discontinuities) and displays (perceptual discontinuities) required during transitions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Devices used in each environment and the necessary changes during the transitions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>There is an important event during the transition between VR-Nav and VR-Manip that is related to a display change (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig16">16</a>c, d). Looking into the continuity properties, such event could be classified as a perceptual discontinuity. We can expected a potential perceptual discontinuity between VR-Nav and WIMP because these environments use distinct displays, MiniCave and LCD, which are inserted in different insertion zones of user interaction (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec5">3.2</a>): personal and central zones, respectively. On the other hand, a potential continuity could be expected from VR-Manip and WIMP transitions once booth interactive environments are using the same display (LCD) placed on the same central insertion zone.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig16_HTML.jpg?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0205-y/MediaObjects/10055_2011_205_Fig16_HTML.jpg" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>
                        <b>a</b>, <b> b</b> Visual signs used during the transitions between the VR-Nav and the VR-Manip environments,<b> c</b>, <b>d</b> change of devices and displays during transitions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0205-y/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Since we opted to use different display that is beneficial for each subtask (manipulation and navigation), we decided to handle such perceptual discontinuity by exploring the cognitive and functional properties. For the cognitive property, we have implemented an animation mechanism as a visual sign to guide the user during the transition, showing what is happening and what to do next. This is an attempt to visually explain the transition from one display to another. For instance, when the user selects an object in VR-Nav, an animation starts to move the virtual camera closer to the selected object. During this approximation, another animation (a virtual LCD moving from left to right and a label showing the name of the incoming environment  VR-Manip) appears, with a sign to move the LCD to the front of the MiniCave (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig16">16</a>a). The transition from VR-Manip back to VR-Nav shows an animation moving the camera back to the location where the object was selected before entering VR-Manip, and, at the same time, another animation shows a virtual LCD moving from right to left indicating the change of displays again (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig16">16</a>b). For the functional continuity property, we also choose interactive techniques with certain similarities in both environments, since VR-Nav and VR-Manip are 3D interactive environments and share the same input device (wand). The technique for locomotion in VR-Nav and the one used in VR-Manip are almost identical because the commands and spatial movements to move the camera in VR-Nav are the same ones to move the selected object behind the LCD in VR-Manip.</p><p>The transitions between the 3D environments (VR-Nav and VR-Manip) and the WIMP interface could be considered abrupt because there is a change in the interaction paradigm, from 3D to 2D. One of the main reasons for such abrupt change is the presence of functional discontinuities along the transitions. These discontinuities occur due to both the change of input devices (mouse/keyboard - wand) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig16">16</a>c, d) and the difference in the nature of the environments in which the devices are used. The continuity we established during the transitions from WIMP to other environments was based on cognition, using interpretation and inferences. For instance, when the user arrives in the WIMP environment from VR-Manip and chooses a file inside the file manager, and opens a context menu, and the command to send the file to VR-Manip is activated (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0205-y#Fig7">7</a>), then VR-Manip reappears, but with a 3D Icon. We believe this visual difference in VR-Manip before and after using the WIMP interface (without the 3D Icon before, and with the 3D Icon after using it) helps the user to infer that this 3D Icon is a shortcut to the selected file in the previous environment. When the user performs the transition from VR-Nav to WIMP by pointing and clicking on a 3D Icon attached to an object in the virtual environment, the WIMP interface appears on the LCD with an open file manager and a selected file. We also believe that such a transition suggests an interpretative link between the environments (cognitive property), i.e., something selected in VR-Nav followed by the appearance of something selected in WIMP can be interpreted as the same object (two representations of the same concept).</p><p>The following section verifies, through an exploratory study, whether such design choices were properly perceived by users.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Exploratory study</h2><div class="c-article-section__content" id="Sec11-content"><p>This section aims to report an exploratory study on the interaction and behavior of some subjects during the transitions between the three environments designed for HybridDesk: VR-Nav, VR-Manip, and WIMP.</p><p>The methodology considers the study of the interactions during the transitions in accordance with the perceptual, cognitive, and functional continuity properties. It also provides data on the effectiveness of the system according to the subjects.</p><p>As we mentioned previously, HybridDesk was developed following the requirements of a 3D annotation task. In order to validate this task, an application prototype consisting of a virtual oil and gas platform was implemented. Thus, the task of annotation might be interpreted, for example, as part of a collaborative task management of CAD modeling of an oil and gas platform.</p><p>Conducted using the talk-aloud approach, this test sought to assess the usability of the transitions and tasks. Also, an analysis of the continuity properties in the transitions/tasks was carried out to evaluate the design decisions, and some guidelines were suggested.</p><h3 class="c-article__sub-heading" id="Sec12">Subjects and tasks</h3><p>This test was conducted with six subjects, all of which had some experience with 3D applications. Instead of receiving an oral explanation of the application scenario (oil and gas virtual scenario), the subjects were given the following written text:</p><p>“You are an experienced user in 3D model graphic visualization but have never used HybridDesk. You were hired by an oil company to verify the modeling quality of 3D objects developed by another group within the company. You will use HybridDesk on a daily basis to perform these verifications and leave annotations with your assessment attached to the verified objects. Your manager left instructions regarding your task in the 3D scene: an annotation file is attached to the top of the staircase at the right side of the heliport of the oil platform. You must navigate at the virtual oil platform using HybridDesk, find the annotation file, and follow the instructions your manager left in this file”.</p><p>The annotation file was a shortcut to a text file located in the file manager. This text file contained the following instructions:</p><p>“Go to VR-Nav, find a crane that has a fissure on its tip, and create an annotation near the fissure to indicate this problem”.</p><p>A training session with each subject was conducted for 10 min.</p><h3 class="c-article__sub-heading" id="Sec13">User test: results of the users’ observations with the talk-aloud approach</h3><p>In order to estimate how the continuity was during the transition interactions, we classified the evaluation issues into three categories, following the continuity properties: cognitive, perceptual, and functional (see Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec3">3</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec10">5</a>). The average user session took 11 min.</p><p>As per Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0205-y#Tab1">1</a>, a large number of usability issues (36) were identified from the talk-aloud sessions. After analyzing the nature of these usability problems, we divided them into two categories: tasks within each environment and transition interactions. Then, for each category, we analyzed the continuity properties: cognitive, perceptual, and functional (see Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec3">3</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0205-y#Sec10">5</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Issues during user observation sessions on test 2 and their classification based on the continuity properties</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-011-0205-y/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The cognitive property of the tasks is related to two problems: users’ disorientation and the unpredicted use of existing signification systems. Signification is the process through which certain systems of signs are established by virtue of social and cultural conventions adopted by the users (De Souza <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="De Souza CS (2005) The semiotic engineering of human-computer interaction. MIT Press, Cambridge" href="/article/10.1007/s10055-011-0205-y#ref-CR11" id="ref-link-section-d25697e2020">2005</a>). In many instances the cause of the problem lay in the users’ interpretation of the meaning of the task, as defined by the task manager, represented by the evaluator of the test (items 12 to 19). Three out of the six participants went initially to the wrong staircase (items 13 and 14) because they did not understand the task correctly, which told them to look for the annotation at the “... staircase to the left of the heliport”. This kind of issue raises various questions, particularly concerning the real cause of this misinterpretation and how to avoid it. One difficulty is that the users are exposed to different signification systems, namely the textual task instruction and the 3D scene, where the staircase had only a visual representation, providing no orientation hint that defines what is left and right. The participants also had some difficulty in interpreting what the actual meaning of “tip of the crane” in the instructions was. Some confused it with the tip of the crane tower, while others confused it with the hook hanging from the crane (items 16, 17, and 18). Regarding users’ disorientation, the lack of marks or maps in the 3D scene was responsible for some cognitive problems during navigation (items 12 and 15).</p><p>The functional property of the tasks is related to execution problems while performing a specific task within a specific environment (items 25–31). One of the issues with greater execution incidence (item 25–4 out of 6 people) was the difficulty in traveling toward the 3D scene from a distant point of view. The users knew what to look for but did not know how to find their way in the 3D scene. This is the reason for the issues identified, although they occurred in the navigation environment (VR-Nav). These subjects mentioned that when an object was far from the observer, the Icon attached to it was too small to be perceived, and when the object was near the Icon, it become too big. In conventional WIMP interfaces, the Icons remain the same size as users browse through the folders of the system, unless they intentionally change the folder configuration options. This difference in Icon sizes reduces the cognitive continuity between the WIMP and 3D environments. It is a scale problem, for which the designer could find a solution. Another difficulty found by users was selecting very small objects, in particular the 3D Icon. Any small arm movement deviates the selection from the intended object to another one.</p><p>The perceptual property of the tasks is related to problems indicating the need to improve the system feedback (items 3–7). The lack of visual feedback when the 3D Icon is attached to an object also caused problems to some participants.</p><p>The cognitive continuity analysis of the transitions is mainly related to the memorization of the commands (items 8–11). The “Home” button on the wand had different meanings, depending on the context, also causing confusion to some participants.</p><p>The perceptual continuity analysis of the transitions is mainly related to the different insertion zones of the displays (items 1 and 2). In order to keep the user interaction focus on his/her task, two perceptive environments were adopted. The environment located in the central zone was used for manipulation and textual tasks, while the one placed on the personal zone was used for navigational tasks. Such design choice generated a perceptive discontinuity in order to support the most adequate environment for each user’s task. Participants had difficulty noticing the result of their actions. Although the system provided visual animations as feedback of transitions, the involuntary environment transitions were not clearly perceived. One participant who did not notice the transition to the manipulation environment and continued interacting with the system using the wand, as if he/she were still in the navigation environment. The fact that the navigation screens of the MiniCave remained apparently active when in the manipulation environment misled this participant.</p><p>The functional continuity analysis of the transitions is mainly related to the execution problems during the transitions. Part of the execution problems was caused by unintentional use of wand commands (items 20–24), and part of these problems was ergonomic (items 32–36), such as fatigued arms, discomfort with the glasses, and LCD manipulation. Two participants chose not to slide the LCD toward the desk center for object manipulation and kept it on the left side. On the other hand, a left-handed participant felt the need to move the LCD to the desk center, with some difficulty, so that he/she could manipulate the object in the LCD with the wand in his/her left hand.</p><p>Despite the short duration of the user interaction during the test session, it was possible to notice the abovementioned ergonomic problems. We believe that such problems might be more evident over a longer period of interaction with HybridDesk. This fact can be reinforced by the informal reports from the HybridDesk developers, who actually spent a lot of time using the system during its development. They reported an important point related to the manipulation of a very detailed object. For this task, users have to perform many hand movements (mostly rotations) to be able to see the chosen object from different angles. This process becomes really tiring because, besides doing these movements, users have to keep an arm suspended for a long time. The problems related to the memorization of commands, in turn, might decrease with the prolonged use of HybridDesk.</p><p>At the moment, the HybridDesk is not used as a daily desktop. It has been used mainly as an immersive environment, like a CAVE or similar setups. Therefore, long-term use issues such as arm fatigue are not prominent. Moreover, users tend to spend much more time in the VR-Nav environment, which is less prone to fatigue than VR-Manip, than in the other environments. We believe that this preference for VR-Nav happens because navigation tasks are more used than manipulation ones and the VR-Nav environment provides a better immersion feeling.</p><p>A detailed user evaluation study of the HybridDesk is presented in (Alencar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Alencar MFC, Raposo AB, Barbosa SDJ (2011) Composition of HCI evaluation methods for hybrid virtual environments. In: proceedings of the 2011 ACM symposium on applied computing, pp 1237–1244" href="/article/10.1007/s10055-011-0205-y#ref-CR1" id="ref-link-section-d25697e2042">2011</a>). This study performed a qualitative evaluation of the HybridDesk by applying traditional usability evaluation methods, like heuristic evaluation, usage observation sessions, questionnaires, and interviews. The study revealed that the main issues were found within the different environments, and not in the transitions themselves, indicating the importance of additional information to facilitate the users’ perception of the application designers’ intentions (for example, visual and audio feedback).</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Conclusion</h2><div class="c-article-section__content" id="Sec14-content"><p>This work introduced a discussion toward the design of transitional interfaces and pointed out the importance of these interfaces for both hybrid user interfaces and a broader range of interfaces with distinct paradigms and technological setups.</p><p>We also discussed issues related to the interaction continuity (perceptual, cognitive, and functional properties) that transitional interfaces should be able to address. Guidelines were suggested, with some steps of the design phase using these continuity properties. These guidelines aim to define interactive environments composed by an interface (of mixed virtuality or mixed reality continuum), input devices, interaction techniques, and displays. Initially, the guidelines propose a task analysis to identify any possible subtasks with different characteristics, and, after that, the definition of interface paradigms for each subtask considering cognitive properties during the transitions between them. The next step focuses on the definition of the input devices, displays, and interaction techniques for each interface. This definition involves the analysis of perceptual and functional properties for transitions. The more different the devices and displays of each interface are, the larger the discontinuity between them will be.</p><p>Following the guidelines, a semi-immersive system composed of three interactive environments was developed: HybridDesk. Then, the transitions between the environments were explored in a 3D annotation task in an oil and gas scenario application.</p><p>These transitions were analyzed by means of an exploratory study with six subjects. We applied a talk-aloud evaluation to capture the subjects’ reactions concerning the design decisions based on the continuity properties. The results indicate a major incidence of functional problems (7 occurrences), followed by cognitive issues (4 occurrences), and finally perceptual ones (3 occurrences).</p><p>These occurrences brought the evaluator’s attention to the fact that the user interaction with HybridDesk actually involves at least three distinct signification systems: “the system” (wand commanding), “the content” (3D scene), and “the task” (textual instructions). Different people produced each of these signification systems: the system designer, the 3D modeler, and the task manager. In standard desktop applications, this interaction inconsistency is not as visible and critical.</p><p>Finally, we conclude that, to date, there is no theory to guide either a systematic design of transitions between interfaces in forthcoming hybrid user interfaces, or their treatment. Although our user test results are still preliminary, this work shows that carrying out a detailed analysis of the requirements of a task and relating them to continuity properties constitute a significant initial approach in the construction of a solid basis for the design of transitional interfaces for hybrid user interfaces. It is important to remember that the evaluation performed in this study focused on transition interactions, and designers can adopt such approach as early as in the design phase of any application involving transitions between contexts in order to minimize potential discontinuities.</p><p>We created some practical guidelines based on our experiments and the issues related to the interaction continuity (perceptual, cognitive, and functional properties) that transitional interfaces should be able to address: </p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Use visual feedback to make users aware of the system state. Improving this feedback with other modalities (such as audio) could be a good choice. This is a very important issue, especially during transitions between interfaces.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Decrease the cognitive load during the use of devices by mapping similar commands to similar physical components of the devices.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Look for devices that could replace or complement the interaction using 6DOF devices. Sometimes, users can get tired of using such devices and not finish the task.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>Analyze the workspace infrastructure and its relation with the interaction devices. For example, the desk in front of the MiniCave provides adequate space for the mouse, but hinders 3D manipulations.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">5.</span>
                    
                      <p>Consider ergonomic aspects while designing the interaction, such as handedness (are users right- or left-handed?), height (are they tall or short?), and users’ position (are they supposed to be seated or standing?).</p>
                    
                  </li>
                </ol>
              <p>As future work, we plan to use a tracked mouse as a universal mouse-pointer to handle both 3D and 2D interactions. This will reduce the functional discontinuity in the transitions between VR-Nav/VR-Manip and WIMP. Besides, we plan to detail a formal modeling of transitional interfaces to encompass the requirements of hybrid user interfaces.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>A video of the system and user interactions is available as supplementary material and at the web address: <a href="http://www.youtube.com/watch?v=Gd4jRlIKJSM">http://www.youtube.com/watch?v=Gd4jRlIKJSM</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>
                              <i>The isomorphic interaction technique</i> suggests “a strict, geometrical, one-to-one correspondence between hand motions in the physical and virtual worlds...” (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bowman D, Kruijff E, LaViola J, Poupyrev I (2005) 3D user interfaces—theory and practice. Addison-Wesley" href="/article/10.1007/s10055-011-0205-y#ref-CR6" id="ref-link-section-d25697e887">2005</a>).</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Alencar MFC, Raposo AB, Barbosa SDJ (2011) Composition of HCI evaluation methods for hybrid virtual environmen" /><p class="c-article-references__text" id="ref-CR1">Alencar MFC, Raposo AB, Barbosa SDJ (2011) Composition of HCI evaluation methods for hybrid virtual environments. In: proceedings of the 2011 ACM symposium on applied computing, pp 1237–1244</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baumgartner S, Ebert A, Deller M (2007) Dimensional congruence for interactive visual data mining and knowledg" /><p class="c-article-references__text" id="ref-CR2">Baumgartner S, Ebert A, Deller M (2007) Dimensional congruence for interactive visual data mining and knowledge discovery. In: EuroVis, Eurographics Association, pp 99–106</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benko H, Ishak E, Feiner S (2005) Cross-dimensional gestural interaction techniques for hybrid immersive envir" /><p class="c-article-references__text" id="ref-CR3">Benko H, Ishak E, Feiner S (2005) Cross-dimensional gestural interaction techniques for hybrid immersive environments. In: Virtual Reality 2005, IEEE, pp 209–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Kato H, Poupyrev I (1999) The magicbook—moving seamlessly between reality and virtuality. In: " /><p class="c-article-references__text" id="ref-CR4">Billinghurst M, Kato H, Poupyrev I (1999) The magicbook—moving seamlessly between reality and virtuality. In: IWAR 99, pp 35–44</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of " /><p class="c-article-references__text" id="ref-CR5">Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of volumetric medical data. In: IEEE symposium on 3D user interfaces, IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Kruijff E, LaViola J, Poupyrev I (2005) 3D user interfaces—theory and practice. Addison-Wesley" /><p class="c-article-references__text" id="ref-CR6">Bowman D, Kruijff E, LaViola J, Poupyrev I (2005) 3D user interfaces—theory and practice. Addison-Wesley</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Butz A, Hollerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: IW" /><p class="c-article-references__text" id="ref-CR7">Butz A, Hollerer T et al (1999) Enveloping users and computers in a collaborative 3D augmented reality. In: IWAR 99, pp 35–44</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Carvalho F, Raposo A, Gattass M (2009) A transitional interface between 2D/3D functional spaces in a desktop s" /><p class="c-article-references__text" id="ref-CR8">Carvalho F, Raposo A, Gattass M (2009) A transitional interface between 2D/3D functional spaces in a desktop semi-immersive system. In: VRCAI 2009 The 8th international ACM conference on virtual-reality continuum and its applications in industry, ACM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Czerwinski M, Tan DS, Robertson GG (2002) Women take a wider view. In: CHI ’02: proceedings of the SIGCHI conf" /><p class="c-article-references__text" id="ref-CR9">Czerwinski M, Tan DS, Robertson GG (2002) Women take a wider view. In: CHI ’02: proceedings of the SIGCHI conference on human factors in computing systems, ACM, New York, pp 195–202</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Demiralp, DB. Karelitz, S. Zhang, DH. Laidlaw, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Demiralp C, Karelitz DB, Zhang S, Laidlaw DH (2006) Cave and fishtank virtual-reality displays: a qualitative " /><p class="c-article-references__text" id="ref-CR10">Demiralp C, Karelitz DB, Zhang S, Laidlaw DH (2006) Cave and fishtank virtual-reality displays: a qualitative and quantitative comparison. IEEE transactions on visualization and computer graphics 12(3):323–330, member-Cullen D. Jackson</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cave%20and%20fishtank%20virtual-reality%20displays%3A%20a%20qualitative%20and%20quantitative%20comparison&amp;journal=IEEE%20transactions%20on%20visualization%20and%20computer%20graphics&amp;volume=12&amp;issue=3&amp;pages=323-330&amp;publication_year=2006&amp;author=Demiralp%2CC&amp;author=Karelitz%2CDB&amp;author=Zhang%2CS&amp;author=Laidlaw%2CDH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="CS. Souza, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="De Souza CS (2005) The semiotic engineering of human-computer interaction. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR11">De Souza CS (2005) The semiotic engineering of human-computer interaction. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20semiotic%20engineering%20of%20human-computer%20interaction&amp;publication_year=2005&amp;author=Souza%2CCS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Dubois, L. Nigay, J. Troccaz, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Uni" /><p class="c-article-references__text" id="ref-CR12">Dubois E, Nigay L, Troccaz J (2002) Assessing continuity and compatibility in augmented reality systems. J Univers Access Inf Soc 1(4):263–273</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10209-002-0024-8" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Assessing%20continuity%20and%20compatibility%20in%20augmented%20reality%20systems&amp;journal=J%20Univers%20Access%20Inf%20Soc&amp;volume=1&amp;issue=4&amp;pages=263-273&amp;publication_year=2002&amp;author=Dubois%2CE&amp;author=Nigay%2CL&amp;author=Troccaz%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Feiner S, Shamash A (1991) Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller" /><p class="c-article-references__text" id="ref-CR13">Feiner S, Shamash A (1991) Hybrid user interfaces: breeding virtually bigger interfaces for physically smaller computers. In: ACM UIST 91, ACM, pp 9–17</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grasset R, Looser J, Billinghurst M (2006) Transitional interface: concept, issues and framework. In: ISMAR, I" /><p class="c-article-references__text" id="ref-CR14">Grasset R, Looser J, Billinghurst M (2006) Transitional interface: concept, issues and framework. In: ISMAR, IEEE, pp 231–232</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grasset R, Duenser A, Billinghurst M (2008) Moving between contexts—a user evaluation of a transitional interf" /><p class="c-article-references__text" id="ref-CR15">Grasset R, Duenser A, Billinghurst M (2008) Moving between contexts—a user evaluation of a transitional interface. In: ICAT 2008: 18th international conference on artificial reality and telexistence, Keio University, Yokohama</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hall ET (1969) The hidden dimension. Anchor Books" /><p class="c-article-references__text" id="ref-CR16">Hall ET (1969) The hidden dimension. Anchor Books</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Ishii, M. Kobayashi, K. Arita, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8):83–97" /><p class="c-article-references__text" id="ref-CR17">Ishii H, Kobayashi M, Arita K (1994) Iterative design of seamless collaboration media. Commun ACM 37(8):83–97</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F179606.179687" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Iterative%20design%20of%20seamless%20collaboration%20media&amp;journal=Commun%20ACM&amp;volume=37&amp;issue=8&amp;pages=83-97&amp;publication_year=1994&amp;author=Ishii%2CH&amp;author=Kobayashi%2CM&amp;author=Arita%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Kiyokawa, H. Takemura, N. Yokoya, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Kiyokawa K, Takemura H, Yokoya N (2000) SeamlessDesign for 3D object creation. IEEE Multimedia IEEE 7(1):22–33" /><p class="c-article-references__text" id="ref-CR18">Kiyokawa K, Takemura H, Yokoya N (2000) SeamlessDesign for 3D object creation. IEEE Multimedia IEEE 7(1):22–33</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F93.839308" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SeamlessDesign%20for%203D%20object%20creation&amp;journal=IEEE%20Multimedia%20IEEE&amp;volume=7&amp;issue=1&amp;pages=22-33&amp;publication_year=2000&amp;author=Kiyokawa%2CK&amp;author=Takemura%2CH&amp;author=Yokoya%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Masso JPM (2008) A structured approach to the development of 3D user interfaces. PhD thesis, ALBACETE, ES, adv" /><p class="c-article-references__text" id="ref-CR19">Masso JPM (2008) A structured approach to the development of 3D user interfaces. PhD thesis, ALBACETE, ES, adviser-Pascual Gonzalez Lopez</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Milgram P, Colquhoun HJ (1999) A taxonomy of real and virtual world display integration. Mixed reality: mergin" /><p class="c-article-references__text" id="ref-CR20">Milgram P, Colquhoun HJ (1999) A taxonomy of real and virtual world display integration. Mixed reality: merging real and virtual worlds. Ohmsha and Springer pp 5–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IECE transactions on information and " /><p class="c-article-references__text" id="ref-CR21">Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IECE transactions on information and systems, pp 1321–1329</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nakashima K, Machida T, Kiyokawa K, Takamura H (2005) A 2D–3D integrated environment for cooperative work. In:" /><p class="c-article-references__text" id="ref-CR22">Nakashima K, Machida T, Kiyokawa K, Takamura H (2005) A 2D–3D integrated environment for cooperative work. In: symposium on virtual reality software and technology, ACM, pp 16–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Polys NF, Kim S, Bowman DA (2005) Effects of information layout, screen size, and field of view on user perfor" /><p class="c-article-references__text" id="ref-CR23">Polys NF, Kim S, Bowman DA (2005) Effects of information layout, screen size, and field of view on user performance in information-rich virtual environments. In: VRST ’05: proceedings of the ACM symposium on virtual reality software and technology, ACM, New York, pp 46–55</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Prabhat, A. Forsberg, M. Katzourin, K. Wharton, M. Slater, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Prabhat, Forsberg A, Katzourin M, Wharton K, Slater M (2008) A comparative study of desktop, fishtank, and cav" /><p class="c-article-references__text" id="ref-CR24">Prabhat, Forsberg A, Katzourin M, Wharton K, Slater M (2008) A comparative study of desktop, fishtank, and cave systems for the exploration of volume rendered confocal data sets. IEEE Trans Visual Comput Gr 14(3):551–563</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2007.70433" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20desktop%2C%20fishtank%2C%20and%20cave%20systems%20for%20the%20exploration%20of%20volume%20rendered%20confocal%20data%20sets&amp;journal=IEEE%20Trans%20Visual%20Comput%20Gr&amp;volume=14&amp;issue=3&amp;pages=551-563&amp;publication_year=2008&amp;author=Prabhat%2C&amp;author=Forsberg%2CA&amp;author=Katzourin%2CM&amp;author=Wharton%2CK&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raja D, Bowman D, Lucas J, North C (2004) Exploring the benefits of immersion in abstract information visualiz" /><p class="c-article-references__text" id="ref-CR25">Raja D, Bowman D, Lucas J, North C (2004) Exploring the benefits of immersion in abstract information visualization. In: 8th international immersive projection technology workshop</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raposo A, Santos I, Soares L, Wagner G, Corseuil E, Gattas M (2009) Environ: integrating VR and CAD in enginee" /><p class="c-article-references__text" id="ref-CR26">Raposo A, Santos I, Soares L, Wagner G, Corseuil E, Gattas M (2009) Environ: integrating VR and CAD in engineering projects. In: IEEE computer graphics and applications, IEEE, vol 29, pp 91–95</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rascar R, Welch G, Cutts M, et al (1998) The office of the future: a unified approach to image-based modeling " /><p class="c-article-references__text" id="ref-CR27">Rascar R, Welch G, Cutts M, et al (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: SIGGRAPH, ACM, pp 179–188</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raymaekers C, Boeck JD, Weyer TD, Coninx K (2005) The effect of display size on navigation in a virtual enviro" /><p class="c-article-references__text" id="ref-CR28">Raymaekers C, Boeck JD, Weyer TD, Coninx K (2005) The effect of display size on navigation in a virtual environment. In: proceedings international conference on enactive interfaces—enactive ’05</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world envi" /><p class="c-article-references__text" id="ref-CR29">Rekimoto J, Nagao K (1995) The world through the computer: computer augmented interaction with real world environments. In: proceedings of the ACM symposium on user interface software and technology, ACM, pp 29–36</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environ" /><p class="c-article-references__text" id="ref-CR30">Rekimoto J, Saitoh M (1999) Augmented surfaces: a spatially continuous work space for hybrid computing environments. In: proceedings of CHI’99, ACM, pp 378–385</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Steed, C. Parker, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Steed A, Parker C (2005) Evaluating effectiveness of interaction techniques across immersive virtual environme" /><p class="c-article-references__text" id="ref-CR31">Steed A, Parker C (2005) Evaluating effectiveness of interaction techniques across immersive virtual environmental systems. Presence Teleoper Virtual Environ 14(5):511–527</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474605774918750" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20effectiveness%20of%20interaction%20techniques%20across%20immersive%20virtual%20environmental%20systems&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=14&amp;issue=5&amp;pages=511-527&amp;publication_year=2005&amp;author=Steed%2CA&amp;author=Parker%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DS. Tan, D. Gergle, P. Scupelli, R. Pausch, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Tan DS, Gergle D, Scupelli P, Pausch R (2006) Physically large displays improve performance on spatial tasks. " /><p class="c-article-references__text" id="ref-CR32">Tan DS, Gergle D, Scupelli P, Pausch R (2006) Physically large displays improve performance on spatial tasks. ACM Trans Comput-Hum Interact 13(1):71–99</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1143518.1143521" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physically%20large%20displays%20improve%20performance%20on%20spatial%20tasks&amp;journal=ACM%20Trans%20Comput-Hum%20Interact&amp;volume=13&amp;issue=1&amp;pages=71-99&amp;publication_year=2006&amp;author=Tan%2CDS&amp;author=Gergle%2CD&amp;author=Scupelli%2CP&amp;author=Pausch%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Trevisan DG (2004) Designing smooth connections between worlds. In: extended abstracts of the 2004 conference " /><p class="c-article-references__text" id="ref-CR33">Trevisan DG (2004) Designing smooth connections between worlds. In: extended abstracts of the 2004 conference on human factors and computing systems CHI2004, Session doctoral consortium, ACM Press, pp 1043–1044</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DG. Trevisan, J. Vanderdonckt, B. Macq, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Trevisan DG, Vanderdonckt J, Macq B (2004) Conceptualizing mixed spaces of interaction for designing continuou" /><p class="c-article-references__text" id="ref-CR34">Trevisan DG, Vanderdonckt J, Macq B (2004) Conceptualizing mixed spaces of interaction for designing continuous interaction. Virtual Real J 8(2):83–95</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-004-0140-2" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Conceptualizing%20mixed%20spaces%20of%20interaction%20for%20designing%20continuous%20interaction&amp;journal=Virtual%20Real%20J&amp;volume=8&amp;issue=2&amp;pages=83-95&amp;publication_year=2004&amp;author=Trevisan%2CDG&amp;author=Vanderdonckt%2CJ&amp;author=Macq%2CB">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0205-y-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors would like to thank all volunteer users who took part in the experiments. Thanks also to Petrobras, the National Council for Scientific and Technological Development (CNPq), and FAPERJ for the financial support of this work.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Computer Graphics Technology Group, Tecgraf, Pontifical Catholic University of Rio de Janeiro, PUC-Rio, Rio de Janeiro, RJ, Brazil</p><p class="c-article-author-affiliation__authors-list">Felipe G. Carvalho</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Computer Institute, Federal Fluminense University, Niteroi, RJ, Brazil</p><p class="c-article-author-affiliation__authors-list">Daniela G. Trevisan</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Informatics, Pontifical Catholic University of Rio de Janeiro, PUC-Rio, Rio de Janeiro, RJ, Brazil</p><p class="c-article-author-affiliation__authors-list">Alberto Raposo</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Felipe_G_-Carvalho"><span class="c-article-authors-search__title u-h3 js-search-name">Felipe G. Carvalho</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Felipe G.+Carvalho&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Felipe G.+Carvalho" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Felipe G.+Carvalho%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Daniela_G_-Trevisan"><span class="c-article-authors-search__title u-h3 js-search-name">Daniela G. Trevisan</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daniela G.+Trevisan&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daniela G.+Trevisan" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniela G.+Trevisan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Alberto-Raposo"><span class="c-article-authors-search__title u-h3 js-search-name">Alberto Raposo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Alberto+Raposo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Alberto+Raposo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alberto+Raposo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0205-y/email/correspondent/c1/new">Daniela G. Trevisan</a>.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec15-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdiv1_eDoCmEy4zTQdhSPaTTUV"><div mi24-video-player="true" video-id="1_eDoCmEy4zTQdhSPaTTUV" player-id="8PcXmCm9nWqE6posBEkd1h" config-type="vmpro" flash-path="//e.video-cdn.net/v2/" api-url="//d.video-cdn.net/play"></div><script src="//e.video-cdn.net/v2/embed.js"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>PDF (5732 KB)</p></div></div>
                </div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Toward%20the%20design%20of%20transitional%20interfaces%3A%20an%20exploratory%20study%20on%20a%20semi-immersive%20hybrid%20user%20interface&amp;author=Felipe%20G.%20Carvalho%20et%20al&amp;contentID=10.1007%2Fs10055-011-0205-y&amp;publication=1359-4338&amp;publicationDate=2011-12-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Carvalho, F.G., Trevisan, D.G. &amp; Raposo, A. Toward the design of transitional interfaces: an exploratory study on a semi-immersive hybrid user interface.
                    <i>Virtual Reality</i> <b>16, </b>271–288 (2012). https://doi.org/10.1007/s10055-011-0205-y</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0205-y.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-05-09">09 May 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-06">06 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-23">23 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-11">November 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0205-y" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0205-y</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Transitional interfaces</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hybrid user interfaces</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Continuity properties</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0205-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=205;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

