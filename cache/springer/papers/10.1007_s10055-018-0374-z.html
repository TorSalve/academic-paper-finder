<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Real-time body tracking in virtual reality using a Vive tracker"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Real-time body tracking in virtual reality using a Vive tracker"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-11-23"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these applications is to enhance the sense of belief that the user is &#8220;present&#8221; in the virtual environment. By tracking the user&#8217;s skeleton in real-time, it is possible to synchronize the avatar&#8217;s motions with the user&#8217;s motions. Although current common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to the lack of positional and rotation data, the current VR applications typically do not represent the user&#8217;s motions. In this paper, we present an accurate, low-latency body tracking approach for VR-based applications using Vive Trackers. Using a HTC Vive headset and Vive Trackers, we have been able to create an immersive VR experience, by animating the motions of the avatar as smoothly, rapidly and as accurately as possible. An evaluation showed our solution is capable of tracking both joint rotation and position with reasonable accuracy and a very low end-to-latency of 
                  
                    
                  
                  $$6.71 \pm 0.80\hbox { ms}$$
                  
                    
                  
                . Due to this merely imperceptible delay and precise tracking, our solution can show the movements of the user in real-time&#160;in order to create deeper immersion."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-11-23"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="155"/>

    <meta name="prism.endingPage" content="168"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0374-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0374-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0374-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0374-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Real-time body tracking in virtual reality using a Vive tracker"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2019/06"/>

    <meta name="citation_online_date" content="2018/11/23"/>

    <meta name="citation_firstpage" content="155"/>

    <meta name="citation_lastpage" content="168"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0374-z"/>

    <meta name="DOI" content="10.1007/s10055-018-0374-z"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0374-z"/>

    <meta name="description" content="Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increa"/>

    <meta name="dc.creator" content="Polona Caserman"/>

    <meta name="dc.creator" content="Augusto Garcia-Agundez"/>

    <meta name="dc.creator" content="Robert Konrad"/>

    <meta name="dc.creator" content="Stefan G&#246;bel"/>

    <meta name="dc.creator" content="Ralf Steinmetz"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Graph Models; citation_title=FABRIK: a fast, iterative solver for the inverse kinematics problem; citation_author=A Aristidou, J Lasenby; citation_volume=73; citation_issue=5; citation_publication_date=2011; citation_pages=243-260; citation_doi=10.1016/j.gmod.2011.05.003; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Proc Natl Acad Sci; citation_title=Illusory ownership of a virtual child body causes overestimation of object sizes and implicit attitude changes; citation_author=D Banakou, R Groten, M Slater; citation_volume=110; citation_issue=31; citation_publication_date=2013; citation_pages=12846-12851; citation_doi=10.1073/pnas.1306779110; citation_id=CR2"/>

    <meta name="citation_reference" content="Bolton J, Lambert M, Lirette D, Unsworth B (2014) PaperDude: a virtual reality cycling exergame. CHI&#8217;14 Extended Abstracts on Human Factors in Computing Systems. CHI EA&#8217;14. ACM, New York, NY, USA, pp 475&#8211;478"/>

    <meta name="citation_reference" content="Botev J, Rothkugel S (2017) High-precision gestural input for immersive large-scale distributed virtual environments. In: Proceedings of the 9th workshop on massively multiuser virtual environments, MMVE&#8217;17. ACM, New York, NY, USA, pp 7&#8211;11"/>

    <meta name="citation_reference" content="Caserman P, Krabbe P, Wojtusch J, von Stryk O (2016) Real-time step detection using the integrated sensors of a head-mounted display. In: 2016 IEEE international conference on systems, man, and cybernetics (SMC), pp 3510&#8211;3515"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Learn Technol; citation_title=A virtual reality dance training system using motion capture technology; citation_author=JCP Chan, H Leung, JKT Tang, T Komura; citation_volume=4; citation_issue=2; citation_publication_date=2011; citation_pages=187-195; citation_doi=10.1109/TLT.2010.27; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=SID Symp Dig Tech Papers; citation_title=Head position model-based latency measurement system for virtual reality head mounted display; citation_author=SW Choi, MW Seo, SL Lee, JH Park, EY Oh, JS Baek, SJ Kang; citation_volume=47; citation_issue=1; citation_publication_date=2016; citation_pages=1381-1384; citation_doi=10.1002/sdtp.10930; citation_id=CR7"/>

    <meta name="citation_reference" content="Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodiment: a pilot study. In: 2017 IEEE virtual reality (VR). IEEE, pp 253&#8211;254"/>

    <meta name="citation_reference" content="citation_journal_title=Eng Technol; citation_title=The teardown: HTC Vive VR headset; citation_author=P Dempsey; citation_volume=11; citation_issue=7&#8211;8; citation_publication_date=2016; citation_pages=80-81; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Eng Trends Technol (IJETT); citation_title=A review paper on oculus rift&#8212;a virtual reality headset; citation_author=PR Desai, PN Desai, KD Ajmera, K Mehta; citation_volume=13; citation_issue=4; citation_publication_date=2014; citation_pages=175-179; citation_doi=10.14445/22315381/IJETT-V13P237; citation_id=CR10"/>

    <meta name="citation_reference" content="Desai K, Raghuraman S, Jin R, Prabhakaran B (2017) QoE studies on interactive 3D tele-immersion. In: 2017 IEEE international symposium on multimedia (ISM), pp 130&#8211;137"/>

    <meta name="citation_reference" content="citation_journal_title=J Pathol Inform; citation_title=Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides; citation_author=N Farahani, R Post, J Duboy, I Ahmed, BJ Kolowitz, T Krinchai, SE Monaco, JL Fine, DJ Hartman, L Pantanowitz; citation_volume=7; citation_publication_date=2016; citation_pages=22; citation_doi=10.4103/2153-3539.181766; citation_id=CR12"/>

    <meta name="citation_reference" content="Fri&#240;riksson FA, Kristj&#225;nsson HS, Sigur&#240;sson DA, Thue D, Vilhj&#225;lmsson HH (2016) Become your avatar: fast skeletal reconstruction from sparse data for fully-tracked VR. In: Proceedings of the 26th international conference on artificial reality and telexistence and the 21st Eurographics symposium on virtual environments: posters and demos, pp 19&#8211;20"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Measuring latency in virtual environments; citation_author=S Friston, A Steed; citation_volume=20; citation_issue=4; citation_publication_date=2014; citation_pages=616-625; citation_doi=10.1109/TVCG.2014.30; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Gait Posture; citation_title=Accuracy of the microsoft kinect sensor for measuring movement in people with Parkinson&#8217;s disease; citation_author=B Galna, G Barry, D Jackson, D Mhiripiri, P Olivier, L Rochester; citation_volume=39; citation_issue=4; citation_publication_date=2014; citation_pages=1062-1068; citation_doi=10.1016/j.gaitpost.2014.01.008; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Curr Eng Technol; citation_title=A review paper on oculus rift &amp; project morpheus; citation_author=I Goradia, J Doshi, L Kurup; citation_volume=4; citation_issue=5; citation_publication_date=2014; citation_pages=3196-3200; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Style-based inverse kinematics; citation_author=K Grochow, SL Martin, A Hertzmann, Z Popovi&#263;; citation_volume=23; citation_issue=3; citation_publication_date=2004; citation_pages=522-531; citation_doi=10.1145/1015706.1015755; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Multi-variate gaussian-based inverse kinematics; citation_author=J Huang, Q Wang, M Fratarcangeli, K Yan, C Pelachaud; citation_volume=36; citation_issue=8; citation_publication_date=2017; citation_pages=418-428; citation_doi=10.1111/cgf.13089; citation_id=CR18"/>

    <meta name="citation_reference" content="Jain D, Sra M, Guo J, Marques R, Wu R, Chiu J, Schmandt C (2016) Immersive terrestrial scuba diving using virtual reality. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, USA, pp 1563&#8211;1569"/>

    <meta name="citation_reference" content="Jiang F, Yang X, Feng L (2016) Real-time full-body motion reconstruction and recognition for off-the-shelf VR devices. In: Proceedings of the 15th ACM SIGGRAPH conference on virtual-reality continuum and its applications in industry&#8212;Volume 1, VRCAI&#8217;16. ACM, pp 309&#8211;318"/>

    <meta name="citation_reference" content="Johnson M, Humer I, Zimmerman B, Shallow J, Tahai L, Pietroszek K (2016) Low-cost latency compensation in motion tracking for smartphone-based head mounted display. In: Proceedings of the international working conference on advanced visual interfaces, AVI&#8217;16. ACM, New York, NY, USA, pp 316&#8211;317"/>

    <meta name="citation_reference" content="Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: changing sense of embodiment by spatial-temporal deformation of virtual human body. In: Proceedings of the 2017 CHI conference on human factors in computing systems, CHI&#8217;17. ACM, New York, NY, USA, pp 6438&#8211;6448"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Fast and efficient skinning of animated meshes; citation_author=L Kavan, PP Sloan, C O&#8217;Sullivan; citation_volume=29; citation_issue=2; citation_publication_date=2010; citation_pages=327-336; citation_doi=10.1111/j.1467-8659.2009.01602.x; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=Int Conf Creat Content Technol; citation_title=Real-time character inverse kinematics using the Gauss&#8211;Seidel iterative approximation method; citation_author=B Kenwright; citation_volume=4; citation_publication_date=2012; citation_pages=63-68; citation_id=CR24"/>

    <meta name="citation_reference" content="Lange B, Rizzo S, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology within virtual environments. In: Interservice/industry training, simulation, and education conference (I/ITSEC)"/>

    <meta name="citation_reference" content="Latoschik ME, Lugrin JL, Habel M, Roth D, Seufert C, Grafe S (2016) Breaking bad behavior: immersive training of class room management. In: Proceedings of the 22nd ACM conference on virtual reality software and technology, VRST&#8217;16. ACM, New York, NY, USA, pp 317&#8211;318"/>

    <meta name="citation_reference" content="Latoschik ME, Roth D, Gall D, Achenbach J, Waltemate T, Botsch M (2017) The effect of avatar realism in immersive social virtual realities. In: Proceedings of the 23rd ACM symposium on virtual reality software and technology, VRST&#8217;17. ACM, New York, NY, USA, pp 39:1&#8211;39:10"/>

    <meta name="citation_reference" content="Martindale J (2018) Oculus Rift vs. HTC Vive. 
                    https://www.digitaltrends.com/virtual-reality/oculus-rift-vs-htc-vive/
                    
                  . Accessed 4 May 2017&#8203;"/>

    <meta name="citation_reference" content="Melo M, Rocha T, Barbosa L, Bessa M (2016) The impact of body position on the usability of multisensory virtual environments: case study of a virtual bicycle. In: Proceedings of the 7th international conference on software development and technologies for enhancing accessibility and fighting info-exclusion, DSAI 2016. ACM, New York, NY, USA, pp 20&#8211;24"/>

    <meta name="citation_reference" content="citation_journal_title=J Dyn Syst Meas Control; citation_title=Inverse kinematic solutions with singularity robustness for robot manipulator control; citation_author=Y Nakamura, H Hanafusa; citation_volume=108; citation_issue=3; citation_publication_date=1986; citation_pages=163-171; citation_doi=10.1115/1.3143764; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Robot Res; citation_title=Efficient computation of the Jacobian for robot manipulators; citation_author=DE Orin, WW Schrader; citation_volume=3; citation_issue=4; citation_publication_date=1984; citation_pages=66-75; citation_doi=10.1177/027836498400300404; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Conscious Cognit; citation_title=Putting yourself in the skin of a black avatar reduces implicit racial bias; citation_author=TC Peck, S Seinfeld, SM Aglioti, M Slater; citation_volume=22; citation_issue=3; citation_publication_date=2013; citation_pages=779-787; citation_doi=10.1016/j.concog.2013.04.016; citation_id=CR31"/>

    <meta name="citation_reference" content="Raaen K, Kjellmo I (2015) Measuring latency in virtual reality systems. In: Chorianopoulos K, Divitini M, Baalsrud&#160;Hauge J, Jaccheri L, Malaka R (eds) Entertainment computing&#8212;ICEC 2015. Springer, Cham, pp 457&#8211;462"/>

    <meta name="citation_reference" content="Roberts D, Duckworth T, Moore C, Wolff R, O&#8217;Hare J (2009) Comparing the end to end latency of an immersive collaborative environment and a video conference. In: Proceedings of the 2009 13th IEEE/ACM international symposium on distributed simulation and real time applications, DS-RT&#8217;09. IEEE Computer Society, Washington, DC, USA, pp 89&#8211;94"/>

    <meta name="citation_reference" content="Schmidt D, Kovacs R, Mehta V, Umapathi U, K&#246;hler S, Cheng LP, Baudisch P (2015) Level-ups: motorized stilts that simulate stair steps in virtual reality. In: Proceedings of the 33rd annual ACM conference extended abstracts on human factors in computing systems, CHI EA&#8217;15. ACM, New York, NY, USA, pp 359&#8211;362"/>

    <meta name="citation_reference" content="Seele S, Misztal S, Buhler H, Herpers R, Schild J (2017) Here&#8217;s looking at you anyway!: how important is realistic gaze behavior in co-located social virtual reality games? In: Proceedings of the annual symposium on computer-human interaction in play, CHI PLAY&#8217;17. ACM, New York, NY, USA, pp 531&#8211;540"/>

    <meta name="citation_reference" content="Shoemake K (1985) Animating rotation with quaternion curves. In: Proceedings of the 12th annual conference on computer graphics and interactive techniques, SIGGRAPH&#8217;85. ACM, New York, NY, USA, pp 245&#8211;254"/>

    <meta name="citation_reference" content="Shum H, Ho ES (2012) Real-time physical modelling of character movements with microsoft kinect. In: Proceedings of the 18th ACM symposium on virtual reality software and technology, VRST&#8217;12. ACM, pp 17&#8211;24"/>

    <meta name="citation_reference" content="Sra M, Schmandt C (2015) MetaSpace II: object and full-body tracking for interaction and navigation in social VR. CoRR abs/1512.02922"/>

    <meta name="citation_reference" content="Steed A (2008) A simple method for estimating the latency of interactive, real-time graphics simulations. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST&#8217;08. ACM, New York, NY, USA, pp 123&#8211;129"/>

    <meta name="citation_reference" content="Tao G, Archambault PS, Levin MF (2013) Evaluation of kinect skeletal tracking in a virtual reality rehabilitation system for upper limb hemiparesis. In: 2013 international conference on virtual rehabilitation (ICVR), pp 164&#8211;165"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE J Transl Eng Health Med; citation_title=Effects of real-world versus virtual environments on joint excursions in full-body reaching tasks; citation_author=JS Thomas, CR France, ST Leitkam, ME Applegate, PE Pidcoe, S Walkowski; citation_volume=4; citation_publication_date=2016; citation_pages=1-8; citation_doi=10.1109/JTEHM.2016.2623787; citation_id=CR42"/>

    <meta name="citation_reference" content="Tsai TC, Chen CY, Su GJ (2015) U-art: your art and ubiquitous art. In: Adjunct proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing and proceedings of the 2015 ACM international symposium on wearable computers, UbiComp/ISWC&#8217;15 Adjunct. ACM, New York, NY, USA, pp 1295&#8211;1302"/>

    <meta name="citation_author" content="Polona Caserman"/>

    <meta name="citation_author_email" content="Polona.Caserman@kom.tu-darmstadt.de"/>

    <meta name="citation_author_institution" content="Multimedia Communications Lab, Technische Universit&#228;t Darmstadt, Darmstadt, Germany"/>

    <meta name="citation_author" content="Augusto Garcia-Agundez"/>

    <meta name="citation_author_email" content="Augusto.Garcia-Agundez@kom.tu-darmstadt.de"/>

    <meta name="citation_author_institution" content="Multimedia Communications Lab, Technische Universit&#228;t Darmstadt, Darmstadt, Germany"/>

    <meta name="citation_author" content="Robert Konrad"/>

    <meta name="citation_author_email" content="Robert.Konrad@kom.tu-darmstadt.de"/>

    <meta name="citation_author_institution" content="Multimedia Communications Lab, Technische Universit&#228;t Darmstadt, Darmstadt, Germany"/>

    <meta name="citation_author" content="Stefan G&#246;bel"/>

    <meta name="citation_author_email" content="Stefan.Gobel@kom.tu-darmstadt.de"/>

    <meta name="citation_author_institution" content="Multimedia Communications Lab, Technische Universit&#228;t Darmstadt, Darmstadt, Germany"/>

    <meta name="citation_author" content="Ralf Steinmetz"/>

    <meta name="citation_author_email" content="Ralf.Steinmetz@kom.tu-darmstadt.de"/>

    <meta name="citation_author_institution" content="Multimedia Communications Lab, Technische Universit&#228;t Darmstadt, Darmstadt, Germany"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0374-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0374-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Real-time body tracking in virtual reality using a Vive tracker"/>
        <meta property="og:description" content="Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these applications is to enhance the sense of belief that the user is “present” in the virtual environment. By tracking the user’s skeleton in real-time, it is possible to synchronize the avatar’s motions with the user’s motions. Although current common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to the lack of positional and rotation data, the current VR applications typically do not represent the user’s motions. In this paper, we present an accurate, low-latency body tracking approach for VR-based applications using Vive Trackers. Using a HTC Vive headset and Vive Trackers, we have been able to create an immersive VR experience, by animating the motions of the avatar as smoothly, rapidly and as accurately as possible. An evaluation showed our solution is capable of tracking both joint rotation and position with reasonable accuracy and a very low end-to-latency of $$6.71 \pm 0.80\hbox { ms}$$ 6.71 ± 0.80 ms . Due to this merely imperceptible delay and precise tracking, our solution can show the movements of the user in real-time&amp;nbsp;in order to create deeper immersion."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Real-time body tracking in virtual reality using a Vive tracker | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0374-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Real-time tracking, Full-body avatar, Low-latency, HTC Vive tracker, Inverse kinematics","kwrd":["Virtual_reality","Real-time_tracking","Full-body_avatar","Low-latency","HTC_Vive_tracker","Inverse_kinematics"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0374-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0374-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-b0018c9f69.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-c02f1b37f0.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=374;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0374-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Real-time body tracking in virtual reality using a Vive tracker
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0374-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0374-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-11-23" itemprop="datePublished">23 November 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Real-time body tracking in virtual reality using a Vive tracker</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Polona-Caserman" data-author-popup="auth-Polona-Caserman" data-corresp-id="c1">Polona Caserman<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-3252-4533"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-3252-4533</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität Darmstadt" /><meta itemprop="address" content="0000 0001 0940 1669, grid.6546.1, Multimedia Communications Lab, Technische Universität Darmstadt, 64283, Darmstadt, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Augusto-Garcia_Agundez" data-author-popup="auth-Augusto-Garcia_Agundez">Augusto Garcia-Agundez</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität Darmstadt" /><meta itemprop="address" content="0000 0001 0940 1669, grid.6546.1, Multimedia Communications Lab, Technische Universität Darmstadt, 64283, Darmstadt, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Robert-Konrad" data-author-popup="auth-Robert-Konrad">Robert Konrad</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität Darmstadt" /><meta itemprop="address" content="0000 0001 0940 1669, grid.6546.1, Multimedia Communications Lab, Technische Universität Darmstadt, 64283, Darmstadt, Germany" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stefan-G_bel" data-author-popup="auth-Stefan-G_bel">Stefan Göbel</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität Darmstadt" /><meta itemprop="address" content="0000 0001 0940 1669, grid.6546.1, Multimedia Communications Lab, Technische Universität Darmstadt, 64283, Darmstadt, Germany" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ralf-Steinmetz" data-author-popup="auth-Ralf-Steinmetz">Ralf Steinmetz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technische Universität Darmstadt" /><meta itemprop="address" content="0000 0001 0940 1669, grid.6546.1, Multimedia Communications Lab, Technische Universität Darmstadt, 64283, Darmstadt, Germany" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">155</span>–<span itemprop="pageEnd">168</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1326 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0374-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these applications is to enhance the sense of belief that the user is “present” in the virtual environment. By tracking the user’s skeleton in real-time, it is possible to synchronize the avatar’s motions with the user’s motions. Although current common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to the lack of positional and rotation data, the current VR applications typically do not represent the user’s motions. In this paper, we present an accurate, low-latency body tracking approach for VR-based applications using Vive Trackers. Using a HTC Vive headset and Vive Trackers, we have been able to create an immersive VR experience, by animating the motions of the avatar as smoothly, rapidly and as accurately as possible. An evaluation showed our solution is capable of tracking both joint rotation and position with reasonable accuracy and a very low end-to-latency of <span class="mathjax-tex">\(6.71 \pm 0.80\hbox { ms}\)</span>. Due to this merely imperceptible delay and precise tracking, our solution can show the movements of the user in real-time in order to create deeper immersion.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Virtual reality (VR) can be experienced wearing novel Head-Mounted Displays (HMDs). In the last few years, there has been a rapid improvement in VR technology, increasing the availability of HMDs to consumers (Choi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Choi SW, Seo MW, Lee SL, Park JH, Oh EY, Baek JS, Kang SJ (2016) Head position model-based latency measurement system for virtual reality head mounted display. SID Symp Dig Tech Papers 47(1):1381–1384" href="/article/10.1007/s10055-018-0374-z#ref-CR7" id="ref-link-section-d49876e410">2016</a>; Friðriksson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Friðriksson FA, Kristjánsson HS, Sigurðsson DA, Thue D, Vilhjálmsson HH (2016) Become your avatar: fast skeletal reconstruction from sparse data for fully-tracked VR. In: Proceedings of the 26th international conference on artificial reality and telexistence and the 21st Eurographics symposium on virtual environments: posters and demos, pp 19–20" href="/article/10.1007/s10055-018-0374-z#ref-CR13" id="ref-link-section-d49876e413">2016</a>). The most advanced HMDs, like Oculus Rift, HTC Vive, and PlayStation VR, already have a high-definition resolution, a wide field-of-view, and a high refresh rate. Furthermore, novel VR systems are capable of positional and rotational tracking of the HMD as well as additional VR devices. Tracking systems in VR can provide new possibilities for a more comfortable, immersive experience and gameplay (Goradia et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Goradia I, Doshi J, Kurup L (2014) A review paper on oculus rift &amp; project morpheus. Int J Curr Eng Technol 4(5):3196–3200" href="/article/10.1007/s10055-018-0374-z#ref-CR16" id="ref-link-section-d49876e416">2014</a>). Oculus Rift, for example, enables tracking through an embedded infrared system (Farahani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (2016) Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides. J Pathol Inform 7:22" href="/article/10.1007/s10055-018-0374-z#ref-CR12" id="ref-link-section-d49876e419">2016</a>). HTC Vive has similar technical specifications to Oculus Rift Consumer Version 1. Both HMDs provide high-definition resolution of <span class="mathjax-tex">\(2160 \times 1200\)</span> pixels, split between each eye and can maintain a frame rate of up to <span class="mathjax-tex">\(90\hbox { Hz}\)</span> (Farahani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (2016) Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides. J Pathol Inform 7:22" href="/article/10.1007/s10055-018-0374-z#ref-CR12" id="ref-link-section-d49876e475">2016</a>; Martindale <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Martindale J (2018) Oculus Rift vs. HTC Vive. &#xA;                    https://www.digitaltrends.com/virtual-reality/oculus-rift-vs-htc-vive/&#xA;                    &#xA;                  . Accessed 4 May 2017​" href="/article/10.1007/s10055-018-0374-z#ref-CR500" id="ref-link-section-d49876e478">2018</a>). But the special feature of the HTC Vive is the Vive Tracker, which allows the developers to bring any real-world object into the virtual environment, e.g., by simply attaching it to sporting equipment like a baseball bat, a golf club or a weapon.<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> The position and orientation of this device are then tracked by two “Lighthouse” stations, based on infrared signals. Each station consist of infrared LEDs, flashing at regular intervals and signaling the start of a cycle (Dempsey <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Dempsey P (2016) The teardown: HTC Vive VR headset. Eng Technol 11(7–8):80–81" href="/article/10.1007/s10055-018-0374-z#ref-CR9" id="ref-link-section-d49876e494">2016</a>). Two little motors project laser beams across the room, one spinning horizontally and the other vertically. The sensors on the VR devices then detect these lasers and can determine its position based on the order its sensors receive the laser sweeps.</p><p>Due to this HMD development, the number of novel and innovative games for rehabilitation, training and exercise activities has greatly increased, e.g., Bolton et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Bolton J, Lambert M, Lirette D, Unsworth B (2014) PaperDude: a virtual reality cycling exergame. CHI’14 Extended Abstracts on Human Factors in Computing Systems. CHI EA’14. ACM, New York, NY, USA, pp 475–478" href="/article/10.1007/s10055-018-0374-z#ref-CR3" id="ref-link-section-d49876e500">2014</a>), Collingwoode-Williams et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodiment: a pilot study. In: 2017 IEEE virtual reality (VR). IEEE, pp 253–254" href="/article/10.1007/s10055-018-0374-z#ref-CR8" id="ref-link-section-d49876e503">2017</a>), Jain et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jain D, Sra M, Guo J, Marques R, Wu R, Chiu J, Schmandt C (2016) Immersive terrestrial scuba diving using virtual reality. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, USA, pp 1563–1569" href="/article/10.1007/s10055-018-0374-z#ref-CR19" id="ref-link-section-d49876e506">2016</a>) and Sra and Schmandt (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Sra M, Schmandt C (2015) MetaSpace II: object and full-body tracking for interaction and navigation in social VR. CoRR abs/1512.02922" href="/article/10.1007/s10055-018-0374-z#ref-CR38" id="ref-link-section-d49876e509">2015</a>). The primary goal of these applications is to give the user an illusion of the presence of “being there” in the VR (Desai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Desai PR, Desai PN, Ajmera KD, Mehta K (2014) A review paper on oculus rift—a virtual reality headset. Int J Eng Trends Technol (IJETT) 13(4):175–179" href="/article/10.1007/s10055-018-0374-z#ref-CR10" id="ref-link-section-d49876e512">2014</a>; Goradia et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Goradia I, Doshi J, Kurup L (2014) A review paper on oculus rift &amp; project morpheus. Int J Curr Eng Technol 4(5):3196–3200" href="/article/10.1007/s10055-018-0374-z#ref-CR16" id="ref-link-section-d49876e516">2014</a>). To create an immersive experience, the connection between the user and the VR, as well as between the player and the avatar, has to be established. Immersive VR can be used to induce ownership over a virtual body that substitutes the real body, as seen from the first-person view (Banakou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Banakou D, Groten R, Slater M (2013) Illusory ownership of a virtual child body causes overestimation of object sizes and implicit attitude changes. Proc Natl Acad Sci 110(31):12846–12851" href="/article/10.1007/s10055-018-0374-z#ref-CR2" id="ref-link-section-d49876e519">2013</a>; Peck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Peck TC, Seinfeld S, Aglioti SM, Slater M (2013) Putting yourself in the skin of a black avatar reduces implicit racial bias. Conscious Cognit 22(3):779–787" href="/article/10.1007/s10055-018-0374-z#ref-CR31" id="ref-link-section-d49876e522">2013</a>). The person exploring VR would then be able to look down and perceive the avatar as her/his own body. Hence, by synchronizing the body movement of the user and their avatar, a positive effect on the cognitive ability of the user as well as the feeling of agency over the avatar can be achieved (Collingwoode-Williams et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodiment: a pilot study. In: 2017 IEEE virtual reality (VR). IEEE, pp 253–254" href="/article/10.1007/s10055-018-0374-z#ref-CR8" id="ref-link-section-d49876e525">2017</a>; Peck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Peck TC, Seinfeld S, Aglioti SM, Slater M (2013) Putting yourself in the skin of a black avatar reduces implicit racial bias. Conscious Cognit 22(3):779–787" href="/article/10.1007/s10055-018-0374-z#ref-CR31" id="ref-link-section-d49876e528">2013</a>). In particular, multiplayer VR games have the requirement of synchronizing the whole body in real time in order to create deeper immersion (Jiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jiang F, Yang X, Feng L (2016) Real-time full-body motion reconstruction and recognition for off-the-shelf VR devices. In: Proceedings of the 15th ACM SIGGRAPH conference on virtual-reality continuum and its applications in industry—Volume 1, VRCAI’16. ACM, pp 309–318" href="/article/10.1007/s10055-018-0374-z#ref-CR20" id="ref-link-section-d49876e531">2016</a>). Recent commercial approaches use infrared VR devices to track the full-body movements in real-time.<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> Furthermore, other developers provided Unity 3D game engine-based asset packages to achieve full-body tracking, e.g., Vive IK Demo<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> and Final IK.<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> However, to the best of our knowledge, there is no research on evaluating these existing commercial kinematic solutions, particularly regarding its accuracy and latency.</p><p>Tracking and representing body movements, regardless of the user orientation, can be challenging. Although common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to this lack of data about a user’s position and orientation in the world, the current VR games typically do not track or represent the body of the user (Farahani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (2016) Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides. J Pathol Inform 7:22" href="/article/10.1007/s10055-018-0374-z#ref-CR12" id="ref-link-section-d49876e577">2016</a>).</p><p>In this paper, we implement a low-latency body tracking approach for immersive VR-based applications. By using only infrared VR controllers, e.g., Vive Tracker it is possible to transfer full-body player’s motions onto a virtual avatar. Additionally, we measure the end-to-end latency. The main research contributions of our work are the following:</p><ul class="u-list-style-dash">
                  <li>
                    <p>We develop a latency measurement tool in order to evaluate the total delay of the proposed method. Using this tool, we want to show that the end-to-end latency of the developed system stays below <span class="mathjax-tex">\(20\hbox { ms}\)</span> since this satisfies the requirement of the VR experiences.</p>
                  </li>
                  <li>
                    <p>In contrast to systems using motion capture suits, we only use a small number of sensors to avoid high initial costs as well as complex setup. We do not explicitly track each body joint. In our research, we only track the position and orientation of the end-effectors (e.g., hands). We then solve the inverse kinematics (IK) problem to determine the angle of other joints in order to enable full-body tracking. This approach satisfies the desire to reduce the amount of sensor.</p>
                  </li>
                  <li>
                    <p>We use infrared VR controller, which do not suffer from occlusion and high latency, such as the Kinect sensor.</p>
                  </li>
                  <li>
                    <p>Our system is entirely based on low-cost hardware and low-level game API. We can easily access the lowest level in order to achieve maximal performance. Additionally, because our body tracking solution should be available, e.g., for researchers to create immersive VR experiences, we include source code, which can be accessed on GitHub.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup></p>
                  </li>
                </ul><p>The rest of this paper is structured as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec2">2</a> provides the related work. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec6">3</a>, the approach of body tracking using infrared sensors and a latency measurement tool are described. An experimental evaluation of the deployed system is given in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec13">4</a>. A discussion of the results and a conclusion follows in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec16">5</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Full-body tracking</h3><p>Many recent research publications add a growing base of evidence to support the use of VR and full-body tracking. Examples recently showed the benefits of having a full-body avatar in a virtual environment by demonstrating the important role of realistic looking virtual humans (Latoschik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Latoschik ME, Roth D, Gall D, Achenbach J, Waltemate T, Botsch M (2017) The effect of avatar realism in immersive social virtual realities. In: Proceedings of the 23rd ACM symposium on virtual reality software and technology, VRST’17. ACM, New York, NY, USA, pp 39:1–39:10" href="/article/10.1007/s10055-018-0374-z#ref-CR27" id="ref-link-section-d49876e672">2017</a>). Furthermore, owning a virtual body and perceiving it from the first-person perspective is also essential when performing reaching tasks in VR (Thomas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Thomas JS, France CR, Leitkam ST, Applegate ME, Pidcoe PE, Walkowski S (2016) Effects of real-world versus virtual environments on joint excursions in full-body reaching tasks. IEEE J Transl Eng Health Med 4:1–8" href="/article/10.1007/s10055-018-0374-z#ref-CR42" id="ref-link-section-d49876e675">2016</a>).</p><p>The most popular game systems, capable of motion-sensing such as Microsoft Kinect<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> or Nintendo Wii<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> already provided significant evidence that exergames are entertaining and motivating (Lange et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Lange B, Rizzo S, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology within virtual environments. In: Interservice/industry training, simulation, and education conference (I/ITSEC)" href="/article/10.1007/s10055-018-0374-z#ref-CR25" id="ref-link-section-d49876e707">2011</a>). However, both Kinect versions suffer from inconsistent tracking, jittering, and unreliable data (Friðriksson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Friðriksson FA, Kristjánsson HS, Sigurðsson DA, Thue D, Vilhjálmsson HH (2016) Become your avatar: fast skeletal reconstruction from sparse data for fully-tracked VR. In: Proceedings of the 26th international conference on artificial reality and telexistence and the 21st Eurographics symposium on virtual environments: posters and demos, pp 19–20" href="/article/10.1007/s10055-018-0374-z#ref-CR13" id="ref-link-section-d49876e710">2016</a>). Kinect V1 sensor is only accurate when tracking gross movements such as sit-to-stand, but is very poor for fine movements such as hand clapping, toe or finger tapping (Galna et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Galna B, Barry G, Jackson D, Mhiripiri D, Olivier P, Rochester L (2014) Accuracy of the microsoft kinect sensor for measuring movement in people with Parkinson’s disease. Gait Posture 39(4):1062–1068" href="/article/10.1007/s10055-018-0374-z#ref-CR15" id="ref-link-section-d49876e713">2014</a>). Due to the new technology, the Kinect V2 which is based on the time-of-flight principle is more accurate in detecting small movements and provides better tracking results. However, the latency of the new version remains high, approximately at <span class="mathjax-tex">\(170\hbox { ms}\)</span> when combined with the Oculus Rift (Botev and Rothkugel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Botev J, Rothkugel S (2017) High-precision gestural input for immersive large-scale distributed virtual environments. In: Proceedings of the 9th workshop on massively multiuser virtual environments, MMVE’17. ACM, New York, NY, USA, pp 7–11" href="/article/10.1007/s10055-018-0374-z#ref-CR4" id="ref-link-section-d49876e743">2017</a>). An additional disadvantage of a single Kinect sensor is that it can only poorly track the rotation of body parts and is incapable of tracking when the user stands sideways (Friðriksson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Friðriksson FA, Kristjánsson HS, Sigurðsson DA, Thue D, Vilhjálmsson HH (2016) Become your avatar: fast skeletal reconstruction from sparse data for fully-tracked VR. In: Proceedings of the 26th international conference on artificial reality and telexistence and the 21st Eurographics symposium on virtual environments: posters and demos, pp 19–20" href="/article/10.1007/s10055-018-0374-z#ref-CR13" id="ref-link-section-d49876e746">2016</a>).</p><p>Although the Kinect suffers from occlusion, provides noise in skeleton tracking and has a high latency, it is the most popular device for body tracking. Due to its adequate accuracy and low cost, many researchers are using this technology to track the user’s movements. Shum and Ho (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Shum H, Ho ES (2012) Real-time physical modelling of character movements with microsoft kinect. In: Proceedings of the 18th ACM symposium on virtual reality software and technology, VRST’12. ACM, pp 17–24" href="/article/10.1007/s10055-018-0374-z#ref-CR37" id="ref-link-section-d49876e752">2012</a>) investigated the major problems of Kinect and developed a framework for a best-matched posture from the captured motion. The proposed solution can overcome the problem of missing Degree of Freedom (DoF) due to occlusions and noises. Sra and Schmandt (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Sra M, Schmandt C (2015) MetaSpace II: object and full-body tracking for interaction and navigation in social VR. CoRR abs/1512.02922" href="/article/10.1007/s10055-018-0374-z#ref-CR38" id="ref-link-section-d49876e755">2015</a>) used Kinect V2 devices to track objects and users. An Oculus Rift DK2 is used for tracking of the head rotation. Collingwoode-Williams et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodiment: a pilot study. In: 2017 IEEE virtual reality (VR). IEEE, pp 253–254" href="/article/10.1007/s10055-018-0374-z#ref-CR8" id="ref-link-section-d49876e758">2017</a>) used Kinect V1 to research the effect of limb and arm synchronization on body ownership in VR. In their study, the user wearing a HMD was able to see a gender-matched avatar in a virtual mirror, that moved its limbs synchronously with the user. Bolton et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Bolton J, Lambert M, Lirette D, Unsworth B (2014) PaperDude: a virtual reality cycling exergame. CHI’14 Extended Abstracts on Human Factors in Computing Systems. CHI EA’14. ACM, New York, NY, USA, pp 475–478" href="/article/10.1007/s10055-018-0374-z#ref-CR3" id="ref-link-section-d49876e761">2014</a>) developed a VR-based exergame, based on the game Paperboy, where the player is wearing a VR headset and driving a bicycle. A Kinect camera tracked the user movements of throwing newspapers into the neighborhood mailboxes. The arms were synchronized to create a high level of immersion. To overcome occlusion problems, other studies even use multiple Kinect devices to track a single user (Desai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Desai K, Raghuraman S, Jin R, Prabhakaran B (2017) QoE studies on interactive 3D tele-immersion. In: 2017 IEEE international symposium on multimedia (ISM), pp 130–137" href="/article/10.1007/s10055-018-0374-z#ref-CR11" id="ref-link-section-d49876e764">2017</a>).</p><p>Unfortunately, the Kinect sensor in combination with VR it inaccurate and will eventually show a false avatar posture (Tao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Tao G, Archambault PS, Levin MF (2013) Evaluation of kinect skeletal tracking in a virtual reality rehabilitation system for upper limb hemiparesis. In: 2013 international conference on virtual rehabilitation (ICVR), pp 164–165" href="/article/10.1007/s10055-018-0374-z#ref-CR41" id="ref-link-section-d49876e770">2013</a>). To provide a more accurate tracking, other recent studies have attempted to use a suit-based motion capture technology. These body tracking suits have attached infrared LED markers which can be then detected by a high-speed camera. Peck et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Peck TC, Seinfeld S, Aglioti SM, Slater M (2013) Putting yourself in the skin of a black avatar reduces implicit racial bias. Conscious Cognit 22(3):779–787" href="/article/10.1007/s10055-018-0374-z#ref-CR31" id="ref-link-section-d49876e773">2013</a>) developed a VR experience, whereby the motions were tracked by the OptiTrack<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup> system with 12 cameras. The movements could be reconstructed at <span class="mathjax-tex">\(100\hbox { Hz}\)</span> and synchronized with the virtual avatar. The users wearing the HMD could see their virtual body from the first-person perspective as well as a reflection in a virtual mirror. Likewise, Banakou et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Banakou D, Groten R, Slater M (2013) Illusory ownership of a virtual child body causes overestimation of object sizes and implicit attitude changes. Proc Natl Acad Sci 110(31):12846–12851" href="/article/10.1007/s10055-018-0374-z#ref-CR2" id="ref-link-section-d49876e815">2013</a>) used 34 cameras to track user’s motions. Chan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chan JCP, Leung H, Tang JKT, Komura T (2011) A virtual reality dance training system using motion capture technology. IEEE Trans Learn Technol 4(2):187–195" href="/article/10.1007/s10055-018-0374-z#ref-CR6" id="ref-link-section-d49876e819">2011</a>) proposed a dance system using a similar optical motion capture system. The user, wearing the motion capture suit, can learn new dance movements by imitating the motions demonstrated by a virtual teacher and listening to the feedback. Since suit-based tracking technology is capable of real-time full-body tracking of multiple users, some authors developed VR multiplayer applications or games, e.g., creating a physical condition control for athletes and dancers (Kasahara et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: changing sense of embodiment by spatial-temporal deformation of virtual human body. In: Proceedings of the 2017 CHI conference on human factors in computing systems, CHI’17. ACM, New York, NY, USA, pp 6438–6448" href="/article/10.1007/s10055-018-0374-z#ref-CR22" id="ref-link-section-d49876e822">2017</a>). In contrast to a single infrared camera, such as Kinect, a wearable motion capture suit is capable of a very accurate body tracking. However, it is very expensive and complicated to use. Using a suit with LED markers for tracking requires a setup area and multiple high-speed cameras. Due to the high initial cost and complex setup, such a motion capture system is in general not applicable for home-based usage.</p><p>For tracking full-body movements also Inertial Measurement Units (IMUs) can be attached to the user’s body. Different commercial tracking systems, such as PrioVR<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup>, Perception Neuron<sup><a href="#Fn10"><span class="u-visually-hidden">Footnote </span>10</a></sup>, or Xsense<sup><a href="#Fn11"><span class="u-visually-hidden">Footnote </span>11</a></sup> are based on IMU. Perception Neuron furthermore utilizes a special data glove with a Vive Tracker in order to track hand position as well as individual fingers. Tsai et a. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Tsai TC, Chen CY, Su GJ (2015) U-art: your art and ubiquitous art. In: Adjunct proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing and proceedings of the 2015 ACM international symposium on wearable computers, UbiComp/ISWC’15 Adjunct. ACM, New York, NY, USA, pp 1295–1302" href="/article/10.1007/s10055-018-0374-z#ref-CR43" id="ref-link-section-d49876e868">2015</a>) developed an own wearable sensor to determine the skeleton posture in real-time. Moreover, measurement values of the integrated sensors of a HMD can be used to recognize steps (Caserman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Caserman P, Krabbe P, Wojtusch J, von Stryk O (2016) Real-time step detection using the integrated sensors of a head-mounted display. In: 2016 IEEE international conference on systems, man, and cybernetics (SMC), pp 3510–3515" href="/article/10.1007/s10055-018-0374-z#ref-CR5" id="ref-link-section-d49876e871">2016</a>). Applying this step detector, the researchers were able to synchronize the feet of the user while walking on a treadmill. The user can then look down and see her/his virtual body from the first-person perspective as she/he would in the real world. In another work, an IMU is attached to a bicycle to detect the steering and breaking information (Melo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Melo M, Rocha T, Barbosa L, Bessa M (2016) The impact of body position on the usability of multisensory virtual environments: case study of a virtual bicycle. In: Proceedings of the 7th international conference on software development and technologies for enhancing accessibility and fighting info-exclusion, DSAI 2016. ACM, New York, NY, USA, pp 20–24" href="/article/10.1007/s10055-018-0374-z#ref-CR28" id="ref-link-section-d49876e875">2016</a>). The player movements have been detected while the player was sitting on the bicycle with the feet on the pedals and hands on the handlebar.</p><h3 class="c-article__sub-heading" id="Sec4">Inverse kinematics</h3><p>Recent studies have attempted to use IK to determine a set of appropriate joint configurations based upon the desired end-effector position. IK approaches based on Jacobian are originally used in robotics in order to control (industrial) manipulators and were already presented in the 80’s (Orin and Schrader <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1984" title="Orin DE, Schrader WW (1984) Efficient computation of the Jacobian for robot manipulators. Int J Robot Res 3(4):66–75" href="/article/10.1007/s10055-018-0374-z#ref-CR30" id="ref-link-section-d49876e886">1984</a>; Nakamura and Hanafusa <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Nakamura Y, Hanafusa H (1986) Inverse kinematic solutions with singularity robustness for robot manipulator control. J Dyn Syst Meas Control 108(3):163–171" href="/article/10.1007/s10055-018-0374-z#ref-CR29" id="ref-link-section-d49876e889">1986</a>). The IK problem, to provide a solution that satisfies the positional and orientational constraints of each specific joint, has been well studied. Kenwright (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kenwright B (2012) Real-time character inverse kinematics using the Gauss–Seidel iterative approximation method. Int Conf Creat Content Technol 4:63–68" href="/article/10.1007/s10055-018-0374-z#ref-CR24" id="ref-link-section-d49876e892">2012</a>) presented a realistic and robust method for solving nonlinear IK problems with angular limits using the Gauss–Seidel iterative method. The proposed method merely requires a small number of iterations and needs only a few milliseconds to compute the solution. Aristidou and Lasenby (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Aristidou A, Lasenby J (2011) FABRIK: a fast, iterative solver for the inverse kinematics problem. Graph Models 73(5):243–260" href="/article/10.1007/s10055-018-0374-z#ref-CR1" id="ref-link-section-d49876e895">2011</a>) proposed a novel heuristic method, combining forward and backward IK. Other recent studies improved IK solutions using a multivariate Gaussian distribution model, which precisely specifies the joint constraints of a kinematic skeleton by integrating biomechanical properties and physical capacity of a human (Huang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Huang J, Wang Q, Fratarcangeli M, Yan K, Pelachaud C (2017) Multi-variate gaussian-based inverse kinematics. Comput Graph Forum 36(8):418–428" href="/article/10.1007/s10055-018-0374-z#ref-CR18" id="ref-link-section-d49876e898">2017</a>). Additionally, IK systems based on a probabilistic model of learned human poses were presented (Grochow et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Grochow K, Martin SL, Hertzmann A, Popović Z (2004) Style-based inverse kinematics. ACM Trans Graph 23(3):522–531" href="/article/10.1007/s10055-018-0374-z#ref-CR17" id="ref-link-section-d49876e902">2004</a>). However, such a system can only produce the most likely pose satisfying the constraints. Other researchers try to improve tracking quality by taking advantage of neural networks to reconstruct the motions, such as walking, jogging, jumping, crouching and turning (Jiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jiang F, Yang X, Feng L (2016) Real-time full-body motion reconstruction and recognition for off-the-shelf VR devices. In: Proceedings of the 15th ACM SIGGRAPH conference on virtual-reality continuum and its applications in industry—Volume 1, VRCAI’16. ACM, pp 309–318" href="/article/10.1007/s10055-018-0374-z#ref-CR20" id="ref-link-section-d49876e905">2016</a>).</p><h3 class="c-article__sub-heading" id="Sec5">Latency</h3><p>To improve the feeling of the presence in the VR, merely tracking user movements in order to synchronize the movements with those in VR is not sufficient (Collingwoode-Williams et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodiment: a pilot study. In: 2017 IEEE virtual reality (VR). IEEE, pp 253–254" href="/article/10.1007/s10055-018-0374-z#ref-CR8" id="ref-link-section-d49876e916">2017</a>; Jain et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jain D, Sra M, Guo J, Marques R, Wu R, Chiu J, Schmandt C (2016) Immersive terrestrial scuba diving using virtual reality. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, USA, pp 1563–1569" href="/article/10.1007/s10055-018-0374-z#ref-CR19" id="ref-link-section-d49876e919">2016</a>). Similarly, the total delay from the time movement occurs, to the time the results of that motion are displayed, should be well considered. A high frame rate and low HMD latency must be ensured in order to create an immersive VR experience (Farahani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (2016) Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides. J Pathol Inform 7:22" href="/article/10.1007/s10055-018-0374-z#ref-CR12" id="ref-link-section-d49876e922">2016</a>). On the one hand, a high latency of the HMD can contribute to cybersickness symptoms of disorientation, headache, nausea, and dizziness (Choi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Choi SW, Seo MW, Lee SL, Park JH, Oh EY, Baek JS, Kang SJ (2016) Head position model-based latency measurement system for virtual reality head mounted display. SID Symp Dig Tech Papers 47(1):1381–1384" href="/article/10.1007/s10055-018-0374-z#ref-CR7" id="ref-link-section-d49876e925">2016</a>; Steed <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Steed A (2008) A simple method for estimating the latency of interactive, real-time graphics simulations. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST’08. ACM, New York, NY, USA, pp 123–129" href="/article/10.1007/s10055-018-0374-z#ref-CR40" id="ref-link-section-d49876e928">2008</a>). On the other hand, a significant delay between a physical movement and an output image can decrease the user’s sense of immersion (Farahani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (2016) Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides. J Pathol Inform 7:22" href="/article/10.1007/s10055-018-0374-z#ref-CR12" id="ref-link-section-d49876e932">2016</a>; Friston and Steed <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Friston S, Steed A (2014) Measuring latency in virtual environments. IEEE Trans Vis Comput Graph 20(4):616–625" href="/article/10.1007/s10055-018-0374-z#ref-CR14" id="ref-link-section-d49876e935">2014</a>). In particular, in VR, the end-to-end latency should not be higher than <span class="mathjax-tex">\(20\,\hbox {ms}\)</span> (Raaen and Kjellmo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Raaen K, Kjellmo I (2015) Measuring latency in virtual reality systems. In: Chorianopoulos K, Divitini M, Baalsrud Hauge J, Jaccheri L, Malaka R (eds) Entertainment computing—ICEC 2015. Springer, Cham, pp 457–462" href="/article/10.1007/s10055-018-0374-z#ref-CR32" id="ref-link-section-d49876e964">2015</a>). Kasahara et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: changing sense of embodiment by spatial-temporal deformation of virtual human body. In: Proceedings of the 2017 CHI conference on human factors in computing systems, CHI’17. ACM, New York, NY, USA, pp 6438–6448" href="/article/10.1007/s10055-018-0374-z#ref-CR22" id="ref-link-section-d49876e967">2017</a>) also showed similar results. The researchers found that a high latency (<span class="mathjax-tex">\(&gt; 30\hbox { ms}\)</span>) will break the sense of agency and body ownership. Therefore, when developing VR experiences, it is essential to keep the end-to-end latency as low as possible.</p><p>Previous works have shown that one or more synchronized cameras can be used to measure the latency in an immersive virtual environment (Friston and Steed <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Friston S, Steed A (2014) Measuring latency in virtual environments. IEEE Trans Vis Comput Graph 20(4):616–625" href="/article/10.1007/s10055-018-0374-z#ref-CR14" id="ref-link-section-d49876e1001">2014</a>; Roberts et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Roberts D, Duckworth T, Moore C, Wolff R, O’Hare J (2009) Comparing the end to end latency of an immersive collaborative environment and a video conference. In: Proceedings of the 2009 13th IEEE/ACM international symposium on distributed simulation and real time applications, DS-RT’09. IEEE Computer Society, Washington, DC, USA, pp 89–94" href="/article/10.1007/s10055-018-0374-z#ref-CR33" id="ref-link-section-d49876e1004">2009</a>). By filming the tracked real object and the associated output of the virtual environment, the delay can be determined using image processing techniques.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Approach</h2><div class="c-article-section__content" id="Sec6-content"><p>To develop a reliable real-time body tracking system, that can be used in an immersive VR experience, the HTC Vive HMD and the Vive Tracker are used to track the movements of the user. By using two base stations and the VR devices with a large number of infrared sensors, such a system suffers much less from occlusion than a single Kinect device. With this technology, we can develop a reliable full-body tracking system which can provide accurate user posture, regardless of the user orientation. Thereby, the technical requirements of using only a small number of sensors and avoiding high costs as well as complex setup (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec1">1</a>) are satisfied. With an accurate real-time body tracking solution and an efficient IK solver, the virtual character can be synchronized with the user. The person wearing a HMD is then able to view the virtual body from the first-person perspective.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="236" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A flowchart for the body tracking system. After obtaining the position and rotation of the Vive Tracker, we solve the IK problem in order to determine the appropriate joint configurations. In the last step, we animate the character</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec7">Development of the body tracking system</h3><p>In this section, the approach of the real-time body tracking system to determine user movements will be described. Because this system should be used to synchronize the virtual avatar with the body movements of the user, an articulated character model with a skeleton must be created. Then, by obtaining the positional and rotational data of the Vive Trackers that are bound to the hands and feet, the full-body motions of the user can be continuously tracked. Through the efficient implementation of the iterative method for solving the IK problem, a set of appropriate joint configurations in an articulated model based upon a desirable end-effector position can be determined in only a few iterations. Finally, the skeleton is animated according to the calculated positions and orientations of the bones. In the following, a detailed description of these individual steps will be given. The flowchart for the body tracking system is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig1">1</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Character model</h4><p>An articulated character with a skeleton was modeled with the MakeHuman<sup><a href="#Fn12"><span class="u-visually-hidden">Footnote </span>12</a></sup> open source tool. A skeleton of a small number of bones was consciously chosen in order to easily define joint constraints to solve the IK problem and to animate the user’s motions, which will be described later. To facilitate the transfer of the character model to the Kore<sup><a href="#Fn13"><span class="u-visually-hidden">Footnote </span>13</a></sup> framework, the Open Game Engine Exchange format (OpenGEX<sup><a href="#Fn14"><span class="u-visually-hidden">Footnote </span>14</a></sup>) is used. Both, the OpenGEX format and the Kore framework are open source projects. OpenGEX exports skinned meshes (vertex data, skeleton, bind-pose transforms, bone influence weighting data) in a human-readable text-based file. Kore is a low-level game library and hardware abstraction framework, which is implemented in the <span class="u-monospace">C++</span> programming language. It provides the necessary functionality to develop games and multimedia applications with high performance.</p><p>A skeleton is defined as a tree structure of bone nodes, where each of these nodes is described by a <span class="mathjax-tex">\(4 \times 4\)</span> transformation matrix. The transformation matrix describes the bind-pose of a bone node. Thus, the default pose of the character mesh is stored before any bone transformation is applied. When the animation is applied, this matrix is used to calculate the new position and orientation. However, in order to prevent unnatural-looking poses while animating the character, we have to define a DoF for each joint. We have to restrict the possible rotations, i.e., rotations around the <i>x-</i>, <i>y-</i>, and <i>z</i>-axis. The generated skeleton gives us 56 DoF in total, as it can be seen in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0374-z#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 DoF of the articulated character model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0374-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The constraints are defined by creating an axis vector <span class="mathjax-tex">\({\mathbf {a}} \in {\mathbb {R}}^{3 \times 1}\)</span> for each joint and setting the angular limits for each of the axes. To prevent such an abnormal pose, like it can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig2">2</a>, a constraint for the knee has to be specified, i.e., <span class="mathjax-tex">\({\mathbf {a}}_{\mathrm{knee}} = \begin{bmatrix}1&amp;0&amp;0 \end{bmatrix}^{\mathrm{T}}\)</span> with angular limits <span class="mathjax-tex">\(\min _{\mathrm{knee}} = 0\)</span> and <span class="mathjax-tex">\(\max _{\mathrm{knee}} = 2\)</span>. Hence, the knee can rotate only around the <i>x</i>-axis and the angle can be only in the range of 0 and 2 radians. When a joint rotates around multiple axes, consequently angular limits for each axis have to be specified.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig2_HTML.jpg" alt="figure2" loading="lazy" width="685" height="428" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Character foot reached the desired end position in both variants. However, only the left body posture is natural. On the right, an unnatural pose is shown</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Position and rotation tracking</h4><p>The position and rotation tracking is the core task of the full-body tracking system to represent the movements of the user in the VR. For tracking Vive Trackers are strapped to hands and feet. The sensors can accurately track the yaw, pitch and roll movements as well as the spatial position.</p><p>The coordinate system of the avatar is attached on the floor and is a right-handed coordinate system, where the <i>x</i>-axis points to the left, the <i>y</i>-axis points backward and the <i>z</i>-axis points upward. To locate the character so that the user wearing HMD can look down and see her/his virtual body, the character has to be transformed, rotated and scaled. First, the character is scaled by a <span class="mathjax-tex">\({\mathbf {S}}_{\mathrm{init}} \in {\mathbb {R}}^{4 \times 4}\)</span> matrix so that the eye height of the character corresponds to the height of the HMD. The sensor measurements of the HMD are provided in a head-fixed coordinate system, where the <i>x</i>-axis points to the right, the <i>y</i>-axis points upward and the <i>z</i>-axis points backward. To calculate the scale factor, we can divide the current height of the user (<i>y</i> position of the HMD, <span class="mathjax-tex">\(p_{y, {\mathrm{hmd}}}\)</span>) by the character height (<i>z</i> position of the character head bone, <span class="mathjax-tex">\(p_{z, {\mathrm{head}}}\)</span>), i.e., <span class="mathjax-tex">\(s = p_{y, {\mathrm{hmd}}} / p_{z,{\mathrm{head}}}\)</span>.</p><p>In addition, we have to rotate the character so that its orientation coincides with the orientation of the user. Let <span class="mathjax-tex">\({\mathbf {q}}_{\mathrm{init}} \in {\mathbb {R}}^{4 \times 1}\)</span> be the initial quaternion that rotates the character so that the virtual body looks in the same direction as the user. Quaternions are used because they are very simple, efficient and do not suffer from Gimbal lock (Shoemake <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Shoemake K (1985) Animating rotation with quaternion curves. In: Proceedings of the 12th annual conference on computer graphics and interactive techniques, SIGGRAPH’85. ACM, New York, NY, USA, pp 245–254" href="/article/10.1007/s10055-018-0374-z#ref-CR36" id="ref-link-section-d49876e1809">1985</a>). However, because the local transformation of the character is calculated by applying the scale, rotation and lastly translation matrix, we have to convert the quaternion to a matrix. To include quaternion calculations in a regular, matrix-based transformation pipeline, we can represent the quaternion <span class="mathjax-tex">\(q = \begin{bmatrix}x&amp;y&amp;z&amp;w\end{bmatrix}^{\mathrm{T}}\)</span> as a matrix (Shoemake <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Shoemake K (1985) Animating rotation with quaternion curves. In: Proceedings of the 12th annual conference on computer graphics and interactive techniques, SIGGRAPH’85. ACM, New York, NY, USA, pp 245–254" href="/article/10.1007/s10055-018-0374-z#ref-CR36" id="ref-link-section-d49876e1877">1985</a>):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {R}}_{\mathrm{init}} = \begin{bmatrix} 1 - 2 y^2 - 2 z^2&amp;\quad 2 x y - 2 z w&amp;\quad 2 x z + 2 y w\\ 2 x y + 2 z w&amp;\quad 1 - 2 x^2 - 2 z^2&amp;\quad 2 y z - 2 x w\\ 2 x z - 2 y w&amp;\quad 2 y z + 2 x w&amp;\quad 1 - 2 x^2 - 2 y^2\\ \end{bmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>Finally, we translate the character to the position of the HMD, represented by a vector <span class="mathjax-tex">\({\mathbf {p}}_{\mathrm{hmd}} \in {\mathbb {R}}^{3 \times 1}\)</span>. Let <span class="mathjax-tex">\({\mathbf {T}}_{\mathrm{init}} \in {\mathbb {R}}^{4 \times 4}\)</span> be the initial transformation matrix and is described as:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {T}}_{\mathrm{init}} = \begin{bmatrix} 1&amp;\quad 0&amp;\quad 0&amp;\quad p_{x,{\mathrm{hmd}}}\\ 0&amp;\quad 1&amp;\quad 0&amp;\quad 0\\ 0&amp;\quad 0&amp;\quad 1&amp;\quad p_{z, {\mathrm{hmd}}}\\ 0&amp;\quad 0&amp;\quad 0&amp;\quad 1\end{bmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Combining all three matrices, the coordinate system of the character is placed so that the user can look down and see her/his virtual body. Multiplying the raw positional vector <span class="mathjax-tex">\({\mathbf {p}}_{\mathrm{raw}}[t] \in {\mathbb {R}}^{4 \times 1}\)</span> of the Vive Tracker at time step <i>t</i> with the inverse transformation matrix <span class="mathjax-tex">\({\mathbf {T}}^{\dagger }\)</span> will transform the sensor measurements to the character local coordinate system:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {T}}^{\dagger }&amp;= \big ({\mathbf {T}}_{\mathrm{init}} \cdot {\mathbf {R}}_{\mathrm{init}} \cdot {\mathbf {S}}_{\mathrm{init}}\big )^{-1}, \quad {\mathbf {T}}^{\dagger } \in {\mathbb {R}}^{4 \times 4} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {p}}_{\mathrm{trans}}[t]&amp;= {\mathbf {T}}^{\dagger } \cdot {\mathbf {p}}_\mathrm{raw}[t], \quad {\mathbf {p}}_{\mathrm{trans}}[t] \in {\mathbb {R}}^{4 \times 1} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Similar, the orientation of the Vive Tracker <span class="mathjax-tex">\({\mathbf {q}}_{\mathrm{raw}}[t] \in {\mathbb {R}}^{4 \times 1}\)</span>, has to be transformed as:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {q}}_{\mathrm{trans}}[t] = {\mathbf {q}}_{\mathrm{init}}^{-1} \cdot {\mathbf {q}}_\mathrm{raw}[t], \quad {\mathbf {q}}_{\mathrm{trans}}[t] \in {\mathbb {R}}^{4 \times 1} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Implementation of inverse kinematics with reduced Jacobian matrix</h4><p>To solve the IK problem, we use the transformed positional vector and the quaternion (rotation) computed in the previous step. With the known desired position and orientation of the end-effector (e.g., a hand or a foot), the angle of each predecessor joint (e.g., an elbow or a knee) can be computed using an iterative, numerical method. An overview of the algorithm is as follows:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Calculate error between desired and actual position as well as rotation</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Check for convergence</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Calculate Jacobian</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Calculate Pseudo-Inverse</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>Calculate joint angles for each bone joint</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">6.</span>
                        
                          <p>Apply quaternions to the transformation matrix</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">7.</span>
                        
                          <p>Update new positions.</p>
                        
                      </li>
                    </ol><p>In each iteration, in the first step, an error between the desired and actual position <span class="mathjax-tex">\(\Delta {\mathbf {e}}_{\mathrm{pos}}\)</span> as well as the desired and current rotation <span class="mathjax-tex">\(\Delta {\mathbf {e}}_{{\mathrm{rot}}}\)</span> of the end-effector has to be calculated. Subsequently, the error <span class="mathjax-tex">\(\Delta {\mathbf {e}} = [\Delta {\mathbf {e}}_{\mathrm{pos}};\,\Delta {\mathbf {e}}_{\mathrm{rot}}]\)</span> is normalized. When checking for convergence in the second step, the error is compared with the maximum error threshold, i.e., <span class="mathjax-tex">\(||\Delta {\mathbf {e}} || &lt; \epsilon\)</span>. When the end-effector is close enough to the desired location or when there is no significant change between current and desired rotation, the algorithm will terminate. Because the end-effector may not be able to reach the desired position, we have to specify the maximal iteration number. This can happen when the desired position is out of range and therefore too far away to be reached. Otherwise, the Jacobian matrix <span class="mathjax-tex">\({\mathbf {J}}\)</span> is calculated in the third step. The position and rotation values of the required axes can be obtained from a combined transformation matrix:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {^0}{}{\mathbf {C}}_j = \begin{bmatrix} {^0}{}{\mathbf {a}}_{xj}&amp;\quad {^0}{}{\mathbf {a}}_{yj}&amp;\quad {^0}{}{\mathbf {a}}_{zj}&amp;\quad {^0}{}{\mathbf {p}}_{j}\\ 0&amp;\quad 0&amp;\quad 0&amp;\quad 1\\ \end{bmatrix} \in {\mathbb {R}}^{4 \times 4}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <span class="mathjax-tex">\({^0}{}{\mathbf {a}}_j \in {\mathbb {R}}^{3 \times 1}\)</span> represents the global rotation around the <i>x-</i>, <i>y-</i> and <i>z</i>-axis and <span class="mathjax-tex">\({^0}{}{\mathbf {p}}_j \in {\mathbb {R}}^{3 \times 1}\)</span> represents the global position of the <i>j</i>th joint with respect to the origin.</p><p>The Jacobian <span class="mathjax-tex">\({\mathbf {J}}\)</span> is defined by the partial derivatives of the joint angles and the difference between the current position and the desired position of the end-effector. It can be determined by computing the cross-product of the joint angle and the change in end-effector location:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {^0}{}{\mathbf {J}}_{n,j} = \frac{\partial {^0}{}{\mathbf {p}}_{n}}{\partial \varvec{\theta }_j} = \begin{bmatrix} {^0}{}{\mathbf {a}}_{j} \times ({^0}{}{\mathbf {p}}_{n} - {^0}{}{\mathbf {p}}_{j})\\ {^0}{}{\mathbf {a}}_{j} \end{bmatrix} , \quad {^0}{}{\mathbf {J}}_{n,j} \in {\mathbb {R}}^{6 \times 1} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>where <span class="mathjax-tex">\({^0}{}{\mathbf {p}}_{n} \in {\mathbb {R}}^{3 \times 1}\)</span> represents the current position of the end-effector, <span class="mathjax-tex">\({^0}{}{\mathbf {a}}_{j} \in {\mathbb {R}}^{3 \times 1}\)</span> the rotation axis and <span class="mathjax-tex">\({^0}{}{\mathbf {p}}_{j} \in {\mathbb {R}}^{3 \times 1}\)</span> the position vector of the <i>j</i>th joint. In order to minimize the computational effort, we calculate for each end-effector only three partial derivatives. In other words, e.g., for a hand to reach the final position, we only determine the position and orientation of the three predecessor joints. The Jacobian <span class="mathjax-tex">\({\mathbf {J}}\)</span> is then build as:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {^0}{}{\mathbf {J}}_3 = \begin{bmatrix} {^0}{}{\mathbf {J}}_{3,1}&amp;{^0}{}{\mathbf {J}}_{3,2}&amp;{^0}{}{\mathbf {J}}_{3,3}\\ \end{bmatrix} \quad \in {\mathbb {R}}^{6 \times 3}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where the 0th joint specifies the root node and the 3rd joint the end-effector. Thus, to manipulate the hand, we rotate the three predecessor joints, i.e., lower arm, upper arm, and clavicle.</p><p>In the fourth step, the pseudo-inverse of the Jacobian <span class="mathjax-tex">\({\mathbf {J}}\)</span> has to be computed. Due to our adjustment of the Jacobian matrix, it will always have the same dimensionality:</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {J}}^{-1}_{\text {left}} = \underbrace{\big ({\mathbf {J}}^{\mathrm{T}} {\mathbf {J}} \big )^{-1} }_{3 \times 3} {\mathbf {J}}^{\mathrm{T}}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>The calculation of the left pseudo-inverse will lead to the determination of a smaller inverse matrix (<span class="mathjax-tex">\(3 \times 3\)</span>) and is therefore advantageous.</p><p>In the fifth step, the joint angles are calculated by multiplying the inverse Jacobian with the difference between desired and current position as well as rotation of the end-effector, as it can be seen in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0374-z#Equ10">10</a>:</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \Delta \varvec{\theta } = {\mathbf {J}}^{-1} \cdot \Delta {\mathbf {e}}, \quad \Delta \varvec{\theta } \in {\mathbb {R}}^{3 \times 1} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>In the sixth step, we can apply the new rotation to the joints:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \varvec{\theta }^{(k+1)} = \varvec{\theta }^{(k)} + \Delta \varvec{\theta } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>Before the quaternion can be applied to the transformation matrix, the angular limits have to be ensured as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec8">3.1.1</a>. Otherwise, the character hand or foot will reach the desired position; however, the individual joints within the kinematic chain can cause unnatural movements. The joint rotations are enforced through clamping between a lower bound (LB) and an upper bound (UB):</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \varvec{\theta }^{(k+1)} = {\left\{ \begin{array}{ll} \text {LB} &amp;{} \quad \text {if } \varvec{\theta }^{(k)} + {\mathbf {J}}^{-1} \Delta {\mathbf {e}} &lt; \text {LB}\\ \text {UB} &amp;{} \quad \text {if } \varvec{\theta }^{(k)} + {\mathbf {J}}^{-1} \Delta {\mathbf {e}} &gt; \text {UB}\\ \varvec{\theta }^{(k)} + {\mathbf {J}}^{-1} \Delta {\mathbf {e}} &amp;{} \quad \text {otherwise}\\ \end{array}\right. } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>To apply the rotation to a joint, the quaternion is first represented as a matrix (see Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0374-z#Equ1">1</a>). The local transformation matrix is then computed by multiplying the bind transformation matrix with the new rotation matrix.</p><p>Finally, in the last iteration step, the new rotation of each joint in the skeleton is calculated by updating the combined transformation matrix:</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {^0}{}{\mathbf {C}}_i = {^0}{}{\mathbf {P}}_i \cdot {\mathbf {L}}_i, \quad {\mathbf {C}}_i \in {\mathbb {R}}^{3 \times 3} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>where <span class="mathjax-tex">\({^0}{}{\mathbf {C}}_i\)</span> is the combined transformation matrix, <span class="mathjax-tex">\({\mathbf {L}}_i\)</span> is the local transformation matrix of the <i>i</i>th bone and <span class="mathjax-tex">\({^0}{}{\mathbf {P}}_i\)</span> is the combined transformation matrix of the <i>i</i>th bone’s parent. Thus, a bone’s combined transformation matrix is determined by first applying its local transformation and then by applying the local transform of its parent. If the maximum number of iterations is not yet reached, we go back to the first step. Otherwise, the algorithm terminates.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Animation</h4><p>To animate the avatar, the calculated new orientations have to be applied to the bone joints. While solving the IK, the quaternion of each joint was updated, depending on the desired position and orientation of the end-effector. However, the difference between the new joint orientation, compared to the joint orientation from the previous frame, is eventually large. In this case, we have to interpolate between the quaternions by applying the SLERP method. Then, to calculate the new skinned vertex position, we first have to calculate a final transformation <span class="mathjax-tex">\({^0}{}{\mathbf {F}}_j\)</span> by multiplying the combined transform <span class="mathjax-tex">\({^0}{}{\mathbf {C}}_j\)</span> with the inverse transform matrix.</p><p>By iterating over all vertices, we calculate the new position of every vertex with respect to the bone rotation. Because each vertex and, above all, vertices near the joint can be influenced by several bones, the new position of the vertex is determined by a weighted average of the influential bone transformations. Therefore, for each vertex, the new position vector is determined by multiplying the final transform matrix of the bone influencing this vertex with the current position vector and the bone weight. The final vertex position <span class="mathjax-tex">\({\mathbf {v}}_{\mathrm{new}}\)</span> is computed as proposed by Kavan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kavan L, Sloan PP, O’Sullivan C (2010) Fast and efficient skinning of animated meshes. Comput Graph Forum 29(2):327–336" href="/article/10.1007/s10055-018-0374-z#ref-CR23" id="ref-link-section-d49876e5434">2010</a>):</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathbf {v}}_{\mathrm{new}} = \sum _{n=0}^{n-1} \left( {^0}{}{\mathbf {F}}_n \cdot {\mathbf {v}}\right) \cdot w_n, \quad {\mathbf {v}}_{\mathrm{new}} \in {\mathbb {R}}^{3 \times 1} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p>where <span class="mathjax-tex">\({\mathbf {v}}\)</span> is the current vertex position, <span class="mathjax-tex">\({\mathbf {F}}_n\)</span> is the final transform of the <i>n</i>th bone, that influences vertex <span class="mathjax-tex">\({\mathbf {v}}\)</span> and <span class="mathjax-tex">\(w_n\)</span> is the weight of the <i>n</i>th bone.</p><p>Finally, the vertex buffer is updated with the new calculated vertex positions and normals. The vertex shader then uses this buffer to draw indexed vertices. In the current implementation, the vertex skinning calculation is done on a Central Processing Unit (CPU), that runs the sequential code as fast as possible. However, especially in the context of VR games, the execution speed has to be considered. Therefore, these computations could also be carried out by a Graphical Processing Unit (GPU), that can do hundreds of calculations in parallel. These would eventually speed-up the calculations of the animation system.</p><p>The currently developed prototype can track multiple end-effectors in order to animate the virtual body. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig3">3</a>, the actual posture of the user and the corresponding VR image is shown. In this case, the Vive Tracker was attached to the foot in order to synchronize the virtual leg with the user’s motions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig3_HTML.jpg" alt="figure3" loading="lazy" width="685" height="420" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The user’s actual posture (left) and a first-person perspective of the user looking down at the virtual body (right)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec12">Development of a latency measurement tool</h3><p>In the second part, a latency measurement tool was implemented to measure the total delay of the implemented body tracking system. The total delay of a VR-based tracking system represents the time at which the motion occurs, to the time the tracking system detects this motion, and the results are displayed on the HMD. Using this latency measurement tool, we expect to show that the total delay of the developed method stays below <span class="mathjax-tex">\(20\hbox { ms}\)</span>. This would satisfy the requirement for a real-time VR experience (Raaen and Kjellmo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Raaen K, Kjellmo I (2015) Measuring latency in virtual reality systems. In: Chorianopoulos K, Divitini M, Baalsrud Hauge J, Jaccheri L, Malaka R (eds) Entertainment computing—ICEC 2015. Springer, Cham, pp 457–462" href="/article/10.1007/s10055-018-0374-z#ref-CR32" id="ref-link-section-d49876e5730">2015</a>; Kasahara et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: changing sense of embodiment by spatial-temporal deformation of virtual human body. In: Proceedings of the 2017 CHI conference on human factors in computing systems, CHI’17. ACM, New York, NY, USA, pp 6438–6448" href="/article/10.1007/s10055-018-0374-z#ref-CR22" id="ref-link-section-d49876e5733">2017</a>).</p><p>The easiest way to measure the latency is to record the Vive Tracker and the output screen at the same time with a high-speed camera. When the Tracker moves in a specific pattern, a video can be analyzed frame by frame in order to identify distinctive motions and to calculate the delay between events. As proposed by Steed (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Steed A (2008) A simple method for estimating the latency of interactive, real-time graphics simulations. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST’08. ACM, New York, NY, USA, pp 123–129" href="/article/10.1007/s10055-018-0374-z#ref-CR40" id="ref-link-section-d49876e5739">2008</a>), the Tracker can be bind to a string in order to swing it. Applying this approach, the features can be identified by extracting the horizontal positions of both objects and detecting the local minima and maxima. By calculating the deviation between the local minimum or maximum of the Tracker and the local minimum or maximum of the virtual object, the frame difference between them can be calculated and finally, the total delay can be determined. Using this approach of identifying the frames at which the Tracker changes the direction and calculating the difference between them, the measurements include only the end-to-end latency of the developed application. However, we cannot make any statement about the accuracy.</p><p>To measure the end-to-end latency, the steps described below should be followed:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Load the video and select two bounding boxes, that contain the Vive Tracker and the corresponding virtual object</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Extract the horizontal position of both bounding boxes in each frame</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Apply the Gaussian kernel to smooth the data and normalize the samples</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Identify distinctive motions by detecting local peaks</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>Calculate the frame difference and determine the latency.</p>
                      
                    </li>
                  </ol><p>Once a capture of the targets in motion has been taken, an algorithm tracks their locations throughout the video. For this purpose, we use a KCF tracking algorithm, which is already implemented by OpenCV<sup><a href="#Fn15"><span class="u-visually-hidden">Footnote </span>15</a></sup> and is able to track multiple objects simultaneously. After the KCF tracker is initialized, a video can be loaded and two initial bounding boxes, one for the marker and one for the VR object, are defined.</p><p>To detect distinctive motions, the horizontal positions of both bounding boxes are smoothed and normalized. By applying the Gaussian kernel at different scales, we reduce the noise and ensure smooth data. Then the peaks, thus, the minima and maxima of both curves have to be identified. When a peak is detected, the frame number is saved in an array. This results in two equally long arrays, one containing frame numbers of the peaks for the Vive Tracker and the other containing frame numbers for the virtual object.</p><p>Once all frames have been processed and the features were extracted, the time difference between the two events <span class="mathjax-tex">\(t_{\text {motion}} - t_{\text {display}}\)</span> is calculated. The latency is calculated as follows:</p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} t = \frac{1}{N} \sum _{i = 0}^{N} \Big (X_i - {\tilde{X}}_i \Big ) \cdot \frac{1000}{\text {FPS}}\text {ms}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>where <i>N</i> is the number of peaks identified in a video, <span class="mathjax-tex">\(X_i\)</span> is the frame number of the <i>i</i>th peak for the Vive Tracker, and <span class="mathjax-tex">\({\tilde{X}}_i\)</span> is the frame number of the <i>i</i>th peak for the virtual object. The total delay is then determined by multiplying the mean difference between two events with the time, the camera needs to capture a new image. The flowchart of the latency measurement tool, including the entire calculation, is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="496" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>A flowchart of the latency measurement tool. The blue curve corresponds to the horizontal positions of the Vive Tracker and the red curve corresponds to the horizontal positions of the virtual object. One can easily recognize, that the red curve is shifted to the right. This horizontal difference of both curves indicates the latency of the distinctive motions (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Results</h2><div class="c-article-section__content" id="Sec13-content"><p>The main objective of this research is to evaluate the end-to-end latency of the proposed method as well as the accuracy and reliability of motions tracking with a Vive Tracker. To evaluate the performance of the tracking system, we determined the total delay with the developed latency measurement tool. We analyzed which limitations and errors can occur and how we can minimize them. Additionally, we evaluated if the person exploring VR can perceive the avatar movements as her/his own.</p><p>For the evaluation, an HTC Vive was connected to a computer running Microsoft Windows 10 to enable full-body tracking of the user. The computer has an 3.30 GHz Intel <span class="mathjax-tex">\(\hbox {Core}^{TM}\)</span> i7-5820K processor with 16 GB RAM and a NVIDIA <span class="mathjax-tex">\(\hbox {GeForce}^{TM}\)</span> GTX 980 graphics card. It has sufficient processing power and it fulfills the minimal requirements for the HTC Vive.<sup><a href="#Fn16"><span class="u-visually-hidden">Footnote </span>16</a></sup></p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig5_HTML.jpg" alt="figure5" loading="lazy" width="685" height="348" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>For hand tracking, the Vive Tracker must be attached to the wrist (left). To track the leg movements, an additional Tracker must be strapped to the ankle (right)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec14">Evaluation of the body tracking system</h3><p>The evaluation of the body tracking system was conducted in the TU Darmstadt, Germany. A total of 13 subjects participated, 1 female and 12 males with an average age of 27 years. First, the participants were asked to fill out the pre-study questionnaire, which included personal questions about gender, age, game consumption habits, VR experience, and body tracking. Afterward, the participants tried out the simulation. They strapped the Vive Tracker to the left and right wrist, as it can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig5">5</a>. The bands were able to fix the Tracker strongly enough so that it could not slip away and could remain in place for the time of the evaluation. For the immersive experience, the subjects were wearing an HTC Vive HMD to view the virtual character from the first-person perspective. The simulation was then run for approximately 5 min. Finally, the participants were again asked to fill out the post-study questionnaire.</p><p>The results of the pre-study questionnaire (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig6">6</a>) showed, that almost the two-fifths (<span class="mathjax-tex">\(38.5\%\)</span>) play video games more than 7 h/week. However, almost one-third (<span class="mathjax-tex">\(30.8\%\)</span>) never play video games. Furthermore, the majority (<span class="mathjax-tex">\(84.6\%\)</span>) had already prior experience with HMDs, e.g., Oculus Rift or HTC Vive. On the one hand, some subjects reported, that they already suffered from cybersickness, such as dizziness, nausea or a headache while wearing a HMD for a longer time. On the other hand, some of them also explicitly stated, that they never feel side effects of any kind. The majority, however, criticized an insufficiently low resolution of the currently available HMDs. Most of the participants (<span class="mathjax-tex">\(84.6\%\)</span>) can imagine body tracking in the context of VR-based games. Three-fourth (<span class="mathjax-tex">\(76.9\%\)</span>) would like to have an avatar, which synchronizes the movements and one-fourth (<span class="mathjax-tex">\(23.1\%\)</span>) is not sure if they want an avatar. However, the results of the post-study questionnaire reveal that almost all participants (<span class="mathjax-tex">\(92.3\%\)</span>) would like to have body tracking in VR games. Body tracking has been proposed in various application scenarios, e.g., goalkeeper, ego-shooter games and other first-person games, where the player can interact with virtual objects. The subjects furthermore suggested body tracking in medicine, where the user can remotely control the surgery with her/his own movements. VR in a combination with the body tracking could be used to train the users to complete a certain task correctly. Further scenarios could include virtual video calls or meetings.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="411" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The results of the pre-study questionnaire</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig7">7</a> shows the mean response of selected questions from the post-study questionnaire, along with their associated standard deviations. The results show that the majority in general likes the VR as well as the idea of body tracking in the VR.<sup><a href="#Fn17"><span class="u-visually-hidden">Footnote </span>17</a></sup> Furthermore, it was found that the subjects could feel as if they were “present” in the virtual environment and could also identify themselves with the avatar.<sup><a href="#Fn18"><span class="u-visually-hidden">Footnote </span>18</a></sup> The subjects wanted to see the full-body avatar and not only the arms, as in the most current first-person games. Some subjects stated they liked that the avatar reflects the movements of the arms; however, they missed the tracking of the legs. Since the current implementation can handle multiple end-effectors, we would only need additional Vive Tracker in order to track the hands and legs simultaneously.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="223" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Rating the simulation, where 1 stands for <i>totally incorrect</i>, 2 for <i>kind of incorrect</i>, 3 for <i>not sure</i>, 4 for <i>kind of correct</i> and 5 for <i>totally correct</i>. The bar shows the mean responses to questions from the post-study questionnaire and the error bars indicate the standard deviations</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The results of the body tracking show that the movements of the avatar corresponded to the real movements of the user and that tracking provided accurate positions.<sup><a href="#Fn19"><span class="u-visually-hidden">Footnote </span>19</a></sup> However, the orientation of the elbow did not always correspond to the reality. Some subjects reported, that the elbow was sometimes twisted or that they could perceive some inaccuracies in arm tracking. To evaluate the accuracy, the subjects were asked to perform various movements, including small and large movements at different speeds. Although the most subject stated that the tracking was quite accurate, sometimes the position of the virtual hands was different from the position of the real hands. This was especially noticeable when touching the own arms or hands. Subjects reported that while the fingers were touching in the real world, the virtual fingers were too far away from each other or they were overlapping. In order to improve the accuracy, the length of the upper and lower arm could be considered. Moreover, collision detection should be incorporated in order to prevent self-collision.</p><p>The results of the tracking itself suggest that the subjects only perceived a low latency.<sup><a href="#Fn20"><span class="u-visually-hidden">Footnote </span>20</a></sup> A low latency was identified for very fast movements by only one participant, who plays the games more than 7 h a week. As it will be described in the next section, the total delay of the tracking remains very low, at <span class="mathjax-tex">\(6.71 \pm \, 0.80\hbox { ms}\)</span>. Some subjects reported that the tracking had some jitter problems.<sup><a href="#Fn21"><span class="u-visually-hidden">Footnote </span>21</a></sup> This sometimes happened, when the Vive Tracker was not able to detect enough laser sweeps from the base stations. When holding an arm in a steady position, no noise or other tracking errors could be identified. Three subjects, however, reported, that the arm was for a short moment locked at some position. Then, after the subjects stretched the arm again, the arm “jumped” to the right position.</p><p>Finally, the results show that almost all subjects would like to have body tracking also in other VR games.<sup><a href="#Fn22"><span class="u-visually-hidden">Footnote </span>22</a></sup> For a more immersive experience, finger recognition should be included, e.g., using a Leap Motion<sup><a href="#Fn23"><span class="u-visually-hidden">Footnote </span>23</a></sup> device or special gloves such as Hi5 VR Gloves,<sup><a href="#Fn24"><span class="u-visually-hidden">Footnote </span>24</a></sup> VRgluv,<sup><a href="#Fn25"><span class="u-visually-hidden">Footnote </span>25</a></sup> HaptX,<sup><a href="#Fn26"><span class="u-visually-hidden">Footnote </span>26</a></sup> and VRtouch<sup><a href="#Fn27"><span class="u-visually-hidden">Footnote </span>27</a></sup>), which can detect the motion of each individual finger. Some subjects stated that they liked to have nothing to hold in their hands. However, the Vive Tracker that was fixed to a hand, is big and is actually developed to be attached to sporting equipment. In the future work, we could also create a smaller and lighter Tracker, e.g., using only a few infrared sensors in a bracelet that can be attached to the hand as well as ankle.</p><p>The total number of 13 participants is too low for any statistical conclusions. In addition to the problem of too few participants, some of them were friends or colleagues. Although the study participants were asked to answer the questions honestly, one cannot rule out that the feedback still was more beneficial. Therefore, an evaluation with more subjects should be carried out in future work.</p><h3 class="c-article__sub-heading" id="Sec15">Evaluation of latency measurements</h3><p>For the second part of the evaluation, an estimation using a latency measuring tool based on an automatic frame counting method using a video camera was made. The Vive Tracker and the output of the virtual environment were captured by a single phone camera (iPhone SE) at 240 FPS and <span class="mathjax-tex">\(1280 \times 720\)</span> pixels resolution. The virtual environment was rendered on a gaming monitor with a <span class="mathjax-tex">\(144\hbox { Hz}\)</span> refresh rate and G-Sync support. Thanks to G-Sync, the frame rate of the output device can be adapted which allows us to maintain the frame rate at the highest possible value of the VR system. Thus, it was possible to ensure a frame rate of 90 FPS.</p><p>In the first step, the latency of the Vive Tracker itself was determined. Thus, the latency of the Tracker as provided by the Vive system, without further processing (thus, without IK or other calculations), was measured. The measurements have shown a latency of <span class="mathjax-tex">\(6.07 \pm 1.36\hbox { ms}\)</span>. Since the developed system cannot obtain a better latency than the one provided by the Vive system, we want to get as close to the value as possible.</p><p>In the second step, the latency of the developed body tracking system using a Vive Tracker is measured. That is the total time between making a movement, sensing it by the Vive system, solving the IK problem and displaying the motion. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0374-z#Tab2">2</a> shows the mean and standard deviation of the measured latency due to the different maximum number of iterations. For each trial (5, 10, 30, 50 and 100 maximum number of iterations), nine measurements were provided and the average (AVG), as well as the standard deviation (STD), were calculated. As one can see, the body tracking system with five maximum number of iterations shows the worst results with an average delay of almost <span class="mathjax-tex">\(200\hbox { ms}\)</span>. With a higher iteration number (10, 30, 50 and 100), the delay will significantly decrease. From these results, we can assume, that with a very small maximum number of iterations, the end-effector will move toward the desired position, but the joint angle will change over time only very slowly. In this case, the IK solver will not be able to provide an appropriate set of joint configurations in order to reach the desired position as smoothly, rapidly and as accurately as possible. The results of the latency evaluation suggest that the maximum number of iterations is very important for the performance of the IK solver.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Measured latency due to the different maximal steps of the IK solver</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0374-z/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0374-z#Fig8">8</a>, with a higher iteration number, the latency will exponentially fall. One would normally expect increasing computational costs in terms of time. As already mentioned before, with fast movements, a very small number of iterations will indeed move the end-effector toward the desired position. However, it will always stay too far away from this desired position. Thus, for fast movements, we obtain better results with a higher number of iterations. If the speed of the movement is very slow, even a lower number of iterations is enough for the convergence because the position in the current frame is almost the same as in the previous frame. As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0374-z#Sec10">3.1.3</a>, the IK solver converges if the end-effector is close enough to the desired location or when there is no significant change between current and desired rotation. However, on average after 95 iterations there is no significant change between desired and current position as well as orientation. If the algorithm would terminate at much higher iteration number, without checking if the end-effector is close enough to the desired position, the latency would increase. Due to many calculations (e.g., calculating an inverse of a non-square matrix), it would not be possible to complete the computations before the next frame needs to be rendered. Thus, the frame rate would drop rapidly, which would decline the performance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0374-z/MediaObjects/10055_2018_374_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="392" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The measured latency. With the higher iteration number, the latency will exponentially fall. However, with a higher maximum number of iteration, the frame rate will drop since we would not be able to complete all computations before the next frame needs to be rendered</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0374-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The end-to-end latency of <span class="mathjax-tex">\(6.71\hbox { ms}\)</span> shows that the implemented solution can reconstruct the motions in real-time. Since the result stays well below <span class="mathjax-tex">\(20\hbox { ms}\)</span>, it meets the requirements for real-time VR experiences (Raaen and Kjellmo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Raaen K, Kjellmo I (2015) Measuring latency in virtual reality systems. In: Chorianopoulos K, Divitini M, Baalsrud Hauge J, Jaccheri L, Malaka R (eds) Entertainment computing—ICEC 2015. Springer, Cham, pp 457–462" href="/article/10.1007/s10055-018-0374-z#ref-CR32" id="ref-link-section-d49876e7725">2015</a>; Kasahara et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: changing sense of embodiment by spatial-temporal deformation of virtual human body. In: Proceedings of the 2017 CHI conference on human factors in computing systems, CHI’17. ACM, New York, NY, USA, pp 6438–6448" href="/article/10.1007/s10055-018-0374-z#ref-CR22" id="ref-link-section-d49876e7728">2017</a>). Compared to the results based on the publication by Jiang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jiang F, Yang X, Feng L (2016) Real-time full-body motion reconstruction and recognition for off-the-shelf VR devices. In: Proceedings of the 15th ACM SIGGRAPH conference on virtual-reality continuum and its applications in industry—Volume 1, VRCAI’16. ACM, pp 309–318" href="/article/10.1007/s10055-018-0374-z#ref-CR20" id="ref-link-section-d49876e7731">2016</a>) with a total latency of <span class="mathjax-tex">\(7\hbox { ms}\)</span>, our solution provides a slightly better result. In this work, two Vive Controllers were used. Similarly, Seele et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Seele S, Misztal S, Buhler H, Herpers R, Schild J (2017) Here’s looking at you anyway!: how important is realistic gaze behavior in co-located social virtual reality games? In: Proceedings of the annual symposium on computer-human interaction in play, CHI PLAY’17. ACM, New York, NY, USA, pp 531–540" href="/article/10.1007/s10055-018-0374-z#ref-CR35" id="ref-link-section-d49876e7761">2017</a>) also used two Vive Controller. However, only the upper body was reconstructed and no latency was measured. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0374-z#Tab3">3</a> summarizes the end-to-end latencies of the related work. All these publications tracked full-body movements and visualized an avatar. As it can be seen, we could achieve similar results or even much lower latency.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Latency results of the related work</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0374-z/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Compared to the latency of the Vive system (without IK solution) with <span class="mathjax-tex">\(6.07\hbox { ms}\)</span>, we can still improve our method. We expected a total delay below <span class="mathjax-tex">\(11.11\hbox { ms}\)</span> since this would satisfy the refresh rate of the HTC Vive HMD, which is <span class="mathjax-tex">\(90\hbox { Hz}\)</span>. Thus, the latency measurements fulfill our expectations. However, because the tracked and the corresponding VR objects were captured with a camera at 240 FPS, a latency below <span class="mathjax-tex">\(4.16\hbox { ms}\)</span> cannot be detected at all. In the future work, an even better camera, which is capable of recording at a high-speed, could be used in order to measure the latency even more accurately.</p><p>The overall evaluation results suggest that the algorithm can be further optimized. On the one hand, the current performance of the implementation can be improved, so that a smaller number of iterations would be needed to obtain the best solution. Therefore, we must first evaluate the performance of computing the Jacobian inverse. In the current implementation, a pseudo-inverse method is used to approximate the inverse of the Jacobian matrix. By applying a more computationally efficient approach to calculate the inverse, e.g., damped least squares, we could reduce the computational cost, complex matrix calculations, and singularity problems. On the other hand, we could minimize the user’s experience of latency by predicting their movements. More specifically, we could analyze the posture of the user and their movements in order to predict the actions in the virtual environment to further reduce the latency.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusion</h2><div class="c-article-section__content" id="Sec16-content"><p>In this paper, a novel body tracking system using IK approach with reduced Jacobian Matrix was developed. Such a real-time solution can be used for immersive VR-based games. By strapping only a small number of Vive Tracker to the player, the full-body motions of the player can be transformed into a virtual avatar. With the tracked motions, even the gestures can be recognized in order to create multiplayer VR experiences. The evaluation with the latency measurement tool showed a very low delay of only <span class="mathjax-tex">\(6.71 \pm 0.80\hbox { ms}\)</span>. Thus, the results show that the proposed method is satisfied with the technical requirement of the HTC Vive HMD and fulfill our expectations. Furthermore, compared to the related work, our latency evaluation shows similar or even better results. Our system can provide an appropriate set of joint configuration in order to reach the desired position as smoothly, rapidly and as accurately as possible in real-time. The evaluation with the participants revealed that the position and orientation of the arms were accurately tracked. Because the movements of the virtual body corresponded to the real movements of the users, the user could feel like they were a part of the VR and could identify themselves with the avatar. The evaluation with the subjects also validated that a minority could perceive only a low end-to-end latency.</p><p>Future research will focus on making the body tracking even more robust and reliable. The effectiveness of the iterative method to solve the IK should be improved. Therefore, we should reduce the computational cost by minimizing the maximum number of iterations that are needed to calculate the appropriate orientation of bones. In particular, because there is no objective evaluation on the accuracy in this research (the accuracy was only evaluated with the subjects), in future work a tool should be developed in order to measure how accurate the developed body tracking system is.</p><p>Since in the current evaluation the participants could see the full-body avatar, but only the hands were animated, also feet should be animated in the future work. By attaching an additional Vive Tracker to the back or hip as well as feet, the user should be able to see an animated avatar while walking, dancing or jumping. To further improve the immersion in the VR experience, the steps of the user could be identified in order to create stepping sound. Furthermore, we could do a comparative study, comparing different presence approaches, e.g., full-body animated avatar versus only animated hands as well as even showing only the Vive Controller or Tracker.</p><p>Additionally, collision detection should be considered, e.g., to interact with the environment. Collision detection is also important in the detection of body movements since we do not want the body limbs to intersect. When the user tries to touch the virtual body, the collision detection should prevent that the hands go through the body. Another important aspect is the appearance of the avatar. Therefore, a tool to personalize the avatar body, e.g., based on muscles, clothes and skin color should be integrated into the pipeline, to create an even more immersive VR experience.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Vive Tracker: <a href="https://www.vive.com/us/vive-tracker/">https://www.vive.com/us/vive-tracker/</a>, last visited on April 3rd, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>IKinema Orion: <a href="https://ikinema.com/orion">https://ikinema.com/orion</a>, last visited on April 3rd, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>Vive IK Demo: <a href="https://github.com/JamesBear/vive_ik_demo">https://github.com/JamesBear/vive_ik_demo</a>, last visited on April 3rd, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>Final IK: <a href="https://assetstore.unity.com/packages/tools/animation/final-ik-14290">https://assetstore.unity.com/packages/tools/animation/final-ik-14290</a>, last visited on April 3rd, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>Body Tracking Demo: <a href="https://github.com/CatCuddler/BodyTracking">https://github.com/CatCuddler/BodyTracking</a>, last visited on April 4th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>Microsoft Kinect: <a href="https://developer.microsoft.com/en-us/windows/kinect">https://developer.microsoft.com/en-us/windows/kinect</a>, last visited on January 28th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>Nintendo Wii: <a href="https://www.nintendo.co.uk/Wii/Wii-94559.html">https://www.nintendo.co.uk/Wii/Wii-94559.html</a>, last visited on January 28th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8"><span class="c-article-footnote--listed__index">8.</span><div class="c-article-footnote--listed__content"><p>OptiTrack system: <a href="http://www.optitrack.com">http://www.optitrack.com</a>, last visited on January 17th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn9"><span class="c-article-footnote--listed__index">9.</span><div class="c-article-footnote--listed__content"><p>PrioVR: <a href="https://yostlabs.com/priovr/">https://yostlabs.com/priovr/</a>, last visited on July 31st, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn10"><span class="c-article-footnote--listed__index">10.</span><div class="c-article-footnote--listed__content"><p>Perception Neuron: <a href="https://neuronmocap.com">https://neuronmocap.com</a>, last visited on July 31st, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn11"><span class="c-article-footnote--listed__index">11.</span><div class="c-article-footnote--listed__content"><p>Xsense: <a href="https://www.xsens.com/">https://www.xsens.com/</a>, last visited on 31st July, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn12"><span class="c-article-footnote--listed__index">12.</span><div class="c-article-footnote--listed__content"><p>MakeHuman: <a href="http://www.makehuman.org">http://www.makehuman.org</a>, last visited on February 3rd, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn13"><span class="c-article-footnote--listed__index">13.</span><div class="c-article-footnote--listed__content"><p>Kore: <a href="https://github.com/Kode/Kore">https://github.com/Kode/Kore</a>, last visited on April 3rd, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn14"><span class="c-article-footnote--listed__index">14.</span><div class="c-article-footnote--listed__content"><p>OpenGEX: <a href="http://opengex.org">http://opengex.org</a>, last visited on February 21st, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn15"><span class="c-article-footnote--listed__index">15.</span><div class="c-article-footnote--listed__content"><p>KCF Tracker: <a href="http://docs.opencv.org/trunk/d2/dff/classcv_1_1TrackerKCF.html">http://docs.opencv.org/trunk/d2/dff/classcv_1_1TrackerKCF.html</a>, last visited on February 17th, 2017.</p></div></li><li class="c-article-footnote--listed__item" id="Fn16"><span class="c-article-footnote--listed__index">16.</span><div class="c-article-footnote--listed__content"><p>Minimum requirements: <a href="https://www.vive.com/us/ready/">https://www.vive.com/us/ready/</a>, last visited on February 5th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn17"><span class="c-article-footnote--listed__index">17.</span><div class="c-article-footnote--listed__content"><p>Question: “I find the VR in general exciting”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.92\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.27\)</span>, question: “I like the idea of body tracking in VR”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.92\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.27\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn18"><span class="c-article-footnote--listed__index">18.</span><div class="c-article-footnote--listed__content"><p>Question: “I felt like I was a part of the VR”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.3\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.48\)</span>, question: “I could identify myself with the avatar”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.07\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.49\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn19"><span class="c-article-footnote--listed__index">19.</span><div class="c-article-footnote--listed__content"><p>Question: “The movements in the VR have corresponded to the real movements”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.23\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.59\)</span>, question: “The tracking was accurate”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.07\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.64\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn20"><span class="c-article-footnote--listed__index">20.</span><div class="c-article-footnote--listed__content"><p>Question: “The movements of the avatar were delayed”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=1.23\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.43\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn21"><span class="c-article-footnote--listed__index">21.</span><div class="c-article-footnote--listed__content"><p>Question: “The tracking had some jitter problems”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=2.37\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,1.25\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn22"><span class="c-article-footnote--listed__index">22.</span><div class="c-article-footnote--listed__content"><p>Question: “I would like body tracking also in other VR games”, five-level Likert scale, <span class="mathjax-tex">\(N=13\)</span>, <span class="mathjax-tex">\({\hbox {AVR}}=4.92\)</span>, <span class="mathjax-tex">\({\hbox {SD}}=\pm \,0.27\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn23"><span class="c-article-footnote--listed__index">23.</span><div class="c-article-footnote--listed__content"><p>Leap Motion: <a href="https://www.leapmotion.com">https://www.leapmotion.com</a>, last visited on January 19th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn24"><span class="c-article-footnote--listed__index">24.</span><div class="c-article-footnote--listed__content"><p>Hi5 VR Glove: <a href="https://hi5vrglove.com">https://hi5vrglove.com</a>, last visited on January 19th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn25"><span class="c-article-footnote--listed__index">25.</span><div class="c-article-footnote--listed__content"><p>VRgluv: <a href="https://vrgluv.com">https://vrgluv.com</a>, last visited on January 19th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn26"><span class="c-article-footnote--listed__index">26.</span><div class="c-article-footnote--listed__content"><p>HaptX: <a href="https://haptx.com">https://haptx.com</a>, last visited on January 19th, 2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn27"><span class="c-article-footnote--listed__index">27.</span><div class="c-article-footnote--listed__content"><p>VRtouch: <a href="https://www.gotouchvr.com/order_vrtouch/">https://www.gotouchvr.com/order_vrtouch/</a>, last visited on January 19th, 2018.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Aristidou, J. Lasenby, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Aristidou A, Lasenby J (2011) FABRIK: a fast, iterative solver for the inverse kinematics problem. Graph Model" /><p class="c-article-references__text" id="ref-CR1">Aristidou A, Lasenby J (2011) FABRIK: a fast, iterative solver for the inverse kinematics problem. Graph Models 73(5):243–260</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.gmod.2011.05.003" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=FABRIK%3A%20a%20fast%2C%20iterative%20solver%20for%20the%20inverse%20kinematics%20problem&amp;journal=Graph%20Models&amp;volume=73&amp;issue=5&amp;pages=243-260&amp;publication_year=2011&amp;author=Aristidou%2CA&amp;author=Lasenby%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Banakou, R. Groten, M. Slater, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Banakou D, Groten R, Slater M (2013) Illusory ownership of a virtual child body causes overestimation of objec" /><p class="c-article-references__text" id="ref-CR2">Banakou D, Groten R, Slater M (2013) Illusory ownership of a virtual child body causes overestimation of object sizes and implicit attitude changes. Proc Natl Acad Sci 110(31):12846–12851</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1306779110" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Illusory%20ownership%20of%20a%20virtual%20child%20body%20causes%20overestimation%20of%20object%20sizes%20and%20implicit%20attitude%20changes&amp;journal=Proc%20Natl%20Acad%20Sci&amp;volume=110&amp;issue=31&amp;pages=12846-12851&amp;publication_year=2013&amp;author=Banakou%2CD&amp;author=Groten%2CR&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bolton J, Lambert M, Lirette D, Unsworth B (2014) PaperDude: a virtual reality cycling exergame. CHI’14 Extend" /><p class="c-article-references__text" id="ref-CR3">Bolton J, Lambert M, Lirette D, Unsworth B (2014) PaperDude: a virtual reality cycling exergame. CHI’14 Extended Abstracts on Human Factors in Computing Systems. CHI EA’14. ACM, New York, NY, USA, pp 475–478</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Botev J, Rothkugel S (2017) High-precision gestural input for immersive large-scale distributed virtual enviro" /><p class="c-article-references__text" id="ref-CR4">Botev J, Rothkugel S (2017) High-precision gestural input for immersive large-scale distributed virtual environments. In: Proceedings of the 9th workshop on massively multiuser virtual environments, MMVE’17. ACM, New York, NY, USA, pp 7–11</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Caserman P, Krabbe P, Wojtusch J, von Stryk O (2016) Real-time step detection using the integrated sensors of " /><p class="c-article-references__text" id="ref-CR5">Caserman P, Krabbe P, Wojtusch J, von Stryk O (2016) Real-time step detection using the integrated sensors of a head-mounted display. In: 2016 IEEE international conference on systems, man, and cybernetics (SMC), pp 3510–3515</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JCP. Chan, H. Leung, JKT. Tang, T. Komura, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Chan JCP, Leung H, Tang JKT, Komura T (2011) A virtual reality dance training system using motion capture tech" /><p class="c-article-references__text" id="ref-CR6">Chan JCP, Leung H, Tang JKT, Komura T (2011) A virtual reality dance training system using motion capture technology. IEEE Trans Learn Technol 4(2):187–195</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTLT.2010.27" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20virtual%20reality%20dance%20training%20system%20using%20motion%20capture%20technology&amp;journal=IEEE%20Trans%20Learn%20Technol&amp;volume=4&amp;issue=2&amp;pages=187-195&amp;publication_year=2011&amp;author=Chan%2CJCP&amp;author=Leung%2CH&amp;author=Tang%2CJKT&amp;author=Komura%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SW. Choi, MW. Seo, SL. Lee, JH. Park, EY. Oh, JS. Baek, SJ. Kang, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Choi SW, Seo MW, Lee SL, Park JH, Oh EY, Baek JS, Kang SJ (2016) Head position model-based latency measurement" /><p class="c-article-references__text" id="ref-CR7">Choi SW, Seo MW, Lee SL, Park JH, Oh EY, Baek JS, Kang SJ (2016) Head position model-based latency measurement system for virtual reality head mounted display. SID Symp Dig Tech Papers 47(1):1381–1384</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fsdtp.10930" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Head%20position%20model-based%20latency%20measurement%20system%20for%20virtual%20reality%20head%20mounted%20display&amp;journal=SID%20Symp%20Dig%20Tech%20Papers&amp;volume=47&amp;issue=1&amp;pages=1381-1384&amp;publication_year=2016&amp;author=Choi%2CSW&amp;author=Seo%2CMW&amp;author=Lee%2CSL&amp;author=Park%2CJH&amp;author=Oh%2CEY&amp;author=Baek%2CJS&amp;author=Kang%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodi" /><p class="c-article-references__text" id="ref-CR8">Collingwoode-Williams T, Gillies M, McCall C, Pan X (2017) The effect of lip and arm synchronization on embodiment: a pilot study. In: 2017 IEEE virtual reality (VR). IEEE, pp 253–254</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Dempsey, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Dempsey P (2016) The teardown: HTC Vive VR headset. Eng Technol 11(7–8):80–81" /><p class="c-article-references__text" id="ref-CR9">Dempsey P (2016) The teardown: HTC Vive VR headset. Eng Technol 11(7–8):80–81</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20teardown%3A%20HTC%20Vive%20VR%20headset&amp;journal=Eng%20Technol&amp;volume=11&amp;issue=7%E2%80%938&amp;pages=80-81&amp;publication_year=2016&amp;author=Dempsey%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PR. Desai, PN. Desai, KD. Ajmera, K. Mehta, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Desai PR, Desai PN, Ajmera KD, Mehta K (2014) A review paper on oculus rift—a virtual reality headset. Int J E" /><p class="c-article-references__text" id="ref-CR10">Desai PR, Desai PN, Ajmera KD, Mehta K (2014) A review paper on oculus rift—a virtual reality headset. Int J Eng Trends Technol (IJETT) 13(4):175–179</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.14445%2F22315381%2FIJETT-V13P237" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20paper%20on%20oculus%20rift%E2%80%94a%20virtual%20reality%20headset&amp;journal=Int%20J%20Eng%20Trends%20Technol%20%28IJETT%29&amp;volume=13&amp;issue=4&amp;pages=175-179&amp;publication_year=2014&amp;author=Desai%2CPR&amp;author=Desai%2CPN&amp;author=Ajmera%2CKD&amp;author=Mehta%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Desai K, Raghuraman S, Jin R, Prabhakaran B (2017) QoE studies on interactive 3D tele-immersion. In: 2017 IEEE" /><p class="c-article-references__text" id="ref-CR11">Desai K, Raghuraman S, Jin R, Prabhakaran B (2017) QoE studies on interactive 3D tele-immersion. In: 2017 IEEE international symposium on multimedia (ISM), pp 130–137</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Farahani, R. Post, J. Duboy, I. Ahmed, BJ. Kolowitz, T. Krinchai, SE. Monaco, JL. Fine, DJ. Hartman, L. Pantanowitz, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (" /><p class="c-article-references__text" id="ref-CR12">Farahani N, Post R, Duboy J, Ahmed I, Kolowitz BJ, Krinchai T, Monaco SE, Fine JL, Hartman DJ, Pantanowitz L (2016) Exploring virtual reality technology and the oculus rift for the examination of digital pathology slides. J Pathol Inform 7:22</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4103%2F2153-3539.181766" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20virtual%20reality%20technology%20and%20the%20oculus%20rift%20for%20the%20examination%20of%20digital%20pathology%20slides&amp;journal=J%20Pathol%20Inform&amp;volume=7&amp;publication_year=2016&amp;author=Farahani%2CN&amp;author=Post%2CR&amp;author=Duboy%2CJ&amp;author=Ahmed%2CI&amp;author=Kolowitz%2CBJ&amp;author=Krinchai%2CT&amp;author=Monaco%2CSE&amp;author=Fine%2CJL&amp;author=Hartman%2CDJ&amp;author=Pantanowitz%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Friðriksson FA, Kristjánsson HS, Sigurðsson DA, Thue D, Vilhjálmsson HH (2016) Become your avatar: fast skelet" /><p class="c-article-references__text" id="ref-CR13">Friðriksson FA, Kristjánsson HS, Sigurðsson DA, Thue D, Vilhjálmsson HH (2016) Become your avatar: fast skeletal reconstruction from sparse data for fully-tracked VR. In: Proceedings of the 26th international conference on artificial reality and telexistence and the 21st Eurographics symposium on virtual environments: posters and demos, pp 19–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Friston, A. Steed, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Friston S, Steed A (2014) Measuring latency in virtual environments. IEEE Trans Vis Comput Graph 20(4):616–625" /><p class="c-article-references__text" id="ref-CR14">Friston S, Steed A (2014) Measuring latency in virtual environments. IEEE Trans Vis Comput Graph 20(4):616–625</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2014.30" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20latency%20in%20virtual%20environments&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=20&amp;issue=4&amp;pages=616-625&amp;publication_year=2014&amp;author=Friston%2CS&amp;author=Steed%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Galna, G. Barry, D. Jackson, D. Mhiripiri, P. Olivier, L. Rochester, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Galna B, Barry G, Jackson D, Mhiripiri D, Olivier P, Rochester L (2014) Accuracy of the microsoft kinect senso" /><p class="c-article-references__text" id="ref-CR15">Galna B, Barry G, Jackson D, Mhiripiri D, Olivier P, Rochester L (2014) Accuracy of the microsoft kinect sensor for measuring movement in people with Parkinson’s disease. Gait Posture 39(4):1062–1068</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.gaitpost.2014.01.008" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Accuracy%20of%20the%20microsoft%20kinect%20sensor%20for%20measuring%20movement%20in%20people%20with%20Parkinson%E2%80%99s%20disease&amp;journal=Gait%20Posture&amp;volume=39&amp;issue=4&amp;pages=1062-1068&amp;publication_year=2014&amp;author=Galna%2CB&amp;author=Barry%2CG&amp;author=Jackson%2CD&amp;author=Mhiripiri%2CD&amp;author=Olivier%2CP&amp;author=Rochester%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Goradia, J. Doshi, L. Kurup, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Goradia I, Doshi J, Kurup L (2014) A review paper on oculus rift &amp; project morpheus. Int J Curr Eng Technol 4(" /><p class="c-article-references__text" id="ref-CR16">Goradia I, Doshi J, Kurup L (2014) A review paper on oculus rift &amp; project morpheus. Int J Curr Eng Technol 4(5):3196–3200</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20paper%20on%20oculus%20rift%20%26%20project%20morpheus&amp;journal=Int%20J%20Curr%20Eng%20Technol&amp;volume=4&amp;issue=5&amp;pages=3196-3200&amp;publication_year=2014&amp;author=Goradia%2CI&amp;author=Doshi%2CJ&amp;author=Kurup%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Grochow, SL. Martin, A. Hertzmann, Z. Popović, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Grochow K, Martin SL, Hertzmann A, Popović Z (2004) Style-based inverse kinematics. ACM Trans Graph 23(3):522–" /><p class="c-article-references__text" id="ref-CR17">Grochow K, Martin SL, Hertzmann A, Popović Z (2004) Style-based inverse kinematics. ACM Trans Graph 23(3):522–531</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1015706.1015755" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Style-based%20inverse%20kinematics&amp;journal=ACM%20Trans%20Graph&amp;volume=23&amp;issue=3&amp;pages=522-531&amp;publication_year=2004&amp;author=Grochow%2CK&amp;author=Martin%2CSL&amp;author=Hertzmann%2CA&amp;author=Popovi%C4%87%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Huang, Q. Wang, M. Fratarcangeli, K. Yan, C. Pelachaud, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Huang J, Wang Q, Fratarcangeli M, Yan K, Pelachaud C (2017) Multi-variate gaussian-based inverse kinematics. C" /><p class="c-article-references__text" id="ref-CR18">Huang J, Wang Q, Fratarcangeli M, Yan K, Pelachaud C (2017) Multi-variate gaussian-based inverse kinematics. Comput Graph Forum 36(8):418–428</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fcgf.13089" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-variate%20gaussian-based%20inverse%20kinematics&amp;journal=Comput%20Graph%20Forum&amp;volume=36&amp;issue=8&amp;pages=418-428&amp;publication_year=2017&amp;author=Huang%2CJ&amp;author=Wang%2CQ&amp;author=Fratarcangeli%2CM&amp;author=Yan%2CK&amp;author=Pelachaud%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jain D, Sra M, Guo J, Marques R, Wu R, Chiu J, Schmandt C (2016) Immersive terrestrial scuba diving using virt" /><p class="c-article-references__text" id="ref-CR19">Jain D, Sra M, Guo J, Marques R, Wu R, Chiu J, Schmandt C (2016) Immersive terrestrial scuba diving using virtual reality. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems. ACM, New York, USA, pp 1563–1569</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jiang F, Yang X, Feng L (2016) Real-time full-body motion reconstruction and recognition for off-the-shelf VR " /><p class="c-article-references__text" id="ref-CR20">Jiang F, Yang X, Feng L (2016) Real-time full-body motion reconstruction and recognition for off-the-shelf VR devices. In: Proceedings of the 15th ACM SIGGRAPH conference on virtual-reality continuum and its applications in industry—Volume 1, VRCAI’16. ACM, pp 309–318</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson M, Humer I, Zimmerman B, Shallow J, Tahai L, Pietroszek K (2016) Low-cost latency compensation in moti" /><p class="c-article-references__text" id="ref-CR21">Johnson M, Humer I, Zimmerman B, Shallow J, Tahai L, Pietroszek K (2016) Low-cost latency compensation in motion tracking for smartphone-based head mounted display. In: Proceedings of the international working conference on advanced visual interfaces, AVI’16. ACM, New York, NY, USA, pp 316–317</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: cha" /><p class="c-article-references__text" id="ref-CR22">Kasahara S, Konno K, Owaki R, Nishi T, Takeshita A, Ito T, Kasuga S, Ushiba J (2017) Malleable embodiment: changing sense of embodiment by spatial-temporal deformation of virtual human body. In: Proceedings of the 2017 CHI conference on human factors in computing systems, CHI’17. ACM, New York, NY, USA, pp 6438–6448</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Kavan, PP. Sloan, C. O’Sullivan, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Kavan L, Sloan PP, O’Sullivan C (2010) Fast and efficient skinning of animated meshes. Comput Graph Forum 29(2" /><p class="c-article-references__text" id="ref-CR23">Kavan L, Sloan PP, O’Sullivan C (2010) Fast and efficient skinning of animated meshes. Comput Graph Forum 29(2):327–336</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2009.01602.x" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20and%20efficient%20skinning%20of%20animated%20meshes&amp;journal=Comput%20Graph%20Forum&amp;volume=29&amp;issue=2&amp;pages=327-336&amp;publication_year=2010&amp;author=Kavan%2CL&amp;author=Sloan%2CPP&amp;author=O%E2%80%99Sullivan%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Kenwright, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Kenwright B (2012) Real-time character inverse kinematics using the Gauss–Seidel iterative approximation metho" /><p class="c-article-references__text" id="ref-CR24">Kenwright B (2012) Real-time character inverse kinematics using the Gauss–Seidel iterative approximation method. Int Conf Creat Content Technol 4:63–68</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20character%20inverse%20kinematics%20using%20the%20Gauss%E2%80%93Seidel%20iterative%20approximation%20method&amp;journal=Int%20Conf%20Creat%20Content%20Technol&amp;volume=4&amp;pages=63-68&amp;publication_year=2012&amp;author=Kenwright%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lange B, Rizzo S, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology wi" /><p class="c-article-references__text" id="ref-CR25">Lange B, Rizzo S, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology within virtual environments. In: Interservice/industry training, simulation, and education conference (I/ITSEC)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Latoschik ME, Lugrin JL, Habel M, Roth D, Seufert C, Grafe S (2016) Breaking bad behavior: immersive training " /><p class="c-article-references__text" id="ref-CR26">Latoschik ME, Lugrin JL, Habel M, Roth D, Seufert C, Grafe S (2016) Breaking bad behavior: immersive training of class room management. In: Proceedings of the 22nd ACM conference on virtual reality software and technology, VRST’16. ACM, New York, NY, USA, pp 317–318</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Latoschik ME, Roth D, Gall D, Achenbach J, Waltemate T, Botsch M (2017) The effect of avatar realism in immers" /><p class="c-article-references__text" id="ref-CR27">Latoschik ME, Roth D, Gall D, Achenbach J, Waltemate T, Botsch M (2017) The effect of avatar realism in immersive social virtual realities. In: Proceedings of the 23rd ACM symposium on virtual reality software and technology, VRST’17. ACM, New York, NY, USA, pp 39:1–39:10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martindale J (2018) Oculus Rift vs. HTC Vive. https://www.digitaltrends.com/virtual-reality/oculus-rift-vs-htc" /><p class="c-article-references__text" id="ref-CR500">Martindale J (2018) Oculus Rift vs. HTC Vive. <a href="https://www.digitaltrends.com/virtual-reality/oculus-rift-vs-htc-vive/">https://www.digitaltrends.com/virtual-reality/oculus-rift-vs-htc-vive/</a>. Accessed 4 May 2017​</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Melo M, Rocha T, Barbosa L, Bessa M (2016) The impact of body position on the usability of multisensory virtua" /><p class="c-article-references__text" id="ref-CR28">Melo M, Rocha T, Barbosa L, Bessa M (2016) The impact of body position on the usability of multisensory virtual environments: case study of a virtual bicycle. In: Proceedings of the 7th international conference on software development and technologies for enhancing accessibility and fighting info-exclusion, DSAI 2016. ACM, New York, NY, USA, pp 20–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Nakamura, H. Hanafusa, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Nakamura Y, Hanafusa H (1986) Inverse kinematic solutions with singularity robustness for robot manipulator co" /><p class="c-article-references__text" id="ref-CR29">Nakamura Y, Hanafusa H (1986) Inverse kinematic solutions with singularity robustness for robot manipulator control. J Dyn Syst Meas Control 108(3):163–171</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1115%2F1.3143764" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0609.93044" aria-label="View reference 30 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Inverse%20kinematic%20solutions%20with%20singularity%20robustness%20for%20robot%20manipulator%20control&amp;journal=J%20Dyn%20Syst%20Meas%20Control&amp;volume=108&amp;issue=3&amp;pages=163-171&amp;publication_year=1986&amp;author=Nakamura%2CY&amp;author=Hanafusa%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DE. Orin, WW. Schrader, " /><meta itemprop="datePublished" content="1984" /><meta itemprop="headline" content="Orin DE, Schrader WW (1984) Efficient computation of the Jacobian for robot manipulators. Int J Robot Res 3(4)" /><p class="c-article-references__text" id="ref-CR30">Orin DE, Schrader WW (1984) Efficient computation of the Jacobian for robot manipulators. Int J Robot Res 3(4):66–75</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F027836498400300404" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20computation%20of%20the%20Jacobian%20for%20robot%20manipulators&amp;journal=Int%20J%20Robot%20Res&amp;volume=3&amp;issue=4&amp;pages=66-75&amp;publication_year=1984&amp;author=Orin%2CDE&amp;author=Schrader%2CWW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TC. Peck, S. Seinfeld, SM. Aglioti, M. Slater, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Peck TC, Seinfeld S, Aglioti SM, Slater M (2013) Putting yourself in the skin of a black avatar reduces implic" /><p class="c-article-references__text" id="ref-CR31">Peck TC, Seinfeld S, Aglioti SM, Slater M (2013) Putting yourself in the skin of a black avatar reduces implicit racial bias. Conscious Cognit 22(3):779–787</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.concog.2013.04.016" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Putting%20yourself%20in%20the%20skin%20of%20a%20black%20avatar%20reduces%20implicit%20racial%20bias&amp;journal=Conscious%20Cognit&amp;volume=22&amp;issue=3&amp;pages=779-787&amp;publication_year=2013&amp;author=Peck%2CTC&amp;author=Seinfeld%2CS&amp;author=Aglioti%2CSM&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raaen K, Kjellmo I (2015) Measuring latency in virtual reality systems. In: Chorianopoulos K, Divitini M, Baal" /><p class="c-article-references__text" id="ref-CR32">Raaen K, Kjellmo I (2015) Measuring latency in virtual reality systems. In: Chorianopoulos K, Divitini M, Baalsrud Hauge J, Jaccheri L, Malaka R (eds) Entertainment computing—ICEC 2015. Springer, Cham, pp 457–462</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roberts D, Duckworth T, Moore C, Wolff R, O’Hare J (2009) Comparing the end to end latency of an immersive col" /><p class="c-article-references__text" id="ref-CR33">Roberts D, Duckworth T, Moore C, Wolff R, O’Hare J (2009) Comparing the end to end latency of an immersive collaborative environment and a video conference. In: Proceedings of the 2009 13th IEEE/ACM international symposium on distributed simulation and real time applications, DS-RT’09. IEEE Computer Society, Washington, DC, USA, pp 89–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schmidt D, Kovacs R, Mehta V, Umapathi U, Köhler S, Cheng LP, Baudisch P (2015) Level-ups: motorized stilts th" /><p class="c-article-references__text" id="ref-CR34">Schmidt D, Kovacs R, Mehta V, Umapathi U, Köhler S, Cheng LP, Baudisch P (2015) Level-ups: motorized stilts that simulate stair steps in virtual reality. In: Proceedings of the 33rd annual ACM conference extended abstracts on human factors in computing systems, CHI EA’15. ACM, New York, NY, USA, pp 359–362</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seele S, Misztal S, Buhler H, Herpers R, Schild J (2017) Here’s looking at you anyway!: how important is reali" /><p class="c-article-references__text" id="ref-CR35">Seele S, Misztal S, Buhler H, Herpers R, Schild J (2017) Here’s looking at you anyway!: how important is realistic gaze behavior in co-located social virtual reality games? In: Proceedings of the annual symposium on computer-human interaction in play, CHI PLAY’17. ACM, New York, NY, USA, pp 531–540</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shoemake K (1985) Animating rotation with quaternion curves. In: Proceedings of the 12th annual conference on " /><p class="c-article-references__text" id="ref-CR36">Shoemake K (1985) Animating rotation with quaternion curves. In: Proceedings of the 12th annual conference on computer graphics and interactive techniques, SIGGRAPH’85. ACM, New York, NY, USA, pp 245–254</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shum H, Ho ES (2012) Real-time physical modelling of character movements with microsoft kinect. In: Proceeding" /><p class="c-article-references__text" id="ref-CR37">Shum H, Ho ES (2012) Real-time physical modelling of character movements with microsoft kinect. In: Proceedings of the 18th ACM symposium on virtual reality software and technology, VRST’12. ACM, pp 17–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sra M, Schmandt C (2015) MetaSpace II: object and full-body tracking for interaction and navigation in social " /><p class="c-article-references__text" id="ref-CR38">Sra M, Schmandt C (2015) MetaSpace II: object and full-body tracking for interaction and navigation in social VR. CoRR abs/1512.02922</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Steed A (2008) A simple method for estimating the latency of interactive, real-time graphics simulations. In: " /><p class="c-article-references__text" id="ref-CR40">Steed A (2008) A simple method for estimating the latency of interactive, real-time graphics simulations. In: Proceedings of the 2008 ACM symposium on virtual reality software and technology, VRST’08. ACM, New York, NY, USA, pp 123–129</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tao G, Archambault PS, Levin MF (2013) Evaluation of kinect skeletal tracking in a virtual reality rehabilitat" /><p class="c-article-references__text" id="ref-CR41">Tao G, Archambault PS, Levin MF (2013) Evaluation of kinect skeletal tracking in a virtual reality rehabilitation system for upper limb hemiparesis. In: 2013 international conference on virtual rehabilitation (ICVR), pp 164–165</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JS. Thomas, CR. France, ST. Leitkam, ME. Applegate, PE. Pidcoe, S. Walkowski, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Thomas JS, France CR, Leitkam ST, Applegate ME, Pidcoe PE, Walkowski S (2016) Effects of real-world versus vir" /><p class="c-article-references__text" id="ref-CR42">Thomas JS, France CR, Leitkam ST, Applegate ME, Pidcoe PE, Walkowski S (2016) Effects of real-world versus virtual environments on joint excursions in full-body reaching tasks. IEEE J Transl Eng Health Med 4:1–8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJTEHM.2016.2623787" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20real-world%20versus%20virtual%20environments%20on%20joint%20excursions%20in%20full-body%20reaching%20tasks&amp;journal=IEEE%20J%20Transl%20Eng%20Health%20Med&amp;volume=4&amp;pages=1-8&amp;publication_year=2016&amp;author=Thomas%2CJS&amp;author=France%2CCR&amp;author=Leitkam%2CST&amp;author=Applegate%2CME&amp;author=Pidcoe%2CPE&amp;author=Walkowski%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tsai TC, Chen CY, Su GJ (2015) U-art: your art and ubiquitous art. In: Adjunct proceedings of the 2015 ACM int" /><p class="c-article-references__text" id="ref-CR43">Tsai TC, Chen CY, Su GJ (2015) U-art: your art and ubiquitous art. In: Adjunct proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing and proceedings of the 2015 ACM international symposium on wearable computers, UbiComp/ISWC’15 Adjunct. ACM, New York, NY, USA, pp 1295–1302</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0374-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Multimedia Communications Lab, Technische Universität Darmstadt, 64283, Darmstadt, Germany</p><p class="c-article-author-affiliation__authors-list">Polona Caserman, Augusto Garcia-Agundez, Robert Konrad, Stefan Göbel &amp; Ralf Steinmetz</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Polona-Caserman"><span class="c-article-authors-search__title u-h3 js-search-name">Polona Caserman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Polona+Caserman&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Polona+Caserman" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Polona+Caserman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Augusto-Garcia_Agundez"><span class="c-article-authors-search__title u-h3 js-search-name">Augusto Garcia-Agundez</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Augusto+Garcia-Agundez&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Augusto+Garcia-Agundez" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Augusto+Garcia-Agundez%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Robert-Konrad"><span class="c-article-authors-search__title u-h3 js-search-name">Robert Konrad</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Robert+Konrad&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Robert+Konrad" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Robert+Konrad%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Stefan-G_bel"><span class="c-article-authors-search__title u-h3 js-search-name">Stefan Göbel</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Stefan+G%C3%B6bel&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Stefan+G%C3%B6bel" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stefan+G%C3%B6bel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ralf-Steinmetz"><span class="c-article-authors-search__title u-h3 js-search-name">Ralf Steinmetz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ralf+Steinmetz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ralf+Steinmetz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ralf+Steinmetz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0374-z/email/correspondent/c1/new">Polona Caserman</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec17-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdiv8zZ3xR6G1BgqStYZLTQ_2M"><div mi24-video-player="true" video-id="8zZ3xR6G1BgqStYZLTQ_2M" player-id="8PcXmCm9nWqE6posBEkd1h" config-type="vmpro" flash-path="//e.video-cdn.net/v2/" api-url="//d.video-cdn.net/play"></div><script src="//e.video-cdn.net/v2/embed.js"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>Supplementary material 1 (mp4 31171 KB)</p></div></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Real-time%20body%20tracking%20in%20virtual%20reality%20using%20a%20Vive%20tracker&amp;author=Polona%20Caserman%20et%20al&amp;contentID=10.1007%2Fs10055-018-0374-z&amp;publication=1359-4338&amp;publicationDate=2018-11-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0374-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0374-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Caserman, P., Garcia-Agundez, A., Konrad, R. <i>et al.</i> Real-time body tracking in virtual reality using a Vive tracker.
                    <i>Virtual Reality</i> <b>23, </b>155–168 (2019). https://doi.org/10.1007/s10055-018-0374-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0374-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-04-04">04 April 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-11-14">14 November 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-11-23">23 November 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-06-01">01 June 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0374-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0374-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Real-time tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Full-body avatar</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Low-latency</span></li><li class="c-article-subject-list__subject"><span itemprop="about">HTC Vive tracker</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Inverse kinematics</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0374-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=374;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

