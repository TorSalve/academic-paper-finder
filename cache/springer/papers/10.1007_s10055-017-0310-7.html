<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Designing a camera placement assistance system for human motion captur"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In multi-camera motion capture systems, determining the optimal camera configuration (camera positions and orientations) is still an unresolved problem. At present, configurations are primarily..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-04-04"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In multi-camera motion capture systems, determining the optimal camera configuration (camera positions and orientations) is still an unresolved problem. At present, configurations are primarily guided by a human operator&#8217;s intuition, which requires expertise and experience, especially with complex, cluttered scenes. In this paper, we propose a solution to automate camera placement for motion capture applications in order to assist a human operator. Our solution is based on the use of a guided genetic algorithm to optimize camera network placement with an appropriate number of cameras. In order to improve the performance of the genetic algorithm (GA), two techniques are described. The first is a distribution and estimation technique, which reduces the search space and generates camera positions for the initial GA population. The second technique is an error metric, which is integrated at GA evaluation level as an optimization function to evaluate the quality of the camera placement in a camera network. Simulation experiments show that our approach is more efficient than other approaches in terms of computation time and quality of the final camera network."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-04-04"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="13"/>

    <meta name="prism.endingPage" content="23"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0310-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0310-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0310-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0310-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2018/03"/>

    <meta name="citation_online_date" content="2017/04/04"/>

    <meta name="citation_firstpage" content="13"/>

    <meta name="citation_lastpage" content="23"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0310-7"/>

    <meta name="DOI" content="10.1007/s10055-017-0310-7"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0310-7"/>

    <meta name="description" content="In multi-camera motion capture systems, determining the optimal camera configuration (camera positions and orientations) is still an unresolved problem. At"/>

    <meta name="dc.creator" content="Azeddine Aissaoui"/>

    <meta name="dc.creator" content="Abdelkrim Ouafi"/>

    <meta name="dc.creator" content="Philippe Pudlo"/>

    <meta name="dc.creator" content="Christophe Gillet"/>

    <meta name="dc.creator" content="Zine-Eddine Baarir"/>

    <meta name="dc.creator" content="Abdelmalik Taleb-Ahmed"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Methods Biomech Biomed Eng; citation_title=Optimisation technique of camera placement for motion capture simulations; citation_author=A Aissaoui, Z Baarir, A Ouafi, P Pudlo, A Taleb-Ahmed, C Gillet, F Deraz; citation_volume=17; citation_issue=sup1; citation_publication_date=2014; citation_pages=122-123; citation_doi=10.1080/10255842.2014.931517; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Opt Lett; citation_title=Optimal placement of stereo sensors; citation_author=M Al Hasan, KK Ramachandran, JE Mitchell; citation_volume=2; citation_issue=1; citation_publication_date=2008; citation_pages=99-111; citation_doi=10.1007/s11590-007-0046-5; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_title=Adaptive and self-adaptive evolutionary computations; citation_inbook_title=Computational intelligence: a dynamic systems perspective; citation_publication_date=1995; citation_pages=152-163; citation_id=CR3; citation_author=PJ Angeline; citation_publisher=IEEE Press"/>

    <meta name="citation_reference" content="citation_journal_title=J Hum Genet; citation_title=Evaluation of next-generation sequencing software in mapping and assembly; citation_author=S Bao, R Jiang, W Kwan, B Wang, X Ma, Y-Q Song; citation_volume=56; citation_issue=6; citation_publication_date=2011; citation_pages=406-414; citation_doi=10.1038/jhg.2011.43; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Mach Vis Appl; citation_title=An occlusion metric for selecting robust camera configurations; citation_author=X Chen, J Davis; citation_volume=19; citation_issue=4; citation_publication_date=2008; citation_pages=217-222; citation_doi=10.1007/s00138-007-0094-y; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=J Comb Theory Ser B; citation_title=A combinatorial theorem in plane geometry; citation_author=V Chvatal; citation_volume=18; citation_issue=1; citation_publication_date=1975; citation_pages=39-41; citation_doi=10.1016/0095-8956(75)90061-1; citation_id=CR6"/>

    <meta name="citation_reference" content="Ercan AO, Yang DB, El&#160;Gamal A, Guibas LJ (2006) Optimal placement and selection of camera network nodes for target localization. In: International conference on distributed computing in sensor systems. Springer, pp 389&#8211;404"/>

    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Genetic algorithms and machine learning; citation_author=DE Goldberg, JH Holland; citation_volume=3; citation_issue=2; citation_publication_date=1988; citation_pages=95-99; citation_doi=10.1023/A:1022602019183; citation_id=CR8"/>

    <meta name="citation_reference" content="Gonzalez-Barbosa J-J, Garc&#237;a-Ram&#237;rez T, Salas J, Hurtado-Ramos J-B, Rico-Jim&#233;nez J-D-J (2009) Optimal camera placement for total coverage. In: Proceedings of the 2009 IEEE international conference on robotics and automation. IEEE Press, pp 3672&#8211;3676"/>

    <meta name="citation_reference" content="H&#246;rster E, Lienhart R (2006) On the optimal placement of multiple visual sensors. In: Proceedings of the 4th ACM international workshop on video surveillance and sensor networks. ACM, pp 111&#8211;120"/>

    <meta name="citation_reference" content="Indu S, Chaudhury S, Mittal NR, Bhattacharyya A (2009) Optimal sensor placement for surveillance of large spaces. In: ICDSC 2009. 3rd ACM/IEEE international conference on distributed smart cameras. IEEE, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Katz I, Aghajan H, Haymaker J (2010) A process for sensor configuration in multi-camera networks. In: Proc. 4th ACM/IEEE int. conf. distributed smart cameras"/>

    <meta name="citation_reference" content="citation_journal_title=Evol Comput; citation_title=A comparison study of self-adaptation in evolution strategies and real-coded genetic algorithms; citation_author=H Kita; citation_volume=9; citation_issue=2; citation_publication_date=2001; citation_pages=223-241; citation_doi=10.1162/106365601750190415; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Inf Theory; citation_title=Computational complexity of art gallery problems; citation_author=D Lee, A Lin; citation_volume=32; citation_issue=2; citation_publication_date=1986; citation_pages=276-282; citation_doi=10.1109/TIT.1986.1057165; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=J Converg; citation_title=Data definition of 3D character modeling and animation using H-Anim; citation_author=MW Lee, CH Jung, MG Lee, D Brutzman; citation_volume=6; citation_issue=2; citation_publication_date=2015; citation_pages=19-29; citation_doi=10.14400/JDC.2015.13.3.19; citation_id=CR15"/>

    <meta name="citation_reference" content="Malik R, Bajcsy P (2008) Automated placement of multiple stereo cameras. In: The 8th workshop on omnidirectional vision, camera networks and non-classical cameras-OMNIVIS"/>

    <meta name="citation_reference" content="MathWorks (1994) Matlab r2015a. 
                    https://www.mathworks.com/products/matlab.html
                    
                  . Accessed 27 Dec 2016"/>

    <meta name="citation_reference" content="citation_title=An introduction to genetic algorithms; citation_publication_date=1998; citation_id=CR18; citation_author=M Mitchell; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=A general method for sensor planning in multi-sensor systems: extension to random occlusion; citation_author=A Mittal, LS Davis; citation_volume=76; citation_issue=1; citation_publication_date=2008; citation_pages=31-52; citation_doi=10.1007/s11263-007-0057-9; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Sens J; citation_title=Particle swarm optimization inspired probability algorithm for optimal camera network placement; citation_author=Y Morsly, N Aouf, MS Djouadi, M Richardson; citation_volume=12; citation_issue=5; citation_publication_date=2012; citation_pages=1402-1412; citation_doi=10.1109/JSEN.2011.2170833; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn; citation_title=Optimal camera placement for accurate reconstruction; citation_author=G Olague, R Mohr; citation_volume=35; citation_issue=4; citation_publication_date=2002; citation_pages=927-944; citation_doi=10.1016/S0031-3203(01)00076-0; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_title=Art gallery theorems and algorithms; citation_publication_date=1987; citation_id=CR22; citation_author=J O&#8217;rourke; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="Piciarelli C, Micheloni C, Foresti GL (2010) Occlusion-aware multiple camera reconfiguration. In: Proceedings of the 4th ACM/IEEE international conference on distributed smart cameras. ACM, pp 88&#8211;94"/>

    <meta name="citation_reference" content="Rahimian P, Kearney JK (2015) Optimal camera placement for motion capture systems in the presence of dynamic occlusion. In: Proceedings of the 21st ACM symposium on virtual reality software and technology. ACM, pp 129&#8211;138"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern Part B (Cybern); citation_title=Adaptive probabilities of crossover and mutation in genetic algorithms; citation_author=M Srinivas, LM Patnaik; citation_volume=24; citation_issue=4; citation_publication_date=1994; citation_pages=656-667; citation_doi=10.1109/21.286385; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_title=Computer vision: algorithms and applications; citation_publication_date=2010; citation_id=CR26; citation_author=R Szeliski; citation_publisher=Springer Science &amp; Business Media"/>

    <meta name="citation_reference" content="citation_title=Bionics in computational intelligence: theory and algorithm; citation_publication_date=2003; citation_id=CR27; citation_author=Z Xu; citation_author=J Zhang; citation_author=Y Zhang; citation_publisher=Science Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern Part B (Cybern); citation_title=Can you see me now? sensor positioning for automated and persistent surveillance; citation_author=Y Yao, C-H Chen, B Abidi, D Page, A Koschan, M Abidi; citation_volume=40; citation_issue=1; citation_publication_date=2010; citation_pages=101-115; citation_doi=10.1109/TSMCB.2009.2017507; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE J Sel Top Sig Process; citation_title=Optimal camera network configurations for visual tagging; citation_author=J Zhao, S-C Cheung, T Nguyen; citation_volume=2; citation_issue=4; citation_publication_date=2008; citation_pages=464-479; citation_doi=10.1109/JSTSP.2008.2001430; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_title=Optimal visual sensor network configuration. Multi-camera networks: principles and applications; citation_publication_date=2009; citation_id=CR30; citation_author=J Zhao; citation_author=S Cheung; citation_author=T Nguyen; citation_publisher=Academic Press"/>

    <meta name="citation_author" content="Azeddine Aissaoui"/>

    <meta name="citation_author_email" content="aissaoui_azeddine@hotmail.fr"/>

    <meta name="citation_author_institution" content="LESIA Laboratory, Biskra University, Biskra, Algeria"/>

    <meta name="citation_author" content="Abdelkrim Ouafi"/>

    <meta name="citation_author_institution" content="LESIA Laboratory, Biskra University, Biskra, Algeria"/>

    <meta name="citation_author" content="Philippe Pudlo"/>

    <meta name="citation_author_institution" content="UVHC, LAMIH, Valenciennes, France"/>

    <meta name="citation_author_institution" content="CNRS, UMR 8201, Valenciennes, France"/>

    <meta name="citation_author_institution" content="University Lille Nord de France, Lille, France"/>

    <meta name="citation_author" content="Christophe Gillet"/>

    <meta name="citation_author_institution" content="UVHC, LAMIH, Valenciennes, France"/>

    <meta name="citation_author_institution" content="CNRS, UMR 8201, Valenciennes, France"/>

    <meta name="citation_author_institution" content="University Lille Nord de France, Lille, France"/>

    <meta name="citation_author" content="Zine-Eddine Baarir"/>

    <meta name="citation_author_institution" content="LESIA Laboratory, Biskra University, Biskra, Algeria"/>

    <meta name="citation_author" content="Abdelmalik Taleb-Ahmed"/>

    <meta name="citation_author_institution" content="UVHC, LAMIH, Valenciennes, France"/>

    <meta name="citation_author_institution" content="CNRS, UMR 8201, Valenciennes, France"/>

    <meta name="citation_author_institution" content="University Lille Nord de France, Lille, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0310-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0310-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm"/>
        <meta property="og:description" content="In multi-camera motion capture systems, determining the optimal camera configuration (camera positions and orientations) is still an unresolved problem. At present, configurations are primarily guided by a human operator’s intuition, which requires expertise and experience, especially with complex, cluttered scenes. In this paper, we propose a solution to automate camera placement for motion capture applications in order to assist a human operator. Our solution is based on the use of a guided genetic algorithm to optimize camera network placement with an appropriate number of cameras. In order to improve the performance of the genetic algorithm (GA), two techniques are described. The first is a distribution and estimation technique, which reduces the search space and generates camera positions for the initial GA population. The second technique is an error metric, which is integrated at GA evaluation level as an optimization function to evaluate the quality of the camera placement in a camera network. Simulation experiments show that our approach is more efficient than other approaches in terms of computation time and quality of the final camera network."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0310-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Multi-camera-based motion capture systems, Optimal camera configurations, Genetic algorithm, Optimization","kwrd":["Multi-camera-based_motion_capture_systems","Optimal_camera_configurations","Genetic_algorithm","Optimization"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0310-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0310-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=310;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0310-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0310-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0310-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-04-04" itemprop="datePublished">04 April 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Azeddine-Aissaoui" data-author-popup="auth-Azeddine-Aissaoui" data-corresp-id="c1">Azeddine Aissaoui<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Biskra University" /><meta itemprop="address" content="grid.442402.4, LESIA Laboratory, Biskra University, BP 145 RP, 07000, Biskra, Algeria" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Abdelkrim-Ouafi" data-author-popup="auth-Abdelkrim-Ouafi">Abdelkrim Ouafi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Biskra University" /><meta itemprop="address" content="grid.442402.4, LESIA Laboratory, Biskra University, BP 145 RP, 07000, Biskra, Algeria" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Philippe-Pudlo" data-author-popup="auth-Philippe-Pudlo">Philippe Pudlo</a></span><sup class="u-js-hide"><a href="#Aff2">2</a>,<a href="#Aff3">3</a>,<a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="UVHC, LAMIH" /><meta itemprop="address" content="0000 0001 2201 3679, grid.473800.8, UVHC, LAMIH, 59313, Valenciennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS, UMR 8201" /><meta itemprop="address" content="CNRS, UMR 8201, 59313, Valenciennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University Lille Nord de France" /><meta itemprop="address" content="0000 0004 1759 9865, grid.412304.0, University Lille Nord de France, 59000, Lille, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Christophe-Gillet" data-author-popup="auth-Christophe-Gillet">Christophe Gillet</a></span><sup class="u-js-hide"><a href="#Aff2">2</a>,<a href="#Aff3">3</a>,<a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="UVHC, LAMIH" /><meta itemprop="address" content="0000 0001 2201 3679, grid.473800.8, UVHC, LAMIH, 59313, Valenciennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS, UMR 8201" /><meta itemprop="address" content="CNRS, UMR 8201, 59313, Valenciennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University Lille Nord de France" /><meta itemprop="address" content="0000 0004 1759 9865, grid.412304.0, University Lille Nord de France, 59000, Lille, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Zine_Eddine-Baarir" data-author-popup="auth-Zine_Eddine-Baarir">Zine-Eddine Baarir</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Biskra University" /><meta itemprop="address" content="grid.442402.4, LESIA Laboratory, Biskra University, BP 145 RP, 07000, Biskra, Algeria" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Abdelmalik-Taleb_Ahmed" data-author-popup="auth-Abdelmalik-Taleb_Ahmed">Abdelmalik Taleb-Ahmed</a></span><sup class="u-js-hide"><a href="#Aff2">2</a>,<a href="#Aff3">3</a>,<a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="UVHC, LAMIH" /><meta itemprop="address" content="0000 0001 2201 3679, grid.473800.8, UVHC, LAMIH, 59313, Valenciennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNRS, UMR 8201" /><meta itemprop="address" content="CNRS, UMR 8201, 59313, Valenciennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University Lille Nord de France" /><meta itemprop="address" content="0000 0004 1759 9865, grid.412304.0, University Lille Nord de France, 59000, Lille, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">13</span>–<span itemprop="pageEnd">23</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">436 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0310-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In multi-camera motion capture systems, determining the optimal camera configuration (camera positions and orientations) is still an unresolved problem. At present, configurations are primarily guided by a human operator’s intuition, which requires expertise and experience, especially with complex, cluttered scenes. In this paper, we propose a solution to automate camera placement for motion capture applications in order to assist a human operator. Our solution is based on the use of a guided genetic algorithm to optimize camera network placement with an appropriate number of cameras. In order to improve the performance of the genetic algorithm (GA), two techniques are described. The first is a distribution and estimation technique, which reduces the search space and generates camera positions for the initial GA population. The second technique is an error metric, which is integrated at GA evaluation level as an optimization function to evaluate the quality of the camera placement in a camera network. Simulation experiments show that our approach is more efficient than other approaches in terms of computation time and quality of the final camera network.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Human motion capture using markers (active or passive) requires cameras to be positioned around the volume of interest so that at least two cameras can view each marker. Three-dimensional marker positions are then calculated by triangulation, and subject movements can thus be defined. When the movement is complex or is produced in the presence of obstacles, it is more difficult to capture markers. Consequently, a human operator tries to find camera configurations that minimize marker occlusion. This task remains difficult and requires time as well as many validation tests, even in the case of an experienced expert.</p><p>Camera configuration has a critical impact on the overall motion capture performance. The configuration quality to determine the marker in 3D is strongly affected by two sources of error: marker visibility and triangulation accuracy. In order to address these error sources, the following points should be taking into consideration:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>A Marker should not be occluded by any obstacle.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>A Marker should be within the camera’s field of view.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>A Marker must be visible given a camera’s resolution, the marker’s size, and its distance from the camera.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>At least two cameras are required to reconstruct a marker.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">5.</span>
                    
                      <p>Cameras should be arranged sufficiently non-parallel so that triangulation calculations are well conditioned.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">6.</span>
                    
                      <p>Incorporating additional cameras leads to over conditioned triangulation. However, it involves a higher cost.</p>
                    
                  </li>
                </ol><p>To assist human operators in configuring camera networks and improving human motion capture, we developed a computer tool using a guided genetic algorithm to simulate the best camera network configuration. Our approach looks at capturing tags (or markers) attached to virtual humanoid performing movements similar to those in the real world. The goal is then to simulate the best camera network for a specified virtual scene.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>The problem of finding optimal camera placement has been studied for a long time. The first research on the subject is based on the famous art gallery problem, of which the main purpose is to find the minimum number of cameras that can monitor a fixed number of paintings in a gallery (Chvatal <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Chvatal V (1975) A combinatorial theorem in plane geometry. J Comb Theory Ser B 18(1):39–41" href="/article/10.1007/s10055-017-0310-7#ref-CR6" id="ref-link-section-d58092e472">1975</a>). Initial formulations lacked realistic models for cameras and the environment (Lee and Lin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Lee D, Lin A (1986) Computational complexity of art gallery problems. IEEE Trans Inf Theory 32(2):276–282" href="/article/10.1007/s10055-017-0310-7#ref-CR14" id="ref-link-section-d58092e475">1986</a>; O’rourke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="O’rourke J (1987) Art gallery theorems and algorithms, vol 57. Oxford University Press, Oxford" href="/article/10.1007/s10055-017-0310-7#ref-CR22" id="ref-link-section-d58092e478">1987</a>). Cameras are assumed to have 360-degree field of view, and there is no resolution degradation with viewing distance. Additionally, both the target space and the locations of cameras are restricted to 2D. Mittal and Davis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Mittal A, Davis LS (2008) A general method for sensor planning in multi-sensor systems: extension to random occlusion. Int J Comput Vis 76(1):31–52" href="/article/10.1007/s10055-017-0310-7#ref-CR19" id="ref-link-section-d58092e481">2008</a>) and Piciarelli et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Piciarelli C, Micheloni C, Foresti GL (2010) Occlusion-aware multiple camera reconfiguration. In: Proceedings of the 4th ACM/IEEE international conference on distributed smart cameras. ACM, pp 88–94" href="/article/10.1007/s10055-017-0310-7#ref-CR23" id="ref-link-section-d58092e484">2010</a>) incorporated more realistic models of cameras and the environment and expanded the search space to 3D.</p><p>Unlike the previous techniques which formulated the camera placement problem in 2D or 3D continuous spaces, most recent approaches consider the problem entirely in the discrete domain (Hörster and Lienhart <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hörster E, Lienhart R (2006) On the optimal placement of multiple visual sensors. In: Proceedings of the 4th ACM international workshop on video surveillance and sensor networks. ACM, pp 111–120" href="/article/10.1007/s10055-017-0310-7#ref-CR10" id="ref-link-section-d58092e490">2006</a>). Instead of optimizing a continuous function using a variational calculation, approaches in the discrete domain quantify the search space from a finite number of candidate positions and look for the best configurations to optimize a given cost function. This strategy naturally leads to combinatorial problems. Efforts have been made to develop discrete camera positioning problems using linear binary programming (Gonzalez-Barbosa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Gonzalez-Barbosa J-J, García-Ramírez T, Salas J, Hurtado-Ramos J-B, Rico-Jiménez J-D-J (2009) Optimal camera placement for total coverage. In: Proceedings of the 2009 IEEE international conference on robotics and automation. IEEE Press, pp 3672–3676" href="/article/10.1007/s10055-017-0310-7#ref-CR9" id="ref-link-section-d58092e493">2009</a>) and quadratic programming (Ercan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ercan AO, Yang DB, El Gamal A, Guibas LJ (2006) Optimal placement and selection of camera network nodes for target localization. In: International conference on distributed computing in sensor systems. Springer, pp 389–404" href="/article/10.1007/s10055-017-0310-7#ref-CR7" id="ref-link-section-d58092e496">2006</a>).</p><p>Moreover, as the majority of these formulations lead to NP-hard problems, a multitude of practical solutions have been proposed: binary integer programming (Zhao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhao J, Cheung S-C, Nguyen T (2008) Optimal camera network configurations for visual tagging. IEEE J Sel Top Sig Process 2(4):464–479" href="/article/10.1007/s10055-017-0310-7#ref-CR29" id="ref-link-section-d58092e502">2008</a>), greedy approach (Al Hasan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Al Hasan M, Ramachandran KK, Mitchell JE (2008) Optimal placement of stereo sensors. Opt Lett 2(1):99–111" href="/article/10.1007/s10055-017-0310-7#ref-CR2" id="ref-link-section-d58092e505">2008</a>), Monte Carlo simulations (Yao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Yao Y, Chen C-H, Abidi B, Page D, Koschan A, Abidi M (2010) Can you see me now? sensor positioning for automated and persistent surveillance. IEEE Trans Syst Man Cybern Part B (Cybern) 40(1):101–115" href="/article/10.1007/s10055-017-0310-7#ref-CR28" id="ref-link-section-d58092e508">2010</a>), genetic algorithms (Olague and Mohr <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Olague G, Mohr R (2002) Optimal camera placement for accurate reconstruction. Pattern Recogn 35(4):927–944" href="/article/10.1007/s10055-017-0310-7#ref-CR21" id="ref-link-section-d58092e511">2002</a>) and particle swarm optimization (Morsly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Morsly Y, Aouf N, Djouadi MS, Richardson M (2012) Particle swarm optimization inspired probability algorithm for optimal camera network placement. IEEE Sens J 12(5):1402–1412" href="/article/10.1007/s10055-017-0310-7#ref-CR20" id="ref-link-section-d58092e514">2012</a>).</p><p>The high dimensionality of the search space and nonlinearity of the camera placement problem for motion capture led us to promote the use of genetic algorithms. In the following, we present research based on the use of genetic algorithms (GA) to solve the camera placement problem.</p><p>
Olague and Mohr (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Olague G, Mohr R (2002) Optimal camera placement for accurate reconstruction. Pattern Recogn 35(4):927–944" href="/article/10.1007/s10055-017-0310-7#ref-CR21" id="ref-link-section-d58092e524">2002</a>) used a genetic algorithm to evaluate a set of camera network configurations based on horizontal and vertical camera orientation (<span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span>). They proposed a 3D reconstruction uncertainty measurement using a scalar function of the covariance matrix. This method proposes the resolution constraint as the only cause of 3D uncertainty. Hence, a quality metric was built by (Chen and Davis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chen X, Davis J (2008) An occlusion metric for selecting robust camera configurations. Mach Vis Appl 19(4):217–222" href="/article/10.1007/s10055-017-0310-7#ref-CR5" id="ref-link-section-d58092e567">2008</a>) taking resolution and occlusion into consideration. Rahimian and Kearney (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Rahimian P, Kearney JK (2015) Optimal camera placement for motion capture systems in the presence of dynamic occlusion. In: Proceedings of the 21st ACM symposium on virtual reality software and technology. ACM, pp 129–138" href="/article/10.1007/s10055-017-0310-7#ref-CR24" id="ref-link-section-d58092e570">2015</a>) show that their optimization algorithm can lead to more significant performance gains over Chen and Davis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chen X, Davis J (2008) An occlusion metric for selecting robust camera configurations. Mach Vis Appl 19(4):217–222" href="/article/10.1007/s10055-017-0310-7#ref-CR5" id="ref-link-section-d58092e574">2008</a>) method both in terms of the visibility metric and in the robustness of full-body motion capture. However, their method does not estimate the minimum number of cameras for a given accuracy threshold and workspace.</p><p>
Indu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Indu S, Chaudhury S, Mittal NR, Bhattacharyya A (2009) Optimal sensor placement for surveillance of large spaces. In: ICDSC 2009. 3rd ACM/IEEE international conference on distributed smart cameras. IEEE, pp 1–8" href="/article/10.1007/s10055-017-0310-7#ref-CR11" id="ref-link-section-d58092e580">2009</a>) proposed an optimization of the camera network placement using a genetic algorithm based on camera position parameters and horizontal and vertical camera orientation (<span class="mathjax-tex">\(x_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(y_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(z_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span>). This study defines a coverage matrix by dividing the area to be photographed into a uniform grid, and each grid cell is classified as a priority area, a non-priority area, or as an obstacle. This method becomes ineffective when performed with a high number of grids elements.</p><p>
Katz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Katz I, Aghajan H, Haymaker J (2010) A process for sensor configuration in multi-camera networks. In: Proc. 4th ACM/IEEE int. conf. distributed smart cameras" href="/article/10.1007/s10055-017-0310-7#ref-CR12" id="ref-link-section-d58092e702">2010</a>) adopted Olague and Mohr (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Olague G, Mohr R (2002) Optimal camera placement for accurate reconstruction. Pattern Recogn 35(4):927–944" href="/article/10.1007/s10055-017-0310-7#ref-CR21" id="ref-link-section-d58092e705">2002</a>) method using the same 3D reconstruction uncertainty measurement but with all the camera position and orientation parameters (<span class="mathjax-tex">\(x_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(y_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(z_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span>, <span class="mathjax-tex">\(\gamma\)</span>) to find an optimal network placement. This method seems ineffective in solving the motion capture problem because it is limited to finding local minima and therefore does not always provide the right camera placement. In addition, the adjustment of the population size or crossover and mutation rates is manually performed by the user; this is disadvantageous to the effectiveness of their method.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Proposed methodology to find preferential camera network placement for motion capture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The main elements that determine the differences between previous GA-based approaches are the number of camera parameters and the way they are used to evaluate placement performance. Researchers have gradually increased the number of camera parameters in order to find adequate solutions that do not require manual intervention. However, these algorithms suffer due to their excessive computation time since they simultaneously handle several solutions. Moreover, incorrect determination of parameters such as population size or mutation rate may limit the effectiveness of the algorithm.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Proposed approach</h2><div class="c-article-section__content" id="Sec3-content"><p>Our camera placement assistance methodology is divided in two modules: <i>graphical modelling</i>, and <i>camera placement optimization</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig1">1</a>).</p><p>Graphical modelling includes scene creation, camera placement, and virtual humanoid animation. Animation involves enabling the virtual humanoid to produce movement similar to that in the real world in order to develop a specific scenario. This scenario can be recorded, modified, or rebuilt.</p><p>The camera placement optimization module applies GGA to the scenario created in order to establish the best camera network placement using an appropriate number of cameras. This module is composed of three blocks. Distribution and estimation technique (DET) block is used to restrict the search space, the error metric block is used to evaluate the performance of the configured camera network, and the adaptive genetic algorithm block starts with the volume estimated by the DET and uses the error metric to optimize the initial camera network.</p><h3 class="c-article__sub-heading" id="Sec4">Module 1: Graphical modelling</h3><p>In this section, the graphical modelling that allows us to create a virtual scenario is defined. This step is considered as the starting point of the optimization process. It includes <i>Scene modelling, Camera modelling</i>, and <i>Human modelling</i>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Scene modelling</h4><p>The scene represents the volume in which the cameras and the animated humanoid can be placed. The user specifies minimum and maximum values for the dimensions of each scene along the <i>x, y</i> and <i>z</i> axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig2">2</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Camera modelling</h4><p>Cameras are modelled in 3D (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig2">2</a>). The location and view direction of the cameras in the scene are described by extrinsic parameters, including:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Three position coordinates: <span class="mathjax-tex">\((x_{{\mathrm{c}}}, y_{{\mathrm{c}}}, z_{{\mathrm{c}}})\)</span>.</p>
                      </li>
                      <li>
                        <p>Rotation matrix <i>R</i> with three angles: (<span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span>, <span class="mathjax-tex">\(\gamma\)</span>). </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R=R_x(\alpha )\times R_y(\beta )\times R_z(\gamma ) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> with </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R_x(\alpha )= &amp; {} \begin{pmatrix} 1 &amp;{}\quad 0 &amp;{}\quad 0\\ 0 &amp;{} \quad \cos (\alpha ) &amp;{} \quad -\sin (\alpha )\\ 0 &amp;{} \quad \sin (\alpha ) &amp;{} \quad \cos (\alpha ) \end{pmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
<div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R_y(\beta )= &amp; {} \begin{pmatrix} \cos (\beta ) &amp;{} \quad 0 &amp;{}\quad \sin (\beta )\\ 0 &amp;{} 1 &amp;{} 0\\ -\sin (\beta ) &amp;{} \quad 0 &amp;{}\quad \cos (\beta ) \end{pmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
<div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R_z(\gamma )= &amp; {} \begin{pmatrix} \cos (\gamma ) &amp;{} \quad -\sin (\gamma )&amp;{} \quad 0\\ \sin (\gamma ) &amp;{} \quad \cos (\gamma ) &amp;{} \quad 0\\ 0 &amp;{}\quad 0 &amp;{}\quad 1 \end{pmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>

                      </li>
                    </ul>
                        <ul class="u-list-style-none">
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(R_x\)</span> rotates <i>y</i> axis towards <i>z</i> axis.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(R_y\)</span> rotates <i>z</i> axis towards <i>x</i> axis.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(R_z\)</span> rotates <i>x</i> axis towards <i>y</i> axis.</p>
                      </li>
                    </ul>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Human modelling</h4><p>A figure is represented as an H-Anim (LOA 1) model (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Lee MW, Jung CH, Lee MG, Brutzman D (2015) Data definition of 3D character modeling and animation using H-Anim. J Converg 6(2):19–29" href="/article/10.1007/s10055-017-0310-7#ref-CR15" id="ref-link-section-d58092e1940">2015</a>), in which each model node corresponds to a body part (e.g. legs, torso, arms). Tags are fixed to specific locations on the virtual humanoid according to their placement on the real actor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig2">2</a>b, c). The virtual humanoid is animated by a sequence of frames. Each frame is displayed on the screen until the next frame overwrites it. This process creates a specific movement (e.g. walking, running, or jumping) that can be captured, modified, or built.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>
                                        <b>a</b> Modelling a motion capture scene in 3D with a virtual humanoid and camera. The camera is placed according to the parameters <span class="mathjax-tex">\(x_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(y_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(z_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span> and <span class="mathjax-tex">\(\gamma\)</span> in relation to scene landmarks. <b>b</b>, <b>c</b> Present an example of the positioning of the tags on the virtual humanoid body segments in accordance with what is planned in the real world, <b>a</b> graphical model, <b>b</b> front view of humanoid, <b>c</b> back view of humanoid</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec8">Module 2: Camera placement optimization</h3><p>In the field of motion capture, GGA improves the genetic algorithm at <i>initialization</i>, <i>evaluation</i>, and <i>reproduction operators</i> levels to address the problems of previous studies.</p><p>The initialization step uses DET for choosing the initial population of the camera networks.</p><p>In the evaluation step, our error metric is used to estimate the visibility of the tags for each camera network, which provides a fitness score. This fitness score is then used to assess the performance of the camera networks.</p><p>In the reproduction step, adaptive strategies are used to provide the best crossover and mutation results.</p><p>We divided this camera placement optimization module into three blocks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig1">1</a>): distribution and estimation technique, error metric, and adaptive genetic algorithm described in detail below.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Distribution and estimation technique</h4><p>DET is used to compute an error metric for marker distribution during the movement to be analysed and propose the best volume to place the initial camera networks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig3">3</a>).</p><p>We define a <span class="mathjax-tex">\({{ tag}}_j(x_j, y_j, z_j)\)</span> as a given 3D point in the scene’s frame, and at each instant t the 3D position of a set of tags is given by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0310-7#Equ5">5</a>:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} L(t)= \begin{bmatrix} {{ tag}}_1(t)\\ \vdots \\ {{ tag}}_j(t) \end{bmatrix} = \begin{bmatrix} x_1(t)\\ y_1(t)\\ z_1(t)\\ \vdots \\ x_j(t)\\ y_j(t)\\ z_j(t) \end{bmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>L</i>(<i>t</i>) is the position of all the tags at instant <i>t</i>.</p><p>
                        <span class="mathjax-tex">\(x_j\)</span>, <span class="mathjax-tex">\(y_j\)</span> and <span class="mathjax-tex">\(z_j\)</span> are the coordinates of <span class="mathjax-tex">\({{ tag}}_j\forall j \in [1\ldots N]\)</span>, where <i>N</i> is the number of tags in the scene. <span class="mathjax-tex">\({{ tag}}_j(t)\)</span> is the <span class="mathjax-tex">\(j{\hbox {th}}\)</span> tag at instant <i>t</i>.</p><p>From Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0310-7#Equ5">5</a>, we define a movement as a position matrix:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \overbrace{ \begin{bmatrix} {{ tag}}_1(t_0)&amp;\cdots&amp;{{ tag}}_1(t_{{{ Nf}}})\\ {{ tag}}_2(t_0)&amp;\cdots&amp;{{ tag}}_2(t_{{{ Nf}}})\\ \vdots&amp;\ddots&amp;\vdots \\ {{ tag}}_j(t_0)&amp;\cdots&amp;{{ tag}}_j(t_{{{ Nf}}}) \end{bmatrix}}^{{{ movement}}} = \begin{bmatrix} x_1(t_0)&amp;\cdots&amp;x_1(t_{{{ Nf}}})\\ y_1(t_0)&amp;\cdots&amp;y_1(t_{{{ Nf}}})\\ z_1(t_0)&amp;\cdots&amp;z_1(t_{{{ Nf}}})\\ x_2(t_0)&amp;\cdots&amp;x_2(t_{{{ Nf}}})\\ y_2(t_0)&amp;\cdots&amp;y_2(t_{{{ Nf}}})\\ z_2(t_0)&amp;\cdots&amp;z_2(t_{{{ Nf}}})\\ \vdots&amp;\ddots&amp;\vdots \\ x_j(t_0)&amp;\cdots&amp;x_j(t_{{{ Nf}}})\\ y_j(t_0)&amp;\cdots&amp;y_j(t_{{{ Nf}}})\\ z_j(t_0)&amp;\cdots&amp;z_j(t_{{{ Nf}}}) \end{bmatrix} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <span class="mathjax-tex">\(t\in [0\ldots {{ Nf}}]\)</span> and <i>Nf</i> is the number of frames.</p><p>Based on Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0310-7#Equ6">6</a>, initial camera locations are generated using the following two steps:</p><p>
                        <i>Determining initial camera positioning zone</i> The purpose of this step is to position the cameras in the proposed solution zone (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig3">3</a>). To define this zone, firstly we determine, from the point clouds, the overall volume of the tags movement. Then, we select the space where tags are visible given a camera’s resolution, the marker’s size, and its distance from the camera.</p><p>
                        <i>Determining camera orientation from an interest point</i> In this step, we calculate the orientation parameters (<span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span>, <span class="mathjax-tex">\(\gamma\)</span>) according to camera position (<span class="mathjax-tex">\(x_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(y_{{\mathrm{c}}}\)</span>, <span class="mathjax-tex">\(z_{{\mathrm{c}}}\)</span>) and an interest point (<i>IP</i>) as a target at which the camera lens is pointed. This interest point is located in the overall volume of the tags movement at (<span class="mathjax-tex">\(x_{{{ IP}}}\)</span>, <span class="mathjax-tex">\(y_{{{ IP}}}\)</span>, <span class="mathjax-tex">\(z_{{{ IP}}}\)</span>) from the centre of the scene <span class="mathjax-tex">\(O_{{\mathrm{s}}}\)</span> according to Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0310-7#Equ7">7</a>.</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} IP= \frac{1}{N} \sum \limits _{i=1}^{N} [[x_i, y_i, z_i]^{{\mathrm{T}}}]o_{{\mathrm{s}}}\Leftrightarrow \left\{ \begin{array}{ll} x_{{{ IP}}}= \frac{1}{N} \sum \limits _{i=1}^{N}x_i \\ y_{{{ IP}}}= \frac{1}{N} \sum \limits _{i=1}^{N}y_i \\ z_{{{ IP}}}= \frac{1}{N} \sum \limits _{i=1}^{N}z_i \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>Then, we calculate the distances <i>xside</i>, <i>yside</i>, and <i>zside</i> between <i>IP</i> and the camera position in each axe as follows:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}\displaystyle {xside}&amp;=|x_{{\mathrm{c}}}-x_{{{ IP}}} | \\ \displaystyle {{ yside}}&amp;= | y_{{\mathrm{c}}}-y_{{{ IP}}} | \\ \displaystyle {{ zside}}&amp;= | z_{{\mathrm{c}}}-z_{{{ IP}}} |\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>Using Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0310-7#Equ8">8</a>, we define <i>d</i> and <i>r</i> as the distances in 2D and 3D Euclidean space as follows:</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} d= &amp; {} \sqrt{({ xside})^2 + ({{{ yside}}})^2} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
<div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} r= &amp; {} \sqrt{d^2 + ({{{ zside}}})^2} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>Thus, according to the centre of the scene <span class="mathjax-tex">\(O_{{\mathrm{s}}}\)</span>, the camera directions <span class="mathjax-tex">\(\alpha\)</span>, <span class="mathjax-tex">\(\beta\)</span> and <span class="mathjax-tex">\(\gamma\)</span> can be calculated as follows:</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \displaystyle \alpha= &amp; {} \arcsin ({{{ yside}}}/r) \nonumber \\ \displaystyle \beta= &amp; {} \arccos ({{{ zside}}}/d) \nonumber \\ \displaystyle \gamma= &amp; {} \arctan ({ xside}/d) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>
                                        <b>a</b> Search zone of initial camera locations (capture zone) for motion capture session according to minimum and maximum resolution. <b>b</b> Camera location and orientation according to points of interest IP. <b>c</b> Calculation of camera orientation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Error metric</h4><p>The goal is to define an optimization function <i>f</i>, which can measure the ability of a camera network to capture tags in a 3D environment. It is then possible, out of all the possible camera network configurations, the one that maximizes this function using a minimum number of cameras.</p><p>For each tag, the number of cameras, for which the tag is visible, is determined in order to estimate the quality <i>Q</i> of the camera locations using the reconstruction error <span class="mathjax-tex">\(E_{{\mathrm{r}}}\)</span>. There are two possible situations for <span class="mathjax-tex">\(E_{{\mathrm{r}}}\)</span>: critical case, when no or only one camera captures the tag; Therefore, <span class="mathjax-tex">\({{ Er}} = 1\)</span>. Favourable case, when two or more cameras capture the tag; Therefore, <span class="mathjax-tex">\({{ Er}} = 0\)</span>.</p><p>This means that:</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;\forall C_i; \forall {{ tag}}_j{:} \left\{ \begin{array}{ll} E_{{\mathrm{r}}}=1 \Leftrightarrow \left\{ \begin{array}{ll} \forall f_k; \sum \limits _{i, j=1}^{Nc, Nf}P_i {{ tag}}_j=0\\ \qquad \text{ or } \\ \forall f_k; \sum \limits _{i, j=1}^{Nc, Nf}P_i {{ tag}}_j=1 \end{array} \right. \\ E_{{\mathrm{r}}}=0 \Leftrightarrow \forall f_k; \sum \limits _{i, j=1}^{Nc, Nf}P_i {{ tag}}_j \ge 2\end{array} \right. \nonumber \\&amp;k \in [1\ldots Nf], Nf=t_{{{ mov}}}/f_e, P=K.R.t, Nc\ge Nc_{{{ min}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where</p><ul class="u-list-style-none">
                      <li>
                        <p>
                                    <span class="mathjax-tex">\({{ Nf}}\)</span> is the number of frames.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(t_{{{ mov}}}\)</span> is the movement duration.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(f_e\)</span> is the acquisition frequency of the movement (60 Hz).</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\({ P}\)</span> is the projection matrix.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\({ K}\)</span> is the intrinsic parameters matrix.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\({ R}\)</span> and <i>t</i> are the rotation and translation parameters. The minimum number of used cameras must be <span class="mathjax-tex">\({{ Nc}}_{{{ min}}}\ge 2\)</span>.</p>
                      </li>
                    </ul><p>We define a binary vector containing reconstructed tags (re) and a matrix <span class="mathjax-tex">\({ A}\)</span> containing the values <span class="mathjax-tex">\({ a}_{ ij}\)</span> of binary vectors whith:</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {{ re}}_j&amp;= {\left\{ \begin{array}{ll} 1 &amp;{} {\text {if}} \quad E_{{\mathrm{r}}}=0 \\ 0 &amp;{} {\text {if}} \quad E_{{\mathrm{r}}}=1 \end{array}\right. }\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div>
<div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} a_{ij}&amp;= {\left\{ \begin{array}{ll} 1 &amp; {\text {if\,tag}}\,j\,{\text {is\,captured\,by\,camera}}\,i\\ 0 &amp; {\text {else}} \end{array}\right. } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div>
<p>In order to satisfy optimum camera locations, three constraints should be considered: <i>Occlusion</i>, <i>camera field of view</i>, and <i>camera resolution</i>.</p><p>
                        <i>Occlusion</i> A marker is completely visible if nothing is blocking the view of the camera. All entities (static, dynamic or the subject itself) can be considered as obstacles <span class="mathjax-tex">\({{ Obs}}_{k}\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig4">4</a>). This constraint is expressed by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0310-7#Equ15">15</a> (Malik and Bajcsy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Malik R, Bajcsy P (2008) Automated placement of multiple stereo cameras. In: The 8th workshop on omnidirectional vision, camera networks and non-classical cameras-OMNIVIS" href="/article/10.1007/s10055-017-0310-7#ref-CR16" id="ref-link-section-d58092e6404">2008</a>):</p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Occ= {\left\{ \begin{array}{ll} {\text {Visible}}{:} &amp;{} \left( {{ SL}}_{CM}\cap \left( {\mathop {\bigcup }\limits _{k=1}^{n}} {{ Obs}}_{k}\right) \right) ={\varnothing } \\ {\text {Occluded}}{:} &amp;{} {\text {else}} \end{array}\right. } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>where <span class="mathjax-tex">\({{ SL}}_{CM}\)</span> is the straight line connecting the camera centre <i>C</i> and marker <i>M</i>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Marker visibility constraints</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>
                        <i>Resolution</i> It represents the minimum and maximum distances from which the camera can accurately view markers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig4">4</a>), and is represented by the following formula (Zhao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zhao J, Cheung S, Nguyen T (2009) Optimal visual sensor network configuration. Multi-camera networks: principles and applications, Academic Press, New York, NY, USA, pp 139–162" href="/article/10.1007/s10055-017-0310-7#ref-CR30" id="ref-link-section-d58092e6631">2009</a>):</p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} D_{{{ min}}} \le \Vert \left[ (X_{{\mathrm{c}}}, Y_{{\mathrm{c}}}, Z_{{\mathrm{c}}})-(X_m, Y_m, Z_m)\right] \cdot v_i\Vert \le D_{{{ max}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (16)
                </div></div><p>where <span class="mathjax-tex">\(v_i\)</span> is the unit vector along the optical axis of the camera’s view trajectory.</p><p>
                        <i>Field of view</i> The camera’s field of view allows for determining if tags can be observed by the camera (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig4">4</a>). The camera’s field of view is calculated as follows:</p><div id="Equ17" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sigma {\text {(in\, degrees)}} = 2\arctan \left( \frac{h}{2 D_{{{{ min}}}}}\right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (17)
                </div></div><p>Thus, we seek to maximize the following optimization function:</p><div id="Equ18" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} f=w_1(\sigma )+w_2(Res)+w_3(Occ) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (18)
                </div></div><p>where <span class="mathjax-tex">\({\textit{w}}_1\)</span>, <span class="mathjax-tex">\({\textit{w}}_2,\)</span> and <span class="mathjax-tex">\({\textit{w}}_3\)</span> are the weights of importance predefined by the designer. This gives:</p><div id="Equ19" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} { Max} \sum \limits _{i}\, f \,{\text { subject\, to}}\, Q=\frac{E_{{\mathrm{r}}}}{Nf\times Nt} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (19)
                </div></div><p>For a minimum number of cameras:</p><div id="Equ20" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} { Min} \sum \limits _{i=1}^{Nc}C_i\,{\text {given}}\sum \limits _{i, j, k=1}^{Nc, Nt, Nf}(a_{ij})_k\ge {{ re}}_{{{ min}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (20)
                </div></div><p>Based on these equations, the objective is to evaluate the quality of the camera network to maximize the visibility of the tags present in the 3D scene.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Adaptive genetic algorithm</h4><p>Our adaptive genetic algorithm proceeds in four steps: initialization, evaluation, selection, and reproduction as follows:</p><p>
                        <i>Initialization</i> A population of camera configurations is created as an initial solution using the DET block. An individual is a vector of real numbers containing the translation and orientation parameters for each camera. We use 4–10 cameras, which results in 24-dimensional vectors of up to 60 vectors. The population consisted of 300 such individuals (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Population of a camera network (initial solution). Each individual contains <i>n</i> cameras and the population consist of <i>m</i> such individuals</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>
                        <i>Evaluation</i> Each individual in the population is evaluated according to the error metric as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0310-7#Sec12">3.2.4</a> and assigned a corresponding fitness score. Our error metric was the 3D point reconstruction error applied to a distribution of tags. In our algorithm, the error from the reconstructed point returns a single score per individual, which allows evaluating its fitness. The evaluation is stopped after a predetermined number of iterations (e.g. <i>60 iterations</i>) or if the reconstruction error is below a certain threshold (e.g. <span class="mathjax-tex">\(f=0.9\)</span>).</p><p>
                        <i>Selection</i> The population is sorted according to the fitness of the evaluated camera networks. Elitism (Goldberg and Holland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Goldberg DE, Holland JH (1988) Genetic algorithms and machine learning. Mach Learn 3(2):95–99" href="/article/10.1007/s10055-017-0310-7#ref-CR8" id="ref-link-section-d58092e7441">1988</a>) is used as the selection method for camera networks exhibiting a fitness score <span class="mathjax-tex">\({\ge }0.5\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Selection of solutions where at least half of the markers are seen (fitness <span class="mathjax-tex">\({\ge }0.5\)</span>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>
                        <i>Reproduction</i> The camera networks selected in the previous step are used to create a new generation of camera networks based on crossover and mutation operators (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig7">7</a>). The choice of crossover probability <span class="mathjax-tex">\(P_{{\mathrm{c}}}\)</span> and mutation probability <span class="mathjax-tex">\(P_m\)</span> has a direct impact on the convergence of the algorithm (Mitchell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Mitchell M (1998) An introduction to genetic algorithms. MIT Press, Cambridge" href="/article/10.1007/s10055-017-0310-7#ref-CR18" id="ref-link-section-d58092e7573">1998</a>; Szeliski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Szeliski R (2010) Computer vision: algorithms and applications. Springer Science &amp; Business Media, Berlin" href="/article/10.1007/s10055-017-0310-7#ref-CR26" id="ref-link-section-d58092e7577">2010</a>). Moreover, it is difficult to find the best values of <span class="mathjax-tex">\(P_{{\mathrm{c}}}\)</span> and <span class="mathjax-tex">\(P_m\)</span> that adapt to all problems. Because of this, we adopted adaptive crossover and mutation strategies.</p><p>Commonly used adaptive operators are detailed in (Angeline and Angeline <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Zhao J, Cheung S-C, Nguyen T (2008) Optimal camera network configurations for visual tagging. IEEE J Sel Top Sig Process 2(4):464–479" href="/article/10.1007/s10055-017-0310-7#ref-CR29" id="ref-link-section-d58092e7632">1995</a>; Kita <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kita H (2001) A comparison study of self-adaptation in evolution strategies and real-coded genetic algorithms. Evol Comput 9(2):223–241" href="/article/10.1007/s10055-017-0310-7#ref-CR13" id="ref-link-section-d58092e7635">2001</a>; Xu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Xu Z, Zhang J, Zhang Y (2003) Bionics in computational intelligence: theory and algorithm. Science Press, Beijing" href="/article/10.1007/s10055-017-0310-7#ref-CR27" id="ref-link-section-d58092e7638">2003</a>). We used crossover and mutation operators from (Kita <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kita H (2001) A comparison study of self-adaptation in evolution strategies and real-coded genetic algorithms. Evol Comput 9(2):223–241" href="/article/10.1007/s10055-017-0310-7#ref-CR13" id="ref-link-section-d58092e7641">2001</a>) to reduce the population size and rapidly converge to the optimal solution. These operators are efficient (Aissaoui et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Aissaoui A, Baarir Z, Ouafi A, Pudlo P, Taleb-Ahmed A, Gillet C, Deraz F (2014) Optimisation technique of camera placement for motion capture simulations. Comput Methods Biomech Biomed Eng 17(sup1):122–123" href="/article/10.1007/s10055-017-0310-7#ref-CR1" id="ref-link-section-d58092e7644">2014</a>).</p><p>The crossover probability is expressed:</p><div id="Equ21" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P_{{\mathrm{c}}} = \left\{ \begin{array}{ll} k_1\left( f_{{{ max}}}-f\right) /\left( f_{{{ max}}}-\bar{f}\right) , &amp;{} f \ge \bar{f} \\ k_2, &amp;{} f &lt;\bar{f} \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (21)
                </div></div><p>The mutation probability is expressed:</p><div id="Equ22" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P_{m} = \left\{ \begin{array}{ll} k_3\left( f_{{{ max}}}-f\right) /\left( f_{{{ max}}}-\bar{f}\right) , &amp;{} f \ge \bar{f} \\ k_4, &amp;{}f &lt;\bar{f} \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (22)
                </div></div><p>where</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(k_1\)</span>, <span class="mathjax-tex">\(k_2\)</span>, <span class="mathjax-tex">\(k_3\)</span>, <span class="mathjax-tex">\(k_4\)</span>: real numbers <span class="mathjax-tex">\({\le } 1.0\)</span>.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(f_{{{ max}}}\)</span>: maximum fitness value of population.</p>
                      </li>
                      <li>
                        <p>
                                    <span class="mathjax-tex">\(\bar{f}\)</span>: average fitness value for each generation.</p>
                      </li>
                      <li>
                        <p>
                                    <i>f</i>: fitness Value of mutant individuals.</p>
                      </li>
                    </ul><p>It is well established in the literature on genetic algorithms (Srinivas and Patnaik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Srinivas M, Patnaik LM (1994) Adaptive probabilities of crossover and mutation in genetic algorithms. IEEE Trans Syst Man Cybern Part B (Cybern) 24(4):656–667" href="/article/10.1007/s10055-017-0310-7#ref-CR25" id="ref-link-section-d58092e8261">1994</a>; Bao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Bao S, Jiang R, Kwan W, Wang B, Ma X, Song Y-Q (2011) Evaluation of next-generation sequencing software in mapping and assembly. J Hum Genet 56(6):406–414" href="/article/10.1007/s10055-017-0310-7#ref-CR4" id="ref-link-section-d58092e8264">2011</a>) that <span class="mathjax-tex">\((P_{{\mathrm{c}}} \ge 0.5)\)</span> and <span class="mathjax-tex">\((P_m \ge 0.01)\)</span> produce good algorithm convergence. Thus, we fixed values of <span class="mathjax-tex">\(k_1\)</span>, <span class="mathjax-tex">\(k_2\)</span> at 0.5 and <span class="mathjax-tex">\(k_3\)</span>, <span class="mathjax-tex">\(k_4\)</span> at 0.01.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Reproduction operators</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Finally, the solution space represents an <i>n</i>-dimensional polyhedron which changes in size and shape whenever a new generation is created. The crossover and mutation operators act as visiting corner points of the polyhedron. Thus, by visiting this corner points an optimal placement solution is proposed when the objective function is maximized.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Simulation and performance evaluation</h2><div class="c-article-section__content" id="Sec21-content"><h3 class="c-article__sub-heading" id="Sec22">Simulation</h3><p>In order to compare the performance of our proposed GGA with that of GA, simulations were conducted on MATLAB 8.5 (MathWorks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="MathWorks (1994) Matlab r2015a. &#xA;                    https://www.mathworks.com/products/matlab.html&#xA;                    &#xA;                  . Accessed 27 Dec 2016" href="/article/10.1007/s10055-017-0310-7#ref-CR17" id="ref-link-section-d58092e8478">1994</a>) using three scenarios: walking, running, and jumping. The necessary simulation parameters are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0310-7#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Simulation parameters</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0310-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec23">Performance evaluation</h3><p>In order to evaluate the performance of GGA from the simulation results, we considered the following three performance measures: optimization quality, execution time, and optimization cost.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Optimization quality</h4><p>This measure is related to the fitness value of each generated camera network. The fitness value defines convergence to the optimal camera network. The closer the value is to “1”, the more efficient the camera network.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig8">8</a> indicates the progression of the average fitness value in 60 iterations for both algorithms GGA and GA. For the 3 scenarios, the GA started with a low fitness value, but GGA started with a significant fitness value (initial iterations). For the 3 scenarios, we notice that the performances in terms of fitness are clearly better for the GGA.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Optimization quality of GGA and GA with respect to average fitness value for evaluated camera networks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Optimization time</h4><p>This represents the maximum time allowed to provide the best camera network for a given scenario. If the algorithm does not provide a response within this limit for a given test, it is considered a failure, even though the algorithm could provide a fair answer with more time.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig9">9</a>, GGA has a significant advantage compared to GA in terms of optimization time. GGA successfully selects the most suitable camera locations for the scenario studied to recover tag coordinates in a reasonable time. Nevertheless, the optimization time of GA varies depending on the movement studied. Thus, as a scenario becomes more complex, the optimization time clearly increases.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Optimization time of camera locations according to the number of reconstructed tags over the number of iterations for GGA and GA</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Optimization cost</h4><p>This measure is essential for motion capture applications. The goal is to minimize the number of cameras used for motion capture, which in turn minimizes the necessary cost for the motion capture setup.</p><p>Concerning the optimization cost, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig10">10</a> demonstrates that GGA reaches a maximum rate of recovered tags (100%) by using only 4 or 6 cameras with a runtime of 15 min. However, GA uses 10 cameras to reach 100% with a runtime of 60 min.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Rate of reconstructed tags with 4, 6, and 10 cameras which refers to the optimization cost for both GGA and GA</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0310-7#Fig11">11</a> shows the best camera network configurations obtained with GGA and GA for the 3 scenarios where, regardless of the complexity of the scenario, the best configuration with GGA is obtained by using a reduced number of cameras compared to GA.</p><p>For scenarios walk, run, and jump, our GGA algorithm finds the best configuration by using 4, 4, and 6 cameras, respectively; however the best configuration of GA is obtained by using 8, 10, 10 cameras, respectively. This means that the GGA is more efficient than the GA in minimizing the number of cameras used and actually less costly.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0310-7/MediaObjects/10055_2017_310_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Best camera network configurations obtained with GGA and GA algorithms for the 3 scenarios</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0310-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec27"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Conclusion</h2><div class="c-article-section__content" id="Sec27-content"><p>This paper presented a method to optimize camera network configuration for a motion capture. The proposed method improves the performances of classical genetic algorithms to deal with the problem of divergence of solutions and decrease the computation time of the optimization. To achieve this goal an error metric that evaluates the quality of capture and a distribution and estimation technique that restricts the search space were developed.</p><p>The work proposed in this paper opens new challenges in the future to improve GGA robustness. We will explore the possibility of combining GGA with other evolutionary techniques to speed up the convergence process and satisfy the requirements of the global optimum problem to optimize camera network placement for motion capture.</p><p>Finally, this work opens new perspectives in terms of the acquisition of human movement in complex environments (e.g. with obstacles). We want to develop a digital application, based on this work, to help position the cameras in an experimental scene, which will save time, and also limit the loss of markers affixed to the subject being tested.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Aissaoui, Z. Baarir, A. Ouafi, P. Pudlo, A. Taleb-Ahmed, C. Gillet, F. Deraz, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Aissaoui A, Baarir Z, Ouafi A, Pudlo P, Taleb-Ahmed A, Gillet C, Deraz F (2014) Optimisation technique of came" /><p class="c-article-references__text" id="ref-CR1">Aissaoui A, Baarir Z, Ouafi A, Pudlo P, Taleb-Ahmed A, Gillet C, Deraz F (2014) Optimisation technique of camera placement for motion capture simulations. Comput Methods Biomech Biomed Eng 17(sup1):122–123</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10255842.2014.931517" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimisation%20technique%20of%20camera%20placement%20for%20motion%20capture%20simulations&amp;journal=Comput%20Methods%20Biomech%20Biomed%20Eng&amp;volume=17&amp;issue=sup1&amp;pages=122-123&amp;publication_year=2014&amp;author=Aissaoui%2CA&amp;author=Baarir%2CZ&amp;author=Ouafi%2CA&amp;author=Pudlo%2CP&amp;author=Taleb-Ahmed%2CA&amp;author=Gillet%2CC&amp;author=Deraz%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Al Hasan, KK. Ramachandran, JE. Mitchell, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Al Hasan M, Ramachandran KK, Mitchell JE (2008) Optimal placement of stereo sensors. Opt Lett 2(1):99–111" /><p class="c-article-references__text" id="ref-CR2">Al Hasan M, Ramachandran KK, Mitchell JE (2008) Optimal placement of stereo sensors. Opt Lett 2(1):99–111</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2363848" aria-label="View reference 2 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11590-007-0046-5" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1145.90037" aria-label="View reference 2 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20placement%20of%20stereo%20sensors&amp;journal=Opt%20Lett&amp;volume=2&amp;issue=1&amp;pages=99-111&amp;publication_year=2008&amp;author=Al%20Hasan%2CM&amp;author=Ramachandran%2CKK&amp;author=Mitchell%2CJE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="PJ. Angeline, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Angeline PJ (1995) Adaptive and self-adaptive evolutionary computations. In: Palaniswami M, Attikiouzel Y (eds" /><p class="c-article-references__text" id="ref-CR3">Angeline PJ (1995) Adaptive and self-adaptive evolutionary computations. In: Palaniswami M, Attikiouzel Y (eds) Computational intelligence: a dynamic systems perspective. IEEE Press, New York, pp 152–163</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20intelligence%3A%20a%20dynamic%20systems%20perspective&amp;pages=152-163&amp;publication_year=1995&amp;author=Angeline%2CPJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Bao, R. Jiang, W. Kwan, B. Wang, X. Ma, Y-Q. Song, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Bao S, Jiang R, Kwan W, Wang B, Ma X, Song Y-Q (2011) Evaluation of next-generation sequencing software in map" /><p class="c-article-references__text" id="ref-CR4">Bao S, Jiang R, Kwan W, Wang B, Ma X, Song Y-Q (2011) Evaluation of next-generation sequencing software in mapping and assembly. J Hum Genet 56(6):406–414</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fjhg.2011.43" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20next-generation%20sequencing%20software%20in%20mapping%20and%20assembly&amp;journal=J%20Hum%20Genet&amp;volume=56&amp;issue=6&amp;pages=406-414&amp;publication_year=2011&amp;author=Bao%2CS&amp;author=Jiang%2CR&amp;author=Kwan%2CW&amp;author=Wang%2CB&amp;author=Ma%2CX&amp;author=Song%2CY-Q">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Chen, J. Davis, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chen X, Davis J (2008) An occlusion metric for selecting robust camera configurations. Mach Vis Appl 19(4):217" /><p class="c-article-references__text" id="ref-CR5">Chen X, Davis J (2008) An occlusion metric for selecting robust camera configurations. Mach Vis Appl 19(4):217–222</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00138-007-0094-y" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1331.68230" aria-label="View reference 5 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20occlusion%20metric%20for%20selecting%20robust%20camera%20configurations&amp;journal=Mach%20Vis%20Appl&amp;volume=19&amp;issue=4&amp;pages=217-222&amp;publication_year=2008&amp;author=Chen%2CX&amp;author=Davis%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Chvatal, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Chvatal V (1975) A combinatorial theorem in plane geometry. J Comb Theory Ser B 18(1):39–41" /><p class="c-article-references__text" id="ref-CR6">Chvatal V (1975) A combinatorial theorem in plane geometry. J Comb Theory Ser B 18(1):39–41</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=372750" aria-label="View reference 6 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0095-8956%2875%2990061-1" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0278.05028" aria-label="View reference 6 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20combinatorial%20theorem%20in%20plane%20geometry&amp;journal=J%20Comb%20Theory%20Ser%20B&amp;volume=18&amp;issue=1&amp;pages=39-41&amp;publication_year=1975&amp;author=Chvatal%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ercan AO, Yang DB, El Gamal A, Guibas LJ (2006) Optimal placement and selection of camera network nodes for ta" /><p class="c-article-references__text" id="ref-CR7">Ercan AO, Yang DB, El Gamal A, Guibas LJ (2006) Optimal placement and selection of camera network nodes for target localization. In: International conference on distributed computing in sensor systems. Springer, pp 389–404</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DE. Goldberg, JH. Holland, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Goldberg DE, Holland JH (1988) Genetic algorithms and machine learning. Mach Learn 3(2):95–99" /><p class="c-article-references__text" id="ref-CR8">Goldberg DE, Holland JH (1988) Genetic algorithms and machine learning. Mach Learn 3(2):95–99</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1022602019183" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Genetic%20algorithms%20and%20machine%20learning&amp;journal=Mach%20Learn&amp;volume=3&amp;issue=2&amp;pages=95-99&amp;publication_year=1988&amp;author=Goldberg%2CDE&amp;author=Holland%2CJH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gonzalez-Barbosa J-J, García-Ramírez T, Salas J, Hurtado-Ramos J-B, Rico-Jiménez J-D-J (2009) Optimal camera p" /><p class="c-article-references__text" id="ref-CR9">Gonzalez-Barbosa J-J, García-Ramírez T, Salas J, Hurtado-Ramos J-B, Rico-Jiménez J-D-J (2009) Optimal camera placement for total coverage. In: Proceedings of the 2009 IEEE international conference on robotics and automation. IEEE Press, pp 3672–3676</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hörster E, Lienhart R (2006) On the optimal placement of multiple visual sensors. In: Proceedings of the 4th A" /><p class="c-article-references__text" id="ref-CR10">Hörster E, Lienhart R (2006) On the optimal placement of multiple visual sensors. In: Proceedings of the 4th ACM international workshop on video surveillance and sensor networks. ACM, pp 111–120</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Indu S, Chaudhury S, Mittal NR, Bhattacharyya A (2009) Optimal sensor placement for surveillance of large spac" /><p class="c-article-references__text" id="ref-CR11">Indu S, Chaudhury S, Mittal NR, Bhattacharyya A (2009) Optimal sensor placement for surveillance of large spaces. In: ICDSC 2009. 3rd ACM/IEEE international conference on distributed smart cameras. IEEE, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz I, Aghajan H, Haymaker J (2010) A process for sensor configuration in multi-camera networks. In: Proc. 4t" /><p class="c-article-references__text" id="ref-CR12">Katz I, Aghajan H, Haymaker J (2010) A process for sensor configuration in multi-camera networks. In: Proc. 4th ACM/IEEE int. conf. distributed smart cameras</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Kita, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Kita H (2001) A comparison study of self-adaptation in evolution strategies and real-coded genetic algorithms." /><p class="c-article-references__text" id="ref-CR13">Kita H (2001) A comparison study of self-adaptation in evolution strategies and real-coded genetic algorithms. Evol Comput 9(2):223–241</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F106365601750190415" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparison%20study%20of%20self-adaptation%20in%20evolution%20strategies%20and%20real-coded%20genetic%20algorithms&amp;journal=Evol%20Comput&amp;volume=9&amp;issue=2&amp;pages=223-241&amp;publication_year=2001&amp;author=Kita%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Lee, A. Lin, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Lee D, Lin A (1986) Computational complexity of art gallery problems. IEEE Trans Inf Theory 32(2):276–282" /><p class="c-article-references__text" id="ref-CR14">Lee D, Lin A (1986) Computational complexity of art gallery problems. IEEE Trans Inf Theory 32(2):276–282</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=838414" aria-label="View reference 14 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIT.1986.1057165" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0593.68035" aria-label="View reference 14 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20complexity%20of%20art%20gallery%20problems&amp;journal=IEEE%20Trans%20Inf%20Theory&amp;volume=32&amp;issue=2&amp;pages=276-282&amp;publication_year=1986&amp;author=Lee%2CD&amp;author=Lin%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MW. Lee, CH. Jung, MG. Lee, D. Brutzman, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Lee MW, Jung CH, Lee MG, Brutzman D (2015) Data definition of 3D character modeling and animation using H-Anim" /><p class="c-article-references__text" id="ref-CR15">Lee MW, Jung CH, Lee MG, Brutzman D (2015) Data definition of 3D character modeling and animation using H-Anim. J Converg 6(2):19–29</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.14400%2FJDC.2015.13.3.19" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20definition%20of%203D%20character%20modeling%20and%20animation%20using%20H-Anim&amp;journal=J%20Converg&amp;volume=6&amp;issue=2&amp;pages=19-29&amp;publication_year=2015&amp;author=Lee%2CMW&amp;author=Jung%2CCH&amp;author=Lee%2CMG&amp;author=Brutzman%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Malik R, Bajcsy P (2008) Automated placement of multiple stereo cameras. In: The 8th workshop on omnidirection" /><p class="c-article-references__text" id="ref-CR16">Malik R, Bajcsy P (2008) Automated placement of multiple stereo cameras. In: The 8th workshop on omnidirectional vision, camera networks and non-classical cameras-OMNIVIS</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="MathWorks (1994) Matlab r2015a. https://www.mathworks.com/products/matlab.html. Accessed 27 Dec 2016" /><p class="c-article-references__text" id="ref-CR17">MathWorks (1994) Matlab r2015a. <a href="https://www.mathworks.com/products/matlab.html">https://www.mathworks.com/products/matlab.html</a>. Accessed 27 Dec 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Mitchell, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Mitchell M (1998) An introduction to genetic algorithms. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR18">Mitchell M (1998) An introduction to genetic algorithms. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20genetic%20algorithms&amp;publication_year=1998&amp;author=Mitchell%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Mittal, LS. Davis, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Mittal A, Davis LS (2008) A general method for sensor planning in multi-sensor systems: extension to random oc" /><p class="c-article-references__text" id="ref-CR19">Mittal A, Davis LS (2008) A general method for sensor planning in multi-sensor systems: extension to random occlusion. Int J Comput Vis 76(1):31–52</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-007-0057-9" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20general%20method%20for%20sensor%20planning%20in%20multi-sensor%20systems%3A%20extension%20to%20random%20occlusion&amp;journal=Int%20J%20Comput%20Vis&amp;volume=76&amp;issue=1&amp;pages=31-52&amp;publication_year=2008&amp;author=Mittal%2CA&amp;author=Davis%2CLS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Morsly, N. Aouf, MS. Djouadi, M. Richardson, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Morsly Y, Aouf N, Djouadi MS, Richardson M (2012) Particle swarm optimization inspired probability algorithm f" /><p class="c-article-references__text" id="ref-CR20">Morsly Y, Aouf N, Djouadi MS, Richardson M (2012) Particle swarm optimization inspired probability algorithm for optimal camera network placement. IEEE Sens J 12(5):1402–1412</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJSEN.2011.2170833" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Particle%20swarm%20optimization%20inspired%20probability%20algorithm%20for%20optimal%20camera%20network%20placement&amp;journal=IEEE%20Sens%20J&amp;volume=12&amp;issue=5&amp;pages=1402-1412&amp;publication_year=2012&amp;author=Morsly%2CY&amp;author=Aouf%2CN&amp;author=Djouadi%2CMS&amp;author=Richardson%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Olague, R. Mohr, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Olague G, Mohr R (2002) Optimal camera placement for accurate reconstruction. Pattern Recogn 35(4):927–944" /><p class="c-article-references__text" id="ref-CR21">Olague G, Mohr R (2002) Optimal camera placement for accurate reconstruction. Pattern Recogn 35(4):927–944</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2801%2900076-0" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0997.68116" aria-label="View reference 21 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20camera%20placement%20for%20accurate%20reconstruction&amp;journal=Pattern%20Recogn&amp;volume=35&amp;issue=4&amp;pages=927-944&amp;publication_year=2002&amp;author=Olague%2CG&amp;author=Mohr%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. O’rourke, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="O’rourke J (1987) Art gallery theorems and algorithms, vol 57. Oxford University Press, Oxford" /><p class="c-article-references__text" id="ref-CR22">O’rourke J (1987) Art gallery theorems and algorithms, vol 57. Oxford University Press, Oxford</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Art%20gallery%20theorems%20and%20algorithms&amp;publication_year=1987&amp;author=O%E2%80%99rourke%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Piciarelli C, Micheloni C, Foresti GL (2010) Occlusion-aware multiple camera reconfiguration. In: Proceedings " /><p class="c-article-references__text" id="ref-CR23">Piciarelli C, Micheloni C, Foresti GL (2010) Occlusion-aware multiple camera reconfiguration. In: Proceedings of the 4th ACM/IEEE international conference on distributed smart cameras. ACM, pp 88–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rahimian P, Kearney JK (2015) Optimal camera placement for motion capture systems in the presence of dynamic o" /><p class="c-article-references__text" id="ref-CR24">Rahimian P, Kearney JK (2015) Optimal camera placement for motion capture systems in the presence of dynamic occlusion. In: Proceedings of the 21st ACM symposium on virtual reality software and technology. ACM, pp 129–138</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Srinivas, LM. Patnaik, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Srinivas M, Patnaik LM (1994) Adaptive probabilities of crossover and mutation in genetic algorithms. IEEE Tra" /><p class="c-article-references__text" id="ref-CR25">Srinivas M, Patnaik LM (1994) Adaptive probabilities of crossover and mutation in genetic algorithms. IEEE Trans Syst Man Cybern Part B (Cybern) 24(4):656–667</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F21.286385" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptive%20probabilities%20of%20crossover%20and%20mutation%20in%20genetic%20algorithms&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20Part%20B%20%28Cybern%29&amp;volume=24&amp;issue=4&amp;pages=656-667&amp;publication_year=1994&amp;author=Srinivas%2CM&amp;author=Patnaik%2CLM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Szeliski, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Szeliski R (2010) Computer vision: algorithms and applications. Springer Science &amp; Business Media, Berlin" /><p class="c-article-references__text" id="ref-CR26">Szeliski R (2010) Computer vision: algorithms and applications. Springer Science &amp; Business Media, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20vision%3A%20algorithms%20and%20applications&amp;publication_year=2010&amp;author=Szeliski%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Z. Xu, J. Zhang, Y. Zhang, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Xu Z, Zhang J, Zhang Y (2003) Bionics in computational intelligence: theory and algorithm. Science Press, Beij" /><p class="c-article-references__text" id="ref-CR27">Xu Z, Zhang J, Zhang Y (2003) Bionics in computational intelligence: theory and algorithm. Science Press, Beijing</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bionics%20in%20computational%20intelligence%3A%20theory%20and%20algorithm&amp;publication_year=2003&amp;author=Xu%2CZ&amp;author=Zhang%2CJ&amp;author=Zhang%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Yao, C-H. Chen, B. Abidi, D. Page, A. Koschan, M. Abidi, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Yao Y, Chen C-H, Abidi B, Page D, Koschan A, Abidi M (2010) Can you see me now? sensor positioning for automat" /><p class="c-article-references__text" id="ref-CR28">Yao Y, Chen C-H, Abidi B, Page D, Koschan A, Abidi M (2010) Can you see me now? sensor positioning for automated and persistent surveillance. IEEE Trans Syst Man Cybern Part B (Cybern) 40(1):101–115</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCB.2009.2017507" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Can%20you%20see%20me%20now%3F%20sensor%20positioning%20for%20automated%20and%20persistent%20surveillance&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20Part%20B%20%28Cybern%29&amp;volume=40&amp;issue=1&amp;pages=101-115&amp;publication_year=2010&amp;author=Yao%2CY&amp;author=Chen%2CC-H&amp;author=Abidi%2CB&amp;author=Page%2CD&amp;author=Koschan%2CA&amp;author=Abidi%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Zhao, S-C. Cheung, T. Nguyen, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Zhao J, Cheung S-C, Nguyen T (2008) Optimal camera network configurations for visual tagging. IEEE J Sel Top S" /><p class="c-article-references__text" id="ref-CR29">Zhao J, Cheung S-C, Nguyen T (2008) Optimal camera network configurations for visual tagging. IEEE J Sel Top Sig Process 2(4):464–479</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJSTSP.2008.2001430" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20camera%20network%20configurations%20for%20visual%20tagging&amp;journal=IEEE%20J%20Sel%20Top%20Sig%20Process&amp;volume=2&amp;issue=4&amp;pages=464-479&amp;publication_year=2008&amp;author=Zhao%2CJ&amp;author=Cheung%2CS-C&amp;author=Nguyen%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Zhao, S. Cheung, T. Nguyen, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Zhao J, Cheung S, Nguyen T (2009) Optimal visual sensor network configuration. Multi-camera networks: principl" /><p class="c-article-references__text" id="ref-CR30">Zhao J, Cheung S, Nguyen T (2009) Optimal visual sensor network configuration. Multi-camera networks: principles and applications, Academic Press, New York, NY, USA, pp 139–162</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20visual%20sensor%20network%20configuration.%20Multi-camera%20networks%3A%20principles%20and%20applications&amp;pages=139-162&amp;publication_year=2009&amp;author=Zhao%2CJ&amp;author=Cheung%2CS&amp;author=Nguyen%2CT">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0310-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported by the Franco-Algerian cooperation programme PHC TASSILI (12MDU876) Grants. Entitled “Assistant System to the Cameras Location in the MOCAP”, the Project gathers members of Automatic Control and Human–Machine Systems of LAMIH Laboratory-Valenciennes University-France, and AISEL Laboratory-Biskra University-Algerie.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">LESIA Laboratory, Biskra University, BP 145 RP, 07000, Biskra, Algeria</p><p class="c-article-author-affiliation__authors-list">Azeddine Aissaoui, Abdelkrim Ouafi &amp; Zine-Eddine Baarir</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">UVHC, LAMIH, 59313, Valenciennes, France</p><p class="c-article-author-affiliation__authors-list">Philippe Pudlo, Christophe Gillet &amp; Abdelmalik Taleb-Ahmed</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">CNRS, UMR 8201, 59313, Valenciennes, France</p><p class="c-article-author-affiliation__authors-list">Philippe Pudlo, Christophe Gillet &amp; Abdelmalik Taleb-Ahmed</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">University Lille Nord de France, 59000, Lille, France</p><p class="c-article-author-affiliation__authors-list">Philippe Pudlo, Christophe Gillet &amp; Abdelmalik Taleb-Ahmed</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Azeddine-Aissaoui"><span class="c-article-authors-search__title u-h3 js-search-name">Azeddine Aissaoui</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Azeddine+Aissaoui&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Azeddine+Aissaoui" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Azeddine+Aissaoui%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Abdelkrim-Ouafi"><span class="c-article-authors-search__title u-h3 js-search-name">Abdelkrim Ouafi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Abdelkrim+Ouafi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Abdelkrim+Ouafi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Abdelkrim+Ouafi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Philippe-Pudlo"><span class="c-article-authors-search__title u-h3 js-search-name">Philippe Pudlo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Philippe+Pudlo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Philippe+Pudlo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Philippe+Pudlo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Christophe-Gillet"><span class="c-article-authors-search__title u-h3 js-search-name">Christophe Gillet</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Christophe+Gillet&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Christophe+Gillet" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Christophe+Gillet%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Zine_Eddine-Baarir"><span class="c-article-authors-search__title u-h3 js-search-name">Zine-Eddine Baarir</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Zine-Eddine+Baarir&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zine-Eddine+Baarir" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zine-Eddine+Baarir%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Abdelmalik-Taleb_Ahmed"><span class="c-article-authors-search__title u-h3 js-search-name">Abdelmalik Taleb-Ahmed</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Abdelmalik+Taleb-Ahmed&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Abdelmalik+Taleb-Ahmed" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Abdelmalik+Taleb-Ahmed%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0310-7/email/correspondent/c1/new">Azeddine Aissaoui</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Designing%20a%20camera%20placement%20assistance%20system%20for%20human%20motion%20capture%20based%20on%20a%20guided%20genetic%20algorithm&amp;author=Azeddine%20Aissaoui%20et%20al&amp;contentID=10.1007%2Fs10055-017-0310-7&amp;publication=1359-4338&amp;publicationDate=2017-04-04&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0310-7" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0310-7" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Aissaoui, A., Ouafi, A., Pudlo, P. <i>et al.</i> Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm.
                    <i>Virtual Reality</i> <b>22, </b>13–23 (2018). https://doi.org/10.1007/s10055-017-0310-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0310-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-01">01 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-03-24">24 March 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-04-04">04 April 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03">March 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0310-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0310-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Multi-camera-based motion capture systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Optimal camera configurations</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Genetic algorithm</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Optimization</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0310-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=310;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

