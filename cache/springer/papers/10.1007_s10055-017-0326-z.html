<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A computational model of perceptual saliency for 3D objects in virtual"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim to select proper reference objects, useful..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A computational model of perceptual saliency for 3D objects in virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-10-06"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London Ltd."/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim to select proper reference objects, useful for locating a target object within a virtual environment (VE), a computational model to identify perceptual saliency is presented. Based on the object&#8217;s features with the major stimulus for the human visual system, three basic features of a 3D object (i.e., color, size, and shape) are individually evaluated and then combined to get a degree of saliency for each 3D object in a virtual scenario. An experiment was conducted to evaluate the extent to which the proposed measure of saliency matches with the people&#8217;s subjective perception of saliency; the results showed a good performance of this computational model."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-10-06"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="221"/>

    <meta name="prism.endingPage" content="234"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London Ltd."/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0326-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0326-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0326-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0326-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A computational model of perceptual saliency for 3D objects in virtual environments"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2018/09"/>

    <meta name="citation_online_date" content="2017/10/06"/>

    <meta name="citation_firstpage" content="221"/>

    <meta name="citation_lastpage" content="234"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0326-z"/>

    <meta name="DOI" content="10.1007/s10055-017-0326-z"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0326-z"/>

    <meta name="description" content="When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim t"/>

    <meta name="dc.creator" content="Graciela Lara"/>

    <meta name="dc.creator" content="Ang&#233;lica De Antonio"/>

    <meta name="dc.creator" content="Adriana Pe&#241;a"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Cognit Process; citation_title=On the assessment of landmark salience for human navigation. Springer; citation_author=D Caduff, T Sabine; citation_volume=9; citation_issue=4; citation_publication_date=2008; citation_pages=249-267; citation_doi=10.1007/s10339-007-0199-2; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Mot Skills; citation_title=Color preferences and cultural variation; citation_author=A Choungourian; citation_volume=26; citation_issue=3_suppl; citation_publication_date=1968; citation_pages=1203-1206; citation_doi=10.2466/pms.1968.26.3c.1203; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Image Process; citation_title=A study of efficiency and accuracy in the transformation from RGB to CIELAB color space; citation_author=C Connolly, T Fliess; citation_volume=6; citation_issue=7; citation_publication_date=1997; citation_pages=1046-1048; citation_doi=10.1109/83.597279; citation_id=CR3"/>

    <meta name="citation_reference" content="de I&#8217;Eclairage CI (1978) Recommendations on uniform color spaces-color difference equations, psyhometric color terms. Supplement no. 2 to CIE publication no. 15 (E-1.3. 1) 1971. TC, 1&#8211;3"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Appl Percept (TAP); citation_title=Computational visual attention systems and their cognitive foundations: a survey; citation_author=S Frintrop, E Rome, HI Christensen; citation_volume=7; citation_issue=1; citation_publication_date=2010; citation_pages=6:1-6:39; citation_id=CR5"/>

    <meta name="citation_reference" content="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519&#8211;536"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit; citation_title=Color-based object recognition. Elsevier; citation_author=T Gevers, AWM Smeulders; citation_volume=32; citation_issue=3; citation_publication_date=1999; citation_pages=453-464; citation_doi=10.1016/S0031-3203(98)00036-3; citation_id=CR7"/>

    <meta name="citation_reference" content="Guo C, Ma Q, Zhang L (2008) Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform. In: Conference on vision and pattern recognition. IEEE, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=Salience of visual parts. Elsevier; citation_author=DD Hoffman, M Singh; citation_volume=68; citation_issue=1; citation_publication_date=1997; citation_pages=29-78; citation_doi=10.1016/S0010-0277(96)00791-3; citation_id=CR9"/>

    <meta name="citation_reference" content="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Computer vision and pattern recognition. IEEE conference on. IEEE Minneapolis, MN, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=Visual Cognit; citation_title=Quantitative modelling of perceptual salience at human eye position. Taylor &amp; Francis Group. Psychology Press; citation_author=L Itti; citation_volume=14; citation_issue=4&#8211;8; citation_publication_date=2006; citation_pages=959-984; citation_doi=10.1080/13506280500195672; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A model of saliency-based visual attention for rapid scence analysis; citation_author=L Itti, C Koch, E Niebur; citation_volume=20; citation_issue=11; citation_publication_date=1998; citation_pages=1254-1259; citation_doi=10.1109/34.730558; citation_id=CR12"/>

    <meta name="citation_reference" content="Ju R, Ge L, Geng W, Ren T, Wu G (2014) Depth saliency based on anisotropic center-surround difference. In: Image processing (ICIP), 2014 IEEE international conference on. IEEE, pp 1115&#8211;1119"/>

    <meta name="citation_reference" content="citation_title=Structural salience of landmarks for route directions; citation_inbook_title=Spatial information theory; citation_publication_date=2005; citation_pages=347-362; citation_id=CR14; citation_author=A Klippel; citation_author=S Winter; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=A computational measure of saliency of the shape of 3D objects; citation_inbook_title=Trends and applications in software engineering; citation_publication_date=2016; citation_pages=235-245; citation_id=CR15; citation_author=G Lara; citation_author=A Antonio; citation_author=A Pe&#241;a; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey J, McKenna S, Trucco E (eds) Proceedings of the British machine vision conference. BMVC Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Visual saliency based on scale-space analysis in the frequency domain; citation_author=J Li, MD Levine, X An, X Xu, H He; citation_volume=35; citation_issue=4; citation_publication_date=2013; citation_pages=996-1010; citation_doi=10.1109/TPAMI.2012.147; citation_id=CR17"/>

    <meta name="citation_reference" content="L&#243;pez F, Valiente JM, Baldrich R, Vanrell M (2005) Fast surface grading using color statistics in the CIE Lab space. In: Iberian conference on pattern recognition and image analysis (LNCS 3523), Springer, Berlin, Heidelberg, vol 2, pp 666&#8211;673"/>

    <meta name="citation_reference" content="citation_title=Sensorische Faktoren Beim Verstehen &#220;berspezifizierter Objektbenennungen; citation_publication_date=1986; citation_id=CR19; citation_author=R Mangold; citation_publisher=Peter Lang"/>

    <meta name="citation_reference" content="Oliva A, Torralba A, Castelhano MS, Henderson JM (2003) Top-dow control of visual attention in object detection. In: Image proceedings, 2003. icip 2003. Proceedings. 2003 international conference on, vol 1, pp 1&#8211;253"/>

    <meta name="citation_reference" content="Raubal M, Winter S (2002) Enriching wayfinding instructions with local landmarks. In: Egenhofer MJ, Mark DM (eds) International conference on geographic information science, vol 2478. Springer, Berlin, Heidelberg, Boulder, CO, USA, pp 243&#8211;259"/>

    <meta name="citation_reference" content="citation_journal_title=Cognit Process; citation_title=The Giessen virtual environment laboratory: human wayfinding and landmark salience Springer-Verlag; citation_author=F R&#246;ser, K Hamburger, M Knauff; citation_volume=12; citation_issue=2; citation_publication_date=2011; citation_pages=209-214; citation_doi=10.1007/s10339-011-0390-3; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=J Spat Sci; citation_title=The structural salience of landmarks: results from an on-line study and a virtual environment experiment. Taylor &amp; Francis Group; citation_author=F R&#246;ser, K Hamburger, A Krumnack, M Knauff; citation_volume=57; citation_issue=1; citation_publication_date=2012; citation_pages=37-50; citation_doi=10.1080/14498596.2012.686362; citation_id=CR23"/>

    <meta name="citation_reference" content="R&#246;ser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: CogSci, pp 3315&#8211;3320"/>

    <meta name="citation_reference" content="citation_journal_title=Geoderma; citation_title=Colour space models for soil science. Elsevier; citation_author=RV Rossel, B Minasny, P Roudier, AB McBratney; citation_volume=133; citation_issue=3; citation_publication_date=2006; citation_pages=320-337; citation_doi=10.1016/j.geoderma.2005.07.017; citation_id=CR25"/>

    <meta name="citation_reference" content="Saleh A, Khalil M, Wahdan A-M (2003) An adaptive image watermarking scheme for color images using S-CIELAB. In: Circuits and systems, 2003 IEEE 46th Midwest symposium, vol 3. IEEE, pp 1575&#8211;1578"/>

    <meta name="citation_reference" content="Sampedro MJ, Blanco M, Ponte D, Leir&#243;s LI (2010) Saliencia Perceptiva y Atenci&#243;n. La Atenci&#243;n (VI). Un enfoque pluridisciplinar, pp 91&#8211;103"/>

    <meta name="citation_reference" content="Shilane P, Min P, Kazhdan M, Furkhouser T (2004) The Princeton shape benchmark. In: Shape modeling applications, 2004. Proceeding. IEEE Washington, DC, USA, pp 167&#8211;178"/>

    <meta name="citation_reference" content="citation_journal_title=Manag Decis; citation_title=Impact of color on marketing; citation_author=S Singh; citation_volume=44; citation_issue=6; citation_publication_date=2006; citation_pages=783-789; citation_doi=10.1108/00251740610673332; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph (TOG); citation_title=Mesh saliency via spectral processing; citation_author=R Song, Y Liu, RR Martin, PL Rosin; citation_volume=33; citation_issue=1; citation_publication_date=2014; citation_pages=6; citation_doi=10.1145/2530691; citation_id=CR30"/>

    <meta name="citation_reference" content="Stoia L (2007) Noun phrase generation for situated dialogs. Ohio State University. Ph.D. thesis"/>

    <meta name="citation_reference" content="Su H, Maji S, Kalogerakis E, Learned-Miller E (2015) Multi-view convulational neural networks for 3D shape recognition. In: Proceedings of the IEEE international conference on computer vision, pp 945&#8211;953"/>

    <meta name="citation_reference" content="Undurraga C, Mery D (2011) Improving tracking algorithms using saliency. In: C&#233;sar SM, Kim S-W (eds) Proceedings 16th Iberoamerican congress. Pattern recognition, vol 7042. Springer-Verlag, Berlin, Heidelberg, Chile, pp 141&#8211;148"/>

    <meta name="citation_reference" content="Undurraga C, Mery D, Sucar SLE (2010) Modelo de Saliencia utilizando el descriptor de covarianza"/>

    <meta name="citation_reference" content="citation_journal_title=Graph Models; citation_title=Mesh saliency with global rarity. ELSEVIER; citation_author=J Wu, X Shen, W Zhu, L Liu; citation_volume=75; citation_issue=5; citation_publication_date=2013; citation_pages=255-264; citation_doi=10.1016/j.gmod.2013.05.002; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_title=Color science: concepts and methods, quantitative data and formulae; citation_publication_date=1982; citation_id=CR36; citation_author=G Wyszecki; citation_author=WS Stiles; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=J Prosthet Dent; citation_title=Defining a natural tooth color space based on a 3-dimensional shade system; citation_author=JC-C Yuan, JD Brewer, EA Monaco, EL Davis; citation_volume=98; citation_issue=2; citation_publication_date=2007; citation_pages=110-119; citation_doi=10.1016/S0022-3913(07)60044-4; citation_id=CR37"/>

    <meta name="citation_reference" content="Zhao Y, Liu Y, Zeng Z (2013) Using region-based saliency for 3D interest points detection. In: International conference on computer analysis of images and patterns. Springer, Berlin, Heidelberg, pp 108&#8211;116"/>

    <meta name="citation_author" content="Graciela Lara"/>

    <meta name="citation_author_email" content="graciela.lara@red.cucei.udg.mx"/>

    <meta name="citation_author_institution" content="CUCEI of the Universidad de Guadalajara, Guadalajara (Jalisco), Mexico"/>

    <meta name="citation_author" content="Ang&#233;lica De Antonio"/>

    <meta name="citation_author_email" content="angelica@fi.upm.es"/>

    <meta name="citation_author_institution" content="Escuela T&#233;cnica Superior de Ingenieros Inform&#225;ticos of the Universidad Polit&#233;cnica de Madrid, Boadilla Del Monte, Spain"/>

    <meta name="citation_author" content="Adriana Pe&#241;a"/>

    <meta name="citation_author_email" content="adriana.pena@cucei.udg.mx"/>

    <meta name="citation_author_institution" content="CUCEI of the Universidad de Guadalajara, Guadalajara (Jalisco), Mexico"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0326-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0326-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A computational model of perceptual saliency for 3D objects in virtual environments"/>
        <meta property="og:description" content="When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim to select proper reference objects, useful for locating a target object within a virtual environment (VE), a computational model to identify perceptual saliency is presented. Based on the object’s features with the major stimulus for the human visual system, three basic features of a 3D object (i.e., color, size, and shape) are individually evaluated and then combined to get a degree of saliency for each 3D object in a virtual scenario. An experiment was conducted to evaluate the extent to which the proposed measure of saliency matches with the people’s subjective perception of saliency; the results showed a good performance of this computational model."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A computational model of perceptual saliency for 3D objects in virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0326-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Reference object, Perceptual salience, Virtual environment, 3D object’s features extraction","kwrd":["Reference_object","Perceptual_salience","Virtual_environment","3D_object’s_features_extraction"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0326-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0326-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=326;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0326-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A computational model of perceptual saliency for 3D objects in virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0326-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0326-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-10-06" itemprop="datePublished">06 October 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A computational model of perceptual saliency for 3D objects in virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Graciela-Lara" data-author-popup="auth-Graciela-Lara">Graciela Lara</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CUCEI of the Universidad de Guadalajara" /><meta itemprop="address" content="0000 0001 2158 0196, grid.412890.6, CUCEI of the Universidad de Guadalajara, Av. Revolución 1500, Col. Olímpica, 44430, Guadalajara (Jalisco), Mexico" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ang_lica-De_Antonio" data-author-popup="auth-Ang_lica-De_Antonio">Angélica De Antonio</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Escuela Técnica Superior de Ingenieros Informáticos of the Universidad Politécnica de Madrid" /><meta itemprop="address" content="Escuela Técnica Superior de Ingenieros Informáticos of the Universidad Politécnica de Madrid, Campus de Montegancedo, 28660, Boadilla Del Monte, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Adriana-Pe_a" data-author-popup="auth-Adriana-Pe_a" data-corresp-id="c1">Adriana Peña<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CUCEI of the Universidad de Guadalajara" /><meta itemprop="address" content="0000 0001 2158 0196, grid.412890.6, CUCEI of the Universidad de Guadalajara, Av. Revolución 1500, Col. Olímpica, 44430, Guadalajara (Jalisco), Mexico" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">221</span>–<span itemprop="pageEnd">234</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">364 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0326-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim to select proper reference objects, useful for locating a target object within a virtual environment (VE), a computational model to identify perceptual saliency is presented. Based on the object’s features with the major stimulus for the human visual system, three basic features of a 3D object (i.e., color, size, and shape) are individually evaluated and then combined to get a degree of saliency for each 3D object in a virtual scenario. An experiment was conducted to evaluate the extent to which the proposed measure of saliency matches with the people’s subjective perception of saliency; the results showed a good performance of this computational model.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Objects have many physical characteristics; some of them make an object more prominent to people than others. When someone observes an image or a group of objects, that person will focus on certain areas to which he/she is attracted, areas that contain information that can be regarded as relevant (Undurraga and Mery <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Undurraga C, Mery D (2011) Improving tracking algorithms using saliency. In: César SM, Kim S-W (eds) Proceedings 16th Iberoamerican congress. Pattern recognition, vol 7042. Springer-Verlag, Berlin, Heidelberg, Chile, pp 141–148" href="/article/10.1007/s10055-017-0326-z#ref-CR33" id="ref-link-section-d25161e331">2011</a>; Undurraga et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Undurraga C, Mery D, Sucar SLE (2010) Modelo de Saliencia utilizando el descriptor de covarianza" href="/article/10.1007/s10055-017-0326-z#ref-CR34" id="ref-link-section-d25161e334">2010</a>). This is the visual saliency of an object, linked to the interaction of its basic features concerning to the other surrounding objects (Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536" href="/article/10.1007/s10055-017-0326-z#ref-CR6" id="ref-link-section-d25161e337">1995</a>; Hoffman and Singh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hoffman DD, Singh M (1997) Salience of visual parts. Elsevier. Cognition 68(1):29–78" href="/article/10.1007/s10055-017-0326-z#ref-CR9" id="ref-link-section-d25161e340">1997</a>; Stoia <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Stoia L (2007) Noun phrase generation for situated dialogs. Ohio State University. Ph.D. thesis" href="/article/10.1007/s10055-017-0326-z#ref-CR31" id="ref-link-section-d25161e343">2007</a>).</p><p>While the ability of the human visual system to detect visual saliency of an object, or a set of objects, is extraordinarily fast and reliable, the computational modeling of this human basic intelligent behavior remains a challenge (Hou and Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Computer vision and pattern recognition. IEEE conference on. IEEE Minneapolis, MN, pp 1–8" href="/article/10.1007/s10055-017-0326-z#ref-CR10" id="ref-link-section-d25161e349">2007</a>). For the proposed model, the aim is for a computer system to measure the perceptual saliency of 3D models in virtual environments (VE), in an automatic mode. The final goal is to get the best reference object or objects, the most salient ones, for the automatic generation of natural language directions, to help users to locate specific objects within a computer-generated scenario.</p><p>The attributes that affect the quality of an object to perform as a reference object can be: (1) those that are context dependent, that is, of its environment, so like of its functional dependencies of the objects (e.g., a plate and a cup) or the prior knowledge of the person that will locate an object; and (2) those related to the specific features of the object. These last (the specific features of the objects) are the aim of this study. According to different studies (Caduff and Sabine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Caduff D, Sabine T (2008) On the assessment of landmark salience for human navigation. Springer. Cognit Process 9(4):249–267" href="/article/10.1007/s10055-017-0326-z#ref-CR1" id="ref-link-section-d25161e355">2008</a>; Frintrop et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Frintrop S, Rome E, Christensen HI (2010) Computational visual attention systems and their cognitive foundations: a survey. ACM Trans Appl Percept (TAP) 7(1):6:1–6:39" href="/article/10.1007/s10055-017-0326-z#ref-CR5" id="ref-link-section-d25161e358">2010</a>; Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536" href="/article/10.1007/s10055-017-0326-z#ref-CR6" id="ref-link-section-d25161e361">1995</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey J, McKenna S, Trucco E (eds) Proceedings of the British machine vision conference. BMVC Press" href="/article/10.1007/s10055-017-0326-z#ref-CR16" id="ref-link-section-d25161e364">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Li J, Levine MD, An X, Xu X, He H (2013) Visual saliency based on scale-space analysis in the frequency domain. IEEE Trans Pattern Anal Mach Intell 35(4):996–1010" href="/article/10.1007/s10055-017-0326-z#ref-CR17" id="ref-link-section-d25161e367">2013</a>; Oliva et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Oliva A, Torralba A, Castelhano MS, Henderson JM (2003) Top-dow control of visual attention in object detection. In: Image proceedings, 2003. icip 2003. Proceedings. 2003 international conference on, vol 1, pp 1–253" href="/article/10.1007/s10055-017-0326-z#ref-CR20" id="ref-link-section-d25161e371">2003</a>; Röser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Röser F, Hamburger K, Knauff M (2011) The Giessen virtual environment laboratory: human wayfinding and landmark salience Springer-Verlag. Cognit Process 12(2):209–214" href="/article/10.1007/s10055-017-0326-z#ref-CR22" id="ref-link-section-d25161e374">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Röser F, Hamburger K, Krumnack A, Knauff M (2012) The structural salience of landmarks: results from an on-line study and a virtual environment experiment. Taylor &amp; Francis Group. J Spat Sci 57(1):37–50" href="/article/10.1007/s10055-017-0326-z#ref-CR23" id="ref-link-section-d25161e377">2012</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: CogSci, pp 3315–3320" href="/article/10.1007/s10055-017-0326-z#ref-CR24" id="ref-link-section-d25161e380">2013</a>), the perceptual saliency is related to the visual characteristics that can capture the attention, such as: color, size, shape, texture, contrast, orientation and motion. In these studies, the object’s characteristics that are more frequently used are those closely related to the human visual system.</p><p>From all the features of an object, an easy to perceive, as well as an attractive is its color. The influence of the colors in the individuals’ nervous system is very complex, but this is undoubtedly a reality. The color is an aspect of the world, while the human being is constantly impacted by its environment (Singh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Singh S (2006) Impact of color on marketing. Manag Decis 44(6):783–789" href="/article/10.1007/s10055-017-0326-z#ref-CR29" id="ref-link-section-d25161e386">2006</a>). Likewise, size is a stimulus, since the larger an object, the greater the chance of attention; a small object is easier to locate by using as reference a bigger object. However, the size of an object is relative to the other objects in the scenario, which might change this context condition. When objects have nearly the same size, their shape is the characteristic that can help to make them distinguishable (Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536" href="/article/10.1007/s10055-017-0326-z#ref-CR6" id="ref-link-section-d25161e389">1995</a>). The visual features of an object with the major stimulus for the human visual system are: color, size, and shape; features that will be used in this computational model (Caduff and Sabine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Caduff D, Sabine T (2008) On the assessment of landmark salience for human navigation. Springer. Cognit Process 9(4):249–267" href="/article/10.1007/s10055-017-0326-z#ref-CR1" id="ref-link-section-d25161e392">2008</a>; Frintrop et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Frintrop S, Rome E, Christensen HI (2010) Computational visual attention systems and their cognitive foundations: a survey. ACM Trans Appl Percept (TAP) 7(1):6:1–6:39" href="/article/10.1007/s10055-017-0326-z#ref-CR5" id="ref-link-section-d25161e395">2010</a>; Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536" href="/article/10.1007/s10055-017-0326-z#ref-CR6" id="ref-link-section-d25161e398">1995</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey J, McKenna S, Trucco E (eds) Proceedings of the British machine vision conference. BMVC Press" href="/article/10.1007/s10055-017-0326-z#ref-CR16" id="ref-link-section-d25161e402">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Li J, Levine MD, An X, Xu X, He H (2013) Visual saliency based on scale-space analysis in the frequency domain. IEEE Trans Pattern Anal Mach Intell 35(4):996–1010" href="/article/10.1007/s10055-017-0326-z#ref-CR17" id="ref-link-section-d25161e405">2013</a>; Oliva et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Oliva A, Torralba A, Castelhano MS, Henderson JM (2003) Top-dow control of visual attention in object detection. In: Image proceedings, 2003. icip 2003. Proceedings. 2003 international conference on, vol 1, pp 1–253" href="/article/10.1007/s10055-017-0326-z#ref-CR20" id="ref-link-section-d25161e408">2003</a>; Röser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Röser F, Hamburger K, Knauff M (2011) The Giessen virtual environment laboratory: human wayfinding and landmark salience Springer-Verlag. Cognit Process 12(2):209–214" href="/article/10.1007/s10055-017-0326-z#ref-CR22" id="ref-link-section-d25161e411">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Röser F, Hamburger K, Krumnack A, Knauff M (2012) The structural salience of landmarks: results from an on-line study and a virtual environment experiment. Taylor &amp; Francis Group. J Spat Sci 57(1):37–50" href="/article/10.1007/s10055-017-0326-z#ref-CR23" id="ref-link-section-d25161e414">2012</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: CogSci, pp 3315–3320" href="/article/10.1007/s10055-017-0326-z#ref-CR24" id="ref-link-section-d25161e417">2013</a>).</p><p>Our proposal comprises to measure the saliency of each object in the scenario, identifying its degree of saliency in relation to the other objects in the VE. This model is characterized by its promptness, simplicity, and efficiency. However, despite its simplicity, empirical results showed that the most salient objects given by our metric matches the most salient objects according to human perception.</p><h3 class="c-article__sub-heading" id="Sec2">Related work</h3><p>In recent years, psychologists and computer vision researchers have applied the concept of saliency to facilitate computer recognition of objects, and for the location of salient areas in images (Itti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Itti L (2006) Quantitative modelling of perceptual salience at human eye position. Taylor &amp; Francis Group. Psychology Press. Visual Cognit 14(4–8):959–984" href="/article/10.1007/s10055-017-0326-z#ref-CR11" id="ref-link-section-d25161e431">2006</a>; Itti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scence analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0326-z#ref-CR12" id="ref-link-section-d25161e434">1998</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey J, McKenna S, Trucco E (eds) Proceedings of the British machine vision conference. BMVC Press" href="/article/10.1007/s10055-017-0326-z#ref-CR16" id="ref-link-section-d25161e437">2011</a>; Raubal and Winter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Raubal M, Winter S (2002) Enriching wayfinding instructions with local landmarks. In: Egenhofer MJ, Mark DM (eds) International conference on geographic information science, vol 2478. Springer, Berlin, Heidelberg, Boulder, CO, USA, pp 243–259" href="/article/10.1007/s10055-017-0326-z#ref-CR21" id="ref-link-section-d25161e440">2002</a>; Sampedro et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Sampedro MJ, Blanco M, Ponte D, Leirós LI (2010) Saliencia Perceptiva y Atención. La Atención (VI). Un enfoque pluridisciplinar, pp 91–103" href="/article/10.1007/s10055-017-0326-z#ref-CR27" id="ref-link-section-d25161e443">2010</a>); several models of perceptive saliency have been proposed in different contexts with different criteria, and with different purposes.</p><p>In order to identify landmarks within a map, Itti et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scence analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259" href="/article/10.1007/s10055-017-0326-z#ref-CR12" id="ref-link-section-d25161e449">1998</a>) proposed a computational model of saliency based on the behavior and the neuronal architecture of the primate’s visual system. The combination of the features: size, intensity, color, and orientation of a multiscale image, is used to create a topographical saliency map. Klippel et al. (Klippel and Winter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Klippel A, Winter S (2005) Structural salience of landmarks for route directions. In: Cohn G, Mark M (eds) Spatial information theory. Springer, Ellicottville, pp 347–362" href="/article/10.1007/s10055-017-0326-z#ref-CR14" id="ref-link-section-d25161e452">2005</a>) approach is a structural salience in a taxonomy of point-like objects with respect to their positions. Their model of saliency is for the cognitive base to support navigation, and to formalize geo semantics. Caduff et al. (Caduff and Sabine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Caduff D, Sabine T (2008) On the assessment of landmark salience for human navigation. Springer. Cognit Process 9(4):249–267" href="/article/10.1007/s10055-017-0326-z#ref-CR1" id="ref-link-section-d25161e455">2008</a>) used a vector of saliency as a trilateral relationship between the observer, the observed object, and the environment in terms of perceptual, cognitive, and contextual saliency. Their system provides automatic generation of route instructions with the metric of geo properties.</p><p>For the identification of visual saliency in 2D images, Hou et al. (Hou and Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Computer vision and pattern recognition. IEEE conference on. IEEE Minneapolis, MN, pp 1–8" href="/article/10.1007/s10055-017-0326-z#ref-CR10" id="ref-link-section-d25161e461">2007</a>) developed an approach not related to the features, categories, or other forms of prior knowledge of the objects, by analyzing the log-spectrum of an input image. Itti (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Itti L (2006) Quantitative modelling of perceptual salience at human eye position. Taylor &amp; Francis Group. Psychology Press. Visual Cognit 14(4–8):959–984" href="/article/10.1007/s10055-017-0326-z#ref-CR11" id="ref-link-section-d25161e464">2006</a>) proposed a computational model treated from three aspects: color, intensity, and orientation. The zones founded by these three levels are linearly combined to obtain the saliency map of an image. For video clips, this same author also developed a simulation framework for the transition of the human eye fixation to the properties of a local image, to operationally define the perceptual saliency not only on the local image properties, but also on how these are captured by the retina. Also applied in videos and natural images, Guo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Guo C, Ma Q, Zhang L (2008) Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform. In: Conference on vision and pattern recognition. IEEE, pp 1–8" href="/article/10.1007/s10055-017-0326-z#ref-CR8" id="ref-link-section-d25161e467">2008</a>) proposed a method called phase spectrum of quaternion Fourier transform (PQFT) based on the phase spectrum of the Fourier transform to obtain a spatiotemporal saliency map. This map considers features like color, spatial salience like the orientation in a single frame, and temporal features between frames for motion. Li et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey J, McKenna S, Trucco E (eds) Proceedings of the British machine vision conference. BMVC Press" href="/article/10.1007/s10055-017-0326-z#ref-CR16" id="ref-link-section-d25161e470">2011</a>) determined the saliency through local and global aspects of an image considering: (1) detection of the saliency of regions, both small and large; (2) detection of saliency in the disordered scenes; and (3) the inhibition of repeated patterns. For them, the objects in the scene, seen by the human visual system, are believed to compete with each other for his/her focus of attention selectively.</p><p>More recently, using 3D visual information in images, Ju et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ju R, Ge L, Geng W, Ren T, Wu G (2014) Depth saliency based on anisotropic center-surround difference. In: Image processing (ICIP), 2014 IEEE international conference on. IEEE, pp 1115–1119" href="/article/10.1007/s10055-017-0326-z#ref-CR13" id="ref-link-section-d25161e476">2014</a>) developed a framework to detect the salient objects based on anisotropic center-surround difference, the method is based on the assumption that salient objects tend to stand out from its surrounding background. They used stereo images from Internet, 3D movies and photographs taken for a Fuji W3 stereo camera for their experiments. Su et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Su H, Maji S, Kalogerakis E, Learned-Miller E (2015) Multi-view convulational neural networks for 3D shape recognition. In: Proceedings of the IEEE international conference on computer vision, pp 945–953" href="/article/10.1007/s10055-017-0326-z#ref-CR32" id="ref-link-section-d25161e479">2015</a>) presented an architecture based on a convolutional neural network (CNN) trained on a fixed set of rendered views of a 3D shape, through saliency maps. They developed view-based descriptors for 3D shapes that are trainable; this method can be used for 3D objects and face recognition.</p><p>Regarding 3D models, in order to detect saliency regions of the surface, Wu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Wu J, Shen X, Zhu W, Liu L (2013) Mesh saliency with global rarity. ELSEVIER. Graph Models 75(5):255–264" href="/article/10.1007/s10055-017-0326-z#ref-CR35" id="ref-link-section-d25161e486">2013</a>) proposed a method considering both contrast and global rarity. The global rarity of each vertex is gotten by contrasting it to all other vertices, where geometrically similar vertices have comparable saliency values. Their method extracts saliency maps in geometric applications including feature-preserved mesh smoothing, simplification, and sampling. Zhao et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Zhao Y, Liu Y, Zeng Z (2013) Using region-based saliency for 3D interest points detection. In: International conference on computer analysis of images and patterns. Springer, Berlin, Heidelberg, pp 108–116" href="/article/10.1007/s10055-017-0326-z#ref-CR38" id="ref-link-section-d25161e489">2013</a>) presented a method based on the selection of interest points by using a smoothing method for features as: color, curvature, shape, and the repeatability of the interest points. The interest points are obtained by the region-based saliency detection. Since it generates spatially consistent high-quality saliency maps, rather than vertex-based, this method is used under different perceptual conditions, such as different viewpoints and noise corruption. Song et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Song R, Liu Y, Martin RR, Rosin PL (2014) Mesh saliency via spectral processing. ACM Trans Graph (TOG) 33(1):6" href="/article/10.1007/s10055-017-0326-z#ref-CR30" id="ref-link-section-d25161e492">2014</a>) developed a method that uses the importance of a local region. It incorporates global aspects in contrast with methods that are typically based on local geometric aspects. They take into account the properties of the log-Laplacian spectrum of the mesh, capturing its attributes. It can be used at multiples spatial scales.</p><p>In this representative set of works related to the automatic detection of saliency, some models analyze only components of the object to determine its most salient part or they use static objects in images, maps, or scenes, to calculate saliency maps. Likewise, there are models that can identify moving objects within a video to understand where people can direct their focus. Some of these systems summarize the properties of a destination object or calculate the saliency of a reference object to draw routes for destinations. However, none of them are applied directly to 3D modeled objects as a whole. To our knowledge, there are no approaches to determine the saliency of the 3D-modeled objects within a VE. In the next section, the computational model of saliency is described in detail.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">General approach of a computational model of perceptual saliency for virtual environments</h2><div class="c-article-section__content" id="Sec3-content"><p>The features or factors of the objects selected, as mentioned for their major stimulus for the human visual system, and used for the design of this computational model of perceptual saliency are: color, size, and shape. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig1">1</a> reflects how the saliency of an object is characterized by a combination of these three features. A relative prominence for these features was established according to Mangold (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Mangold R (1986) Sensorische Faktoren Beim Verstehen Überspezifizierter Objektbenennungen. Peter Lang, Frankfurt" href="/article/10.1007/s10055-017-0326-z#ref-CR19" id="ref-link-section-d25161e510">1986</a>), who’s study stated that, in objects’ identification tasks, color is a higher dominant feature than the size or the shape; and, likewise, size is more easily identified than shape.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Scheme of computational model of perceptual saliency</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The following describes the process of characterization based on these three features (i.e., color, size, and shape) and how the saliency is evaluated for each feature of the set of 3D objects in the scenario, before mixing them in an individual saliency measure.</p><h3 class="c-article__sub-heading" id="Sec4">Color characterization</h3><p>There are a number of color models (also called color systems or spaces) useful for the characterization of color as perceived by the human eye, like the RGB, CMYK, CIE-XYZ tristimulus, CIE-Lab, CIE-LUV, among others. While RGB is the color model most commonly used in digital image processing (e.g., scanners, cameras, and monitors), this system presents a number of disadvantages regarding to human visual perception:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                <i>High correlation between its components</i> The RGB system has a dependency on the light intensity of its three components: red, green, and blue. These components are measured with a scale from 0 to 255, and each color is defined by a set of values. The RGB values depend on the ability and specific sensitivity of each device for capture and visualization; thus, the RGB color space is device-dependent. However, with methods for devices calibration, the RGB space can be transformed into a linear perceptually uniform color space (Gevers and Smeulders <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gevers T, Smeulders AWM (1999) Color-based object recognition. Elsevier. Pattern Recognit 32(3):453–464" href="/article/10.1007/s10055-017-0326-z#ref-CR7" id="ref-link-section-d25161e550">1999</a>).</p>
                    </li>
                    <li>
                      <p>
                                <i>Not psychologically intuitive</i> RGB is not based on any psychological perception model. This psychological non-intuitiveness is a serious problem for humans that present, in some cases, the trouble of a bad visualization of colors defined by its three components (Gevers and Smeulders <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gevers T, Smeulders AWM (1999) Color-based object recognition. Elsevier. Pattern Recognit 32(3):453–464" href="/article/10.1007/s10055-017-0326-z#ref-CR7" id="ref-link-section-d25161e562">1999</a>).</p>
                    </li>
                    <li>
                      <p>
                                <i>Not uniformity distance</i> RGB space does not correspond to perceptual distance (Connolly and Fliess <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Connolly C, Fliess T (1997) A study of efficiency and accuracy in the transformation from RGB to CIELAB color space. IEEE Trans Image Process 6(7):1046–1048" href="/article/10.1007/s10055-017-0326-z#ref-CR3" id="ref-link-section-d25161e574">1997</a>; Gevers and Smeulders <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gevers T, Smeulders AWM (1999) Color-based object recognition. Elsevier. Pattern Recognit 32(3):453–464" href="/article/10.1007/s10055-017-0326-z#ref-CR7" id="ref-link-section-d25161e577">1999</a>). It does not have perceptual uniformity, i.e., the low correlation between the perceived difference of two colors and the Euclidian distance in the RGB space, is an evident problem to take this model for the computation of perceptual saliency.</p>
                    </li>
                  </ul>
<p>The RGB model is then oriented toward hardware, it is not intuitive and it is not related to the concepts of hue, saturation, and brightness. Therefore, for the suitability of the color in RGB format, in our computational model based on the human visual system, it is necessary to resort to the transformation of its values into a uniform color space where the distance between the points is directly proportional to the difference in the perceived color. This target color space is known as CIE-Lab, designed to be perceptually uniform (López et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="López F, Valiente JM, Baldrich R, Vanrell M (2005) Fast surface grading using color statistics in the CIE Lab space. In: Iberian conference on pattern recognition and image analysis (LNCS 3523), Springer, Berlin, Heidelberg, vol 2, pp 666–673" href="/article/10.1007/s10055-017-0326-z#ref-CR18" id="ref-link-section-d25161e586">2005</a>; Yuan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Yuan JC-C, Brewer JD, Monaco EA, Davis EL (2007) Defining a natural tooth color space based on a 3-dimensional shade system. J Prosthet Dent 98(2):110–119" href="/article/10.1007/s10055-017-0326-z#ref-CR37" id="ref-link-section-d25161e589">2007</a>). The CIE-Lab color space (strictly known as CIE 1976 <i>L</i> * <i>a</i> * <i>b</i> *) allows quantifying color differences that can be expressed in terms of human visual perception. Differences (<i>∆E*</i>) between two colors can be obtained from CIE 1976 <i>L</i> * <i>a</i> * <i>b</i>* with the following Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ1">1</a>) (López et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="López F, Valiente JM, Baldrich R, Vanrell M (2005) Fast surface grading using color statistics in the CIE Lab space. In: Iberian conference on pattern recognition and image analysis (LNCS 3523), Springer, Berlin, Heidelberg, vol 2, pp 666–673" href="/article/10.1007/s10055-017-0326-z#ref-CR18" id="ref-link-section-d25161e617">2005</a>; Saleh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Saleh A, Khalil M, Wahdan A-M (2003) An adaptive image watermarking scheme for color images using S-CIELAB. In: Circuits and systems, 2003 IEEE 46th Midwest symposium, vol 3. IEEE, pp 1575–1578" href="/article/10.1007/s10055-017-0326-z#ref-CR26" id="ref-link-section-d25161e621">2003</a>):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\Delta E^{*} = \sqrt[2]{{\left[ {\left( {\Delta L^{*} } \right)^{2} + \left( {\Delta a^{*} } \right)^{2} + \left( {\Delta b^{*} } \right)^{2} } \right]}}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
<p>The color difference (<i>∆E*</i>) is often used for the evaluation of color reproduction quality in an image processing system (Rossel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rossel RV, Minasny B, Roudier P, McBratney AB (2006) Colour space models for soil science. Elsevier. Geoderma 133(3):320–337" href="/article/10.1007/s10055-017-0326-z#ref-CR25" id="ref-link-section-d25161e762">2006</a>).</p><p>In order to transform a color of the RGB system to the CIE-Lab system, it has to be first transformed into the CIE-XYZ system (also called XYZ tristimulus system) as an intermediary for the determination of perceptually uniform color systems as CIE-Lab or CIE-LUV. In the XYZ tristimulus system, CIE defines colors with the variables <i>X</i>, <i>Y</i>, and <i>Z</i>, where <i>Y</i> represents the color luminosity, and the variables <i>X</i> and <i>Z</i> represent the virtual components of the primary specters for the calculation of all colors through its addition (Wyszecki and Stiles <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Wyszecki G, Stiles WS (1982) Color science: concepts and methods, quantitative data and formulae, vol 8. Wiley, New York" href="/article/10.1007/s10055-017-0326-z#ref-CR36" id="ref-link-section-d25161e787">1982</a>).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig2">2</a> shows this transformations sequence for the characterization of the color of a 3D object in a numerical format. The proposed procedure includes three steps: (1) the 3D object color is obtained in the RGB system; (2) then RGB color values are converted to the CIE-XYZ color system; and (3) the values of CIE-XYZ are used to convert the color to the CIE-Lab system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Sequence of transformations of the color of a 3D object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>To convert a color of RGB standard to the XYZ tristimulus system is then used the transformation matrix proposed by Wyszecki et al. (Wyszecki and Stiles <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Wyszecki G, Stiles WS (1982) Color science: concepts and methods, quantitative data and formulae, vol 8. Wiley, New York" href="/article/10.1007/s10055-017-0326-z#ref-CR36" id="ref-link-section-d25161e818">1982</a>):</p><ul class="u-list-style-none">
                    <li>
                      <p>RGB to XYZ:</p>
                    </li>
                  </ul>
<div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\left[ {\begin{array}{*{20}c} X \\ Y \\ Z \\ \end{array} } \right] = \left[ {\begin{array}{*{20}c} {0.412453} &amp; {0.357580} &amp; {0.180423} \\ {0.212671} &amp; {0.715160} &amp; {0.072169} \\ {0.019334} &amp; {0.119193} &amp; {0.950227} \\ \end{array} } \right]\left[ {\begin{array}{*{20}c} R \\ G \\ B \\ \end{array} } \right]$$</span></div></div>
<p>Afterward, the tristimulus value is transformed to the CIE-Lab Cartesian coordinate system, using the following linear equations (de I’Eclairage <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="de I’Eclairage CI (1978) Recommendations on uniform color spaces-color difference equations, psyhometric color terms. Supplement no. 2 to CIE publication no. 15 (E-1.3. 1) 1971. TC, 1–3" href="/article/10.1007/s10055-017-0326-z#ref-CR4" id="ref-link-section-d25161e1041">1978</a>):</p><p>XYZ to CIE-Lab: Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ2">2</a>)–(<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ4">4</a>)</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$b = 200\left[ {\left( {\frac{Y}{Yn}} \right)^{1/3} - \left( {\frac{Z}{Zn}} \right)^{1/3} } \right]$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
<div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$a = 500 \left[ {\left( {\frac{X}{Xn}} \right)^{1/3} - \left( {\frac{Y}{Yn}} \right)^{1/3} } \right]$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
<div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$b = 200\left[ {\left( {\frac{Y}{Yn}} \right)^{1/3} - \left( {\frac{Z}{Zn}} \right)^{1/3} } \right]$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Here <i>Xn</i>, <i>Yn</i>, and <i>Zn</i> represent the values of <i>X</i>, <i>Y</i>, and <i>Z</i> for the illuminant (reference white point). With these transformations is obtained the characterization of the color of an object in a three-digit numeric format. Within the computational model of perceptual saliency, the individual color of the object is represented with the variable (<i>C</i>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Saliency by color</h4><p>In order to identify the most salient colors to the users’ view, the saliency by color, the color group selected was the one specified by Choungourian in Choungourian (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1968" title="Choungourian A (1968) Color preferences and cultural variation. Percept Mot Skills 26(3_suppl):1203–1206" href="/article/10.1007/s10055-017-0326-z#ref-CR2" id="ref-link-section-d25161e1359">1968</a>). This author stated, based on empirical studies that the colors with major stimuli to the human visual system are: red, orange, yellow, yellow-green, green, blue-green, blue, and purple. His results also showed that there are no differences in sex or culture for these preferences.</p><p>For the computational model to calculate the color saliency, a three-step process is followed:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Using Euclidean distances, the distance of the color of all the objects in the environment is calculated with respect to the group of most salient colors.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Then, the salient color with less distance to the color of the object is selected, and its three coordinates (<i>L</i>, <i>a</i>, <i>b</i>) are located to assign a saliency value according to ranges in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0326-z#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Saliency value by color considering the distance value</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0326-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>

                        
                      </li>
                    </ol>
<p>We are aware that this is a rough categorization for color saliency, but to our knowledge there are no graduating intermediate values on this regard.</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Finally, the three obtained values (one for each coordinate) are averaged to get the salience by color of each object.</p>
                        
                      </li>
                    </ol>
<p>The value of saliency by color of the object is represented in the computational model by the variable <i>Sc</i>
                        <sub>[0–1]</sub>.</p><p>In this initial approach, the computational model of saliency is only for 3D objects with one solid color. The RGB color of the 3D object can be recovered from the software tool with which it was modeled.</p><h3 class="c-article__sub-heading" id="Sec6">Size characterization</h3><p>An object can also be attractive to the view of the human beings simply by its size. A solid object represents a three-dimensional geometric body that occupies a place in space. With the volume measurement of an object, it can be identified its size or physical extent in three dimensions (i.e., height, length, and width) using, for example, cubic centimeters (cm<sup>3</sup>) or cubic meters (m<sup>3</sup>) as measure unit.</p><p>Because objects in the real world or in a VE are frequently not regular geometric figures, the mathematical expressions that are used to calculate the volume of geometric figures are not appropriate for all objects. An automatic measure for the volume of regular and irregular 3D objects in a VE is required, with no regards of their position and orientation. Therefore, we propose a volume measure through a pre-voxelized object, which implies to consider the following three previous steps:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>The 3D object is first voxelized. The voxelization process is the segmentation of an object into small cubic portions, a unit called voxel, which conforms and represents the three-dimensional object, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig3">3</a>. The voxelization process helps us to characterize both the size and the shape of an object. Regarding the size of the voxel (<i>vsi</i>), in VEs is common to work with an approximation in terms of centimeters; then, the size of the voxel is here treated as one cubic centimeter.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>View of a voxelized 3D object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                                
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>So, the total number of voxels (<i>nv</i>) for the object is calculated through the voxelization algorithm. During the voxelization process, a voxel is detected as correct when two halves form one, a voxel, is counted when it is detected as correct.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Therefore, the number of voxels (<i>nv</i>) is multiplied by the voxel size (<i>vsi</i>) to obtain the object volume (<i>vo</i>) in cubic centimeters.</p>
                      
                    </li>
                  </ol>
                <p>Additionally to the volume, we use other three parameters to fully characterize the size of a voxelized 3D object: length, width, and height of its bounding box. The characterization of the object’s size in this computational model of perceptual saliency is represented by the variable (<i>Si</i>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Saliency by size</h4><p>After the color, according to the literature as mentioned, the size of an object is the second characteristic with major influence for determining its saliency. With the values of length, width, and height of the bounding box and the volume of each object, is made a list of the objects in the VE. This list is organized in an ontology for a semantic definition, that is, a description of the features of each object. From the list of objects, the largest object in each of its size parameters (i.e., length, width, height, and volume) is used to establish a proportional value of saliency for the rest of the objects. For example, to calculate the saliency of the length, the next Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ5">5</a>) is used:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$Sl_{{\left[ {0 - 1} \right]}} = \frac{l}{hlv}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>Sl</i>
                        <sub>[0–1]</sub> represents the normalized value of saliency by length, calculated by dividing the object’s length (<i>l</i>) by the highest length value of the objects in the VE (<i>hlv</i>). Likewise are calculated width <i>Sw</i>
                        <sub>[0–1]</sub>, height <i>Sh</i>
                        <sub>[0–1]</sub>, and volume <i>Sv</i>
                        <sub>[0–1]</sub>. The four weighted values are then averaged to get the saliency by size of the 3D object, represented in the computational model by the variable <i>Ssi</i>
                        <sub>[0–1]</sub>.</p><h3 class="c-article__sub-heading" id="Sec8">Shape characterization</h3><p>According to the measurement obtained by our model, the object with the largest volume will be mathematically most salient than the objects with lower volume. However, not always the largest object in an environment is the most salient. In some cases, it is possible for a small object to be the most salient, for example, if it contrasts with respect to the other objects in the scene (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Contrast by size</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>On the other hand, if all the objects have the same size, other characteristics such as the shape (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig5">5</a>a), the orientation (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig5">5</a>b), or the distance between objects (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig5">5</a>c) could help to distinguish one object from the others (Gapp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536" href="/article/10.1007/s10055-017-0326-z#ref-CR6" id="ref-link-section-d25161e1769">1995</a>). In order to deal with these exceptions, the shape of the object was included in our computational model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Other characteristics of an object: <b>a</b> the shape, <b>b</b> the orientation, and <b>c</b> the distance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>As mentioned, the shape of an object contributes to its visual attraction. When the visual interest of an object is its shape, the measure could be specified by the deviation of its shape from a rectangle or by its shape factor, which represents the height-to-width ratio. For example, skyscrapers have a high shape factor, whereas long and low buildings have a low shape factor (Raubal and Winter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Raubal M, Winter S (2002) Enriching wayfinding instructions with local landmarks. In: Egenhofer MJ, Mark DM (eds) International conference on geographic information science, vol 2478. Springer, Berlin, Heidelberg, Boulder, CO, USA, pp 243–259" href="/article/10.1007/s10055-017-0326-z#ref-CR21" id="ref-link-section-d25161e1803">2002</a>) (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Shape factor: high and low</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>This could be a simple way to specify the shape of a 3D object, although not with enough detail. One of the toughest challenges of this computational model of saliency was the characterization of shape, due to the large amount of information it possesses, making it a complex characteristic. Typically, the shape of a 3D object is recognized by its vertices, edges, and sides; aspects normally projected toward the observer’s viewpoint in VEs. However, this appreciation is a complicated process during the implementation.</p><p>With the purpose of getting an optimal way to retrieve the data of the shape of a 3D object, we proposed a measure based on the central idea of the volume of a pre-voxelized object. Assuming that the flatter is the surface of an object, the less salient it is, and inversely, objects with high pointedness tend to be perceptually more salient, we propose to calculate the proportion of empty space and full space in voxels, in the bounding box of the 3D object. This approach was previously evaluated in (Lara et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Lara G, De Antonio A, Peña A (2016) A computational measure of saliency of the shape of 3D objects. In: Mejia J, Muñoz M, Rocha Á, Calvo-Manzano J (eds) Trends and applications in software engineering. Springer, Cham, pp 235–245" href="/article/10.1007/s10055-017-0326-z#ref-CR15" id="ref-link-section-d25161e1834">2016</a>).</p><p>For this, first the volume of the bounding box in voxels (<i>siBBox</i>) is obtained, and then the volume in voxels of the object (<i>vo</i>) is subtracted from this number. The result of this operation is the “Empty space” (<i>Es</i>). It represents the number of free voxels within the bounding box that are not part of the object, while the volume of the object in voxels corresponds to the “Full space” (<i>Fs</i>) in the bounding box. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig7">7</a> illustrates the empty and full spaces of the bounding box of a 3D object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Empty and full spaces of the bounding box of a 3D object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Finally, the empty space (<i>Es</i>) divided by the total of voxels in the bounding box (<i>siBBox</i>) represents its empty space proportion, used as measure for the model. The characterization of the shape is represented in the computational model with the variable (<i>Sh</i>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Saliency by shape</h4><p>With the above calculation, we provide a direct estimation of the saliency of an object’s shape. It is worth to mention that a normalization process is required when the feature that determines the saliency of the object is a comparison with the surrounding objects like in the case of size, in contrast with the shape or the color. The saliency by shape variable is represented in the model by <i>Ssh</i>
                        <sub>[0–1]</sub>.</p><h3 class="c-article__sub-heading" id="Sec10">Individual saliency of the 3D object</h3><p>The calculation of the saliency for each object within the scenario, the individual saliency by object, is made through a weighted sum. According to Gapp (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536" href="/article/10.1007/s10055-017-0326-z#ref-CR6" id="ref-link-section-d25161e1907">1995</a>) and Mangold (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Mangold R (1986) Sensorische Faktoren Beim Verstehen Überspezifizierter Objektbenennungen. Peter Lang, Frankfurt" href="/article/10.1007/s10055-017-0326-z#ref-CR19" id="ref-link-section-d25161e1910">1986</a>), the color is the most salient factor, followed by the size and shape, as a result a weight was assigned to each factor as follows: 0.5 to the color, 0.3 to the size, and 0.2 to the shape. Equation. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ6">6</a>) is then proposed to calculate an individual saliency for each 3D object in the scenario.</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$IS = \left( {Sc_{{\left[ {0 - 1} \right]}} *0.5} \right) + \left( {Ssi_{{\left[ {0 - 1} \right]}} *0.3} \right) + \left( {Ssh_{{\left[ {0 - 1} \right]}} *0.2} \right)$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <i>IS</i> represents the individual saliency of each 3D object and <i>Sc</i>
                    <sub>[0–1]</sub>, <i>Ssi</i>
                    <sub>[0–1]</sub>, and <i>Ssh</i>
                    <sub>[0–1]</sub> correspond to its saliency by color, size, and shape, respectively.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Experimental evaluation</h2><div class="c-article-section__content" id="Sec11-content"><p>With the aim to verify our hypothesis that people perception of saliency is similar to the results of our computational model of saliency, it was designed and carried out the experiment next described.</p><h3 class="c-article__sub-heading" id="Sec12">Method</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Participants</h4><p>A group of thirty people, undergraduate students, professors, and administrative employees of a Computer Science school at a university, seventeen male and thirteen female, with ages in the range of 20–67 years, voluntarily participated.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Materials, devices, and situation</h4><p>An ad hoc application was developed to measure 3D objects saliency according to the computational model here proposed. The game engine Unity 3D™ and a number of scripts in C# programming language were used. Particularly, for the 3D objects voxelization process, a master script of Unity 3D™ was adjusted to extract the number of voxels and so calculating the volume of each object. An ontology data, designed and managed with the Protégé™ editor, was incorporated. Also, a semantic editor–exporter was developed in the EditorWindow of Unity 3D™, and through it, data can be collected from the 3D objects in the scenes and exported to the ontology.</p><p>The experiment was carried out in a laboratory with suitable lighting condition. Each participant’s session was run in a SONY™ laptop computer, model VGN-CS270T, with a processor Intel <sup>®</sup> Core™2 Duo CPU P8600, 2.40 GHz, 4.00 GB memory, with a mouse and a keyboard.</p><p>Data were statistically analyzed using the SPSS™ (Statistical Product and Service Solutions) application.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Design and procedure</h4><p>From the Princeton Shape Benchmark (PSB), which contains 1814 3D models and is public available on the World Wide Web (Shilane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Shilane P, Min P, Kazhdan M, Furkhouser T (2004) The Princeton shape benchmark. In: Shape modeling applications, 2004. Proceeding. IEEE Washington, DC, USA, pp 167–178" href="/article/10.1007/s10055-017-0326-z#ref-CR28" id="ref-link-section-d25161e2111">2004</a>), were randomly extracted 3D objects used and evaluated in a previous experiment related to the saliency of shape (see Lara et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Lara G, De Antonio A, Peña A (2016) A computational measure of saliency of the shape of 3D objects. In: Mejia J, Muñoz M, Rocha Á, Calvo-Manzano J (eds) Trends and applications in software engineering. Springer, Cham, pp 235–245" href="/article/10.1007/s10055-017-0326-z#ref-CR15" id="ref-link-section-d25161e2114">2016</a>). Nine of them were randomly selected as follows: three evaluated as with high saliency, three as having medium saliency, and three with low saliency according to its shape. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig8">8</a>, can be seen the selected models and their given name in the PSB. In the first row are the ones with high salience in shape (Ferrari F380, Spider and Mobile Phone), in row two the medium ones (GarbageCan, C64Chip, and BeerBottle), and in the last row the less salient in shape objects (Pencil, Dice and BriefCase).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The set of nine objects selected for the experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>The nine selected 3D objects were resized adjusting each of them to three different scales: 1 for a large size, 0.65 for medium, and 0.25 for a small size; getting a set of 27 different objects which were used twice, giving a total of 54 objects.</p><p>Two groups of colors were used in these 54 objects: the group formed by the eight most salient colors identified by Choungourian (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1968" title="Choungourian A (1968) Color preferences and cultural variation. Percept Mot Skills 26(3_suppl):1203–1206" href="/article/10.1007/s10055-017-0326-z#ref-CR2" id="ref-link-section-d25161e2144">1968</a>); and a second group of eight colors randomly selected from the RGB system. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig9">9</a> are presented the 16 colors, with their RBG and CIE-Lab systems number specifications in the bottom.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The colors used in the objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The two same figure and size objects were colored: one with a salient color (from the first group of colors) and one with a random color (from the second group of colors), taking care that no color was repeated in the same figure object. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0326-z#Tab2">2</a> summarizes this information: in the first column is the name of the object, in the second column its size, and in the third and fourth columns the salient and the random colors used in the two same figure and size objects.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Colors used in the set of objects</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0326-z/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The snapshot of each figure was randomly placed in a slide of a PowerPoint™ presentation with a white background. The presentation was projected on a 21.5″ HP ProDisplay P223a Monitor (X7R62AA). Three different sequences with the slides were made, each presented to three groups of ten persons. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig10">10</a> shows the sequence of three of the slides, from the slide 20–22 of one of the three presentations sequences.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Three slides of one presentation of the objects for the participants</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>Participants were asked to provide basic personal information such as age or gender. Then they wrote their answers in a designed paper format with two parts: a semantic differential test and a selection test.</p><p>The participants were informed that their task was to observe a set of objects. Then they were instructed as follows: <i>“Imagine each object within a room among other objects, and then answer: ‘To what extent do you think that the object would attract your attention?’ by marking with a vertical line within the horizontal line of the object in a scale of 0 to 10</i> (the line was 10 cm and had a 0 on the left and a 10 on the right [see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig11">11</a>, which constitutes a semantic differential test]) . <i>Then please mark with an X the feature or features you think are the most salient of the object</i> (the options were: color ‘C’, size ‘Si’ and shape ‘Sh’), <i>if you think it is none of them, then do not mark any.”</i> Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig11">11</a> shows a part of the areas where participants wrote their answers. Then the presentation was shown to them, and it was explained to use the forward arrow to see the next figures with the next instruction: <i>“Please do not go back on the presentation.”</i>
                        </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>View of the areas where participants wrote their answers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <p>Each person lasted about 15 min to complete the test.</p><h3 class="c-article__sub-heading" id="Sec16">Data</h3><p>Data from the two parts of the participants’ answers, and the three dependent variables were treated as follows:</p><p>A set of continuous data from the semantic differential part of the test in the experiment was gotten. The lines marked by the participants were measured in centimeters and millimeters. With these values, the participants implicitly expressed their opinions about the salience for each observed object. The average of these measures (<i>AV</i>) was then calculated. Also, the frequency of each feature (color, size, and shape) of the object that was marked by the participants was obtained from the second part of the test, and so we measured the other dependent variables.</p><p>Additionally, the 54 objects were processed by the developed application to get a saliency by color (<i>Sc</i>), size (<i>Ssi</i>), and shape (<i>Ssh</i>) to obtain its individual saliency (<i>IS</i>) according to the given weights for each feature.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Results and discussion</h2><div class="c-article-section__content" id="Sec17-content"><p>First, we analyzed the results of the test. In order to understand the measure of saliency given by the participants to each object and its relation with the marked as outstanding objects’ characteristic, frequency of their selection was obtained and separated in three ranges. With a minimum value of 2 and a maximum of 27, the frequency ranges were labeled as:</p><ul class="u-list-style-none">
                  <li>
                    <p>From 2 to 10 = “Low-frequency” range;</p>
                  </li>
                  <li>
                    <p>From 11 to 18 = “Medium-frequency” range; and</p>
                  </li>
                  <li>
                    <p>From 19 to 27 = “High-frequency” range</p>
                  </li>
                </ul>
<p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0326-z#Tab3">3</a> summarizes the numbers by ranges of frequencies. In the first column are the names of object’s features and in the second the number of objects with this feature; columns 3, 4, and 5 present the number of ranges within each frequency range.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Ranges of frequencies by feature</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0326-z/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>Based on the information in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0326-z#Tab3">3</a>, we can state that:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>In general, the shape is the characteristic that was found by the participants fewer times as attractive, followed by size and the color as the most attractive, which is consistent with the literature review. The shape of the objects was the feature with the highest number in the low range (40 of 54 objects); the only object with a high range was the Spider, but this occurred when it had large size and red color. The Ferrari F380 and the Spider had the major medium range regarding the shape, while the rest of objects were mainly in the low-frequency range categorization.</p>
                  </li>
                  <li>
                    <p>The eight proposed colors by Choungourian (Choungourian <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1968" title="Choungourian A (1968) Color preferences and cultural variation. Percept Mot Skills 26(3_suppl):1203–1206" href="/article/10.1007/s10055-017-0326-z#ref-CR2" id="ref-link-section-d25161e3943">1968</a>) were clearly more prominent to the participants compared with the random colors. Most of them were in the medium or high range of frequencies (92,6%). The color that highly stands out was the red one, with no regard to the size or shape of the object. Two of the randomly selected colors had one high range: random 6 and random 7; the shapes of the objects in which the participants found them attractive were: the Spider and the Mobile Phone, which have a high salient shape in (Lara et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Lara G, De Antonio A, Peña A (2016) A computational measure of saliency of the shape of 3D objects. In: Mejia J, Muñoz M, Rocha Á, Calvo-Manzano J (eds) Trends and applications in software engineering. Springer, Cham, pp 235–245" href="/article/10.1007/s10055-017-0326-z#ref-CR15" id="ref-link-section-d25161e3946">2016</a>), with a large and medium size, respectively.</p>
                  </li>
                  <li>
                    <p>Regarding the size of the objects, as expected, the large size had higher frequency ranges followed by the medium size and the small one.</p>
                  </li>
                </ul>
<p>To corroborate the measure given by the participants to the objects and their relation with their marked features as relevant, a multiple linear regression model was calculated, with the average (<i>AV</i>) of the measure given by the participants in the first part of the test as the dependent variable, and the frequency of the marked as attractive features: color, shape, and size, as independent variables. Results are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig12">12</a>, with an adjusted <i>R</i>
                <sup>2</sup> of 0.63, the variation in the measure given by the participants can be assumed as explained by the independent variables. The ANOVA corroborates that the correlation is not due to chance (<i>Sig</i> &lt; <i>0.000 or 0.003</i>). And the coefficient values indicate that the three independent variables are significant for the model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Results of the multiple regression model based in the participants’ answers in the test: <b>a</b> Model Summary, <b>b</b> ANOVA<sup>b</sup>, and <b>c</b> Coefficients<sup>a</sup>
                            </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>However, based on the standardized coefficients, the size has a higher relative value than the color, contrary to what was expected. That is to say, participants marked the color as the most salient feature, but the size explains better the value they give to the object as attractive.</p><p>The three independent variables, i.e., color, size, and shape, explain 65% of the saliency average given by the participants to the objects (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig12">12</a>a “Model Summary”).</p><p>According to the critical level (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig12">12</a>b “ANOVA<sup>b</sup>”), there is a linear relation among the response of the participants with the used object features.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig12">12</a>c “Coefficients<sup>a</sup>” contains the information to construct Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ7">7</a>), which predicts the saliency value of the objects.</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\text{Prognosis}}\;{\text{in}}\;{\text{salience}} = 0.109\;({\text{constant}}) + (0.104*{\text{color}}) + (176*{\text{size}}) + (0.076*{\text{shape}})$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
<h3 class="c-article__sub-heading" id="Sec18">Computational model compared with the participants’ appreciation of saliency</h3><p>A Pearson’s correlation coefficient was calculated to compare the average of the participants’ measure of saliency for each object (<i>AV</i>) with our metric value, giving a correlation index of (<i>r</i>
                    <sub>
                        <i>p</i>
                    </sub> = 0.60), Sig. = 0.000 significant at the 0.01 level 2-tailed; a scatter diagram is present in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig13">13</a> to appreciate their linear relation. This metric value was calculated based on the weights given by the literature review; see Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0326-z#Equ6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0326-z/MediaObjects/10055_2017_326_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Dispersion diagrams showing the correlation between the value of individual saliency and the value of average of votes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0326-z/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Even though the participants marked the color as a more prominent characteristic of the object, the statistical model reflects that the size had a higher weight in the correlation with their evaluation of saliency for the objects. If we adjust the weights of our computational model, that is, to the measure of saliency for the three characteristics, to the relative weight of the independent variables of the regression model (see the standard coefficients in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig12">12</a>), the correlation between the participants measure of saliency and the computational model increases to (<i>r</i>
                    <sub>
                        <i>p</i>
                    </sub> = 0.85).</p><p>This adjustment to better fit the participants’ perception of saliency with our model obeys probably to the conditions of the experiment, in which only one object was shown to the participants in a white background (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0326-z#Fig10">10</a>) with the intention to isolate it from distractions. In any case, as mentioned, the size is a relative visual salient characteristic because it is compared with the other objects in the scenario.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusions and future work</h2><div class="c-article-section__content" id="Sec19-content"><p>We propose a computational model of perceptual saliency for 3D objects in virtual environments based on the color, size, and shape; the visually salient features of the objects, the better perceived by the human visual system. The model follows three steps: (1) characterization of color, size, and shape, that is, to apply different mechanisms for extracting and representing these three basic features of a 3D object; (2) apply a process that allows to establish a normalized saliency for each feature; and (3) calculate individual saliency of each object in the virtual environment by giving a weight to the obtained saliency by feature.</p><p>An experiment was conducted to validate the values of the model regarding the three selected features of the objects. Results demonstrated that the metric of our computational model is efficient and fits, to an extent, with the perception of saliency of the participants. The measures of saliency by feature can be weighted according to certain circumstances.</p><p>During the experiment, only one object on a white background was shown to the participants at a time, where there seems to have a higher weight the size than the color. This condition, that clearly changes when the purpose is to locate an object in a scenario, must affect the given weights to the measures of saliency by features more suitable for previous studies.</p><p>It is necessary to conduct additional experiments trying to avoid the contextual influences on the size, to verify if the weights obtained in this experiment from the point of view of the human perception better represent the relative weight of each one the features considered in this model than those initially proposed.</p><p>Our algorithm is easy to implement and requires minimal space and computational time. Also, our metric of saliency can be applied to 3D objects with regular or irregular geometry, and it is adaptable to the scales of the 3D models.</p><p>As part of our future work, we would like to extend this computational model mainly in three aspects: (a) to include objects’ texture, (b) treating the variability of appearances that might have the shape of an object as: changing perspectives, different views and occlusions, and (c) measure the saliency of objects from their context. This context-dependent evaluation of saliency should deal with the rarity of the object’s features within a context, and the impact of the point of view of the observer, including the identification and management of occlusions.</p><p>This computational model is intended to be useful for the automatic generation of natural language directions to locate objects in virtual environments. By determining the individual saliency of each object, it is useful to identify both the objects to be located and the better candidate of reference objects, very significant during the generation of indications for the localization of objects. We also believe that our model could be useful for additional applications such as 3D object identification. Indeed, by using our metric of perceptual saliency, the regions of potential interest in a scene could be more easily identified.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Caduff, T. Sabine, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Caduff D, Sabine T (2008) On the assessment of landmark salience for human navigation. Springer. Cognit Proces" /><p class="c-article-references__text" id="ref-CR1">Caduff D, Sabine T (2008) On the assessment of landmark salience for human navigation. Springer. Cognit Process 9(4):249–267</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10339-007-0199-2" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20assessment%20of%20landmark%20salience%20for%20human%20navigation.%20Springer&amp;journal=Cognit%20Process&amp;volume=9&amp;issue=4&amp;pages=249-267&amp;publication_year=2008&amp;author=Caduff%2CD&amp;author=Sabine%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Choungourian, " /><meta itemprop="datePublished" content="1968" /><meta itemprop="headline" content="Choungourian A (1968) Color preferences and cultural variation. Percept Mot Skills 26(3_suppl):1203–1206" /><p class="c-article-references__text" id="ref-CR2">Choungourian A (1968) Color preferences and cultural variation. Percept Mot Skills 26(3_suppl):1203–1206</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2466%2Fpms.1968.26.3c.1203" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Color%20preferences%20and%20cultural%20variation&amp;journal=Percept%20Mot%20Skills&amp;volume=26&amp;issue=3_suppl&amp;pages=1203-1206&amp;publication_year=1968&amp;author=Choungourian%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Connolly, T. Fliess, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Connolly C, Fliess T (1997) A study of efficiency and accuracy in the transformation from RGB to CIELAB color " /><p class="c-article-references__text" id="ref-CR3">Connolly C, Fliess T (1997) A study of efficiency and accuracy in the transformation from RGB to CIELAB color space. IEEE Trans Image Process 6(7):1046–1048</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F83.597279" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20study%20of%20efficiency%20and%20accuracy%20in%20the%20transformation%20from%20RGB%20to%20CIELAB%20color%20space&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=6&amp;issue=7&amp;pages=1046-1048&amp;publication_year=1997&amp;author=Connolly%2CC&amp;author=Fliess%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="de I’Eclairage CI (1978) Recommendations on uniform color spaces-color difference equations, psyhometric color" /><p class="c-article-references__text" id="ref-CR4">de I’Eclairage CI (1978) Recommendations on uniform color spaces-color difference equations, psyhometric color terms. Supplement no. 2 to CIE publication no. 15 (E-1.3. 1) 1971. TC, 1–3</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Frintrop, E. Rome, HI. Christensen, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Frintrop S, Rome E, Christensen HI (2010) Computational visual attention systems and their cognitive foundatio" /><p class="c-article-references__text" id="ref-CR5">Frintrop S, Rome E, Christensen HI (2010) Computational visual attention systems and their cognitive foundations: a survey. ACM Trans Appl Percept (TAP) 7(1):6:1–6:39</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20visual%20attention%20systems%20and%20their%20cognitive%20foundations%3A%20a%20survey&amp;journal=ACM%20Trans%20Appl%20Percept%20%28TAP%29&amp;volume=7&amp;issue=1&amp;pages=6%3A1-6%3A39&amp;publication_year=2010&amp;author=Frintrop%2CS&amp;author=Rome%2CE&amp;author=Christensen%2CHI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on s" /><p class="c-article-references__text" id="ref-CR6">Gapp K-P (1995) Object localization: selection of optimal reference objects. In: International conference on spatial information theory. Springer, Berlin, Heidelberg, pp 519–536</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Gevers, AWM. Smeulders, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Gevers T, Smeulders AWM (1999) Color-based object recognition. Elsevier. Pattern Recognit 32(3):453–464" /><p class="c-article-references__text" id="ref-CR7">Gevers T, Smeulders AWM (1999) Color-based object recognition. Elsevier. Pattern Recognit 32(3):453–464</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0031-3203%2898%2900036-3" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Color-based%20object%20recognition.%20Elsevier&amp;journal=Pattern%20Recognit&amp;volume=32&amp;issue=3&amp;pages=453-464&amp;publication_year=1999&amp;author=Gevers%2CT&amp;author=Smeulders%2CAWM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Guo C, Ma Q, Zhang L (2008) Spatio-temporal saliency detection using phase spectrum of quaternion Fourier tran" /><p class="c-article-references__text" id="ref-CR8">Guo C, Ma Q, Zhang L (2008) Spatio-temporal saliency detection using phase spectrum of quaternion Fourier transform. In: Conference on vision and pattern recognition. IEEE, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DD. Hoffman, M. Singh, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Hoffman DD, Singh M (1997) Salience of visual parts. Elsevier. Cognition 68(1):29–78" /><p class="c-article-references__text" id="ref-CR9">Hoffman DD, Singh M (1997) Salience of visual parts. Elsevier. Cognition 68(1):29–78</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0010-0277%2896%2900791-3" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Salience%20of%20visual%20parts.%20Elsevier&amp;journal=Cognition&amp;volume=68&amp;issue=1&amp;pages=29-78&amp;publication_year=1997&amp;author=Hoffman%2CDD&amp;author=Singh%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Computer vision and pattern recogn" /><p class="c-article-references__text" id="ref-CR10">Hou X, Zhang L (2007) Saliency detection: a spectral residual approach. In: Computer vision and pattern recognition. IEEE conference on. IEEE Minneapolis, MN, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Itti, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Itti L (2006) Quantitative modelling of perceptual salience at human eye position. Taylor &amp; Francis Group. Psy" /><p class="c-article-references__text" id="ref-CR11">Itti L (2006) Quantitative modelling of perceptual salience at human eye position. Taylor &amp; Francis Group. Psychology Press. Visual Cognit 14(4–8):959–984</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F13506280500195672" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantitative%20modelling%20of%20perceptual%20salience%20at%20human%20eye%20position.%20Taylor%20%26%20Francis%20Group.%20Psychology%20Press&amp;journal=Visual%20Cognit&amp;volume=14&amp;issue=4%E2%80%938&amp;pages=959-984&amp;publication_year=2006&amp;author=Itti%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Itti, C. Koch, E. Niebur, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scence analysis. IEEE Tra" /><p class="c-article-references__text" id="ref-CR12">Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scence analysis. IEEE Trans Pattern Anal Mach Intell 20(11):1254–1259</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.730558" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20model%20of%20saliency-based%20visual%20attention%20for%20rapid%20scence%20analysis&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=20&amp;issue=11&amp;pages=1254-1259&amp;publication_year=1998&amp;author=Itti%2CL&amp;author=Koch%2CC&amp;author=Niebur%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ju R, Ge L, Geng W, Ren T, Wu G (2014) Depth saliency based on anisotropic center-surround difference. In: Ima" /><p class="c-article-references__text" id="ref-CR13">Ju R, Ge L, Geng W, Ren T, Wu G (2014) Depth saliency based on anisotropic center-surround difference. In: Image processing (ICIP), 2014 IEEE international conference on. IEEE, pp 1115–1119</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Klippel, S. Winter, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Klippel A, Winter S (2005) Structural salience of landmarks for route directions. In: Cohn G, Mark M (eds) Spa" /><p class="c-article-references__text" id="ref-CR14">Klippel A, Winter S (2005) Structural salience of landmarks for route directions. In: Cohn G, Mark M (eds) Spatial information theory. Springer, Ellicottville, pp 347–362</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20information%20theory&amp;pages=347-362&amp;publication_year=2005&amp;author=Klippel%2CA&amp;author=Winter%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Lara, A. Antonio, A. Peña, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Lara G, De Antonio A, Peña A (2016) A computational measure of saliency of the shape of 3D objects. In: Mejia " /><p class="c-article-references__text" id="ref-CR15">Lara G, De Antonio A, Peña A (2016) A computational measure of saliency of the shape of 3D objects. In: Mejia J, Muñoz M, Rocha Á, Calvo-Manzano J (eds) Trends and applications in software engineering. Springer, Cham, pp 235–245</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Trends%20and%20applications%20in%20software%20engineering&amp;pages=235-245&amp;publication_year=2016&amp;author=Lara%2CG&amp;author=Antonio%2CA&amp;author=Pe%C3%B1a%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey " /><p class="c-article-references__text" id="ref-CR16">Li J, Levine M, An X, He H (2011) Saliency detection based on frequency and spatial domain analysis. In: Hoey J, McKenna S, Trucco E (eds) Proceedings of the British machine vision conference. BMVC Press</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Li, MD. Levine, X. An, X. Xu, H. He, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Li J, Levine MD, An X, Xu X, He H (2013) Visual saliency based on scale-space analysis in the frequency domain" /><p class="c-article-references__text" id="ref-CR17">Li J, Levine MD, An X, Xu X, He H (2013) Visual saliency based on scale-space analysis in the frequency domain. IEEE Trans Pattern Anal Mach Intell 35(4):996–1010</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2012.147" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20saliency%20based%20on%20scale-space%20analysis%20in%20the%20frequency%20domain&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=35&amp;issue=4&amp;pages=996-1010&amp;publication_year=2013&amp;author=Li%2CJ&amp;author=Levine%2CMD&amp;author=An%2CX&amp;author=Xu%2CX&amp;author=He%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="López F, Valiente JM, Baldrich R, Vanrell M (2005) Fast surface grading using color statistics in the CIE Lab " /><p class="c-article-references__text" id="ref-CR18">López F, Valiente JM, Baldrich R, Vanrell M (2005) Fast surface grading using color statistics in the CIE Lab space. In: Iberian conference on pattern recognition and image analysis (LNCS 3523), Springer, Berlin, Heidelberg, vol 2, pp 666–673</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Mangold, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Mangold R (1986) Sensorische Faktoren Beim Verstehen Überspezifizierter Objektbenennungen. Peter Lang, Frankfu" /><p class="c-article-references__text" id="ref-CR19">Mangold R (1986) Sensorische Faktoren Beim Verstehen Überspezifizierter Objektbenennungen. Peter Lang, Frankfurt</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensorische%20Faktoren%20Beim%20Verstehen%20%C3%9Cberspezifizierter%20Objektbenennungen&amp;publication_year=1986&amp;author=Mangold%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oliva A, Torralba A, Castelhano MS, Henderson JM (2003) Top-dow control of visual attention in object detectio" /><p class="c-article-references__text" id="ref-CR20">Oliva A, Torralba A, Castelhano MS, Henderson JM (2003) Top-dow control of visual attention in object detection. In: Image proceedings, 2003. icip 2003. Proceedings. 2003 international conference on, vol 1, pp 1–253</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raubal M, Winter S (2002) Enriching wayfinding instructions with local landmarks. In: Egenhofer MJ, Mark DM (e" /><p class="c-article-references__text" id="ref-CR21">Raubal M, Winter S (2002) Enriching wayfinding instructions with local landmarks. In: Egenhofer MJ, Mark DM (eds) International conference on geographic information science, vol 2478. Springer, Berlin, Heidelberg, Boulder, CO, USA, pp 243–259</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Röser, K. Hamburger, M. Knauff, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Röser F, Hamburger K, Knauff M (2011) The Giessen virtual environment laboratory: human wayfinding and landmar" /><p class="c-article-references__text" id="ref-CR22">Röser F, Hamburger K, Knauff M (2011) The Giessen virtual environment laboratory: human wayfinding and landmark salience Springer-Verlag. Cognit Process 12(2):209–214</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10339-011-0390-3" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Giessen%20virtual%20environment%20laboratory%3A%20human%20wayfinding%20and%20landmark%20salience%20Springer-Verlag&amp;journal=Cognit%20Process&amp;volume=12&amp;issue=2&amp;pages=209-214&amp;publication_year=2011&amp;author=R%C3%B6ser%2CF&amp;author=Hamburger%2CK&amp;author=Knauff%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Röser, K. Hamburger, A. Krumnack, M. Knauff, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Röser F, Hamburger K, Krumnack A, Knauff M (2012) The structural salience of landmarks: results from an on-lin" /><p class="c-article-references__text" id="ref-CR23">Röser F, Hamburger K, Krumnack A, Knauff M (2012) The structural salience of landmarks: results from an on-line study and a virtual environment experiment. Taylor &amp; Francis Group. J Spat Sci 57(1):37–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F14498596.2012.686362" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20structural%20salience%20of%20landmarks%3A%20results%20from%20an%20on-line%20study%20and%20a%20virtual%20environment%20experiment.%20Taylor%20%26%20Francis%20Group&amp;journal=J%20Spat%20Sci&amp;volume=57&amp;issue=1&amp;pages=37-50&amp;publication_year=2012&amp;author=R%C3%B6ser%2CF&amp;author=Hamburger%2CK&amp;author=Krumnack%2CA&amp;author=Knauff%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: CogSci, pp 33" /><p class="c-article-references__text" id="ref-CR24">Röser F, Krumnack A, Hamburger K (2013) The influence of perceptual and structural salience. In: CogSci, pp 3315–3320</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RV. Rossel, B. Minasny, P. Roudier, AB. McBratney, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Rossel RV, Minasny B, Roudier P, McBratney AB (2006) Colour space models for soil science. Elsevier. Geoderma " /><p class="c-article-references__text" id="ref-CR25">Rossel RV, Minasny B, Roudier P, McBratney AB (2006) Colour space models for soil science. Elsevier. Geoderma 133(3):320–337</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.geoderma.2005.07.017" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Colour%20space%20models%20for%20soil%20science.%20Elsevier&amp;journal=Geoderma&amp;volume=133&amp;issue=3&amp;pages=320-337&amp;publication_year=2006&amp;author=Rossel%2CRV&amp;author=Minasny%2CB&amp;author=Roudier%2CP&amp;author=McBratney%2CAB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saleh A, Khalil M, Wahdan A-M (2003) An adaptive image watermarking scheme for color images using S-CIELAB. In" /><p class="c-article-references__text" id="ref-CR26">Saleh A, Khalil M, Wahdan A-M (2003) An adaptive image watermarking scheme for color images using S-CIELAB. In: Circuits and systems, 2003 IEEE 46th Midwest symposium, vol 3. IEEE, pp 1575–1578</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sampedro MJ, Blanco M, Ponte D, Leirós LI (2010) Saliencia Perceptiva y Atención. La Atención (VI). Un enfoque" /><p class="c-article-references__text" id="ref-CR27">Sampedro MJ, Blanco M, Ponte D, Leirós LI (2010) Saliencia Perceptiva y Atención. La Atención (VI). Un enfoque pluridisciplinar, pp 91–103</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shilane P, Min P, Kazhdan M, Furkhouser T (2004) The Princeton shape benchmark. In: Shape modeling application" /><p class="c-article-references__text" id="ref-CR28">Shilane P, Min P, Kazhdan M, Furkhouser T (2004) The Princeton shape benchmark. In: Shape modeling applications, 2004. Proceeding. IEEE Washington, DC, USA, pp 167–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Singh, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Singh S (2006) Impact of color on marketing. Manag Decis 44(6):783–789" /><p class="c-article-references__text" id="ref-CR29">Singh S (2006) Impact of color on marketing. Manag Decis 44(6):783–789</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1108%2F00251740610673332" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Impact%20of%20color%20on%20marketing&amp;journal=Manag%20Decis&amp;volume=44&amp;issue=6&amp;pages=783-789&amp;publication_year=2006&amp;author=Singh%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Song, Y. Liu, RR. Martin, PL. Rosin, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Song R, Liu Y, Martin RR, Rosin PL (2014) Mesh saliency via spectral processing. ACM Trans Graph (TOG) 33(1):6" /><p class="c-article-references__text" id="ref-CR30">Song R, Liu Y, Martin RR, Rosin PL (2014) Mesh saliency via spectral processing. ACM Trans Graph (TOG) 33(1):6</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2530691" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1288.68239" aria-label="View reference 30 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mesh%20saliency%20via%20spectral%20processing&amp;journal=ACM%20Trans%20Graph%20%28TOG%29&amp;volume=33&amp;issue=1&amp;publication_year=2014&amp;author=Song%2CR&amp;author=Liu%2CY&amp;author=Martin%2CRR&amp;author=Rosin%2CPL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoia L (2007) Noun phrase generation for situated dialogs. Ohio State University. Ph.D. thesis" /><p class="c-article-references__text" id="ref-CR31">Stoia L (2007) Noun phrase generation for situated dialogs. Ohio State University. Ph.D. thesis</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Su H, Maji S, Kalogerakis E, Learned-Miller E (2015) Multi-view convulational neural networks for 3D shape rec" /><p class="c-article-references__text" id="ref-CR32">Su H, Maji S, Kalogerakis E, Learned-Miller E (2015) Multi-view convulational neural networks for 3D shape recognition. In: Proceedings of the IEEE international conference on computer vision, pp 945–953</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Undurraga C, Mery D (2011) Improving tracking algorithms using saliency. In: César SM, Kim S-W (eds) Proceedin" /><p class="c-article-references__text" id="ref-CR33">Undurraga C, Mery D (2011) Improving tracking algorithms using saliency. In: César SM, Kim S-W (eds) Proceedings 16th Iberoamerican congress. Pattern recognition, vol 7042. Springer-Verlag, Berlin, Heidelberg, Chile, pp 141–148</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Undurraga C, Mery D, Sucar SLE (2010) Modelo de Saliencia utilizando el descriptor de covarianza" /><p class="c-article-references__text" id="ref-CR34">Undurraga C, Mery D, Sucar SLE (2010) Modelo de Saliencia utilizando el descriptor de covarianza</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Wu, X. Shen, W. Zhu, L. Liu, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Wu J, Shen X, Zhu W, Liu L (2013) Mesh saliency with global rarity. ELSEVIER. Graph Models 75(5):255–264" /><p class="c-article-references__text" id="ref-CR35">Wu J, Shen X, Zhu W, Liu L (2013) Mesh saliency with global rarity. ELSEVIER. Graph Models 75(5):255–264</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.gmod.2013.05.002" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mesh%20saliency%20with%20global%20rarity.%20ELSEVIER&amp;journal=Graph%20Models&amp;volume=75&amp;issue=5&amp;pages=255-264&amp;publication_year=2013&amp;author=Wu%2CJ&amp;author=Shen%2CX&amp;author=Zhu%2CW&amp;author=Liu%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Wyszecki, WS. Stiles, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Wyszecki G, Stiles WS (1982) Color science: concepts and methods, quantitative data and formulae, vol 8. Wiley" /><p class="c-article-references__text" id="ref-CR36">Wyszecki G, Stiles WS (1982) Color science: concepts and methods, quantitative data and formulae, vol 8. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Color%20science%3A%20concepts%20and%20methods%2C%20quantitative%20data%20and%20formulae&amp;publication_year=1982&amp;author=Wyszecki%2CG&amp;author=Stiles%2CWS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC-C. Yuan, JD. Brewer, EA. Monaco, EL. Davis, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Yuan JC-C, Brewer JD, Monaco EA, Davis EL (2007) Defining a natural tooth color space based on a 3-dimensional" /><p class="c-article-references__text" id="ref-CR37">Yuan JC-C, Brewer JD, Monaco EA, Davis EL (2007) Defining a natural tooth color space based on a 3-dimensional shade system. J Prosthet Dent 98(2):110–119</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0022-3913%2807%2960044-4" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Defining%20a%20natural%20tooth%20color%20space%20based%20on%20a%203-dimensional%20shade%20system&amp;journal=J%20Prosthet%20Dent&amp;volume=98&amp;issue=2&amp;pages=110-119&amp;publication_year=2007&amp;author=Yuan%2CJC-C&amp;author=Brewer%2CJD&amp;author=Monaco%2CEA&amp;author=Davis%2CEL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhao Y, Liu Y, Zeng Z (2013) Using region-based saliency for 3D interest points detection. In: International c" /><p class="c-article-references__text" id="ref-CR38">Zhao Y, Liu Y, Zeng Z (2013) Using region-based saliency for 3D interest points detection. In: International conference on computer analysis of images and patterns. Springer, Berlin, Heidelberg, pp 108–116</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0326-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>Graciela Lara holds a PROMEP scholarship in partnership with the UDG (UDG-685), Mexico. We also thank the students Adrián Calle Murillo, Roberto Mendoza Vasquez, and Álvaro Iturmendi Muñoz for their help in the implementation of the metric and the experimental software application and materials.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CUCEI of the Universidad de Guadalajara, Av. Revolución 1500, Col. Olímpica, 44430, Guadalajara (Jalisco), Mexico</p><p class="c-article-author-affiliation__authors-list">Graciela Lara &amp; Adriana Peña</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Escuela Técnica Superior de Ingenieros Informáticos of the Universidad Politécnica de Madrid, Campus de Montegancedo, 28660, Boadilla Del Monte, Spain</p><p class="c-article-author-affiliation__authors-list">Angélica De Antonio</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Graciela-Lara"><span class="c-article-authors-search__title u-h3 js-search-name">Graciela Lara</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Graciela+Lara&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Graciela+Lara" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Graciela+Lara%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ang_lica-De_Antonio"><span class="c-article-authors-search__title u-h3 js-search-name">Angélica De Antonio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ang%C3%A9lica+De Antonio&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ang%C3%A9lica+De Antonio" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ang%C3%A9lica+De Antonio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Adriana-Pe_a"><span class="c-article-authors-search__title u-h3 js-search-name">Adriana Peña</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Adriana+Pe%C3%B1a&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Adriana+Pe%C3%B1a" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Adriana+Pe%C3%B1a%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0326-z/email/correspondent/c1/new">Adriana Peña</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20computational%20model%20of%20perceptual%20saliency%20for%203D%20objects%20in%20virtual%20environments&amp;author=Graciela%20Lara%20et%20al&amp;contentID=10.1007%2Fs10055-017-0326-z&amp;publication=1359-4338&amp;publicationDate=2017-10-06&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0326-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0326-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lara, G., De Antonio, A. &amp; Peña, A. A computational model of perceptual saliency for 3D objects in virtual environments.
                    <i>Virtual Reality</i> <b>22, </b>221–234 (2018). https://doi.org/10.1007/s10055-017-0326-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0326-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-09-30">30 September 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-09-20">20 September 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-10-06">06 October 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-09">September 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0326-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0326-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Reference object</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Perceptual salience</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D object’s features extraction</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0326-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=326;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

