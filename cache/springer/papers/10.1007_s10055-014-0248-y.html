<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="New wireless connection between user and VE using speech processing"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as a speech source and streams the results of..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="New wireless connection between user and VE using speech processing"/>

    <meta name="dc.source" content="Virtual Reality 2014 18:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-07-20"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as a speech source and streams the results of the process through a Wi-Fi network. The proposed VRPN server provides a handy, portable and wireless human machine interface that can facilitate interaction in a variety interfaces and application domains including HMD- and CAVE-based virtual reality systems, flight and driving simulators and many others. The VRPN server is based on a speech processing software development kits and VRPN library in C++. Speak-to-VR VRPN works well even in the presence of background noise or the voices of other users in the vicinity. The speech processing algorithm is not sensitive to the user&#8217;s accent because it is trained while it is operating. Speech recognition parameters are trained by hidden Markov model in real time. The advantages and disadvantages of the speak-to-VR server are studied under different configurations. Then, the efficiency and the precision of the speak-to-VR server for a real application are validated via a formal user study with ten participants. Two experimental test setups are implemented on a CAVE system by using either Kinect Xbox or array microphone as input device. Each participant is asked to navigate in a virtual environment and manipulate an object. The experimental data analysis shows promising results and motivates additional research opportunities."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-07-20"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="235"/>

    <meta name="prism.endingPage" content="243"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0248-y"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0248-y"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0248-y.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0248-y"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="New wireless connection between user and VE using speech processing"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2014/11"/>

    <meta name="citation_online_date" content="2014/07/20"/>

    <meta name="citation_firstpage" content="235"/>

    <meta name="citation_lastpage" content="243"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0248-y"/>

    <meta name="DOI" content="10.1007/s10055-014-0248-y"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0248-y"/>

    <meta name="description" content="This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as "/>

    <meta name="dc.creator" content="M. Ali Mirzaei"/>

    <meta name="dc.creator" content="Frederic Merienne"/>

    <meta name="dc.creator" content="James H. Oliver"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Pick your top geek gift-cosmic log; citation_author=A Boyle; citation_volume=3147; citation_publication_date=2008; citation_pages=10; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_title=The cognitive effects of delayed visual feedback: working memory disruption while driving in virtual environments; citation_inbook_title=Cognitive technology: instruments of mind; citation_publication_date=2001; citation_pages=75-82; citation_id=CR2; citation_author=PN Day; citation_author=PO Holt; citation_author=GT Russell; citation_publisher=Springer"/>

    <meta name="citation_reference" content="DiVerdi S, Rakkolainen I, H&#246;llerer T, Olwal A (2006) A novel walk-through 3d display. In: Electronic imaging 2006, p 605519. International Society for Optics and Photonics"/>

    <meta name="citation_reference" content="Fischbach M, Wiebusch D, Giebler-Schubert A, Latoschik ME, Rehfeld S, Tramberend H (2011) Sixton&#8217;s curse-simulator x demonstration. In: 2011 IEEE virtual reality conference (VR), pp 255&#8211;256. IEEE"/>

    <meta name="citation_reference" content="Intel. Voice recognition and synthesis using the intel perceptual computing sdk, 2013"/>

    <meta name="citation_reference" content="iSpeech. Speech processing sdk for mobile developer, 8 2011"/>

    <meta name="citation_reference" content="citation_journal_title=J Guangxi Acad Sci; citation_title=Implement of speech application program based on speech sdk [j]; citation_author=G Jinghui, J Zijing, H Jinming; citation_volume=3; citation_publication_date=2005; citation_pages=169-172; citation_id=CR7"/>

    <meta name="citation_reference" content="Joystiq. Kinect: the company behind the tech explains how it works, March 21 2011"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Aviat Psychol; citation_title=Simulator sickness questionnaire: an enhanced method for quantifying simulator sickness; citation_author=RS Kennedy, NE Lane, KS Berbaum, MG Lilienthal; citation_volume=3; citation_issue=3; citation_publication_date=1993; citation_pages=203-220; citation_doi=10.1207/s15327108ijap0303_3; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Narotama; citation_title=Hidden Markov models; citation_author=R Lyngs&#248;; citation_volume=1; citation_publication_date=2012; citation_pages=1-24; citation_id=CR10"/>

    <meta name="citation_reference" content="Nilsson M, Ejnarsson M (2002) Speech recognition using hidden Markov model. Master&#8217;s thesis, Department of Telecommunications and Speech Processing, Blekinge Institute of Technology"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Process; citation_title=Bandwidth extension of telephone speech using a neural network and a filter bank implementation for highband mel spectrum; citation_author=H Pulakka, P Alku; citation_volume=19; citation_issue=7; citation_publication_date=2011; citation_pages=2170-2183; citation_doi=10.1109/TASL.2011.2118206; citation_id=CR12"/>

    <meta name="citation_reference" content="R. M. T. II. (2008) Vrpn 07.30&#8212;
                    http://www.cs.unc.edu/research/vrpn/
                    
                  
                "/>

    <meta name="citation_reference" content="Retrieved P (2010) Kinect xbox 360 specification, July 2 2010. This information is based on specifications supplied by manufacturers and should be used for guidance only"/>

    <meta name="citation_reference" content="Rodr&#237;guez-Andina J, Fagundes RDR, Junior DB (2001) A fpga-based viterbi algorithm implementation for speech recognition systems. In: 2001 IEEE international conference on acoustics, speech, and signal processing, 2001 (Proceedings ICASSP&#8217;01), vol 2, pp 1217&#8211;1220, IEEE"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Robust adaptive beamforming using multidimensional covariance fitting; citation_author=M Rubsamen, AB Gershman; citation_volume=60; citation_issue=2; citation_publication_date=2012; citation_pages=740-753; citation_doi=10.1109/TSP.2011.2174233; citation_id=CR16"/>

    <meta name="citation_reference" content="SAR (2005) Sri language modeling toolkit and speech sdk"/>

    <meta name="citation_reference" content="citation_journal_title=Adv Sci Lett; citation_title=A new partially adaptive minimum variance distortionless response beamformer with constrained stability least mean squares algorithm; citation_author=W Shao, Z Qian; citation_volume=19; citation_issue=4; citation_publication_date=2013; citation_pages=1071-1074; citation_doi=10.1166/asl.2013.4430; citation_id=CR18"/>

    <meta name="citation_reference" content="Stone JE, Kohlmeyer A, Vandivort KL, Schulten K (2010) Immersive molecular visualization and interactive modeling with commodity hardware. In: Advances in visual computing. Springer, Berlin, pp 382&#8211;393"/>

    <meta name="citation_reference" content="Store M (2010) Kinect for xbox 360, 7 July 2010. Array of 4 microphones supporting single speaker voice recognition"/>

    <meta name="citation_reference" content="Suma EA, Lange B, Rizzo A, Krum DM, Bolas M (2011) Faast: The flexible action and articulated skeleton toolkit. In: 2011 IEEE virtual reality conference (VR), pp 247&#8211;248, IEEE"/>

    <meta name="citation_reference" content="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 55&#8211;61, ACM"/>

    <meta name="citation_reference" content="Zhu F-W, Li D-Q, Yuan Z-P, Wu J-Q, Cheng X (2004) An ar tracker based on planar marker. J Shanghai Univ (Nat Sci Ed) 5:005"/>

    <meta name="citation_author" content="M. Ali Mirzaei"/>

    <meta name="citation_author_email" content="mirzai142.nri@gmail.com"/>

    <meta name="citation_author_institution" content="Lab. Le2i, Institute Image, Paris-Tech, Paris, France"/>

    <meta name="citation_author" content="Frederic Merienne"/>

    <meta name="citation_author_institution" content="Lab. Le2i, Institute Image, Paris-Tech, Paris, France"/>

    <meta name="citation_author" content="James H. Oliver"/>

    <meta name="citation_author_email" content="oliver@iastate.edu"/>

    <meta name="citation_author_institution" content="Virtual Reality Application Center, Iowa State University, Ames, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0248-y&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0248-y"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="New wireless connection between user and VE using speech processing"/>
        <meta property="og:description" content="This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as a speech source and streams the results of the process through a Wi-Fi network. The proposed VRPN server provides a handy, portable and wireless human machine interface that can facilitate interaction in a variety interfaces and application domains including HMD- and CAVE-based virtual reality systems, flight and driving simulators and many others. The VRPN server is based on a speech processing software development kits and VRPN library in C++. Speak-to-VR VRPN works well even in the presence of background noise or the voices of other users in the vicinity. The speech processing algorithm is not sensitive to the user’s accent because it is trained while it is operating. Speech recognition parameters are trained by hidden Markov model in real time. The advantages and disadvantages of the speak-to-VR server are studied under different configurations. Then, the efficiency and the precision of the speak-to-VR server for a real application are validated via a formal user study with ten participants. Two experimental test setups are implemented on a CAVE system by using either Kinect Xbox or array microphone as input device. Each participant is asked to navigate in a virtual environment and manipulate an object. The experimental data analysis shows promising results and motivates additional research opportunities."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>New wireless connection between user and VE using speech processing | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0248-y","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Speak-to-VR, Wi-Fi network, Speech processing, VRPN server","kwrd":["Speak-to-VR","Wi-Fi_network","Speech_processing","VRPN_server"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0248-y","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0248-y","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=248;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0248-y">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            New wireless connection between user and VE using speech processing
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0248-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0248-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-07-20" itemprop="datePublished">20 July 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">New wireless connection between user and VE using speech processing</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-M__Ali-Mirzaei" data-author-popup="auth-M__Ali-Mirzaei" data-corresp-id="c1">M. Ali Mirzaei<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Institute Image, Paris-Tech" /><meta itemprop="address" content="grid.434207.6, 0000 0001 2194 6047, Lab. Le2i, Institute Image, Paris-Tech, Paris, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Frederic-Merienne" data-author-popup="auth-Frederic-Merienne">Frederic Merienne</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Institute Image, Paris-Tech" /><meta itemprop="address" content="grid.434207.6, 0000 0001 2194 6047, Lab. Le2i, Institute Image, Paris-Tech, Paris, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-James_H_-Oliver" data-author-popup="auth-James_H_-Oliver">James H. Oliver</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Iowa State University" /><meta itemprop="address" content="grid.34421.30, 0000000419367312, Virtual Reality Application Center, Iowa State University, Ames, IA, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">235</span>–<span itemprop="pageEnd">243</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">311 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0248-y/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        <ul class="c-article-events"><li class="c-article-events__item"><span>An erratum to this article can be found at <a href="http://dx.doi.org/10.1007/s10055-015-0262-8">http://dx.doi.org/10.1007/s10055-015-0262-8</a>.</span></li></ul>
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as a speech source and streams the results of the process through a Wi-Fi network. The proposed VRPN server provides a handy, portable and wireless human machine interface that can facilitate interaction in a variety interfaces and application domains including HMD- and CAVE-based virtual reality systems, flight and driving simulators and many others. The VRPN server is based on a speech processing software development kits and VRPN library in C++. Speak-to-VR VRPN works well even in the presence of background noise or the voices of other users in the vicinity. The speech processing algorithm is not sensitive to the user’s accent because it is trained while it is operating. Speech recognition parameters are trained by hidden Markov model in real time. The advantages and disadvantages of the speak-to-VR server are studied under different configurations. Then, the efficiency and the precision of the speak-to-VR server for a real application are validated via a formal user study with ten participants. Two experimental test setups are implemented on a CAVE system by using either Kinect Xbox or array microphone as input device. Each participant is asked to navigate in a virtual environment and manipulate an object. The experimental data analysis shows promising results and motivates additional research opportunities.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Since the virtual-reality peripheral network (VRPN) system was released in 2001 (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 55–61, ACM" href="/article/10.1007/s10055-014-0248-y#ref-CR22" id="ref-link-section-d40094e355">2001</a>), many of VRPN servers for different devices have been developed and attached to the library (VRPN 07.30 R. M. T. II <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="R. M. T. II. (2008) Vrpn 07.30—&#xA;                    http://www.cs.unc.edu/research/vrpn/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0248-y#ref-CR13" id="ref-link-section-d40094e358">2008</a>). This includes Dream Cheeky USB drum (Boyle <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Boyle A et al (2008) Pick your top geek gift-cosmic log. Science 3147:10" href="/article/10.1007/s10055-014-0248-y#ref-CR1" id="ref-link-section-d40094e361">2008</a>), Hillcrest Labs’ Free-space (DiVerdi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="DiVerdi S, Rakkolainen I, Höllerer T, Olwal A (2006) A novel walk-through 3d display. In: Electronic imaging 2006, p 605519. International Society for Optics and Photonics" href="/article/10.1007/s10055-014-0248-y#ref-CR3" id="ref-link-section-d40094e364">2006</a>), the Nintendo Wii Remote (Fischbach et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Fischbach M, Wiebusch D, Giebler-Schubert A, Latoschik ME, Rehfeld S, Tramberend H (2011) Sixton’s curse-simulator x demonstration. In: 2011 IEEE virtual reality conference (VR), pp 255–256. IEEE" href="/article/10.1007/s10055-014-0248-y#ref-CR4" id="ref-link-section-d40094e367">2011</a>), 3DConnexion Space-Navigator, Logitech Magellan, Spaceball 6DOF motion controllers (Stone et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Stone JE, Kohlmeyer A, Vandivort KL, Schulten K (2010) Immersive molecular visualization and interactive modeling with commodity hardware. In: Advances in visual computing. Springer, Berlin, pp 382–393" href="/article/10.1007/s10055-014-0248-y#ref-CR19" id="ref-link-section-d40094e371">2010</a>), Neat Gadget (TNGs) from MindTel (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 55–61, ACM" href="/article/10.1007/s10055-014-0248-y#ref-CR22" id="ref-link-section-d40094e374">2001</a>), fly-stick, mouse (ZHU et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhu F-W, Li D-Q, Yuan Z-P, Wu J-Q, Cheng X (2004) An ar tracker based on planar marker. J Shanghai Univ (Nat Sci Ed) 5:005" href="/article/10.1007/s10055-014-0248-y#ref-CR23" id="ref-link-section-d40094e377">2004</a>), keyboard, 3D mouse, the Xbox 360 game controller (Suma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Suma EA, Lange B, Rizzo A, Krum DM, Bolas M (2011) Faast: The flexible action and articulated skeleton toolkit. In: 2011 IEEE virtual reality conference (VR), pp 247–248, IEEE" href="/article/10.1007/s10055-014-0248-y#ref-CR21" id="ref-link-section-d40094e380">2011</a>) and so on (for more information see R. M. T. II <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="R. M. T. II. (2008) Vrpn 07.30—&#xA;                    http://www.cs.unc.edu/research/vrpn/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0248-y#ref-CR13" id="ref-link-section-d40094e383">2008</a>).</p><p>The VRPN system provides a device independent and network-transparent interface to virtual reality peripherals and simulation platforms. VRPN now supports thousands of applications in VR research (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 55–61, ACM" href="/article/10.1007/s10055-014-0248-y#ref-CR22" id="ref-link-section-d40094e389">2001</a>) and industrial products such as large-scale simulation, modeling and visualization.</p><p>Speech processing technology has progressed recently both in terms of processing speed and accuracy. Different software development kits (SDK) have been proposed by specialists in this arena. A company called iSpeech (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="iSpeech. Speech processing sdk for mobile developer, 8 2011" href="/article/10.1007/s10055-014-0248-y#ref-CR6" id="ref-link-section-d40094e395">2011</a>) has released a free speech processing SDK for mobile developers building apps for iOS, Android and BlackBerry. SAR (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="SAR (2005) Sri language modeling toolkit and speech sdk" href="/article/10.1007/s10055-014-0248-y#ref-CR17" id="ref-link-section-d40094e398">2005</a>) proposed an SDK to enable the engineers to embed speech processing into their products and applications. Intel (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Intel. Voice recognition and synthesis using the intel perceptual computing sdk, 2013" href="/article/10.1007/s10055-014-0248-y#ref-CR5" id="ref-link-section-d40094e401">2013</a>) has released a SDK for voice processing and synthesis. Microsoft Speech SDK 5.1 (Jinghui et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Jinghui G, Zijing J, Jinming H (2005) Implement of speech application program based on speech sdk [j]. J Guangxi Acad Sci 3:169–172" href="/article/10.1007/s10055-014-0248-y#ref-CR7" id="ref-link-section-d40094e404">2005</a>) promoted the features of the previous version of the Speech SDK. Now, it is possible to use the Win32, Win64 Speech API (SAPI) to develop speech applications with Visual C++, basic, ECMA-Script and other Automation languages. The SDK also includes freely distributable text-to-speech (TTS) and speech recognition (SR) engines.</p><p>Speech-to-VR VRPN server has two distinctive parts, speech processing and streaming through the network. Speech-to-VR incorporates a Wi-Fi network for data streaming since wireless connections are more comfortable for end users. In addition, for CAVE-based applications, the walls of the display make wired connections cumbersome. Thus, speak-to-VR is implemented with Wi-Fi connection to a local area network (LAN).</p><p>The aim of the paper was to describe a general purpose and robust speech interface using Microsoft Speech SDK. In particular, speak-to-VR enhances the performance of the Microsoft Speech SDK by implementing a minimum variance distortionless response (MVDR) algorithm for a microphone array. The second objective is to develop a navigation/manipulation application in a VE to test user performance during usage of the speak-to-VR interface. The paper organized as follows: in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0248-y#Sec2">2</a>, the principle of MVDR algorithm is briefly explained and microphone array architecture for speech recognition is introduced. This section also describes how the result of speech processing is converted to VRPN data type and streamed through a wireless network. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0248-y#Sec3">3</a> describes the performance of the proposed speech-based HMI will be tested by navigation/ manipulation tasks inside a CAVE-based virtual environment. The result of a subjective user study and some advantages of this speak-to-VR VRPN serve are discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0248-y#Sec4">4</a>. Some useful information about the performance of the speak-to-VR VRPN server is also explained.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Theory behind speech recognition based on microphone array</h2><div class="c-article-section__content" id="Sec2-content"><p>A primary requirement for speak-to-VR is a broad application domain characterized by a less than ideal sound environment that may include noise, echoes and reverberation. Thus, a robust real-time speech processing approach is required that also minimizes delay, in order to facilitate interactive performance. Thus, speak-to-VR is implemented with a microphone array that provides spatially discriminated input to the MVDR algorithm. The channel impulse responses <span class="mathjax-tex">\(h_i (r,t)\)</span> describe sound propagation from the source to the individual microphones. The discrete-time beam-former is modeled by fast Fourier transform (FFT) overlap-add filter bank (Pulakka and Alku <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Pulakka H, Alku P (2011) Bandwidth extension of telephone speech using a neural network and a filter bank implementation for highband mel spectrum. IEEE Trans Audio Speech Lang Process 19(7):2170–2183" href="/article/10.1007/s10055-014-0248-y#ref-CR12" id="ref-link-section-d40094e472">2011</a>). The MVDR (Shao and Qian <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shao W, Qian Z (2013) A new partially adaptive minimum variance distortionless response beamformer with constrained stability least mean squares algorithm. Adv Sci Lett 19(4):1071–1074" href="/article/10.1007/s10055-014-0248-y#ref-CR18" id="ref-link-section-d40094e475">2013</a>) beam-former algorithm in the frequency domain is used to analyze this multi-channel system. An MVDR beam-former optimizes the power of the output signal under the constraint that signals from the desired direction are maintained (Rubsamen and Gershman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Rubsamen M, Gershman AB (2012) Robust adaptive beamforming using multidimensional covariance fitting. IEEE Trans Signal Process 60(2):740–753" href="/article/10.1007/s10055-014-0248-y#ref-CR16" id="ref-link-section-d40094e478">2012</a>). Optimization constraints (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ1">1</a>) can be solved using Lagrange’s method (2):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf w _{o} = \,\,^{\rm argmin}_{w} \left\{ {\mathbf{ w} }^{H} {\mathbf{ S}} _{xx} {\mathbf {w} }\right\} , \hbox{with}\;\mathbf w ^{H}\mathbf h =1\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \nabla _w\left[ \mathbf w ^{H}\mathbf S _{xx}\mathbf w +\lambda \left( \mathbf w ^{H}\mathbf h -1\right) \right] =\mathbf S _{vv}\mathbf w +\lambda \mathbf h =0 \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(\mathbf S _{xx},\mathbf w ,\nabla _{w}\)</span> are spatio-spectral correlation matrix, beam-former weights and the gradient with respect to the weight vector, respectively. Superscripts <i>H</i> and <i>h</i> denote the conjugate transpose and the channel transfer function vector. Combining the constraint equation from (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ1">1</a>) with (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ2">2</a>) leads to the well-known solution for the optimum weight vector</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf{w}_{o}=\frac{\mathbf{S}^{-1}_{vv}\mathbf{h}}{\mathbf{h}^{H}\mathbf{S}^{-1}_{vv}\mathbf{h}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>If the noise is assumed as homogeneous diffuse noise and if <span class="mathjax-tex">\(\mathbf S _{vv}\)</span> is estimated for each signal frame with index <i>m</i> by</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;\mathbf S _{vv}\left( e^{j\theta },m\right) \nonumber \\&amp;\quad =\alpha \mathbf S _{vv}\left( e^{j\theta },m-1\right) +\left( 1-\alpha \right) \mathbf v \left( e^{j\theta },m\right) \mathbf v ^{H}\left( e^{j\theta },m\right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <span class="mathjax-tex">\(\theta =2\pi \frac{f}{f_{s}}\)</span> is the frequency variable, the optimum weight vector can be found iteratively with a steepest descent algorithm expressed by,</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf w _{k+1}&amp;= \mathbf w _{k}-\mu {\nabla }_{w}\left[ \mathbf w _{k}^{H}\mathbf S _{xx}\mathbf w _{k}+\lambda \left( \mathbf w _{k}^{H}\mathbf h -1\right) \right] \nonumber \\&amp;= \mathbf w _{k}-\mu \left( \mathbf S _{vv}\mathbf w _{k}+\lambda \mathbf h \right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>The Lagrange multiplier, <span class="mathjax-tex">\(\lambda\)</span>, is calculated by substituting the second constraint of (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ1">1</a>) into (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ2">2</a>). By eliminating <span class="mathjax-tex">\(\lambda\)</span> from (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ5">5</a>), the final update equation is given by,</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf{w}_{k+1}=\mathbf{w}_{k}-\mu \underbrace{\left( I-\frac{\mathbf{hh}^{H}}{\left\| \mathbf{h}\right\| ^2}\right) \mathbf{S}_{vv}\mathbf{w} _{k}}_{\left( \mathbf{g}_{k}\right) } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>In (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ6">6</a>), the weight vector is updated by using <span class="mathjax-tex">\(\mathbf S _{vv}\)</span> estimated from (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ4">4</a>) and iterating in each frame. Furthermore, convergence speed is improved by computing an optimum step size factor <span class="mathjax-tex">\(\mu\)</span>. Step size is chosen so that it minimizes the noise power at the beam-former output in each iteration.</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{\partial \left( \mathbf w _{k+1}^{H}\mathbf S _{vv}\mathbf w _{k+1}\right) }{\partial \mu ^{*}}=0 \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>Combining (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ6">6</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0248-y#Equ7">7</a>) yields,</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mu _{k}=\frac{\mathbf{g}_{k}^{H}\mathbf{S}_{vv}\mathbf{w}_{k}}{\mathbf{g}_{k}^{H}\mathbf{S}_{vv}\mathbf{g}_{k}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>The entire process is summarized in the flowchart shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Flow chart for weight vector calculation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Implementation of speech processing</h2><div class="c-article-section__content" id="Sec3-content"><p>The proposed speech recognition system consists of six components as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig2">2</a>. Among them, the key signal processing components include feature analysis, unit matching system, lexical coding and syntactic analysis. The speech signal is analyzed in the frequency and time domain to extract observation vectors, which can be used to train a hidden Markov model (HMM) (Lyngsø <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Lyngsø R (2012) Hidden Markov models. Narotama 1:1–24" href="/article/10.1007/s10055-014-0248-y#ref-CR10" id="ref-link-section-d40094e1885">2012</a>). The HMM is a useful algorithm for characterizing various speech sounds.</p><p>The reason why HMM algorithms are popular in speech processing is because they can be trained automatically and computationally feasible to use. The HMM is an stochastic approach which models the given problem as a “doubly stochastic process”. The detail of the HMM training is well described in Nilsson and Ejnarsson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Nilsson M, Ejnarsson M (2002) Speech recognition using hidden Markov model. Master’s thesis, Department of Telecommunications and Speech Processing, Blekinge Institute of Technology" href="/article/10.1007/s10055-014-0248-y#ref-CR11" id="ref-link-section-d40094e1891">2002</a>).</p><p>A speech processing algorithm is a chain of different processes. First, a choice of speech recognition must be made by the unit matching system to detect subwords. Possibilities include linguistically basic subword units as well as derivative units. For the current application, it is both reasonable and practical to consider the word as a basic speech unit. Thus, for the current implementation, only words listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab1">1</a> are considered.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The architecture of speech recognition and coding system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 The vocabulary list in the dictionary of speech processing system (lexicon)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The lexical coding process places constraints on the unit matching system so that the paths investigated are those corresponding to sequences of speech units, which are in a word dictionary (lexicon). This procedure implies that the word extracted by speech recognition must be specified in terms of basic units chosen for recognition. Syntactic analysis adds further constraints to the set of recognition search paths. One way in which semantic constraints are utilized is via a dynamic model of the state of the recognizer. For each word recognized, the associated code (fourth row in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab1">1</a>) is selected and server data are generated and sent to the client side through the wireless network. Then, the corresponding function is activated in the client application.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4"> Experiments and validation setup</h2><div class="c-article-section__content" id="Sec4-content"><p>Test environment as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig3">3</a> was established to evaluate the performance of the proposed VRPN server. The environment consists of CAVE system, a microphone array and its interfacing system, a laptop computer equipped with wireless router, as well as an Ethernet router, LAN Hub and VE workstation computer along with its accessories. A test software platform was developed to manage the CAVE display system. The platform uses OpenSceneGraph to render the 3D model and incorporate the properties of the virtual environment into the model. The model is projected in the display system via MPI and four NVidia Quadroplex GPUs. All the C++ functions were wrapped under Java script functions in the platform to make development faster and easier for programmers.</p><p>Every microphone array can be used as audio input device; however, Kinect Xbox 360 is used in these experiments. The microphone array features four microphone capsules (Joystiq <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Joystiq. Kinect: the company behind the tech explains how it works, March 21 2011" href="/article/10.1007/s10055-014-0248-y#ref-CR8" id="ref-link-section-d40094e2122">2011</a>; Store <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Store M (2010) Kinect for xbox 360, 7 July 2010. Array of 4 microphones supporting single speaker voice recognition" href="/article/10.1007/s10055-014-0248-y#ref-CR20" id="ref-link-section-d40094e2125">2010</a>) and operates with each channel processing 16-bit audio at a sampling rate of 16 kHz (Retrieved <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Retrieved P (2010) Kinect xbox 360 specification, July 2 2010. This information is based on specifications supplied by manufacturers and should be used for guidance only" href="/article/10.1007/s10055-014-0248-y#ref-CR14" id="ref-link-section-d40094e2128">2010</a>). The distance between the user and the microphone array is limited to 1–3 meters because the array is located inside the CAVE. The speak-to-VR server ran on a Dell Precision laptop (Intel Core<span class="mathjax-tex">\(^{\mathrm{TM}}\)</span> i7 processors, RAM 32GB 1333MHz, 7200rpm hard drive 350GB, graphics NVIDIA Quadro 1000M, integrated noise canceling digital array microphones, USB 2.0 and IEEE 1394 I/O ports, Windows 7 Professional 64-Bit) and streamed the Speech Processing (SP) result as 7 buttons over a Wi-Fi network. VRPN has different data type such as “Button”, “Digital”, “Tracker” and so on. Button data type is selected because it is easier for the development. The microphone array and the distance between the elements of the array are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Test-bench and wireless infra-structure for voice data streaming in speak-to-VR VRPN server</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Micro-phone array configuration</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The speak-to-VR VRPN server processes spoken commands and compares the result with the dictionary content (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab1">1</a>) after receiving a word as a 1D signal from microphone array. Then, the associated button is activated and sent to the client application through the wireless network. Navigation and object manipulation applications were developed in the client side to evaluate user performance. Since it is difficult to use only the speak-to-VR interface for both navigation and object manipulation, this interface was used in combination with other HMI. The detail of these two tasks will be explained separately below.</p><h3 class="c-article__sub-heading" id="Sec5">Navigation</h3><p>Navigation in large-scale 3D VEs can be implemented by the combination of two types of movement, i.e., translation and rotation. These movements can be initiated either by clicking on a button or pressing a joy-stick handle when a navigation device such as fly-stick is implemented. Similarly, the movement can be initiated and terminated by using speak-to-VR VRPN server buttons (voice commands) as explained above (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab1">1</a>). We asked the user to go from position A to B using two interfaces (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig5">5</a>). In the first interface, the task is performed by using only “start”, “stop”, “turn left” and “turn right” commands from speak-to-VR VRPN server. In the second interface, “start” and “stop” commands from speak-to-VR and rotation from AR-Tracker (ZHU et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhu F-W, Li D-Q, Yuan Z-P, Wu J-Q, Cheng X (2004) An ar tracker based on planar marker. J Shanghai Univ (Nat Sci Ed) 5:005" href="/article/10.1007/s10055-014-0248-y#ref-CR23" id="ref-link-section-d40094e2215">2004</a>) VRPN server were employed to complete the navigation task. The screen shot of the server and client side of speak-to-VR VRPN are shown at the top right corner in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig5">5</a>. As seen, the server side has sent a code associated with “backward”, in “button 3”, by setting 4th digital bit active. Consecutively, “Button 1” became active when “stop” had been told. In the client side, the assigned function starts navigation/manipulation task immediately after receiving the code from speak-VRPN server.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Navigation inside a real-scale 3D model using speak-to-VR as an input device</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec6">Object manipulation</h3><p>Object manipulation was implemented by fusing data from speak-to-VR and a Magic bracelet VRPN server. The Magic bracelet is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig6">6</a>. For the manipulation task, the user is asked to “select”, “move” and “place” an object (a cube of 30 × 30 × 30) from location A to B. The object is selected by saying “select” when the bracelet is close enough (30 cm) to the target object. Then, the object and the bracelet become one object and by the movement of the bracelet it moves. The object is placed in the last location of the bracelet when the command “place” is said by the user.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Pair of magic bracelets</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Result and discussion</h2><div class="c-article-section__content" id="Sec7-content"><p>The dictionary is not limited to the vocabulary listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab1">1</a>. It can be extended, but a larger dictionary and longer words increase the processing time significantly. Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab3">3</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab4">4</a> show the relation between speech processing time (SPT) and total processing time (TPT) with the length and the number of the words in the dictionary, respectively. The SPT is only the time speech takes to be processed, and the TPT includes the speech processing time plus the transit time in the VRPN network. To compute these times, one high-resolution timer is started when the speech signal received from the microphone array and stopped when the 7-bit code is received by to the client application. The difference between these two instances is total processing time. The same strategy was used to measure the speech processing time.</p><h3 class="c-article__sub-heading" id="Sec8">Parameters study</h3><p>Two dictionaries have been chosen to study the usability and the restriction of the proposed speech-based HMI for the navigation task, dictionary 1 (15 words with min 5 and max 20 letters) and dictionary 2 (8 words with min and max similar to dictionary 1) as seen in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab2">2</a>.</p><p>As expected, speech recognition performance decreases with larger dictionary. For example, for dictionary 1 SPT and TPT for “start” in dictionary 1 are 106 (ms) and 177(ms), respectively, while for dictionary 2 they are 82(ms) and 141(ms). The SPT increases 29.3 % when the number of the words in the dictionary is doubled (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab3">3</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab4">4</a>). The difference of two processing time is calculated by <span class="mathjax-tex">\((\hbox{SPT}_{2}-\hbox{SPT}_{1})/\hbox{SPT}_{1}\times 100\,\%\)</span> and <span class="mathjax-tex">\((\hbox{TPT}_{2}-\hbox{TPT}_{1})/\hbox{TPT}_{1}\times 100\,\%\)</span> where subscripts 1 and 2 indicate the dictionary. TPT increases 25.5 %, for the same word, by increasing the number of the words. On average, SPT and TPT increase 35.7 and 33.3 %, respectively, when the number of the words is doubled. However, when the length of the words (#letters) increases in the same dictionary (e.g. dictionary 1), SPT and TPT increase quite rapidly.</p><p>SPT associated with “start” and “Increase speed” (with 5 and 13 letters) is 106(ms) and 691.6(ms), respectively, which leads to 5.5 times slower processing. TPT becomes 5.1 times slower for the same words. As seen, TPT and SPT get 5 times slower when the length increases almost 2.5 times. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig7">7</a> shows the summary of this reduction in TPT and SPT for all the words in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab3">3</a>. All of the words in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab3">3</a> were compared to the word “start” to calculate values in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig7">7</a>. Clearly, the relation between the length of the words and TPT and SPT increase is approximately exponential which in turn means the length of the word is extremely important and the words need to be carefully selected.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Relation between the length of the words and reduction in the processing speed (increasing processing delay)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Two dictionaries used in the navigation study</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Speech processing and total processing time for navigation dictionary 1 in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab2">2</a>
                        </b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Speech processing and total processing time for navigation dictionary 2 in table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab2">2</a>
                        </b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Two dictionary used in manipulation study</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Speech processing and total processing time for manipulation dictionary 1 in table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab5">5</a>
                        </b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 Speech processing and total processing time for manipulation dictionary 2 in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab5">5</a>
                        </b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/7"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Similarly, two dictionaries have been chosen for manipulation task, dictionary 1 (9 words with min 4 and max 20 letters) and dictionary 2 (6 words with min and max similar to dictionary 1) as seen in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab5">5</a>. STP and TPT for “place” (with 5 letters in length) increase 26.9 and 21.9 % when vocabulary increases by 1.5 times (Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab6">6</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab7">7</a>). Comparison of “move” and “Rotate clockwise” shows SPT and TPT increase 13.9 and 12.2 times, respectively. Similar to the navigation dictionary, the processing time grows exponentially with the length of words. Since cyber sickness is frequently attributed to lag time between input and response (Day et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Day PN, Holt PO, Russell GT (2001) The cognitive effects of delayed visual feedback: working memory disruption while driving in virtual environments. Cognitive technology: instruments of mind. Springer, Berlin, pp 75–82" href="/article/10.1007/s10055-014-0248-y#ref-CR2" id="ref-link-section-d40094e3442">2001</a>), appropriate selection of vocabulary for navigation and manipulation is critical.</p><p>Based on this heuristic, more than 10 words in the dictionary make the server quite slow. Moreover, the length of the words is important, words longer than 10 letters decrease the processing speed dramatically. As a result, it is recommended to select shorter words and include fewer words in the dictionary to make it as fast as possible.</p><p>The precision of the server is also important. The precision is defined as the number of successful instances of speech recognition compared to the total number of trials. To test the precision of the speak-to-VR VRPN server, four pairs of words were selected and spoken in front of the array while the detection result was recording. The test word pairs were chosen to be intentionally close in terms of pronunciation. The result of this test is shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0248-y#Tab8">8</a>. The recognition rate is high on average (<i>M</i> = 96.25 %); however, when the words are very close, for instance “start” and “stop”, the detection rate decreases somewhat (95.6, 94.8 %). On the contrary, the recognition precision increases when the words are different, for instance “left” and “right” (98.2, 97.7 %).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 Random accuracy test of speak-to-VR VRPN server</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/tables/8"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec9">Evaluation by subjective user study</h3><p>A subjective user study was conducted to evaluate the user performance of the speak-to-VR VRPN server. A simulator sickness questionnaire (SSQ Kennedy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Kennedy RS, Lane NE, Berbaum KS, Lilienthal MG (1993) Simulator sickness questionnaire: an enhanced method for quantifying simulator sickness. Int J Aviat Psychol 3(3):203–220" href="/article/10.1007/s10055-014-0248-y#ref-CR9" id="ref-link-section-d40094e3638">1993</a>) and postexposure question were used as two evaluation methods. Ten subjects (6 males and 4 females: ages <span class="mathjax-tex">\(31.58\pm 12.69\)</span> years and weights <span class="mathjax-tex">\(74.65\pm 15.22\)</span> kg) participated in the experiment. They performed two tasks as explained above: navigation and manipulation. Participants were asked to score navigation/manipulation tasks using speak-to-VR HMI on a Likert scale of 1-10 (1: bad, 10: very good) in the postexposure questionnaire. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig8">8</a> shows the result of this evaluation.</p><p>A dependent-samples <i>t</i> test was conducted to compare manipulation (<i>M</i> = 8.4, SD = 1.43) and navigation (<i>M</i> = 6.5, SD = 1.96). This analysis yields <i>t</i>(9) = 2.3, <i>P</i> = 0.046 and since <span class="mathjax-tex">\(p\prec 0.05\)</span> the difference is significant. As seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig8">8</a>, most of the participants found manipulation more comfortable with the speak-to-VR interface than navigation. This is because, there are substantial delays using only the speak-to-VR HMI which induces general discomfort for users when they are navigating. To avoid this general discomfort, we combined the interface with AR-Tracker as mentioned above.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The level of user satisfaction using speak-to-VR interface for interaction and navigation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The postexposure sickness score after navigating by the first and the second interfaces (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0248-y#Sec5">4.1</a>) for each participant is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0248-y#Fig9">9</a>. A dependent-samples t-test was conducted to compare navigation using only the speak-to-VR interface (M = 138.2, SD = 72.2) and navigation by combining two interfaces (<i>M</i> = 71.2, SD = 44.9), speak-to-VR and AR-Tracker, via Kennedy SSQ (Kennedy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Kennedy RS, Lane NE, Berbaum KS, Lilienthal MG (1993) Simulator sickness questionnaire: an enhanced method for quantifying simulator sickness. Int J Aviat Psychol 3(3):203–220" href="/article/10.1007/s10055-014-0248-y#ref-CR9" id="ref-link-section-d40094e3774">1993</a>). The analysis yields<i> t</i>(9) = 4.76, P = 0.0098. this means, in average, participants felt less discomfort with the combination of two HMI.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0248-y/MediaObjects/10055_2014_248_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Comparison of two navigation interface based on speak-to-VR VRPN server</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0248-y/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In addition to the previous evaluations, the accuracy of the speech-based HMI is evaluated in the presence of the accent changes. Four subjects with different accents (Italian, Persian, Indian, Chinese) were selected and were trained for nearly 10 min. They were asked to repeat words “start”, “stop”, “turn left” and “turn right” several trial (35 times) during training phase and real test. As expected, the precision of the speak-to-VR for training and real test is above 85 and 94 %, respectively. Moreover, the difference between subjects is only (<span class="mathjax-tex">\(Min = 94.3\,\%, Max = 97.14\,\%\)</span>) 3 %, which means when the accent changes the precision does not change significantly.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Conclusion</h2><div class="c-article-section__content" id="Sec10-content"><p>The speak-to-VR VRPN server described in this paper is based on the Microsoft speech processing SDK and uses a Wi-Fi network to provide a handy, portable and wireless connection between user and a VR system. The subjective user study shows that this HMI is more useful for interaction tasks (<i>t</i>(9) = 2.3, <span class="mathjax-tex">\(p\prec 0.05\)</span>). However, if this HMI (for translation) is combined with another HMI (rotation), it can be used for navigation as well (<i>t</i>(9) = 4.76, <span class="mathjax-tex">\(p\prec 0.001\)</span>). The proposed VRPN server works independent of user’s accent and functions precisely (higher than 90 % on average) with the presence of other users and background noise.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Future research</h2><div class="c-article-section__content" id="Sec11-content"><p>The delay of the speak-to-VR VRPN server restricts the usage of the speech-based HMI in a real-time VR applications. Rodriguez et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Rodríguez-Andina J, Fagundes RDR, Junior DB (2001) A fpga-based viterbi algorithm implementation for speech recognition systems. In: 2001 IEEE international conference on acoustics, speech, and signal processing, 2001 (Proceedings ICASSP’01), vol 2, pp 1217–1220, IEEE" href="/article/10.1007/s10055-014-0248-y#ref-CR15" id="ref-link-section-d40094e3930">2001</a>) proposed a reconfigurable FPGA-based architecture to accelerate speech recognition on a FPGA. Therefore, one solution for this problem is to implement speech processing on a hardware such as a GPU or FPGA with parallel processing approaches. Then, this solution can be combined with VRPN server to make the speak-to-VR faster for a real-time application.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Boyle, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Boyle A et al (2008) Pick your top geek gift-cosmic log. Science 3147:10" /><p class="c-article-references__text" id="ref-CR1">Boyle A et al (2008) Pick your top geek gift-cosmic log. Science 3147:10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pick%20your%20top%20geek%20gift-cosmic%20log&amp;journal=Science&amp;volume=3147&amp;publication_year=2008&amp;author=Boyle%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="PN. Day, PO. Holt, GT. Russell, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Day PN, Holt PO, Russell GT (2001) The cognitive effects of delayed visual feedback: working memory disruption" /><p class="c-article-references__text" id="ref-CR2">Day PN, Holt PO, Russell GT (2001) The cognitive effects of delayed visual feedback: working memory disruption while driving in virtual environments. Cognitive technology: instruments of mind. Springer, Berlin, pp 75–82</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20technology%3A%20instruments%20of%20mind&amp;pages=75-82&amp;publication_year=2001&amp;author=Day%2CPN&amp;author=Holt%2CPO&amp;author=Russell%2CGT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DiVerdi S, Rakkolainen I, Höllerer T, Olwal A (2006) A novel walk-through 3d display. In: Electronic imaging 2" /><p class="c-article-references__text" id="ref-CR3">DiVerdi S, Rakkolainen I, Höllerer T, Olwal A (2006) A novel walk-through 3d display. In: Electronic imaging 2006, p 605519. International Society for Optics and Photonics</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fischbach M, Wiebusch D, Giebler-Schubert A, Latoschik ME, Rehfeld S, Tramberend H (2011) Sixton’s curse-simul" /><p class="c-article-references__text" id="ref-CR4">Fischbach M, Wiebusch D, Giebler-Schubert A, Latoschik ME, Rehfeld S, Tramberend H (2011) Sixton’s curse-simulator x demonstration. In: 2011 IEEE virtual reality conference (VR), pp 255–256. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Intel. Voice recognition and synthesis using the intel perceptual computing sdk, 2013" /><p class="c-article-references__text" id="ref-CR5">Intel. Voice recognition and synthesis using the intel perceptual computing sdk, 2013</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="iSpeech. Speech processing sdk for mobile developer, 8 2011" /><p class="c-article-references__text" id="ref-CR6">iSpeech. Speech processing sdk for mobile developer, 8 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Jinghui, J. Zijing, H. Jinming, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Jinghui G, Zijing J, Jinming H (2005) Implement of speech application program based on speech sdk [j]. J Guang" /><p class="c-article-references__text" id="ref-CR7">Jinghui G, Zijing J, Jinming H (2005) Implement of speech application program based on speech sdk [j]. J Guangxi Acad Sci 3:169–172</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Implement%20of%20speech%20application%20program%20based%20on%20speech%20sdk%20%5Bj%5D&amp;journal=J%20Guangxi%20Acad%20Sci&amp;volume=3&amp;pages=169-172&amp;publication_year=2005&amp;author=Jinghui%2CG&amp;author=Zijing%2CJ&amp;author=Jinming%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Joystiq. Kinect: the company behind the tech explains how it works, March 21 2011" /><p class="c-article-references__text" id="ref-CR8">Joystiq. Kinect: the company behind the tech explains how it works, March 21 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RS. Kennedy, NE. Lane, KS. Berbaum, MG. Lilienthal, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Kennedy RS, Lane NE, Berbaum KS, Lilienthal MG (1993) Simulator sickness questionnaire: an enhanced method for" /><p class="c-article-references__text" id="ref-CR9">Kennedy RS, Lane NE, Berbaum KS, Lilienthal MG (1993) Simulator sickness questionnaire: an enhanced method for quantifying simulator sickness. Int J Aviat Psychol 3(3):203–220</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327108ijap0303_3" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simulator%20sickness%20questionnaire%3A%20an%20enhanced%20method%20for%20quantifying%20simulator%20sickness&amp;journal=Int%20J%20Aviat%20Psychol&amp;volume=3&amp;issue=3&amp;pages=203-220&amp;publication_year=1993&amp;author=Kennedy%2CRS&amp;author=Lane%2CNE&amp;author=Berbaum%2CKS&amp;author=Lilienthal%2CMG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Lyngsø, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Lyngsø R (2012) Hidden Markov models. Narotama 1:1–24" /><p class="c-article-references__text" id="ref-CR10">Lyngsø R (2012) Hidden Markov models. Narotama 1:1–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hidden%20Markov%20models&amp;journal=Narotama&amp;volume=1&amp;pages=1-24&amp;publication_year=2012&amp;author=Lyngs%C3%B8%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nilsson M, Ejnarsson M (2002) Speech recognition using hidden Markov model. Master’s thesis, Department of Tel" /><p class="c-article-references__text" id="ref-CR11">Nilsson M, Ejnarsson M (2002) Speech recognition using hidden Markov model. Master’s thesis, Department of Telecommunications and Speech Processing, Blekinge Institute of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Pulakka, P. Alku, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Pulakka H, Alku P (2011) Bandwidth extension of telephone speech using a neural network and a filter bank impl" /><p class="c-article-references__text" id="ref-CR12">Pulakka H, Alku P (2011) Bandwidth extension of telephone speech using a neural network and a filter bank implementation for highband mel spectrum. IEEE Trans Audio Speech Lang Process 19(7):2170–2183</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTASL.2011.2118206" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bandwidth%20extension%20of%20telephone%20speech%20using%20a%20neural%20network%20and%20a%20filter%20bank%20implementation%20for%20highband%20mel%20spectrum&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Process&amp;volume=19&amp;issue=7&amp;pages=2170-2183&amp;publication_year=2011&amp;author=Pulakka%2CH&amp;author=Alku%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="R. M. T. II. (2008) Vrpn 07.30—http://www.cs.unc.edu/research/vrpn/&#xA;                " /><p class="c-article-references__text" id="ref-CR13">R. M. T. II. (2008) Vrpn 07.30—<a href="http://www.cs.unc.edu/research/vrpn/">http://www.cs.unc.edu/research/vrpn/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Retrieved P (2010) Kinect xbox 360 specification, July 2 2010. This information is based on specifications sup" /><p class="c-article-references__text" id="ref-CR14">Retrieved P (2010) Kinect xbox 360 specification, July 2 2010. This information is based on specifications supplied by manufacturers and should be used for guidance only</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rodríguez-Andina J, Fagundes RDR, Junior DB (2001) A fpga-based viterbi algorithm implementation for speech re" /><p class="c-article-references__text" id="ref-CR15">Rodríguez-Andina J, Fagundes RDR, Junior DB (2001) A fpga-based viterbi algorithm implementation for speech recognition systems. In: 2001 IEEE international conference on acoustics, speech, and signal processing, 2001 (Proceedings ICASSP’01), vol 2, pp 1217–1220, IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Rubsamen, AB. Gershman, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Rubsamen M, Gershman AB (2012) Robust adaptive beamforming using multidimensional covariance fitting. IEEE Tra" /><p class="c-article-references__text" id="ref-CR16">Rubsamen M, Gershman AB (2012) Robust adaptive beamforming using multidimensional covariance fitting. IEEE Trans Signal Process 60(2):740–753</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2011.2174233" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2919474" aria-label="View reference 16 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20adaptive%20beamforming%20using%20multidimensional%20covariance%20fitting&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=60&amp;issue=2&amp;pages=740-753&amp;publication_year=2012&amp;author=Rubsamen%2CM&amp;author=Gershman%2CAB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SAR (2005) Sri language modeling toolkit and speech sdk" /><p class="c-article-references__text" id="ref-CR17">SAR (2005) Sri language modeling toolkit and speech sdk</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Shao, Z. Qian, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Shao W, Qian Z (2013) A new partially adaptive minimum variance distortionless response beamformer with constr" /><p class="c-article-references__text" id="ref-CR18">Shao W, Qian Z (2013) A new partially adaptive minimum variance distortionless response beamformer with constrained stability least mean squares algorithm. Adv Sci Lett 19(4):1071–1074</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1166%2Fasl.2013.4430" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20partially%20adaptive%20minimum%20variance%20distortionless%20response%20beamformer%20with%20constrained%20stability%20least%20mean%20squares%20algorithm&amp;journal=Adv%20Sci%20Lett&amp;volume=19&amp;issue=4&amp;pages=1071-1074&amp;publication_year=2013&amp;author=Shao%2CW&amp;author=Qian%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stone JE, Kohlmeyer A, Vandivort KL, Schulten K (2010) Immersive molecular visualization and interactive model" /><p class="c-article-references__text" id="ref-CR19">Stone JE, Kohlmeyer A, Vandivort KL, Schulten K (2010) Immersive molecular visualization and interactive modeling with commodity hardware. In: Advances in visual computing. Springer, Berlin, pp 382–393</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Store M (2010) Kinect for xbox 360, 7 July 2010. Array of 4 microphones supporting single speaker voice recogn" /><p class="c-article-references__text" id="ref-CR20">Store M (2010) Kinect for xbox 360, 7 July 2010. Array of 4 microphones supporting single speaker voice recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Suma EA, Lange B, Rizzo A, Krum DM, Bolas M (2011) Faast: The flexible action and articulated skeleton toolkit" /><p class="c-article-references__text" id="ref-CR21">Suma EA, Lange B, Rizzo A, Krum DM, Bolas M (2011) Faast: The flexible action and articulated skeleton toolkit. In: 2011 IEEE virtual reality conference (VR), pp 247–248, IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-tr" /><p class="c-article-references__text" id="ref-CR22">Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001) Vrpn: a device-independent, network-transparent vr peripheral system. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 55–61, ACM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhu F-W, Li D-Q, Yuan Z-P, Wu J-Q, Cheng X (2004) An ar tracker based on planar marker. J Shanghai Univ (Nat S" /><p class="c-article-references__text" id="ref-CR23">Zhu F-W, Li D-Q, Yuan Z-P, Wu J-Q, Cheng X (2004) An ar tracker based on planar marker. J Shanghai Univ (Nat Sci Ed) 5:005</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0248-y-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Lab. Le2i, Institute Image, Paris-Tech, Paris, France</p><p class="c-article-author-affiliation__authors-list">M. Ali Mirzaei &amp; Frederic Merienne</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Virtual Reality Application Center, Iowa State University, Ames, IA, USA</p><p class="c-article-author-affiliation__authors-list">James H. Oliver</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-M__Ali-Mirzaei"><span class="c-article-authors-search__title u-h3 js-search-name">M. Ali Mirzaei</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;M. Ali+Mirzaei&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=M. Ali+Mirzaei" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22M. Ali+Mirzaei%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Frederic-Merienne"><span class="c-article-authors-search__title u-h3 js-search-name">Frederic Merienne</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Frederic+Merienne&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Frederic+Merienne" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Frederic+Merienne%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-James_H_-Oliver"><span class="c-article-authors-search__title u-h3 js-search-name">James H. Oliver</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;James H.+Oliver&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=James H.+Oliver" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22James H.+Oliver%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0248-y/email/correspondent/c1/new">M. Ali Mirzaei</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=New%20wireless%20connection%20between%20user%20and%20VE%20using%20speech%20processing&amp;author=M.%20Ali%20Mirzaei%20et%20al&amp;contentID=10.1007%2Fs10055-014-0248-y&amp;publication=1359-4338&amp;publicationDate=2014-07-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0248-y" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0248-y" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Mirzaei, M.A., Merienne, F. &amp; Oliver, J.H. New wireless connection between user and VE using speech processing.
                    <i>Virtual Reality</i> <b>18, </b>235–243 (2014). https://doi.org/10.1007/s10055-014-0248-y</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0248-y.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-11-21">21 November 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-06-23">23 June 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-07-20">20 July 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-11">November 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0248-y" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0248-y</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Speak-to-VR</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Wi-Fi network</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Speech processing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">VRPN server</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0248-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=248;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

