<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An augmented reality application for improving shopping experience in "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In several large retail stores, such as malls, sport or food stores, the customer often feels lost due to the difficulty in finding a product. Although these large stores usually have visual signs..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An augmented reality application for improving shopping experience in large retail stores"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-02-24"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In several large retail stores, such as malls, sport or food stores, the customer often feels lost due to the difficulty in finding a product. Although these large stores usually have visual signs to guide customers toward specific products, sometimes these signs are also hard to find and are not updated. In this paper, we propose a system that jointly combines deep learning and augmented reality techniques to provide the customer with useful information. First, the proposed system learns the visual appearance of different areas in the store using a deep learning architecture. Then, customers can use their mobile devices to take a picture of the area where they are located within the store. Uploading this image to the system trained for image classification, we are able to identify the area where the customer is located. Then, using this information and novel augmented reality techniques, we provide information about the area where the customer is located: route to another area where a product is available, 3D product visualization, user location, analytics, etc. The system developed is able to successfully locate a user in an example store with 98% accuracy. The combination of deep learning systems together with augmented reality techniques shows promising results toward improving user experience in retail/commerce applications: branding, advance visualization, personalization, enhanced customer experience, etc."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-02-24"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="281"/>

    <meta name="prism.endingPage" content="291"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0338-3"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0338-3"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0338-3.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0338-3"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An augmented reality application for improving shopping experience in large retail stores"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2019/09"/>

    <meta name="citation_online_date" content="2018/02/24"/>

    <meta name="citation_firstpage" content="281"/>

    <meta name="citation_lastpage" content="291"/>

    <meta name="citation_article_type" content="S.I. : Virtual Reality, Augmented Reality and Commerce"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0338-3"/>

    <meta name="DOI" content="10.1007/s10055-018-0338-3"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0338-3"/>

    <meta name="description" content="In several large retail stores, such as malls, sport or food stores, the customer often feels lost due to the difficulty in finding a product. Although the"/>

    <meta name="dc.creator" content="Edmanuel Cruz"/>

    <meta name="dc.creator" content="Sergio Orts-Escolano"/>

    <meta name="dc.creator" content="Francisco Gomez-Donoso"/>

    <meta name="dc.creator" content="Carlos Rizo"/>

    <meta name="dc.creator" content="Jose Carlos Rangel"/>

    <meta name="dc.creator" content="Higinio Mora"/>

    <meta name="dc.creator" content="Miguel Cazorla"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ahn J, Williamson J, Gartrell M, Han R, Lv Q, Mishra S (2015) Supporting healthy grocery shopping via mobile augmented reality. ACM Trans Multimedia Comput Commun Appl 12(1s):16:1&#8211;16:24. ISSN 1551-6857. 
                    https://doi.org/10.1145/2808207
                    
                  
                        "/>

    <meta name="citation_reference" content="Akgul O, Penekli HI, Genc Y (2016) Applying deep learning in augmented reality tracking. In: 12th international conference on signal-image technology internet-based systems (SITIS), 2016, pp 47&#8211;54. 
                    https://doi.org/10.1109/SITIS.2016.17
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Recent advances in augmented reality; citation_author=R Azuma, Y Baillot, R Behringer, S Feiner, S Julier, B MacIntyre; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=34-37; citation_doi=10.1109/38.963459; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Trans Neural Netw; citation_title=Learning long-term dependencies with gradient descent is difficult; citation_author=Y Bengio, P Simard, P Frasconi; citation_volume=5; citation_issue=2; citation_publication_date=1994; citation_pages=157-166; citation_doi=10.1109/72.279181; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Electron Commer; citation_title=Fashion shopping in multichannel retail: the role of technology in enhancing the customer experience; citation_author=M Blazquez; citation_volume=18; citation_issue=4; citation_publication_date=2014; citation_pages=97-116; citation_doi=10.2753/JEC1086-4415180404; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Multimed Tools Appl; citation_title=Augmented reality technologies, systems and applications; citation_author=J Carmigniani, B Furht, M Anisetti, P Ceravolo, E Damiani, M Ivkovic; citation_volume=51; citation_issue=1; citation_publication_date=2011; citation_pages=341-377; citation_doi=10.1007/s11042-010-0660-6; citation_id=CR6"/>

    <meta name="citation_reference" content="Chatzopoulos D, Bermejo C, Huang Z, Hui P (2017) Mobile augmented reality survey: from where we are to where we go. IEEE Access 5:6917&#8211;6950. ISSN 2169-3536. 
                    https://doi.org/10.1109/ACCESS.2017.2698164
                    
                  
                        "/>

    <meta name="citation_reference" content="Clarifai (2015) Clarifai: Amplifying Intelligence with a Vision. 
                    http://www.clarifai.com/
                    
                  . Accessed 1 June 2017"/>

    <meta name="citation_reference" content="Csurka G, Dance CR, Fan L, Willamowski J, Bray C (2004) Visual categorization with bags of keypoints. In: Workshop on statistical learning in computer vision, ECCV, pp 1&#8211;22"/>

    <meta name="citation_reference" content="Goldman A (2001) The transfer of retail formats into developing economies: the example of china. J Retail 77(2):221&#8211;242. ISSN 0022-4359. 
                    https://doi.org/10.1016/S0022-4359(01)00044-6
                    
                  . 
                    http://www.sciencedirect.com/science/article/pii/S0022435901000446
                    
                  
                        "/>

    <meta name="citation_reference" content="Grewal D, Roggeveen AL, Nordflt J (2017) The future of retailing. J Retail 93(1):1&#8211;6. ISSN 0022-4359. 
                    https://doi.org/10.1016/j.jretai.2016.12.008
                    
                  . 
                    http://www.sciencedirect.com/science/article/pii/S0022435916300872
                    
                  . (The Future of Retailing)"/>

    <meta name="citation_reference" content="He K, Zhang X, Ren S, Sun J (2015) Deep residual learning for image recognition. CoRR. 
                    arXiv:1512.03385
                    
                  
                        "/>

    <meta name="citation_reference" content="Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. CoRR. 
                    arXiv:1412.6980
                    
                  
                        "/>

    <meta name="citation_reference" content="Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the 8th IEEE international symposium on mixed and augmented reality, 2009, ISMAR &#8217;09, Washington, DC, USA. IEEE Computer Society, pp 83&#8211;86. ISBN 978-1-4244-5390-0. 
                    https://doi.org/10.1109/ISMAR.2009.5336495
                    
                  
                        "/>

    <meta name="citation_reference" content="Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ (eds) Proceedings of the 25th international conference on neural information processing systems. NIPS&#39;12, vol 1. Curran Associates Inc., USA, pp 1097&#8211;1105"/>

    <meta name="citation_reference" content="Lazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. In: Proceedings of the 2006 IEEE computer society conference on computer vision and pattern recognition&#8212;CVPR &#8217;06, Washington, DC, USA, vol 2. IEEE Computer Society, pp 2169&#8211;2178. ISBN 0-7695-2597-0. 
                    https://doi.org/10.1109/CVPR.2006.68
                    
                  
                        "/>

    <meta name="citation_reference" content="LeCun Y, Kavukcuoglu K, Farabet C (2010) Convolutional networks and applications in vision. In: Proceedings of IEEE international symposium on circuits and systems (ISCAS), 2010. IEEE, pp 253&#8211;256"/>

    <meta name="citation_reference" content="citation_journal_title=J Mark; citation_title=Understanding customer experience throughout the customer journey; citation_author=KN Lemon, PC Verhoef; citation_volume=80; citation_issue=6; citation_publication_date=2016; citation_pages=69-96; citation_doi=10.1509/jm.15.0420; citation_id=CR17"/>

    <meta name="citation_reference" content="Li Z, Hoiem D (2016) Learning without forgetting. Springer International Publishing, Cham, pp 614&#8211;629. ISBN 978-3-319-46493-0"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Semant Web Inf Syst; citation_title=Property-based semantic similarity and relatedness for improving recommendation accuracy and diversity; citation_author=S Likavec, F Osborne, F Cena; citation_volume=11; citation_issue=4; citation_publication_date=2008; citation_pages=1-40; citation_doi=10.4018/IJSWIS.2015100101; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=ICML; citation_title=Multiple-instance learning for natural scene classification; citation_author=O Maron, AL Ratan; citation_volume=98; citation_publication_date=1998; citation_pages=341-349; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Adv Robot Syst; citation_title=A taxonomy of vision systems for ground mobile robots; citation_author=J Mart&#237;nez-G&#243;mez, A Fern&#225;ndez-Caballero, I Garc&#237;a-Varea, L Rodr&#237;guez, C Romero-Gonz&#225;lez; citation_volume=11; citation_publication_date=2014; citation_pages=1-11; citation_doi=10.5772/58900; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Robot Res; citation_title=Vidrilo: the visual and depth robot indoor localization with objects information dataset; citation_author=J Martinez-Gomez, I Garcia-Varea, M Cazorla, V Morell; citation_volume=34; citation_issue=14; citation_publication_date=2015; citation_pages=1681-1687; citation_doi=10.1177/0278364915596058; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Univ Access Inf Soc; citation_title=Augmented reality navigation systems; citation_author=W Narzt, G Pomberger, A Ferscha, D Kolb, R M&#252;ller, J Wieghardt, H H&#246;rtner, C Lindinger; citation_volume=4; citation_issue=3; citation_publication_date=2006; citation_pages=177-187; citation_doi=10.1007/s10209-005-0017-5; citation_id=CR23"/>

    <meta name="citation_reference" content="Newcombe RA, Lovegrove SJ, Davison AJ (2011) DTAM: dense tracking and mapping in real-time. In: Proceedings of the international conference on computer vision, 2011, ICCV &#8217;11, Washington, DC, USA. IEEE Computer Society, pp 2320&#8211;2327. ISBN 978-1-4577-1101-5. 
                    https://doi.org/10.1109/ICCV.2011.6126513
                    
                  
                        "/>

    <meta name="citation_reference" content="Ortiz-Catalan M, Gumundsdttir RA, Kristoffersen MB, Zepeda-Echavarria A, Caine-Winterberger K, Kulbacka-Ortiz K, Widehammar C, Eriksson K, Stockselius A, Ragn C, Pihlar Z, Burger H, Hermansson L (2016) Phantom motor execution facilitated by machine learning and augmented reality as treatment for phantom limb pain: a single group, clinical trial in patients with chronic intractable phantom limb pain. Lancet 388(10062):2885&#8211;2894. ISSN 0140-6736"/>

    <meta name="citation_reference" content="Pascanu R, Mikolov T, Bengio Y (2012) On the difficulty of training recurrent neural networks. ArXiv e-prints"/>

    <meta name="citation_reference" content="Pauly O, Diotte B, Fallavollita P, Weidert S, Euler E, Navab N (2015) Machine learning-based augmented reality for improved surgical scene understanding. Computerized Medical Imaging and Graphics 41(Supplement C):55&#8211;60. ISSN 0895-6111. (Machine Learning in Medical Imaging)"/>

    <meta name="citation_reference" content="citation_journal_title=Adv Robot; citation_title=Scene classification from semantic labeling; citation_author=JC Rangel, M Cazorla, I Garcia-Varea, J Martinez-Gomez, E Fromont, M Sebban; citation_volume=30; citation_issue=11&#8211;12; citation_publication_date=2016; citation_pages=758-769; citation_doi=10.1080/01691864.2016.1164621; citation_id=CR28"/>

    <meta name="citation_reference" content="Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, Berg AC, Fei-Fei L (2015) Imagenet large scale visual recognition challenge. Int J Comput Vis 1&#8211;42. ISSN 0920-5691. 
                    https://doi.org/10.1007/s11263-015-0816-y
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Int J Semant Web Inf Syst; citation_title=Mobile ontologies: concept, development, usage, and business potential; citation_author=J Veijalainen; citation_volume=4; citation_publication_date=2008; citation_pages=20-34; citation_doi=10.4018/jswis.2008010102; citation_id=CR30"/>

    <meta name="citation_reference" content="Willems K, Smolders A, Brengman M, Luyten K, Schning J (2017) The path-to-purchase is paved with digital opportunities: an inventory of shopper-oriented retail technologies. Technol Forecast Soc Change 124(Supplement C):228&#8211;242. ISSN 0040-1625"/>

    <meta name="citation_reference" content="Wu J, Christensen H, Rehg JM et al (2009) Visual place categorization: problem, dataset, and algorithm. In: IEEE/RSJ international conference on intelligent robots and systems, 2009. IROS 2009. IEEE, pp 4763&#8211;4770"/>

    <meta name="citation_author" content="Edmanuel Cruz"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Sergio Orts-Escolano"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Francisco Gomez-Donoso"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Carlos Rizo"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Jose Carlos Rangel"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Higinio Mora"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_author" content="Miguel Cazorla"/>

    <meta name="citation_author_email" content="miguel.cazorla@ua.es"/>

    <meta name="citation_author_institution" content="Instituto Universitario de Investigaci&#243;n Inform&#225;tica, Universidad de Alicante, Alicante, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0338-3&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0338-3"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An augmented reality application for improving shopping experience in large retail stores"/>
        <meta property="og:description" content="In several large retail stores, such as malls, sport or food stores, the customer often feels lost due to the difficulty in finding a product. Although these large stores usually have visual signs to guide customers toward specific products, sometimes these signs are also hard to find and are not updated. In this paper, we propose a system that jointly combines deep learning and augmented reality techniques to provide the customer with useful information. First, the proposed system learns the visual appearance of different areas in the store using a deep learning architecture. Then, customers can use their mobile devices to take a picture of the area where they are located within the store. Uploading this image to the system trained for image classification, we are able to identify the area where the customer is located. Then, using this information and novel augmented reality techniques, we provide information about the area where the customer is located: route to another area where a product is available, 3D product visualization, user location, analytics, etc. The system developed is able to successfully locate a user in an example store with 98% accuracy. The combination of deep learning systems together with augmented reality techniques shows promising results toward improving user experience in retail/commerce applications: branding, advance visualization, personalization, enhanced customer experience, etc."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An augmented reality application for improving shopping experience in large retail stores | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0338-3","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Smart shopping, Deep learning, Augmented reality, Retail stores, User experience, Human–computer interaction, 3D visualization","kwrd":["Smart_shopping","Deep_learning","Augmented_reality","Retail_stores","User_experience","Human–computer_interaction","3D_visualization"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0338-3","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0338-3","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=338;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0338-3">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An augmented reality application for improving shopping experience in large retail stores
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0338-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0338-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">S.I. : Virtual Reality, Augmented Reality and Commerce</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-02-24" itemprop="datePublished">24 February 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An augmented reality application for improving shopping experience in large retail stores</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Edmanuel-Cruz" data-author-popup="auth-Edmanuel-Cruz">Edmanuel Cruz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sergio-Orts_Escolano" data-author-popup="auth-Sergio-Orts_Escolano">Sergio Orts-Escolano</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Francisco-Gomez_Donoso" data-author-popup="auth-Francisco-Gomez_Donoso">Francisco Gomez-Donoso</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Carlos-Rizo" data-author-popup="auth-Carlos-Rizo">Carlos Rizo</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jose_Carlos-Rangel" data-author-popup="auth-Jose_Carlos-Rangel">Jose Carlos Rangel</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Higinio-Mora" data-author-popup="auth-Higinio-Mora">Higinio Mora</a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-8591-0710"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-8591-0710</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Miguel-Cazorla" data-author-popup="auth-Miguel-Cazorla" data-corresp-id="c1">Miguel Cazorla<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Alicante" /><meta itemprop="address" content="0000 0001 2168 1800, grid.5268.9, Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">281</span>–<span itemprop="pageEnd">291</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3007 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0338-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In several large retail stores, such as malls, sport or food stores, the customer often feels lost due to the difficulty in finding a product. Although these large stores usually have visual signs to guide customers toward specific products, sometimes these signs are also hard to find and are not updated. In this paper, we propose a system that jointly combines deep learning and augmented reality techniques to provide the customer with useful information. First, the proposed system learns the visual appearance of different areas in the store using a deep learning architecture. Then, customers can use their mobile devices to take a picture of the area where they are located within the store. Uploading this image to the system trained for image classification, we are able to identify the area where the customer is located. Then, using this information and novel augmented reality techniques, we provide information about the area where the customer is located: route to another area where a product is available, 3D product visualization, user location, analytics, etc. The system developed is able to successfully locate a user in an example store with 98% accuracy. The combination of deep learning systems together with augmented reality techniques shows promising results toward improving user experience in retail/commerce applications: branding, advance visualization, personalization, enhanced customer experience, etc.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Key business leaders in the technology sector are currently committing to the trend of augmented reality, as is the case of the technology giants Apple, Google and Microsoft.</p><p>The current level of technological development is the result of several key factors such as hardware improvement and the increase in mobile devices with high computing capabilities. Moreover, recent trends such as Internet of the Things (IoT), deep learning (DL) and augmented reality (AR) approaches have also contributed to the way technological solutions are used and developed. AR consists of adding virtual information to the physical world, allowing the user to enrich their environment perception. The basic goal of an AR system is to enhance the user’s perception of an interaction with the real world through supplementing the real world with 3D virtual objects that appear to coexist in the same space as the real world (Azuma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–37" href="/article/10.1007/s10055-018-0338-3#ref-CR3" id="ref-link-section-d32891e425">2001</a>). Besides, industrial IoT and machine learning (ML) are two of the areas where the potential of AR can be visualized in the near future (Pauly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Pauly O, Diotte B, Fallavollita P, Weidert S, Euler E, Navab N (2015) Machine learning-based augmented reality for improved surgical scene understanding. Computerized Medical Imaging and Graphics 41(Supplement C):55–60. ISSN 0895-6111. (Machine Learning in Medical Imaging)" href="/article/10.1007/s10055-018-0338-3#ref-CR27" id="ref-link-section-d32891e428">2015</a>; Ahn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Ahn J, Williamson J, Gartrell M, Han R, Lv Q, Mishra S (2015) Supporting healthy grocery shopping via mobile augmented reality. ACM Trans Multimedia Comput Commun Appl 12(1s):16:1–16:24. ISSN 1551-6857. &#xA;                    https://doi.org/10.1145/2808207&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR1" id="ref-link-section-d32891e431">2015</a>; Ortiz-Catalan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Ortiz-Catalan M, Gumundsdttir RA, Kristoffersen MB, Zepeda-Echavarria A, Caine-Winterberger K, Kulbacka-Ortiz K, Widehammar C, Eriksson K, Stockselius A, Ragn C, Pihlar Z, Burger H, Hermansson L (2016) Phantom motor execution facilitated by machine learning and augmented reality as treatment for phantom limb pain: a single group, clinical trial in patients with chronic intractable phantom limb pain. Lancet 388(10062):2885–2894. ISSN 0140-6736" href="/article/10.1007/s10055-018-0338-3#ref-CR25" id="ref-link-section-d32891e434">2016</a>).</p><p>Nowadays global economy is driven by daily trading operations made by customers at stores (Goldman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Goldman A (2001) The transfer of retail formats into developing economies: the example of china. J Retail 77(2):221–242. ISSN 0022-4359. &#xA;                    https://doi.org/10.1016/S0022-4359(01)00044-6&#xA;                    &#xA;                  . &#xA;                    http://www.sciencedirect.com/science/article/pii/S0022435901000446&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR9" id="ref-link-section-d32891e440">2001</a>). Customers are permanently seeking to make the most of their incomes and so compare not only product quality but also the complete purchase experience (Lemon and Verhoef <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Lemon KN, Verhoef PC (2016) Understanding customer experience throughout the customer journey. J Mark 80(6):69–96" href="/article/10.1007/s10055-018-0338-3#ref-CR17" id="ref-link-section-d32891e443">2016</a>; Blazquez <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Blazquez M (2014) Fashion shopping in multichannel retail: the role of technology in enhancing the customer experience. Int J Electron Commer 18(4):97–116" href="/article/10.1007/s10055-018-0338-3#ref-CR5" id="ref-link-section-d32891e446">2014</a>). Thus, stores must be adaptive to this type of consumer. Knowledge about products is a crucial aspect that increasingly interests consumers. This knowledge may be accessed by digital means (Willems et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Willems K, Smolders A, Brengman M, Luyten K, Schning J (2017) The path-to-purchase is paved with digital opportunities: an inventory of shopper-oriented retail technologies. Technol Forecast Soc Change 124(Supplement C):228–242. ISSN 0040-1625" href="/article/10.1007/s10055-018-0338-3#ref-CR31" id="ref-link-section-d32891e449">2017</a>). Nevertheless, the digital experience has not yet been carefully considered by commercial stores. Therefore, we aim to apply AR techniques to enrich user experience when shopping for a product in a physical store.</p><p>Commercial stores are one of the sectors with the greatest possibilities to implement AR in the short term. Therefore, this work proposes the development of an AR mobile application that initially allows to determine the location inside a commercial store based on visual information coming from a monocular camera (color image). These images are captured using the mobile phone camera. Next, the mobile application guides the user to the location of a desired product as well as shows a virtual version of the product and related information.</p><p>The main intent of this research work is to improve shopping experience using current ML and AR techniques. In this direction, we proposed a system that enables shopping experience by showing AR models to the users directly at the store, so products that are boxed or not available can be seen as if the were physically there. Besides, the system provides user navigation, improving the localization of certain products at the store; this feature can be extremely convenient at large stores/warehouses.</p><p>The rest of the paper is organized as follows: First, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec2">2</a> the state of the art in the field is presented. Next, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec3">3</a> details a description of the proposal. This is followed by Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec10">4</a>, where the procedures for testing the proposed approach are described and the results of the experiments are presented. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec15">6</a> includes the discussion and conclusions of the work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">State of the art</h2><div class="c-article-section__content" id="Sec2-content"><p>
Grewal et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Grewal D, Roggeveen AL, Nordflt J (2017) The future of retailing. J Retail 93(1):1–6. ISSN 0022-4359. &#xA;                    https://doi.org/10.1016/j.jretai.2016.12.008&#xA;                    &#xA;                  . &#xA;                    http://www.sciencedirect.com/science/article/pii/S0022435916300872&#xA;                    &#xA;                  . (The Future of Retailing)" href="/article/10.1007/s10055-018-0338-3#ref-CR10" id="ref-link-section-d32891e482">2017</a>) predicts that AR is one of the emerging applications that will define the future of retailing. Thanks to the use of this technology, the customer decision-making process will be improved, including interactive visualization of products and advertisement.</p><p>The use of AR inside large commercial areas (LCA) has not yet been exploited. This kind of areas needs a special attention for helping customers (Likavec et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Likavec S, Osborne F, Cena F (2008) Property-based semantic similarity and relatedness for improving recommendation accuracy and diversity. Int J Semant Web Inf Syst 11(4):1–40" href="/article/10.1007/s10055-018-0338-3#ref-CR19" id="ref-link-section-d32891e488">2008</a>). There exist different AR systems able to apply this technology to different scenarios (Carmigniani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Carmigniani J, Furht B, Anisetti M, Ceravolo P, Damiani E, Ivkovic M (2011) Augmented reality technologies, systems and applications. Multimed Tools Appl 51(1):341–377" href="/article/10.1007/s10055-018-0338-3#ref-CR6" id="ref-link-section-d32891e491">2011</a>). Although there are several LCAs that have stands where a customer can find products, the use of a mobile device could improve the customer experience (Veijalainen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Veijalainen J (2008) Mobile ontologies: concept, development, usage, and business potential. Int J Semant Web Inf Syst 4:20–34" href="/article/10.1007/s10055-018-0338-3#ref-CR30" id="ref-link-section-d32891e494">2008</a>). A comprehensive review of mobile AR is found in Chatzopoulos et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Chatzopoulos D, Bermejo C, Huang Z, Hui P (2017) Mobile augmented reality survey: from where we are to where we go. IEEE Access 5:6917–6950. ISSN 2169-3536. &#xA;                    https://doi.org/10.1109/ACCESS.2017.2698164&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR7" id="ref-link-section-d32891e497">2017</a>). A large gap in this area is the use of AR to guide the customer inside a LCA, although this has been used for navigation in outdoor applications (Narzt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Narzt W, Pomberger G, Ferscha A, Kolb D, Müller R, Wieghardt J, Hörtner H, Lindinger C (2006) Augmented reality navigation systems. Univ Access Inf Soc 4(3):177–187" href="/article/10.1007/s10055-018-0338-3#ref-CR23" id="ref-link-section-d32891e500">2006</a>).</p><p>The proposed system uses a scene classification method for user location. The scene classification or indoor place categorization problem may be defined as the problem of classifying an image as belonging to a scene category from a set of predefined labels (Maron and Ratan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Maron O, Ratan AL (1998) Multiple-instance learning for natural scene classification. ICML 98:341–349" href="/article/10.1007/s10055-018-0338-3#ref-CR20" id="ref-link-section-d32891e506">1998</a>). Scene classifiers are also helpful for specific tasks (Martínez-Gómez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Martínez-Gómez J, Fernández-Caballero A, García-Varea I, Rodríguez L, Romero-González C (2014) A taxonomy of vision systems for ground mobile robots. Int J Adv Robot Syst 11:1–11" href="/article/10.1007/s10055-018-0338-3#ref-CR21" id="ref-link-section-d32891e509">2014</a>) such as autonomous navigation, high-level planning, simultaneous location and mapping (SLAM), or human–robot interaction (HRI).</p><p>Scene classification is commonly addressed as a supervised classification process (Wu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Wu J, Christensen H, Rehg JM et al (2009) Visual place categorization: problem, dataset, and algorithm. In: IEEE/RSJ international conference on intelligent robots and systems, 2009. IROS 2009. IEEE, pp 4763–4770" href="/article/10.1007/s10055-018-0338-3#ref-CR32" id="ref-link-section-d32891e515">2009</a>), where input data correspond to perceptions, and classes to semantic scene categories. Current approaches are based on a two-stage building process: (a) selecting the appropriate descriptors to be extracted from perceptions and (b) choosing a classification model to be able to deal with the extracted descriptors.</p><p>Relying on the use of images as the main perception mechanism, the descriptor generation problem is addressed by using computer vision techniques. In this process, the organization of the data extracted from the images plays an important role. This is presented clearly in two of the most widely used approaches: the Bag-of-Words (BoW) (Csurka et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Csurka G, Dance CR, Fan L, Willamowski J, Bray C (2004) Visual categorization with bags of keypoints. In: Workshop on statistical learning in computer vision, ECCV, pp 1–22" href="/article/10.1007/s10055-018-0338-3#ref-CR8" id="ref-link-section-d32891e522">2004</a>; Martinez-Gomez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Martinez-Gomez J, Garcia-Varea I, Cazorla M, Morell V (2015) Vidrilo: the visual and depth robot indoor localization with objects information dataset. Int J Robot Res 34(14):1681–1687. &#xA;                    https://doi.org/10.1177/0278364915596058&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR22" id="ref-link-section-d32891e525">2015</a>) and the spatial pyramid (Lazebnik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Lazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. In: Proceedings of the 2006 IEEE computer society conference on computer vision and pattern recognition—CVPR ’06, Washington, DC, USA, vol 2. IEEE Computer Society, pp 2169–2178. ISBN 0-7695-2597-0. &#xA;                    https://doi.org/10.1109/CVPR.2006.68&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR15" id="ref-link-section-d32891e528">2006</a>). These two approaches allow the generation of fixed-dimensionality descriptors, required for most of the state-of-the-art classification models, built from any type of local features.</p><p>The use of DL techniques is considered a notable milestone in the research areas of computer vision and robotics (LeCun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="LeCun Y, Kavukcuoglu K, Farabet C (2010) Convolutional networks and applications in vision. In: Proceedings of IEEE international symposium on circuits and systems (ISCAS), 2010. IEEE, pp 253–256" href="/article/10.1007/s10055-018-0338-3#ref-CR16" id="ref-link-section-d32891e534">2010</a>). DL provides classifiers capable not only of classifying data but also of automatically extracting intermediate features. This technique has been applied to image tagging with surprising results.</p><p>For instance, since 2012 the winners of ImageNet competition  (Russakovsky et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, Berg AC, Fei-Fei L (2015) Imagenet large scale visual recognition challenge. Int J Comput Vis 1–42. ISSN 0920-5691. &#xA;                    https://doi.org/10.1007/s11263-015-0816-y&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR29" id="ref-link-section-d32891e540">2015</a>) have use convolutional neural network (CNN), for example Alexnet (Krizhevsky et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ (eds) Proceedings of the 25th international conference on neural information processing systems. NIPS'12, vol 1. Curran Associates Inc., USA, pp 1097–1105" href="/article/10.1007/s10055-018-0338-3#ref-CR14" id="ref-link-section-d32891e543">2012</a>) and Clarifai (Clarifai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Clarifai (2015) Clarifai: Amplifying Intelligence with a Vision. &#xA;                    http://www.clarifai.com/&#xA;                    &#xA;                  . Accessed 1 June 2017" href="/article/10.1007/s10055-018-0338-3#ref-CR100" id="ref-link-section-d32891e546">2015</a>). In addition to very large amounts of annotated data for training, DL requires high processing capabilities for classification. Recently, in Rangel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Rangel JC, Cazorla M, Garcia-Varea I, Martinez-Gomez J, Fromont E, Sebban M (2016) Scene classification from semantic labeling. Adv Robot 30(11–12):758–769. &#xA;                    https://doi.org/10.1080/01691864.2016.1164621&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR28" id="ref-link-section-d32891e549">2016</a>) it has been demonstrated that the use of DL techniques improves results in scene classification.</p><p>To the best of our knowledge, DL techniques have only been used for feature extraction and feature matching to improve current AR systems. In Akgul et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Akgul O, Penekli HI, Genc Y (2016) Applying deep learning in augmented reality tracking. In: 12th international conference on signal-image technology internet-based systems (SITIS), 2016, pp 47–54. &#xA;                    https://doi.org/10.1109/SITIS.2016.17&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR2" id="ref-link-section-d32891e555">2016</a>) a DL technique is used to improve the tracking of a given target for AR applications.</p><p>In this work, we propose a novel system that improves customer experience through the use of AR techniques. To do so, we utilize state-of-the-art artificial intelligence techniques that help create a smart experience through the use of an AR application. Thanks to the use of a DL-based classifier, we are able to accurately locate a customer within a large retail store. Based on this information, we can create a rich, customized experience that helps the customer through the shopping process.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">System description</h2><div class="c-article-section__content" id="Sec3-content"><p>The system is composed of a mobile phone application and a web service that performs the classification task; it takes an image of an aisle and returns a label. Therefore, the mobile application will communicate with the server to obtain information related to where the user is positioned. The development of this system is divided into two phases, the first for collecting the data and training the classifier and the second for linking the trained classifier with a mobile device to test the proposed approach. Further details of these two main stages are provided below. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig1">1</a> shows an overall description of the proposed system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Proposal flowchart. The user is located in (Bikes) and the item they are looking for is in (Golf). The path and location are computed by our mobile application</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec4">User localization using a DL-based classifier on a remote web service</h3><p>In order to suggest where to find a certain item, first we have to locate the user. To do so, the customer must use our mobile application to take a picture of the aisle where they are currently located and the application will detect their location to suggest the path to the item they are looking for. To locate a user from a picture of an aisle, we rely on CNNs. These kinds of networks are able to classify an input image based on their features and assign them a category.</p><p>Finally, the classification model is integrated in a web service in order to make it available to external clients. As making predictions with a CNN is computationally demanding, it is too costly to implement it on a mobile device. Thus, when a user takes the image for location, the image is resized and sent to the web service. It classifies the image and returns the aisle where the user is located.</p><p>Next, we present the details of the user location subsystem.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Architecture</h4><p>To locate a person in the store given a picture, we rely on a ResNet50 (He et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="He K, Zhang X, Ren S, Sun J (2015) Deep residual learning for image recognition. CoRR. &#xA;                    arXiv:1512.03385&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR11" id="ref-link-section-d32891e609">2015</a>) deep convolutional neural network.</p><p>The ResNet50 DL architecture is currently the state-of-the-art CNN in image recognition, achieving a top-1 error (22.85%) on the ImageNet validation split. The main feature of this architecture is the inclusion of the “residual” term. It consists of the aggregation of the input image to the output image of a convolution block. As a result, the output of a convolution block may be seen as the input image where the features activated by the filters are highlighted. In contrast, the output of a convolution layer in a default CNN is only the result of the neurons activation. If a neuron is not triggered in a certain region of the input image, the output remains with lower or null activation values. When the network computes the weights update in the backpropagation stage, the values in non-activated regions lead to very low upgrades, eventually even resulting in not upgrade at all, which causes the learning to block. This issue is known as the vanishing gradient problem (Bengio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Bengio Y, Simard P, Frasconi P (1994) Learning long-term dependencies with gradient descent is difficult. Trans Neural Netw 5(2):157–166" href="/article/10.1007/s10055-018-0338-3#ref-CR4" id="ref-link-section-d32891e615">1994</a>; Pascanu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Pascanu R, Mikolov T, Bengio Y (2012) On the difficulty of training recurrent neural networks. ArXiv e-prints" href="/article/10.1007/s10055-018-0338-3#ref-CR26" id="ref-link-section-d32891e618">2012</a>). The inclusion of the “residual” term helps fighting the vanishing gradient problem and allows the design of even deeper architectures. Currently, the best performer on several tasks of the ImageNet challenge is based on the “residual” approach introduced by ResNet.</p><p>Our proposal makes use of a default ResNet50, with a minor modification: The number of neurons was modified to 20 in the final fully connected layer in order to fit our problem.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Dataset and training</h4><p>In order to train the CNN, it is necessary to have labeled sequences of images belonging to the different aisles in the store. Hence, we recorded a video while passing through every aisle in a large sports store. The store was open at the time the dataset was recorded, so besides the items, there are other customers in the store. This is desired feature because will force the learned model to deal with partially occluded scenes as we cannot expect a full view of an aisles or shelves with no other customers in it. Three different human-operated RGB cameras were used in this process. The videos were split into clips and hand-labeled according to the aisle in which a clip was recorded. Then, the frames of every clip were extracted and resized to <span class="mathjax-tex">\(224 \times 224\)</span> in order to feed the CNN. Blurry frames were manually selected and excluded from the dataset. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0338-3#Tab1">1</a> shows the description of each class and the number of samples. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig2">2</a> shows a view of the environment (large retail store) where the proposed system was tested.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Description of the aisles that our localization system can detect</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0338-3/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Environment where the experiments were carried out. Only 4 of 20 categories are shown</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The samples were grouped by label, shuffled and split into training and test subsets with a <span class="mathjax-tex">\(20\%\)</span> test split ratio.</p><p>The time investment on the dataset creation is distributed as follows: First, the time committed to film the aisles depends on the size of the store. As no special equipment (besides the RGB camera) or special requirements are needed, the time invested in this step is the time you spend wandering all the aisles of the store once. Then, the labeling process that consists on extracting the frames of the videos and assigning them a class is also a low time-consuming task. As extracting the frames from a video is a fast and automatic task, we find the time of this task negligible. Also, assigning a category for each frame is also quite straight-forward as there should be a video per aisle. By far, the most time-consuming is the model training process. This could take from several hours to a few days, depending on the hardware and the data. Nonetheless, this is an automatic process and no personnel is involved but for the configuration of the parameters at the beginning of the training process.</p><p>Overall, we think that the investment of time is negligible given the potential value that the mobile application would provide.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Dealing with environment changes (visual appearance)</h4><p>As a matter of fact, changes in the organization of the shelves, the modification of the packaging or products, or other may modify the visual appearance of the aisles. As the localization procedure relies on visual features, these events will make the system fail.</p><p>If there are slight changes in the environment (like changing a certain model of a shoe for another model), the deep neural network architecture is inherently able to deal with them. If there are major changes (like complete aisle reorganization or the addition of a new category of items), it is required to retrain the network. In this case, two methodologies for new data acquisition can be followed: If the major changes are gradually introduced, the users would provide enough training data when the localization fails and they manually choose the location, as explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec9">3.3</a>. If there are dramatic and sudden changes, a new data acquisition procedure is needed, so the affected aisled must be filmed. Either cases, the network must be retrained. Besides, we are considering implementing continuous learning techniques, where data acquired by the user is leveraged for incremental training [Learning without Forgetting (Li and Hoiem <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Li Z, Hoiem D (2016) Learning without forgetting. Springer International Publishing, Cham, pp 614–629. ISBN 978-3-319-46493-0" href="/article/10.1007/s10055-018-0338-3#ref-CR18" id="ref-link-section-d32891e1160">2016</a>)].</p><h3 class="c-article__sub-heading" id="Sec8">Augmented reality mobile application</h3><p>In this work, we have developed an AR mobile application that utilizes DL techniques to provide a helpful experience when shopping in large stores.</p><p>Customers frequently feel lost when shopping in big stores. Thanks to the developed mobile application, users can easily locate themselves in the store, as explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec4">3.1</a>, and use mobile application guidance to find and retrieve information about the product they are looking for. Using AR techniques, we are also able to provide the user with a richer experience, being able to visualize a 3D model of different products available in the store. Thanks to current AR techniques, we are able to blend realistic off-line captured 3D models with the real world. This feature allows the user to know the availability of the different types or models of the products, e.g., size and colors. It also helps companies reduce the amount of products customers return every year, as well avoids users having to remove the product from its packaging before buying it. According to the National Retail Federation (NRF),<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> Americans returned <span class="mathjax-tex">\(\$260\)</span> billion in merchandise in 2015, a 66% increase from 5 years ago. The reason for these returns is, in the majority of the cases, the lack of knowledge of the product by the final customers. Therefore, using our mobile application, customers have the option of knowing several features of the product before its purchase, avoiding the returning of the product to the store. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig3">3</a> shows a visual example of how AR is used in our application by displaying a 3D model of a product and enabling the user to visualize the different customization choices of the item. Also, the user can rotate the model, perform zooming, change its features from the available pool or even check the in-store availability.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Example of 3D models displayed in a store using AR techniques (markerless tracking)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">Mobile application workflow</h3><p>As stated before, our mobile application is able to automatically detect where the user is located and provide guidance to the item that the user is looking for. Furthermore, the user can project and manipulate virtual items in an AR fashion in order to check whether the item satisfies their personal preferences (such as size or color) or not.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Mobile Application workflow</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The mobile application workflow, which is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig4">4</a>, is as follows:</p><p>Once the users launch the mobile application, they must provide an image of the aisle where they are currently located using the mobile application. This image is sent to the web server, and then the category for the image is predicted. The category is inferred from available aisles in the store taking advantage of DL methods, as explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec4">3.1</a>. The category assigned by the web server to the image determines the location of the user in the store.</p><p>The localization result is reflected on the mobile application UI and then, if it is correct, it asks the user what kind of product they are interested in. The mobile application features a complete searcher with filtering and different sorting option in order to enable a fast item searching process. Once an item is selected, the user can navigate to that object or project it in an AR fashion. The user can interact with the 3D model of the object displayed on the mobile device screen by rotating it, changing its color or singular features of each item available for purchasing. Then, the user can navigate to the item if the user chooses to. At all times, the user can query the in-store availability or customization options of the item.</p><p>The navigation screen projects an arrow in an AR manner and behaves like a compass, pointing toward the next waypoint. Finally, if the users follow the directions, it will lead them to the location of the object they have chosen. In this point, the user could interact with a 3D model of the item, as explained before.</p><p>As stated earlier, the first step of the mobile application is the automatic localization of the user. In case that this process fails, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig10">10</a>, the user could manually localize itself in order to use the guidance capabilities of the mobile application. Since the automatic localization is performed from an image, if it is not accurate and the user must use the manual localization, both the image and the correct localization chosen by the user are sent to the server for further analysis.</p><p>As explained, our system mixes techniques from AR and DL to provide the user with a complete experience when visiting a store and purchasing a product.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Experiments</h2><div class="c-article-section__content" id="Sec10-content"><h3 class="c-article__sub-heading" id="Sec11">Experimental scenario</h3><p>The experiments in this work were carried out using the aforementioned dataset. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig5">5</a> shows representative images for 8 of the 20 available categories in the store and training dataset.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Examples of 8 from the 20 different sections in the store</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec12">User localization using deep learning</h3><p>The architecture described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec5">3.1.1</a> was trained to take a picture of the users surroundings as an input and to infer which aisle appears in the input image. The details about the dataset we used for training and testing are shown in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec6">3.1.2</a>. As previously mentioned, the dataset was split in training and testing using an 80 and <span class="mathjax-tex">\(20\%\)</span> ratio, respectively. The optimizer of choice was Adam (Kingma and Ba <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. CoRR. &#xA;                    arXiv:1412.6980&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR12" id="ref-link-section-d32891e1359">2014</a>) with a learning rate of 0.00001. Test loss and accuracy rates were used as early stopping criteria. At the end of the training process, it achieved a <span class="mathjax-tex">\(98.97\%\)</span> test accuracy. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig6">6</a> shows the confusion matrix related to this experiment.</p><p>The model generated is implemented as a web service to perform inference from remote devices. The system only takes 0.014 s at runtime, so it can be comfortably used in real-time applications. Consuming the web service from a mobile phone adds some latency. We measured this latency on different mobile phones and under different load conditions (WiFi, 3G/4G network) and, on average, it adds <span class="mathjax-tex">\(\sim 1.5\)</span> s of latency.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Confusion matrix for the test split</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>All timings, training procedures and results were obtained by conducting the experiments in the following test setup: Intel Core i5-3570 with 8 GB of Kingston HyperX 1333 MHz DDR3 RAM on an Asus P8H77-M PRO motherboard (Intel H77 chipset). Secondary storage was provided by a Seagate Desktop HDD 3 TB. Additionally, the system included a NVIDIA GTX1080Ti GPU used for training and inference.</p><p>The framework of choice was Keras 2.0.1 with Tensorflow 1.2.0 as the backbone. CUDA 8.0 and cuDNN were enabled.</p><p>The experiments were carried out using a real mobile phone device inside the store. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig7">7</a> (left) shows the mobile application interface for taking a picture of the current location in the store, asking the user to locate himself in the place.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Mobile application screen captures</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig7">7</a> (right) shows a product being displayed with AR for the virtual interaction with the customer inside the store.</p><p>The mobile application has been tested on several mobile devices with different hardware (graphics capabilities) specifications. The mobile application was developed using an AR library (Kudan<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>) which provides us with markerless tracking of the scene (Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the 8th IEEE international symposium on mixed and augmented reality, 2009, ISMAR ’09, Washington, DC, USA. IEEE Computer Society, pp 83–86. ISBN 978-1-4244-5390-0. &#xA;                    https://doi.org/10.1109/ISMAR.2009.5336495&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR13" id="ref-link-section-d32891e1485">2009</a>; Newcombe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Newcombe RA, Lovegrove SJ, Davison AJ (2011) DTAM: dense tracking and mapping in real-time. In: Proceedings of the international conference on computer vision, 2011, ICCV ’11, Washington, DC, USA. IEEE Computer Society, pp 2320–2327. ISBN 978-1-4577-1101-5. &#xA;                    https://doi.org/10.1109/ICCV.2011.6126513&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0338-3#ref-CR24" id="ref-link-section-d32891e1488">2011</a>). The mobile application was developed using Unity3D and tested on mobile phones based on the Android and iOS operating systems.</p><h3 class="c-article__sub-heading" id="Sec13">Results</h3><p>We tested the proposed mobile application in a real scenario. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig8">8</a> shows several examples in which the mobile application provides the category where the customer is located.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Several results obtained by the proposed mobile application in a real scenario</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The mobile application developed also provides the user with guidance, being able to guide them to the aisle where the product they are looking for is located. Instructions are provided using AR techniques on the store floor. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig9">9</a> shows an example of the system developed for customer guidance. Guidance behavior is similar to the way a compass works, the arrow turns looking in the direction that the customer has to follow to find the right path as explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0338-3#Sec9">3.3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Guidance example</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Discussion</h2><div class="c-article-section__content" id="Sec14-content"><p>Having an Internet connection is one of the main limitations of the presented work. Since the use of DL techniques requires high-performance computing to process input data in real time, it is not possible to perform the image recognition process on the mobile phone. Besides, it would dramatically increase power consumption. The second limitation of the presented system is related to ML part of the presented system. Since the DL model was trained using product and store images acquired at an specific time, if the visual appearance of these products changes, the system would need to be retrained with new acquired images. However, this is a problem that is being currently studied and there already exists multiple approaches to perform Learning without forgetting (Li and Hoiem <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Li Z, Hoiem D (2016) Learning without forgetting. Springer International Publishing, Cham, pp 614–629. ISBN 978-3-319-46493-0" href="/article/10.1007/s10055-018-0338-3#ref-CR18" id="ref-link-section-d32891e1554">2016</a>). In this way, the DL technique continuously retrained adding new data without forgetting previous knowledge.</p><p>We have detected that our system has some failure cases. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0338-3#Fig10">10</a> shows an example of this. These cases occur when the user takes a picture and the system provides a different localization (corridor) from the real one. In that case, the system is unable to guide the customer correctly. We notice that this situation only happens 1 out of 20 predictions. We mitigate this error including the following: When the predicted localization is provided to the customer, he/she is asked if that is correct. If the localization is wrong and the customer confirms the localization, the system would guide to an incorrect destination. But, if the customer knows his/her correct localization and could rectify it, then the system uses that information to modify the learned model and improve the future predictions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0338-3/MediaObjects/10055_2018_338_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Failure cases in a real scenario</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0338-3/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Conclusions</h2><div class="c-article-section__content" id="Sec15-content"><p>In this work, we present a mobile application using AR to improve the shopping experience in large retail stores. The contributions of this work are manifold. First, we have combined DL techniques to locate the customer in the store. Thus, we are able to guide the customer toward the place where the desired product is located. Second, using AR, the customer is able to visualize the products and models in the store. The customer is not only able to visualize a product but also to get useful information related to the product, specifications, other available models, sizes and so on.</p><p>For future works, we aim to provide a web application where a given store can upload images of their store and the system will be able to reconfigure itself to work in that new environment. Moreover, we plan to study different compression algorithms for efficient storage and transmission of 3D models using mobile networks.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p><a href="https://nrf.com/sites/default/files/Images/Media%20Center/NRF%20Retail%20Return%20Fraud%20Final_0.pdf">https://nrf.com/sites/default/files/Images/Media Center/NRF Retail Return Fraud Final_0.pdf</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p><a href="https://www.kudan.eu/">https://www.kudan.eu/</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ahn J, Williamson J, Gartrell M, Han R, Lv Q, Mishra S (2015) Supporting healthy grocery shopping via mobile a" /><p class="c-article-references__text" id="ref-CR1">Ahn J, Williamson J, Gartrell M, Han R, Lv Q, Mishra S (2015) Supporting healthy grocery shopping via mobile augmented reality. ACM Trans Multimedia Comput Commun Appl 12(1s):16:1–16:24. ISSN 1551-6857. <a href="https://doi.org/10.1145/2808207">https://doi.org/10.1145/2808207</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Akgul O, Penekli HI, Genc Y (2016) Applying deep learning in augmented reality tracking. In: 12th internationa" /><p class="c-article-references__text" id="ref-CR2">Akgul O, Penekli HI, Genc Y (2016) Applying deep learning in augmented reality tracking. In: 12th international conference on signal-image technology internet-based systems (SITIS), 2016, pp 47–54. <a href="https://doi.org/10.1109/SITIS.2016.17">https://doi.org/10.1109/SITIS.2016.17</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. MacIntyre, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. " /><p class="c-article-references__text" id="ref-CR3">Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–37</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963459" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=6&amp;pages=34-37&amp;publication_year=2001&amp;author=Azuma%2CR&amp;author=Baillot%2CY&amp;author=Behringer%2CR&amp;author=Feiner%2CS&amp;author=Julier%2CS&amp;author=MacIntyre%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Bengio, P. Simard, P. Frasconi, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Bengio Y, Simard P, Frasconi P (1994) Learning long-term dependencies with gradient descent is difficult. Tran" /><p class="c-article-references__text" id="ref-CR4">Bengio Y, Simard P, Frasconi P (1994) Learning long-term dependencies with gradient descent is difficult. Trans Neural Netw 5(2):157–166</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F72.279181" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult&amp;journal=Trans%20Neural%20Netw&amp;volume=5&amp;issue=2&amp;pages=157-166&amp;publication_year=1994&amp;author=Bengio%2CY&amp;author=Simard%2CP&amp;author=Frasconi%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Blazquez, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Blazquez M (2014) Fashion shopping in multichannel retail: the role of technology in enhancing the customer ex" /><p class="c-article-references__text" id="ref-CR5">Blazquez M (2014) Fashion shopping in multichannel retail: the role of technology in enhancing the customer experience. Int J Electron Commer 18(4):97–116</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2753%2FJEC1086-4415180404" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fashion%20shopping%20in%20multichannel%20retail%3A%20the%20role%20of%20technology%20in%20enhancing%20the%20customer%20experience&amp;journal=Int%20J%20Electron%20Commer&amp;volume=18&amp;issue=4&amp;pages=97-116&amp;publication_year=2014&amp;author=Blazquez%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Carmigniani, B. Furht, M. Anisetti, P. Ceravolo, E. Damiani, M. Ivkovic, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Carmigniani J, Furht B, Anisetti M, Ceravolo P, Damiani E, Ivkovic M (2011) Augmented reality technologies, sy" /><p class="c-article-references__text" id="ref-CR6">Carmigniani J, Furht B, Anisetti M, Ceravolo P, Damiani E, Ivkovic M (2011) Augmented reality technologies, systems and applications. Multimed Tools Appl 51(1):341–377</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11042-010-0660-6" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20technologies%2C%20systems%20and%20applications&amp;journal=Multimed%20Tools%20Appl&amp;volume=51&amp;issue=1&amp;pages=341-377&amp;publication_year=2011&amp;author=Carmigniani%2CJ&amp;author=Furht%2CB&amp;author=Anisetti%2CM&amp;author=Ceravolo%2CP&amp;author=Damiani%2CE&amp;author=Ivkovic%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chatzopoulos D, Bermejo C, Huang Z, Hui P (2017) Mobile augmented reality survey: from where we are to where w" /><p class="c-article-references__text" id="ref-CR7">Chatzopoulos D, Bermejo C, Huang Z, Hui P (2017) Mobile augmented reality survey: from where we are to where we go. IEEE Access 5:6917–6950. ISSN 2169-3536. <a href="https://doi.org/10.1109/ACCESS.2017.2698164">https://doi.org/10.1109/ACCESS.2017.2698164</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Clarifai (2015) Clarifai: Amplifying Intelligence with a Vision. http://www.clarifai.com/. Accessed 1 June 201" /><p class="c-article-references__text" id="ref-CR100">Clarifai (2015) Clarifai: Amplifying Intelligence with a Vision. <a href="http://www.clarifai.com/">http://www.clarifai.com/</a>. Accessed 1 June 2017</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Csurka G, Dance CR, Fan L, Willamowski J, Bray C (2004) Visual categorization with bags of keypoints. In: Work" /><p class="c-article-references__text" id="ref-CR8">Csurka G, Dance CR, Fan L, Willamowski J, Bray C (2004) Visual categorization with bags of keypoints. In: Workshop on statistical learning in computer vision, ECCV, pp 1–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Goldman A (2001) The transfer of retail formats into developing economies: the example of china. J Retail 77(2" /><p class="c-article-references__text" id="ref-CR9">Goldman A (2001) The transfer of retail formats into developing economies: the example of china. J Retail 77(2):221–242. ISSN 0022-4359. <a href="https://doi.org/10.1016/S0022-4359(01)00044-6">https://doi.org/10.1016/S0022-4359(01)00044-6</a>. <a href="http://www.sciencedirect.com/science/article/pii/S0022435901000446">http://www.sciencedirect.com/science/article/pii/S0022435901000446</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grewal D, Roggeveen AL, Nordflt J (2017) The future of retailing. J Retail 93(1):1–6. ISSN 0022-4359. https://" /><p class="c-article-references__text" id="ref-CR10">Grewal D, Roggeveen AL, Nordflt J (2017) The future of retailing. J Retail 93(1):1–6. ISSN 0022-4359. <a href="https://doi.org/10.1016/j.jretai.2016.12.008">https://doi.org/10.1016/j.jretai.2016.12.008</a>. <a href="http://www.sciencedirect.com/science/article/pii/S0022435916300872">http://www.sciencedirect.com/science/article/pii/S0022435916300872</a>. (<b>The Future of Retailing</b>)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="He K, Zhang X, Ren S, Sun J (2015) Deep residual learning for image recognition. CoRR. arXiv:1512.03385&#xA;      " /><p class="c-article-references__text" id="ref-CR11">He K, Zhang X, Ren S, Sun J (2015) Deep residual learning for image recognition. CoRR. <a href="http://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. CoRR. arXiv:1412.6980&#xA;                     " /><p class="c-article-references__text" id="ref-CR12">Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. CoRR. <a href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the 8th IEEE inte" /><p class="c-article-references__text" id="ref-CR13">Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the 8th IEEE international symposium on mixed and augmented reality, 2009, ISMAR ’09, Washington, DC, USA. IEEE Computer Society, pp 83–86. ISBN 978-1-4244-5390-0. <a href="https://doi.org/10.1109/ISMAR.2009.5336495">https://doi.org/10.1109/ISMAR.2009.5336495</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural networks. I" /><p class="c-article-references__text" id="ref-CR14">Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ (eds) Proceedings of the 25th international conference on neural information processing systems. NIPS'12, vol 1. Curran Associates Inc., USA, pp 1097–1105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: spatial pyramid matching for recognizing natural" /><p class="c-article-references__text" id="ref-CR15">Lazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. In: Proceedings of the 2006 IEEE computer society conference on computer vision and pattern recognition—CVPR ’06, Washington, DC, USA, vol 2. IEEE Computer Society, pp 2169–2178. ISBN 0-7695-2597-0. <a href="https://doi.org/10.1109/CVPR.2006.68">https://doi.org/10.1109/CVPR.2006.68</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LeCun Y, Kavukcuoglu K, Farabet C (2010) Convolutional networks and applications in vision. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR16">LeCun Y, Kavukcuoglu K, Farabet C (2010) Convolutional networks and applications in vision. In: Proceedings of IEEE international symposium on circuits and systems (ISCAS), 2010. IEEE, pp 253–256</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KN. Lemon, PC. Verhoef, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Lemon KN, Verhoef PC (2016) Understanding customer experience throughout the customer journey. J Mark 80(6):69" /><p class="c-article-references__text" id="ref-CR17">Lemon KN, Verhoef PC (2016) Understanding customer experience throughout the customer journey. J Mark 80(6):69–96</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1509%2Fjm.15.0420" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20customer%20experience%20throughout%20the%20customer%20journey&amp;journal=J%20Mark&amp;volume=80&amp;issue=6&amp;pages=69-96&amp;publication_year=2016&amp;author=Lemon%2CKN&amp;author=Verhoef%2CPC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li Z, Hoiem D (2016) Learning without forgetting. Springer International Publishing, Cham, pp 614–629. ISBN 97" /><p class="c-article-references__text" id="ref-CR18">Li Z, Hoiem D (2016) Learning without forgetting. Springer International Publishing, Cham, pp 614–629. ISBN 978-3-319-46493-0</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Likavec, F. Osborne, F. Cena, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Likavec S, Osborne F, Cena F (2008) Property-based semantic similarity and relatedness for improving recommend" /><p class="c-article-references__text" id="ref-CR19">Likavec S, Osborne F, Cena F (2008) Property-based semantic similarity and relatedness for improving recommendation accuracy and diversity. Int J Semant Web Inf Syst 11(4):1–40</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4018%2FIJSWIS.2015100101" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Property-based%20semantic%20similarity%20and%20relatedness%20for%20improving%20recommendation%20accuracy%20and%20diversity&amp;journal=Int%20J%20Semant%20Web%20Inf%20Syst&amp;volume=11&amp;issue=4&amp;pages=1-40&amp;publication_year=2008&amp;author=Likavec%2CS&amp;author=Osborne%2CF&amp;author=Cena%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Maron, AL. Ratan, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Maron O, Ratan AL (1998) Multiple-instance learning for natural scene classification. ICML 98:341–349" /><p class="c-article-references__text" id="ref-CR20">Maron O, Ratan AL (1998) Multiple-instance learning for natural scene classification. ICML 98:341–349</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple-instance%20learning%20for%20natural%20scene%20classification&amp;journal=ICML&amp;volume=98&amp;pages=341-349&amp;publication_year=1998&amp;author=Maron%2CO&amp;author=Ratan%2CAL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Martínez-Gómez, A. Fernández-Caballero, I. García-Varea, L. Rodríguez, C. Romero-González, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Martínez-Gómez J, Fernández-Caballero A, García-Varea I, Rodríguez L, Romero-González C (2014) A taxonomy of v" /><p class="c-article-references__text" id="ref-CR21">Martínez-Gómez J, Fernández-Caballero A, García-Varea I, Rodríguez L, Romero-González C (2014) A taxonomy of vision systems for ground mobile robots. Int J Adv Robot Syst 11:1–11</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.5772%2F58900" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20taxonomy%20of%20vision%20systems%20for%20ground%20mobile%20robots&amp;journal=Int%20J%20Adv%20Robot%20Syst&amp;volume=11&amp;pages=1-11&amp;publication_year=2014&amp;author=Mart%C3%ADnez-G%C3%B3mez%2CJ&amp;author=Fern%C3%A1ndez-Caballero%2CA&amp;author=Garc%C3%ADa-Varea%2CI&amp;author=Rodr%C3%ADguez%2CL&amp;author=Romero-Gonz%C3%A1lez%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Martinez-Gomez, I. Garcia-Varea, M. Cazorla, V. Morell, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Martinez-Gomez J, Garcia-Varea I, Cazorla M, Morell V (2015) Vidrilo: the visual and depth robot indoor locali" /><p class="c-article-references__text" id="ref-CR22">Martinez-Gomez J, Garcia-Varea I, Cazorla M, Morell V (2015) Vidrilo: the visual and depth robot indoor localization with objects information dataset. Int J Robot Res 34(14):1681–1687. <a href="https://doi.org/10.1177/0278364915596058">https://doi.org/10.1177/0278364915596058</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F0278364915596058" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vidrilo%3A%20the%20visual%20and%20depth%20robot%20indoor%20localization%20with%20objects%20information%20dataset&amp;journal=Int%20J%20Robot%20Res&amp;doi=10.1177%2F0278364915596058&amp;volume=34&amp;issue=14&amp;pages=1681-1687&amp;publication_year=2015&amp;author=Martinez-Gomez%2CJ&amp;author=Garcia-Varea%2CI&amp;author=Cazorla%2CM&amp;author=Morell%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Narzt, G. Pomberger, A. Ferscha, D. Kolb, R. Müller, J. Wieghardt, H. Hörtner, C. Lindinger, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Narzt W, Pomberger G, Ferscha A, Kolb D, Müller R, Wieghardt J, Hörtner H, Lindinger C (2006) Augmented realit" /><p class="c-article-references__text" id="ref-CR23">Narzt W, Pomberger G, Ferscha A, Kolb D, Müller R, Wieghardt J, Hörtner H, Lindinger C (2006) Augmented reality navigation systems. Univ Access Inf Soc 4(3):177–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10209-005-0017-5" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20navigation%20systems&amp;journal=Univ%20Access%20Inf%20Soc&amp;volume=4&amp;issue=3&amp;pages=177-187&amp;publication_year=2006&amp;author=Narzt%2CW&amp;author=Pomberger%2CG&amp;author=Ferscha%2CA&amp;author=Kolb%2CD&amp;author=M%C3%BCller%2CR&amp;author=Wieghardt%2CJ&amp;author=H%C3%B6rtner%2CH&amp;author=Lindinger%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Newcombe RA, Lovegrove SJ, Davison AJ (2011) DTAM: dense tracking and mapping in real-time. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR24">Newcombe RA, Lovegrove SJ, Davison AJ (2011) DTAM: dense tracking and mapping in real-time. In: Proceedings of the international conference on computer vision, 2011, ICCV ’11, Washington, DC, USA. IEEE Computer Society, pp 2320–2327. ISBN 978-1-4577-1101-5. <a href="https://doi.org/10.1109/ICCV.2011.6126513">https://doi.org/10.1109/ICCV.2011.6126513</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ortiz-Catalan M, Gumundsdttir RA, Kristoffersen MB, Zepeda-Echavarria A, Caine-Winterberger K, Kulbacka-Ortiz " /><p class="c-article-references__text" id="ref-CR25">Ortiz-Catalan M, Gumundsdttir RA, Kristoffersen MB, Zepeda-Echavarria A, Caine-Winterberger K, Kulbacka-Ortiz K, Widehammar C, Eriksson K, Stockselius A, Ragn C, Pihlar Z, Burger H, Hermansson L (2016) Phantom motor execution facilitated by machine learning and augmented reality as treatment for phantom limb pain: a single group, clinical trial in patients with chronic intractable phantom limb pain. Lancet 388(10062):2885–2894. ISSN 0140-6736</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pascanu R, Mikolov T, Bengio Y (2012) On the difficulty of training recurrent neural networks. ArXiv e-prints" /><p class="c-article-references__text" id="ref-CR26">Pascanu R, Mikolov T, Bengio Y (2012) On the difficulty of training recurrent neural networks. ArXiv e-prints</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pauly O, Diotte B, Fallavollita P, Weidert S, Euler E, Navab N (2015) Machine learning-based augmented reality" /><p class="c-article-references__text" id="ref-CR27">Pauly O, Diotte B, Fallavollita P, Weidert S, Euler E, Navab N (2015) Machine learning-based augmented reality for improved surgical scene understanding. Computerized Medical Imaging and Graphics 41(Supplement C):55–60. ISSN 0895-6111. (<b>Machine Learning in Medical Imaging</b>)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC. Rangel, M. Cazorla, I. Garcia-Varea, J. Martinez-Gomez, E. Fromont, M. Sebban, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Rangel JC, Cazorla M, Garcia-Varea I, Martinez-Gomez J, Fromont E, Sebban M (2016) Scene classification from s" /><p class="c-article-references__text" id="ref-CR28">Rangel JC, Cazorla M, Garcia-Varea I, Martinez-Gomez J, Fromont E, Sebban M (2016) Scene classification from semantic labeling. Adv Robot 30(11–12):758–769. <a href="https://doi.org/10.1080/01691864.2016.1164621">https://doi.org/10.1080/01691864.2016.1164621</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F01691864.2016.1164621" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scene%20classification%20from%20semantic%20labeling&amp;journal=Adv%20Robot&amp;doi=10.1080%2F01691864.2016.1164621&amp;volume=30&amp;issue=11%E2%80%9312&amp;pages=758-769&amp;publication_year=2016&amp;author=Rangel%2CJC&amp;author=Cazorla%2CM&amp;author=Garcia-Varea%2CI&amp;author=Martinez-Gomez%2CJ&amp;author=Fromont%2CE&amp;author=Sebban%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, Berg AC, " /><p class="c-article-references__text" id="ref-CR29">Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, Berg AC, Fei-Fei L (2015) Imagenet large scale visual recognition challenge. Int J Comput Vis 1–42. ISSN 0920-5691. <a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Veijalainen, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Veijalainen J (2008) Mobile ontologies: concept, development, usage, and business potential. Int J Semant Web " /><p class="c-article-references__text" id="ref-CR30">Veijalainen J (2008) Mobile ontologies: concept, development, usage, and business potential. Int J Semant Web Inf Syst 4:20–34</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4018%2Fjswis.2008010102" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mobile%20ontologies%3A%20concept%2C%20development%2C%20usage%2C%20and%20business%20potential&amp;journal=Int%20J%20Semant%20Web%20Inf%20Syst&amp;volume=4&amp;pages=20-34&amp;publication_year=2008&amp;author=Veijalainen%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Willems K, Smolders A, Brengman M, Luyten K, Schning J (2017) The path-to-purchase is paved with digital oppor" /><p class="c-article-references__text" id="ref-CR31">Willems K, Smolders A, Brengman M, Luyten K, Schning J (2017) The path-to-purchase is paved with digital opportunities: an inventory of shopper-oriented retail technologies. Technol Forecast Soc Change 124(Supplement C):228–242. ISSN 0040-1625</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu J, Christensen H, Rehg JM et al (2009) Visual place categorization: problem, dataset, and algorithm. In: IE" /><p class="c-article-references__text" id="ref-CR32">Wu J, Christensen H, Rehg JM et al (2009) Visual place categorization: problem, dataset, and algorithm. In: IEEE/RSJ international conference on intelligent robots and systems, 2009. IROS 2009. IEEE, pp 4763–4770</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0338-3-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work has been supported by the Spanish Government TIN2016-76515-R Grant, supported with Feder funds. It has also been supported by the University of Alicante Project GRE16-19.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Instituto Universitario de Investigación Informática, Universidad de Alicante, Alicante, Spain</p><p class="c-article-author-affiliation__authors-list">Edmanuel Cruz, Sergio Orts-Escolano, Francisco Gomez-Donoso, Carlos Rizo, Jose Carlos Rangel, Higinio Mora &amp; Miguel Cazorla</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Edmanuel-Cruz"><span class="c-article-authors-search__title u-h3 js-search-name">Edmanuel Cruz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Edmanuel+Cruz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Edmanuel+Cruz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Edmanuel+Cruz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Sergio-Orts_Escolano"><span class="c-article-authors-search__title u-h3 js-search-name">Sergio Orts-Escolano</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Sergio+Orts-Escolano&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sergio+Orts-Escolano" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sergio+Orts-Escolano%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Francisco-Gomez_Donoso"><span class="c-article-authors-search__title u-h3 js-search-name">Francisco Gomez-Donoso</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Francisco+Gomez-Donoso&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Francisco+Gomez-Donoso" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Francisco+Gomez-Donoso%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Carlos-Rizo"><span class="c-article-authors-search__title u-h3 js-search-name">Carlos Rizo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Carlos+Rizo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Carlos+Rizo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Carlos+Rizo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jose_Carlos-Rangel"><span class="c-article-authors-search__title u-h3 js-search-name">Jose Carlos Rangel</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jose Carlos+Rangel&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jose Carlos+Rangel" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jose Carlos+Rangel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Higinio-Mora"><span class="c-article-authors-search__title u-h3 js-search-name">Higinio Mora</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Higinio+Mora&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Higinio+Mora" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Higinio+Mora%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Miguel-Cazorla"><span class="c-article-authors-search__title u-h3 js-search-name">Miguel Cazorla</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Miguel+Cazorla&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Miguel+Cazorla" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Miguel+Cazorla%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0338-3/email/correspondent/c1/new">Miguel Cazorla</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20augmented%20reality%20application%20for%20improving%20shopping%20experience%20in%20large%20retail%20stores&amp;author=Edmanuel%20Cruz%20et%20al&amp;contentID=10.1007%2Fs10055-018-0338-3&amp;publication=1359-4338&amp;publicationDate=2018-02-24&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0338-3" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0338-3" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Cruz, E., Orts-Escolano, S., Gomez-Donoso, F. <i>et al.</i> An augmented reality application for improving shopping experience in large retail stores.
                    <i>Virtual Reality</i> <b>23, </b>281–291 (2019). https://doi.org/10.1007/s10055-018-0338-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0338-3.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-07-28">28 July 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-02-16">16 February 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-02-24">24 February 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-09-01">01 September 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0338-3" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0338-3</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Smart shopping</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Deep learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Retail stores</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User experience</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D visualization</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0338-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=338;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

