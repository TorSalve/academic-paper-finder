<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Evaluating the effects of frame of reference on spatial collaboration "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Spatial collaboration is an everyday activity in which people work together to solve a spatial problem. For example, a group of people will often arrange furniture together or exchange directions..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/7/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2004 7:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2004-05-04"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Spatial collaboration is an everyday activity in which people work together to solve a spatial problem. For example, a group of people will often arrange furniture together or exchange directions with one another. Collaborative virtual environments using desktop PCs are particularly useful for spatial activities when the participants are distributed. This work investigates ways to enhance distributed, collaborative spatial activities. This paper explores how different frames of reference affect spatial collaboration. Specifically, it reports on an experiment that examines different combinations of exocentric and egocentric frames of reference with two users. Tasks involve manipulating an object, where one participant knows the objective (director) and the other performs the interactions (actor). It discusses the advantages and disadvantages of the different combinations for a spatial collaboration task. Findings from this study demonstrate that frames of reference affect collaboration in a variety of ways and simple exocentric-egocentric combinations do not always provide the most usable solution."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2004-05-04"/>

    <meta name="prism.volume" content="7"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="164"/>

    <meta name="prism.endingPage" content="174"/>

    <meta name="prism.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0123-3"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0123-3"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0123-3.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0123-3"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments"/>

    <meta name="citation_volume" content="7"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2004/06"/>

    <meta name="citation_online_date" content="2004/05/04"/>

    <meta name="citation_firstpage" content="164"/>

    <meta name="citation_lastpage" content="174"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0123-3"/>

    <meta name="DOI" content="10.1007/s10055-004-0123-3"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0123-3"/>

    <meta name="description" content="Spatial collaboration is an everyday activity in which people work together to solve a spatial problem. For example, a group of people will often arrange f"/>

    <meta name="dc.creator" content="Wendy A. Schafer"/>

    <meta name="dc.creator" content="Doug A. Bowman"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comp Graph Applic; citation_author=null Benford; citation_volume=20; citation_publication_date=2000; citation_pages=66; citation_doi=10.1109/38.844374; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Virt Real: Res Develop App; citation_title=Collaborative virtual environments: an introductory review of issues and systems; citation_author=EF Churchill, D Snowdon; citation_volume=3; citation_issue=1; citation_publication_date=1998; citation_pages=3-15; citation_id=CR2"/>

    <meta name="citation_reference" content="Dourish P, Bellotti V (1992) Awareness and coordination in shared workspaces. In: Proceedings of ACM Computer Supported Cooperative Work, 107&#8211;114"/>

    <meta name="citation_reference" content="Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative virtual interfaces. In: Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology, Asheville, NC, November 1999"/>

    <meta name="citation_reference" content="Gutwin C, Greenberg S (1998) Design for individuals, design for groups: tradeoffs between power and workspace awareness. In: Proceedings of Computer Supported Cooperative Work, 207&#8211;216"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput-Hum Interact; citation_title=The effects of workspace awareness support on the usability of real-time distributed groupware; citation_author=C Gutwin, S Greenberg; citation_volume=6; citation_issue=3; citation_publication_date=1999; citation_pages=243-281; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=J Collab Comput; citation_title=A descriptive framework of workspace awareness for real-time groupware; citation_author=C Gutwin, S Greenberg; citation_volume=11; citation_issue=3&#8211;4; citation_publication_date=2002; citation_pages=411-446; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput-Hum Interact; citation_title=Object-focused interaction In collaborative virtual environments; citation_author=J Hindmarsh, M Fraser, C Heath, S Benford, C Greenhalgh; citation_volume=7; citation_issue=4; citation_publication_date=2000; citation_pages=477-509; citation_id=CR8"/>

    <meta name="citation_reference" content="Lau LMS, Curson J, Drew R, Drew PM, Leigh C (1999) Use of virtual science park resource rooms to support group work in a learning environment. In: Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work, Phoenix, AZ, November 1999"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph App; citation_author=null Leigh; citation_volume=16; citation_publication_date=1996; citation_pages=47; citation_doi=10.1109/38.511853; citation_id=CR10"/>

    <meta name="citation_reference" content="Leigh J, Johnson AE, DeFanti TA (1996) CALVIN: an Immersimedia design environment utilizing heterogeneous perspectives. In: Proceedings of the IEEE International Conference on Multimedia Computing and Systems, Hiroshima, Japan, June 1996"/>

    <meta name="citation_reference" content="Leigh J, Johnson AE, Vasilakis CA, DeFanti TA (1996) Multiple-perspective collaborative design in persistent networked virtual environments. In: Proceedings of the IEEE Virtual Reality Annual International Symposium, Santa Clara, CA, 30 March-3 April 1996"/>

    <meta name="citation_reference" content="Mastaglio TW, Williamson J (1995) User-centered development of a large-scale complex networked virtual environment. In: Proceedings of the Conference on Human Factors and Computing Systems, Denver, CO, May 1995"/>

    <meta name="citation_reference" content="Schafer WA, Bowman DA (2003) A comparison of traditional and fisheye radar view techniques for spatial collaboration. In: Proceedings of Graphics Interface, 39&#8211;46"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Off Info Sys; citation_author=null Stefik; citation_volume=5; citation_publication_date=1987; citation_pages=147; citation_doi=10.1145/27636.28056; citation_id=CR15"/>

    <meta name="citation_reference" content="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of Human Factors and Computer Systems, 265&#8211;272"/>

    <meta name="citation_reference" content="citation_journal_title=J Exper Psych Appl; citation_author=null Wickens; citation_volume=1; citation_publication_date=1995; citation_pages=110; citation_doi=10.1037//1076-898X.1.2.110; citation_id=CR17"/>

    <meta name="citation_reference" content="Wickens CD (1999) Frames of reference for navigation. In: Gopher D, Koriat A (eds) Attention and performance XVII, MIT Press, Cambridge, MA"/>

    <meta name="citation_reference" content="Yang H (2002) Multiple perspectives for collaborative navigation in CVE. In: Extended abstracts for the ACM Conference on Human Factors in Computer Systems, Springer, Berlin Heidelberg New York"/>

    <meta name="citation_reference" content="Yang H, Olson GM (2002) Exploring collaborative navigation: the effect of perspectives on group performance. In: Proceedings of the 4th International Conference on Collaborative Virtual Environments, Bonn, Germany, 30 September-2 October 2002"/>

    <meta name="citation_author" content="Wendy A. Schafer"/>

    <meta name="citation_author_email" content="wschafer@vt.edu"/>

    <meta name="citation_author_institution" content="Virginia Polytechnic Institute and State University, Virginia Tech, Blacksburg, USA"/>

    <meta name="citation_author" content="Doug A. Bowman"/>

    <meta name="citation_author_institution" content="Virginia Polytechnic Institute and State University, Virginia Tech, Blacksburg, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0123-3&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0123-3"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments"/>
        <meta property="og:description" content="Spatial collaboration is an everyday activity in which people work together to solve a spatial problem. For example, a group of people will often arrange furniture together or exchange directions with one another. Collaborative virtual environments using desktop PCs are particularly useful for spatial activities when the participants are distributed. This work investigates ways to enhance distributed, collaborative spatial activities. This paper explores how different frames of reference affect spatial collaboration. Specifically, it reports on an experiment that examines different combinations of exocentric and egocentric frames of reference with two users. Tasks involve manipulating an object, where one participant knows the objective (director) and the other performs the interactions (actor). It discusses the advantages and disadvantages of the different combinations for a spatial collaboration task. Findings from this study demonstrate that frames of reference affect collaboration in a variety of ways and simple exocentric-egocentric combinations do not always provide the most usable solution."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0123-3","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Awareness, Collaborative virtual environment (CVE), Multiple perspectives","kwrd":["Awareness","Collaborative_virtual_environment_(CVE)","Multiple_perspectives"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0123-3","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0123-3","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=123;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0123-3">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0123-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0123-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2004-05-04" itemprop="datePublished">04 May 2004</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Wendy_A_-Schafer" data-author-popup="auth-Wendy_A_-Schafer" data-corresp-id="c1">Wendy A. Schafer<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Virginia Tech" /><meta itemprop="address" content="grid.438526.e, 0000000106944940, Virginia Polytechnic Institute and State University, Virginia Tech, Blacksburg, VA 24061, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Doug_A_-Bowman" data-author-popup="auth-Doug_A_-Bowman">Doug A. Bowman</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Virginia Tech" /><meta itemprop="address" content="grid.438526.e, 0000000106944940, Virginia Polytechnic Institute and State University, Virginia Tech, Blacksburg, VA 24061, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 7</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">164</span>–<span itemprop="pageEnd">174</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">121 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0123-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Spatial collaboration is an everyday activity in which people work together to solve a spatial problem. For example, a group of people will often arrange furniture together or exchange directions with one another. Collaborative virtual environments using desktop PCs are particularly useful for spatial activities when the participants are distributed. This work investigates ways to enhance distributed, collaborative spatial activities. This paper explores how different frames of reference affect spatial collaboration. Specifically, it reports on an experiment that examines different combinations of exocentric and egocentric frames of reference with two users. Tasks involve manipulating an object, where one participant knows the objective (director) and the other performs the interactions (actor). It discusses the advantages and disadvantages of the different combinations for a spatial collaboration task. Findings from this study demonstrate that frames of reference affect collaboration in a variety of ways and simple exocentric-egocentric combinations do not always provide the most usable solution.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Introduction</h2><div class="c-article-section__content" id="Sec2-content"><p>Every day, people are engaged in complex activities and projects that require them to think spatially. From the decisions people make in deciding which route to take to work to the hobbies people enjoy such as hiking and traveling, people are consistently thinking about spaces. In many cases, people collaborate in groups to solve problems related to spaces. The task might be a question of how to get from point A to point B, or how to layout the existing furniture in a new office area, or how to ensure a set of hiking trails is properly maintained collaboratively. In each of these cases, people are thinking about spatial relations and communicating with others to achieve a goal.</p><p>Spatial collaboration describes the combined work of multiple participants to solve problems involving physical space (Schafer and Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Schafer WA, Bowman DA (2003) A comparison of traditional and fisheye radar view techniques for spatial collaboration. In: Proceedings of Graphics Interface, 39–46" href="/article/10.1007/s10055-004-0123-3#ref-CR14" id="ref-link-section-d72801e285">2003</a>). The physical place is typically a large space people occupy and interact with. The place might be a building, such as a house or a multiple-story office building, or an outdoor area. There are many scenarios for any given spatial collaboration activity. The objectives of the activity can vary as well as the number of people engaged and the different roles each person plays. The common factor in all spatial collaboration is that more than one person is focused on a spatial problem concerning an inhabitable, physical area.</p><p>Collaborative virtual environments (CVEs) provide a way for multiple, distributed users to synchronously work together (Churchill and Snowdown <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Churchill EF, Snowdon D (1998) Collaborative virtual environments: an introductory review of issues and systems. Virt Real: Res Develop App 3(1):3–15" href="/article/10.1007/s10055-004-0123-3#ref-CR2" id="ref-link-section-d72801e291">1998</a>). The applications of CVEs range from teaching online university courses (Lau, Curson, Drew, Drew and Leigh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lau LMS, Curson J, Drew R, Drew PM, Leigh C (1999) Use of virtual science park resource rooms to support group work in a learning environment. In: Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work, Phoenix, AZ, November 1999" href="/article/10.1007/s10055-004-0123-3#ref-CR9" id="ref-link-section-d72801e294">1999</a>) to team-based military training (Mastaglio and Williamson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Mastaglio TW, Williamson J (1995) User-centered development of a large-scale complex networked virtual environment. In: Proceedings of the Conference on Human Factors and Computing Systems, Denver, CO, May 1995" href="/article/10.1007/s10055-004-0123-3#ref-CR13" id="ref-link-section-d72801e297">1995</a>) to poetry performances (Benford, Reynard, Greenhalgh, Snowdon and Bullock <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Benford S, Reynard G, Greenhalgh C, Snowdon D, Bullock A (2000) A poetry performance in a collaborative virtual environment. IEEE Comp Graph Applic 20(3):66–75" href="/article/10.1007/s10055-004-0123-3#ref-CR1" id="ref-link-section-d72801e300">2000</a>). This research focuses on CVE applications in which the structure of the environment is meaningful. The environment represents the spatial problem the collaborators are working to resolve. With this goal in mind, the users’ view of the environment becomes a critical factor for supporting spatial collaboration.</p><p>The objective of this work is to investigate how frames of reference affect spatial collaboration. Frames of reference have been explored in previous aviation research. Investigating different frames of reference with respect to flight navigation, the displays differed in their viewpoints on the space (Wickens and Prevett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Wickens CD, Prevett TT (1995) Exploring the dimensions of egocentricity in aircraft navigation displays. J Exper Psych Appl 1(2):110–135" href="/article/10.1007/s10055-004-0123-3#ref-CR17" id="ref-link-section-d72801e306">1995</a>; Wickens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Wickens CD (1999) Frames of reference for navigation. In: Gopher D, Koriat A (eds) Attention and performance XVII, MIT Press, Cambridge, MA" href="/article/10.1007/s10055-004-0123-3#ref-CR18" id="ref-link-section-d72801e309">1999</a>). Similarly, when interacting with a virtual environment, a user might be situated within the spatial representation so that one has a first-person view or egocentric frame of reference. Alternatively, one could look at the space from an external, or exocentric, frame of reference. Many virtual environments permit the user to “fly” into and out of the space, thereby blurring the distinction between a strict exocentric and egocentric frame of reference. These systems provide multiple perspectives on the space and create a flexible, single user environment. This work constrains the user’s navigation to either an exocentric or egocentric perspective and examines the direct effects of frame of reference combinations as two collaborators navigate independently. The task is to manipulate objects in a CVE with both fine and coarse grain movements. The study also focuses on the use of desktop computers because they are prominent in homes, offices, and some public places. This technology is currently used for everyday collaboration through the use of email, instant messaging, and online forums. It is also likely to be utilized for many spatial collaboration activities in comparison to more expensive and more immersive displays. The following sections review the related research and describe the details of the experiment. The results are presented along with a discussion of the findings.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Related work</h2><div class="c-article-section__content" id="Sec3-content"><p>Some of the earlier work using multiple perspectives in CVEs was part of the CALVIN project (Collaborative Architectural Layout Via Immersive Navigation). The objective was to support the more creative aspects of architectural design by exploiting multiple viewpoints (Leigh, Johnson, Vasilakis and DeFanti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Leigh J, Johnson AE, Vasilakis CA, DeFanti TA (1996) Multiple-perspective collaborative design in persistent networked virtual environments. In: Proceedings of the IEEE Virtual Reality Annual International Symposium, Santa Clara, CA, 30 March-3 April 1996" href="/article/10.1007/s10055-004-0123-3#ref-CR12" id="ref-link-section-d72801e320">1996</a>; Leigh, Johnson and DeFanti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Leigh J, Johnson AE, DeFanti TA (1996) CALVIN: an Immersimedia design environment utilizing heterogeneous perspectives. In: Proceedings of the IEEE International Conference on Multimedia Computing and Systems, Hiroshima, Japan, June 1996" href="/article/10.1007/s10055-004-0123-3#ref-CR11" id="ref-link-section-d72801e323">1996</a>). Using different immersive hardware configurations, they found that the users with the exocentric viewpoint took charge of the large object movements in a layout task, while the users with the egocentric viewpoint performed fine adjustments (Leigh and Johnson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Leigh J, Johnson AE (1996) Supporting transcontinental collaborative work in persistent virtual environments. IEEE Comput Graph App 16(4):47–51" href="/article/10.1007/s10055-004-0123-3#ref-CR10" id="ref-link-section-d72801e326">1996</a>). These results correspond to the belief that the first-hand view affords fine grain manipulations, while the global view is preferable for gross manipulations (Stoakley, Conway and Pausch <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of Human Factors and Computer Systems, 265–272" href="/article/10.1007/s10055-004-0123-3#ref-CR16" id="ref-link-section-d72801e329">1995</a>). This work explores this concept further as the collaborators interact with a desktop display.</p><p>Yang and Olson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Yang H, Olson GM (2002) Exploring collaborative navigation: the effect of perspectives on group performance. In: Proceedings of the 4th International Conference on Collaborative Virtual Environments, Bonn, Germany, 30 September-2 October 2002" href="/article/10.1007/s10055-004-0123-3#ref-CR20" id="ref-link-section-d72801e335">2002</a>) have also looked at various egocentric-exocentric implementations within a fabricated virtual environment created from a scientific data set. Their goal was to enable a distributed group to examine trends in a space of three-dimensional data points using a desktop (Yang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Yang H (2002) Multiple perspectives for collaborative navigation in CVE. In: Extended abstracts for the ACM Conference on Human Factors in Computer Systems, Springer, Berlin Heidelberg New York" href="/article/10.1007/s10055-004-0123-3#ref-CR19" id="ref-link-section-d72801e338">2002</a>). Yang and Olson found that when different perspectives are employed, groups struggle to perform the spatial, mental transformations required to see the same data objects. However, they also observed that an exocentric user could quickly provide directions so an egocentric partner navigated his/her view to include the target object. This study investigates the effects of frames of reference in a spatial collaboration, and is interested in environments that represent realistic locations, where egocentric navigation is constrained to a plane, and the task involves object manipulation rather than data exploration.</p><p>The University of Nottingham and King’s College London have explored object-focused interactions in desktop virtual environments. They observed that two users with egocentric viewpoints experience awareness problems when trying to arrange furniture in a single room (Hindmarsh, Fraser, Heath, Benford and Greenhalgh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction In collaborative virtual environments. ACM Trans Comput-Hum Interact 7(4):477–509" href="/article/10.1007/s10055-004-0123-3#ref-CR8" id="ref-link-section-d72801e344">2000</a>). The participants could not see their partner’s avatar and the objects of discussion with the same viewpoint. They also had difficulty seeing what their partner was doing and understanding what their partner could see. To address these issues, peripheral lenses and explicit representations of actions and viewing fields were implemented in a second version (Fraser, Benford, Hindmarsh and Heath <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative virtual interfaces. In: Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology, Asheville, NC, November 1999" href="/article/10.1007/s10055-004-0123-3#ref-CR4" id="ref-link-section-d72801e347">1999</a>). This work aims to address these types of awareness issues through frame of reference combinations.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">A spatial collaboration experiment</h2><div class="c-article-section__content" id="Sec4-content"><p>In collaborations, people are typically engaged in both individual and shared efforts (Gutwin and Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Gutwin C, Greenberg S (1998) Design for individuals, design for groups: tradeoffs between power and workspace awareness. In: Proceedings of Computer Supported Cooperative Work, 207–216" href="/article/10.1007/s10055-004-0123-3#ref-CR5" id="ref-link-section-d72801e358">1998</a>; Dourish and Bellotti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Dourish P, Bellotti V (1992) Awareness and coordination in shared workspaces. In: Proceedings of ACM Computer Supported Cooperative Work, 107–114" href="/article/10.1007/s10055-004-0123-3#ref-CR3" id="ref-link-section-d72801e361">1992</a>). To support this mixed-focus collaboration, the CVE in this experiment uses the<i>
relaxed what you see is what I see</i> technique (Stefik, Bobrow, Foster, Lanning and Tatar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Stefik M, Bobrow DG, Foster G, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser interfaces. ACM Trans Off Info Sys 5(2):147–167" href="/article/10.1007/s10055-004-0123-3#ref-CR15" id="ref-link-section-d72801e367">1987</a>). Each participant navigates the virtual environment individually, while avatars provide a visual indication for each user’s location and orientation. When an object is moved, all participants witness this manipulation from their own displays.</p><h3 class="c-article__sub-heading" id="Sec5">Variables of interest</h3><p>This investigation focuses on frame of reference combinations and their effects on both the overall collaboration session and the participant’s awareness of each other’s location and activities. The study conducted examined two frames of reference, and had the participants play two distinct roles. Both of these roles can have an effect on any given spatial collaboration task.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Exocentric-egocentric combinations</h4><p>In examining frames of reference, this experiment investigated an exocentric frame of reference technique and an egocentric technique. Given the presence of two collaborators, participants could both have an egocentric frame of reference, an exocentric frame of reference, or a combination of an egocentric view and an exocentric view.</p><p>The effects of different egocentric and exocentric combinations on a spatial collaboration task can vary greatly. Using the same frame of reference could be advantageous because both collaborators will have a similar view of the representation, thus establishing a common reference system for the task. The collaborators might approach the task together and take turns interacting with the environment. However, if the collaborators use different frames of reference, this could provide more insight into the task. Each collaborator would have a unique view, possibly making him/her a purveyor of information or a specialist for certain interactions within the environment. For instance, the participant with the egocentric viewpoint might contribute precise object positioning, knowledge of small details, or a first-person perspective of the space. The user with the exocentric perspective might add large-scale object movements or knowledge of the spatial layout. Differing frames of reference might also enable a group to solve complex spatial problems more efficiently because the unique viewpoints provide multiple views of the same problem.</p><p>Frame of reference combinations can also have an effect on the collaborator’s awareness of one another’s efforts. Awareness has many dimensions, which include knowing where one’s partners are located, what they can see, what they are currently focused on, and what they are doing (Gutwin and Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Gutwin C, Greenberg S (2002) A descriptive framework of workspace awareness for real-time groupware. J Collab Comput 11(3–4):411–446" href="/article/10.1007/s10055-004-0123-3#ref-CR7" id="ref-link-section-d72801e387">2002</a>). When two collaborators use the same frame of reference technique, their similar views might provide for greater awareness because one collaborator can easily align his view with what his partner sees. A disadvantage of this scenario occurs when a user cannot see the other’s avatar, making it difficult to discuss the environment together, or to coordinate their individual views of the space. A group of users may have problems knowing where everyone is located and what actions each user is performing. Given different frames of reference, more situations may exist where the collaborators can see their partners’ location and orientation. For example, a user with the exocentric view easily will be able to see the avatar of person with an egocentric view. At the same time, a user with the egocentric view can always know his exocentric partner has an external view of the representation. However, the frames of reference might be such that it is difficult to discern what features of the space each partner can see, or what actions each is performing.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">User roles to encourage awareness</h4><p>Each participant was assigned a distinct role for the task. The two roles employed were a director and an actor, similar to the work conducted by Gutwin and Greenberg (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput-Hum Interact 6(3):243–281" href="/article/10.1007/s10055-004-0123-3#ref-CR6" id="ref-link-section-d72801e398">1999</a>). The director knew the specifics of the task and his/her responsibility was to see that the task was completed correctly. The actor, conversely, was charged with performing the actions required.</p><p>In this way, the directors needed to be aware of where their partners were located and what they could see in order to provide the instructions. The director also needed to know what the actor was doing to ensure proper task completion. Likewise, the actor often needed to be aware of his/her partner because the director frequently provided directions relative to his location and orientation. For example, a helpful director might provide instructional phrases such as “in front of me”, “follow me”, or “it needs to go to my left”. All of these statements demand an understanding of the director’s perspective. A director could also use statements such as “beside you”, or “near the fireplace”, but these required less awareness of the director from the actor.</p><p>Given these roles, the CVE implementation enabled both participants to navigate the spatial representation, but only the actor could perform the actions. This encouraged collaboration and participation as the task responsibilities were divided between both users. Furthermore, the tasks were designed so that one user could not complete the task alone.</p><h3 class="c-article__sub-heading" id="Sec8">Tasks</h3><p>The director-actor pairs completed four tasks corresponding to four similar but unique virtual environments. Each displayed an interior design of a house and contained familiar elements such as doorways, windows, furniture, and appliances. Each task required the director to guide the actor in repositioning a piece of furniture. The director was provided with a spatial idea that included which object to move and the new location. The actor made sure he/she understood this idea and performed the object manipulation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0123-3#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0123-3#Fig2">2</a>). In each case, two pieces of furniture were manipulatable by the actor. These objects were located near each other and required the director to be specific in his instructions. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0123-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0123-3/MediaObjects/s10055-004-0123-3fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0123-3/MediaObjects/s10055-004-0123-3fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p> An egocentric actor has selected the orange basketball, which can be moved with buttons in the pop-up window. The floating head avatar represents the egocentric director’s position and orientation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0123-3/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0123-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0123-3/MediaObjects/s10055-004-0123-3fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0123-3/MediaObjects/s10055-004-0123-3fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p> An exocentric actor and an egocentric director work together to position a picture frame centered above the bed and flush against the wall</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0123-3/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The specifics of each task were fairly complex. These included hanging a picture frame on a wall, placing a basketball in between two shelves, moving a chair behind a bookshelf, and positioning a table next to another to form one long table. Each of these required the actor to move the object in the x, y, and z dimensions, as well as rotate the object around the y-axis. The tasks also required both users to navigate and adjust their orientation, as the objects needed to move to an opposite side of the house. Tasks were considered complete when the appropriate object was positioned within a certain level of precision.</p><h3 class="c-article__sub-heading" id="Sec9">The user interface</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Interactions</h4><p>Both the egocentric and exocentric techniques implemented constrained the user’s navigation. The egocentric frame of reference restricted the user to a first-person view of the space. One could navigate forward and backward and effectively sidestep to the left and right within the same plane using the arrow keys on a keyboard. Changing orientation was also possible by looking left, right, up, and down by clicking and dragging the mouse. Similarly, the exocentric frame of reference was restricted to an external view of the space. The user navigated with the arrow keys along the surface of an imaginary half sphere that surrounded the top half of the space. One was always oriented toward the bottom center of the representation and the viewpoints were arranged so that the user always could see the entire space (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0123-3#Fig3">3</a>). There was no support for zooming to prevent a user from seeing a more egocentric view. Occlusion also prevented a user from seeing every object in the environment from every viewpoint, as objects in the environment would block one’s view of other features. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0123-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0123-3/MediaObjects/s10055-004-0123-3flb3.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0123-3/MediaObjects/s10055-004-0123-3flb3.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p> The exocentric frame of reference enabled a user to navigate a half sphere surrounding the representation. One was always oriented toward the center of the representation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0123-3/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Selection and manipulation interactions were the same for both exocentric and egocentric users. Right-clicking over an object selected it, and this was indicated by a red-lined bounding box. A popup window was displayed with buttons to move a selected object forward, backward, left, right, up, and down, and to rotate the object left and right around the y-axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0123-3#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0123-3#Fig2">2</a>). Both forward/backward and left/right movements were based on the user’s viewing angle. Selecting forward/backward moved the object in and out of the screen, and left/right moved the object across the screen. When the object was positioned within a predefined level of precision it changed colors to indicate task completion.</p><h3 class="c-article__sub-heading" id="Sec11">The experimental design</h3><p>32 people participated in the experiment, including computer scientists, biologists, human factors experts, and ecologists. Of this group, 16 males and sixteen females completed the task with five male-male pairs, five female-female pairs, and six male-female pairs to balance the effects due to gender. The average age of the participants was 25, with the youngest person being 19 years old and the oldest being 54 years old. Almost all of the participants had experienced a virtual environment before. Many had viewed a demonstration of an immersive virtual environment with the CAVE, or a head-mounted display, and six of the participants played three-dimensional games on a regular basis.</p><p>Each participant followed a similar two-session procedure, including a practice and an evaluation. The individual practice allowed the participants to learn the interactions required to navigate their viewpoint, select an object, and manipulate the object selected. This practice allowed the users to experience the interfaces before completing the timed tasks two to four days later.</p><p>In the evaluation session, the two participants were randomly paired. After being introduced and reminded how to interact with the software, the pairs completed four unique tasks together. They were seated back to back, so as not to be able to see one another’s face, or each other’s screen. They were asked to not turn around, but to talk freely about the task. Before collaborating with each interior house design, the pair was notified of their different roles and perspectives. Then, after completing the furniture manipulation task, the users individually rated the treatment condition. Informal debriefing discussions were conducted with the pairs at the conclusion of all tasks before the group left.</p><p>The experiment used a 2×2 factorial design in blocks such that each subject played the actor and the director for two tasks. Each group worked with the four possible assignments of roles and frames of reference (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab1">1</a>). The ordering of these combinations and the assignment of the users to the roles varied with each pair. The four furniture tasks were also randomly assigned to each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1.  Both frames of reference were assigned to the actor and director roles, which corresponded to the four tasks completed by a pair of participants</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Experimental results</h2><div class="c-article-section__content" id="Sec12-content"><p>Two metrics were collected during the study. Each participant completed a questionnaire at the end of a task and the software automatically collected timing data. Tables with mean values are provided for each interaction.</p><h3 class="c-article__sub-heading" id="Sec13">Questionnaire results</h3><p>The questionnaire completed at the end of each task contained three sections: Collaboration Rating, Perceived Effort, and Awareness Rating. Answers in each section were based on 7-point Likert scale and analyzed using an Analysis of Variance.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">The collaboration rating</h4><p>The collaboration rating consisted of three statements shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab2">2</a>. Participants rated their level of agreement with each statement. A low rating corresponds to “strongly agreed” and a high rating corresponds to “strongly disagreed.” </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2.  Collaboration rating statements</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Participants agreed with statement 1 more when the actor had an egocentric view (F = 2.81, p = 0.0973, df = 1). Users also agreed with this statement more when the director had an egocentric view (F = 2.99, p = 0.0872, df = 1).</p><p>There was a trend for an interaction for statement 2 with the director and actor view assignments (F = 3.33, p = 0.0713, df = 1). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab3">3</a> indicates the mean values for the conditions. Comparing these means using least significant differences, the egocentric-egocentric combination is significantly less than the others. In other words, subjects understood one another more clearly when both partners had egocentric views. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3.  Mean ratings for collaboration statement 2. The egocentric-egocentric combination is significantly different</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">The perceived effort rating</h4><p>The perceived effort section also included three statements where a lower rating corresponded to agreement and higher numbers meant disagreement (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab4">4</a>). </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4.  Perceived effort statements</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Averaged together for an overall perceived effort, there was an interaction trend in the participants’ role and the director view assignments (F = 2.92, p = 0.0913, df = 1). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab5">5</a> shows how the actor’s response with the exocentric director corresponded to significantly greater perceived effort than the other combinations. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5.  Mean ratings for all perceived effort statements. Actors responded significantly differently when collaborating with an exocentric director</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Analyzing the individual perceived effort statements, there was a significant interaction for statement 1 with the frame of reference assignments (F = 4.08, p = 0.0466, df = 1). Subjects felt the task was more difficult to complete when both users had exocentric views. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab6">6</a> provides the mean values for each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6.  Mean ratings for perceived effort statement 1. Responses were significantly higher for the exocentric pair combination</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Participants also responded differently to the second perceived effort statement “The task required little effort.” The participants felt the task required less effort with an egocentric actor (F = 2.92, p = 0.0910, df = 1).</p><p>In the third statement, there was a significant difference between the actor’s response with an exocentric view and the director’s response with an exocentric actor (F = 3.55, p = 0.0628, df = 1). Users felt the task required greater concentration when they played the role of an exocentric actor and less concentration as the director with an exocentric actor. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab7">7</a> provides the mean results for each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7.  Mean ratings for perceived effort statement 3. Exocentric actor responses were significantly different from director responses with an exocentric actor</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/7"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">The awareness rating</h4><p>The awareness section of the questionnaire included the four questions listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab8">8</a>. Participants responded to each question on a 7-point numerical scale, where 1 corresponded to “always” and 7 was labeled as “never.” </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8.  Awareness questions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/8"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Averaging the participant’s responses for the four questions, it can be seen that the director’s view assignment had a significant effect (F = 13.03, p = 0.0005, df = 1). With the egocentric director combinations, participants indicated that they had a greater awareness of their partner’s location, viewpoint, and actions.</p><p>Examining the individual awareness questions, there was an interaction trend for the first question (“How often did you know where your partner was located?”). Participants responded differently depending on their role and the director’s assignment (F = 2.81, p = 0.097, df = 1). Comparing the means listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab9">9</a> using the least significant differences, directors with an exocentric frame of reference indicated having the least awareness followed by actors who worked with an exocentric director. The greatest amount of awareness corresponded to both egocentric directors and actors who worked with egocentric directors. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9.  Mean ratings for awareness question 1. With an egocentric director, both participants indicated an equal level of awareness, which was greater than the other conditions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/9"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec17">Timing results</h3><p>The software automatically logged timing data for each task. The timing started when one user clicked the large “Start” button covering both displays and ended when the correct object was positioned within the level of precision and unselected by the actor. Participants were asked to perform the tasks as quickly as possible.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Time to select</h4><p>Examining the time required for the participants to select the correct object, the egocentric pairs required significantly more time than both the exocentric-egocentric combinations (F = 4.06, p = 0.054, df = 1). The egocentric pairs also took more time than the exocentric pairs, but the difference was not statistically significant. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab10">10</a> indicates the mean times for each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b id="Tab10" data-test="table-caption">Table 10.  Mean times for selecting the correct object. The egocentric pair combination required significantly more time than both the exocentric-egocentric combinations</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/10"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">The selection and coarse positioning time</h4><p>There were also differences when combining the time required to select the object with the time used in positioning it within a coarse level of precision. Egocentric actors required significantly more time for the selection and manipulation than the exocentric actors (F = 8.58, p = 0.007, df = 1).</p><p>In conducting the study, each participant played the role of the director twice. If it is assumed that there was not a user effect with regards to which participant played this role in the four tasks, there was an additional significant difference in the times listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab11">11</a>. When both participants had an egocentric frame of reference, the pairs required significantly more time to select and coarse position the object. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-11"><figure><figcaption class="c-article-table__figcaption"><b id="Tab11" data-test="table-caption">Table 11.  Mean times for selecting the correct object and positioning within a coarse grain level of precision. Egocentric pairs required significantly more time</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/11"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">The coarse positioning time</h4><p>Analyzing only the time required to position an object with a coarse level of precision also differed significantly. Again, egocentric actors required much more time than their exocentric counterparts (F = 8.07, p = 0.008, df = 1).</p><p>Assuming that there were no user effects due to the director assignment, the egocentric pair required significantly more time than both the exocentric pair and the exocentric actor-egocentric director conditions. The egocentric pairs also took longer than the egocentric actor-exocentric director for this part of the task, but this difference was not significant. The mean times for each combination are listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab12">12</a>. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-12"><figure><figcaption class="c-article-table__figcaption"><b id="Tab12" data-test="table-caption">Table 12.  Mean times for positioning the object with a coarse level of precision. The egocentric pair combination required significantly more time than both the combinations with an exocentric actor</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/12"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">A fine positioning time</h4><p>There was a significant difference in the time required to finely position the object after it had been located within a coarse level of precision. Egocentric actors required significantly less time than their exocentric counterparts (F = 3.17, p = 0.086, df = 1).</p><p>Looking at the mean times and assuming no effect of director assignment, the exocentric actor-egocentric director combination required significantly more time than the other conditions. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab13">13</a> lists the mean times for each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-13"><figure><figcaption class="c-article-table__figcaption"><b id="Tab13" data-test="table-caption">Table 13.  Mean times for finely positioning the object. The exocentric actor-egocentric director combination required significantly more time</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/13"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">A positioning time</h4><p>Assuming no effect of director assignment and analyzing the time used to finely position the object after selection, the exocentric pair required significantly less than time than both the exocentric actor-egocentric director and egocentric pair conditions. The exocentric pairs also took less time than the egocentric actor-exocentric director, but this difference was not significant. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab14">14</a> lists the mean times for each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-14"><figure><figcaption class="c-article-table__figcaption"><b id="Tab14" data-test="table-caption">Table 14.  Mean times for positioning the object, including coarse and fine movements. The exocentric pairs required the least amount of time, which was significantly different from the egocentric director combinations</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/14"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">The total time</h4><p>Overall, the egocentric pair condition required significantly more than time than both the exocentric pair and the egocentric actor-exocentric director conditions, assuming that there was no effect due to director assignment. The egocentric pair also took more time than the exocentric actor-egocentric director, but this difference was not significant. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0123-3#Tab15">15</a> lists the mean times for each condition. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-15"><figure><figcaption class="c-article-table__figcaption"><b id="Tab15" data-test="table-caption">Table 15.  Mean times for completing the task. The egocentric pairs required the most amount of time, significantly more than the pairs with an exocentric director</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/tables/15"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Discussion</h2><div class="c-article-section__content" id="Sec24-content"><p>These results provide insight into how frames of reference affect spatial collaboration. They reveal the advantages and disadvantages of combining the actor and director roles with different viewpoints. They also explore how the frame of reference combinations affected the collaboration. A discussion of the results follows.</p><h3 class="c-article__sub-heading" id="Sec25">Actor and director frames of reference</h3><p>When analyzing these results it is interesting to consider how the participants’ responses on the questionnaire agree and disagree with the timing metrics. One example where the users expressed problems that corresponded to increased time involved an exocentric actor. Participants felt that the collaboration session proceeded more smoothly and with fewer problems when the actor had an egocentric view. They also responded that the task required more effort and greater concentration with an exocentric actor. This can be attributed to the actor’s ability to see more of the details of the environment with an egocentric frame of reference. When located in the environment the user could easily recognize the correct object to move, the target location, and the small differences between an object’s current and target location. An exocentric actor, in contrast, struggled to position the object with the fine-grained movements required in the final manipulation stages. Such an actor cannot see the details of the environment as clearly and it is important to position oneself to achieve an optimal view of the object and the target space. The timing results support this finding. The pairs with an exocentric actor required more time to finely position an object once it was within a coarse grain level of precision. This indicates that the exocentric actor condition faced object manipulation issues in the later parts of the task.</p><p>However, the user’s belief that the collaboration session went more smoothly and was less problematic with an egocentric actor is not apparent in the timing metrics. In fact, the participants took longer to complete two aspects of the task when the actor used an egocentric frame of reference. Both the time to position the object with a coarse grain level of precision and the time to select and position within a coarse grain level required significantly more time with an egocentric actor. The participants did not reveal issues with the egocentric actor in their responses, but the selection part of the task demands additional time and effort because the actor has to find the object by exploring the space. The director can help or hinder this process depending on his/her own knowledge of the space. Also, in manipulating the object an egocentric actor must rely on his spatial knowledge of the environment. This knowledge is unlike an exocentric actor’s and most users cannot determine the shortest path to the target. Such a path often involves moving the object through walls and other objects in the environment. It also involves positioning oneself so that the object moves along the straightest trajectory toward the target. This is not easy to accomplish because it requires that the user knows precisely which way to face, where he/she should position himself/herself with respect to the object’s location, and whether the object should be moved forward in a pulling motion or moved backward in pushing movement.</p><p>There is also a discrepancy in the questionnaire responses regarding the director’s perspective. Participants felt that the collaboration session occurred more smoothly and with fewer problems when the director had an egocentric view. The actors also indicated that the tasks required more effort with an exocentric director. This again could be due to the ability to see more of the details with the egocentric frame of reference. Providing these details to the director may enable the user to be more involved with the task, placing less of a burden on the actor to determine the specifics of environment. Another possibility is that users in general are less familiar with an exocentric view and so they are less helpful in providing important information to their partners. However, the timing results do not provide evidence for these issues. The times associated with selection, coarse positioning, coarse and fine positioning, and total task completion time indicate that the combinations with an exocentric director are similar to and even faster than their egocentric director counterparts. This implies that the participants may have encountered problems with an exocentric director, but it did not affect their overall performance.</p><p>Similarly, the questionnaire results favored an egocentric director with respect to greater awareness. Participants expressed knowing more often about their partners’ location, what they could see, what they were directly looking at, and what they were doing with an egocentric director. This may be because the director is in a better position to see the details of the environment that are highly important for the task and, in turn, are frequently discussed by the actor. These details involve identifying the correct object to select, confirming the correct target location, and ensuring the object is correctly positioned. As a result, the director maintained a greater sense of the actor and his actions, regardless of his partner’s frame of reference. Also, with the director physically located inside the environment, the instructions one provides can be more specific because the director can reference his avatar’s specific location. However, the timing results support an alternative explanation, as the pairs required significantly more time with the exocentric actor-egocentric director combination to finely position an object after a coarse positioning. This reflects the exocentric actor’s difficulties in finely tuning the position, as well as the pair’s combination to collaborate effectively. The egocentric director knows the task specifics and can see the object in motion, but this user is less helpful in conveying the specifics required by the exocentric actor. This is most likely due to the director’s lack of awareness and the actor’s impatience. For example, the exocentric actor might see the correct object and move it to the general target area before the egocentric director has navigated to the location of the object, or the target location. These results demonstrate that an egocentric director can be both beneficial and problematic with respect to awareness issues for spatial collaboration.</p><h3 class="c-article__sub-heading" id="Sec26">Frame of reference combinations</h3><p>In addition to the results of the actor and director assignments, there are also some interesting findings related to the three frame of reference combinations. When both participants used an egocentric frame of reference, the users felt that they more clearly understood one another. This confirms that similar egocentric frames of reference have an effect on collaboration where it provides collaborators with a familiar first-person view of the environment and enables them to share ideas with greater ease. However, the timing results describe a different conclusion. The two egocentric views corresponded to the greatest amount of time for selection, selection and coarse positioning, coarse positioning, and total time. This demonstrates how the two egocentric frames of reference are at a disadvantage in comparison with the other combinations. One explanation is that neither participant can easily see the object at the start of the task or has an understanding of the environment’s layout. They must explore the environment to gain this spatial knowledge and find the object. Once the object is selected, the pair next has to find the target location and move the object through the environment using their spatial understanding.</p><p>In comparison to the egocentric pairs, both egocentric-exocentric combinations corresponded to an easier task completion rating and the least amount of time for the selection. This agrees with the hypothesis that when working together the users were able to see the fine grain details as well as the overall layout of the environment, enabling them to solve the task with greater ease. The users also realized and commented on the potential value of the different frames of reference, leading to favorable survey responses due to intuition as well as experience. Conversely, the surveys also indicated that the pairs had the most problems understanding one another with different frames of reference. Using different frames of reference, the users had to deal with issues that were different from having the same frame of reference. Their views into the space were unique in that an exocentric view of an object looked very different from an egocentric view of the same object. This confirms that different frames of reference can have a negative effect on collaboration.</p><p>Particularly interesting are the timing results from the egocentric actor and exocentric director combination. The time required for selection was similar to that of the exocentric actor, egocentric director condition and both exocentric frames of reference. This implies that the director was very helpful in guiding the actor to the correct object. The coarse positioning time was also not significantly different from these two combinations, extending the benefits of the combination even further. This shows that an egocentric actor, aided by an exocentric director, can perform large-scale object manipulations similar to that of an exocentric actor. A similar comparison also exists with the total task completion time, suggesting that this combination supported both fine and coarse grain movements.</p><p>Lastly, the paired exocentric frames of reference presented unique issues to the collaborators. In the survey, the participants felt that the task was most difficult to complete with this combination. The precise movements required by the task and the need to distinguish between objects in the environment led to problems. Participants were also observed discussing the layout with respect to their viewpoint using references such as “bottom, left corner” and forgetting that their partner’s view was different. It was also more difficult to use the avatars for collaboration in an exocentric frame of reference. If you see an avatar in the egocentric view, spatial references such as “behind you” are possible, and are much more prevalent. Seeing an avatar in the exocentric view more often confirmed that the other person was present and that this partner had an exocentric frame of reference as well. However, these disadvantages were not revealed in the timing results. In fact, the exocentric paired condition required significantly less than time than both the exocentric actor, egocentric director and the egocentric paired conditions with respect to the time required to finely position the object once it was selected. This is probably due to the limited time required by two exocentric users to perform large-scale manipulation of an object across the space. Both can see the object in respect to the spatial layout and can ensure the object moves along the straightest path toward the target location.</p></div></div></section><section aria-labelledby="Sec27"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Conclusions and future work</h2><div class="c-article-section__content" id="Sec27-content"><p>This research investigated the effects of similar and different frames of reference in a distributed and collaborative spatial environment. In particular, this experiment compared three frames of reference combinations (exocentric-exocentric, egocentric-egocentric, and exocentric-egocentric) using two roles (actor and director) to investigate awareness issues.</p><p>The study contributes to the design of collaborative virtual environments as it presents advantages and disadvantages in assigning the actor and director of a task specific exocentric and egocentric frames of reference. An egocentric actor allows a user to experience the environment first hand, by exploring the space, seeing the details, and recognizing fine-grained tasks. This frame of reference was less efficient in finding objects within the space and moving them large distances across the space. The exocentric actor was much faster in accomplishing these tasks. With a view of the entire space, an exocentric user could quickly identify objects and manipulate them along the straightest path toward the target. The exocentric actor struggled, where the egocentric was successful, in positioning an object with fine manipulations. In assigning the director to the different frames of reference, an egocentric director was beneficial to the collaboration because the user could see and contribute with the details of the object manipulation task. The usefulness of such a director was limited, however, as an exocentric actor continued to have issues with fine-grained manipulation.</p><p>Examining the frame of reference combinations, the two egocentric users took longer to complete the task, but felt that they could understand each other the best. The exocentric pairs often required the least amount of time, but the participants felt that the task was most difficult to complete. The egocentric actor and exocentric director combination was particularly promising. This pairing seemed to take advantage of the different frames of reference in performing the task efficiently.</p><p>This work looks at how people collaborate on spatial problems in a distributed setting using a collaborative virtual environment. It offers some lessons about this collaboration context and encourages further exploration. In the future, this research will continue to explore different implementations of the exocentric and egocentric frames of reference to determine if these results are repeatable. It also will explore the collaborators’ behaviors when they are not restricted to one frame of reference but rather can navigate the space using both first person and more external viewpoints. Given the same roles, it would be interesting to see how the users approach similar tasks, and how their performance compares with the results reported here. Lastly, the findings from this study encourage an exploration of other spatial collaboration tasks. Tasks that require varied object manipulations and other forms of interaction could yield different results.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Benford, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Benford S, Reynard G, Greenhalgh C, Snowdon D, Bullock A (2000) A poetry performance in a collaborative virtua" /><p class="c-article-references__text" id="ref-CR1">Benford S, Reynard G, Greenhalgh C, Snowdon D, Bullock A (2000) A poetry performance in a collaborative virtual environment. IEEE Comp Graph Applic 20(3):66–75</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.844374" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=IEEE%20Comp%20Graph%20Applic&amp;volume=20&amp;publication_year=2000&amp;author=Benford%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EF. Churchill, D. Snowdon, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Churchill EF, Snowdon D (1998) Collaborative virtual environments: an introductory review of issues and system" /><p class="c-article-references__text" id="ref-CR2">Churchill EF, Snowdon D (1998) Collaborative virtual environments: an introductory review of issues and systems. Virt Real: Res Develop App 3(1):3–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaborative%20virtual%20environments%3A%20an%20introductory%20review%20of%20issues%20and%20systems&amp;journal=Virt%20Real%3A%20Res%20Develop%20App&amp;volume=3&amp;issue=1&amp;pages=3-15&amp;publication_year=1998&amp;author=Churchill%2CEF&amp;author=Snowdon%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dourish P, Bellotti V (1992) Awareness and coordination in shared workspaces. In: Proceedings of ACM Computer " /><p class="c-article-references__text" id="ref-CR3">Dourish P, Bellotti V (1992) Awareness and coordination in shared workspaces. In: Proceedings of ACM Computer Supported Cooperative Work, 107–114</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative vi" /><p class="c-article-references__text" id="ref-CR4">Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative virtual interfaces. In: Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology, Asheville, NC, November 1999</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutwin C, Greenberg S (1998) Design for individuals, design for groups: tradeoffs between power and workspace " /><p class="c-article-references__text" id="ref-CR5">Gutwin C, Greenberg S (1998) Design for individuals, design for groups: tradeoffs between power and workspace awareness. In: Proceedings of Computer Supported Cooperative Work, 207–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Gutwin, S. Greenberg, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distribu" /><p class="c-article-references__text" id="ref-CR6">Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput-Hum Interact 6(3):243–281</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%20workspace%20awareness%20support%20on%20the%20usability%20of%20real-time%20distributed%20groupware&amp;journal=ACM%20Trans%20Comput-Hum%20Interact&amp;volume=6&amp;issue=3&amp;pages=243-281&amp;publication_year=1999&amp;author=Gutwin%2CC&amp;author=Greenberg%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Gutwin, S. Greenberg, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Gutwin C, Greenberg S (2002) A descriptive framework of workspace awareness for real-time groupware. J Collab " /><p class="c-article-references__text" id="ref-CR7">Gutwin C, Greenberg S (2002) A descriptive framework of workspace awareness for real-time groupware. J Collab Comput 11(3–4):411–446</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20descriptive%20framework%20of%20workspace%20awareness%20for%20real-time%20groupware&amp;journal=J%20Collab%20Comput&amp;volume=11&amp;issue=3%E2%80%934&amp;pages=411-446&amp;publication_year=2002&amp;author=Gutwin%2CC&amp;author=Greenberg%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Hindmarsh, M. Fraser, C. Heath, S. Benford, C. Greenhalgh, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction In collaborative vir" /><p class="c-article-references__text" id="ref-CR8">Hindmarsh J, Fraser M, Heath C, Benford S, Greenhalgh C (2000) Object-focused interaction In collaborative virtual environments. ACM Trans Comput-Hum Interact 7(4):477–509</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Object-focused%20interaction%20In%20collaborative%20virtual%20environments&amp;journal=ACM%20Trans%20Comput-Hum%20Interact&amp;volume=7&amp;issue=4&amp;pages=477-509&amp;publication_year=2000&amp;author=Hindmarsh%2CJ&amp;author=Fraser%2CM&amp;author=Heath%2CC&amp;author=Benford%2CS&amp;author=Greenhalgh%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lau LMS, Curson J, Drew R, Drew PM, Leigh C (1999) Use of virtual science park resource rooms to support group" /><p class="c-article-references__text" id="ref-CR9">Lau LMS, Curson J, Drew R, Drew PM, Leigh C (1999) Use of virtual science park resource rooms to support group work in a learning environment. In: Proceedings of the International ACM SIGGROUP Conference on Supporting Group Work, Phoenix, AZ, November 1999</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Leigh, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Leigh J, Johnson AE (1996) Supporting transcontinental collaborative work in persistent virtual environments. " /><p class="c-article-references__text" id="ref-CR10">Leigh J, Johnson AE (1996) Supporting transcontinental collaborative work in persistent virtual environments. IEEE Comput Graph App 16(4):47–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.511853" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=IEEE%20Comput%20Graph%20App&amp;volume=16&amp;publication_year=1996&amp;author=Leigh%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leigh J, Johnson AE, DeFanti TA (1996) CALVIN: an Immersimedia design environment utilizing heterogeneous pers" /><p class="c-article-references__text" id="ref-CR11">Leigh J, Johnson AE, DeFanti TA (1996) CALVIN: an Immersimedia design environment utilizing heterogeneous perspectives. In: Proceedings of the IEEE International Conference on Multimedia Computing and Systems, Hiroshima, Japan, June 1996</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leigh J, Johnson AE, Vasilakis CA, DeFanti TA (1996) Multiple-perspective collaborative design in persistent n" /><p class="c-article-references__text" id="ref-CR12">Leigh J, Johnson AE, Vasilakis CA, DeFanti TA (1996) Multiple-perspective collaborative design in persistent networked virtual environments. In: Proceedings of the IEEE Virtual Reality Annual International Symposium, Santa Clara, CA, 30 March-3 April 1996</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mastaglio TW, Williamson J (1995) User-centered development of a large-scale complex networked virtual environ" /><p class="c-article-references__text" id="ref-CR13">Mastaglio TW, Williamson J (1995) User-centered development of a large-scale complex networked virtual environment. In: Proceedings of the Conference on Human Factors and Computing Systems, Denver, CO, May 1995</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schafer WA, Bowman DA (2003) A comparison of traditional and fisheye radar view techniques for spatial collabo" /><p class="c-article-references__text" id="ref-CR14">Schafer WA, Bowman DA (2003) A comparison of traditional and fisheye radar view techniques for spatial collaboration. In: Proceedings of Graphics Interface, 39–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Stefik, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Stefik M, Bobrow DG, Foster G, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser int" /><p class="c-article-references__text" id="ref-CR15">Stefik M, Bobrow DG, Foster G, Lanning S, Tatar D (1987) WYSIWIS revised: early experiences with multiuser interfaces. ACM Trans Off Info Sys 5(2):147–167</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F27636.28056" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=ACM%20Trans%20Off%20Info%20Sys&amp;volume=5&amp;publication_year=1987&amp;author=Stefik%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedi" /><p class="c-article-references__text" id="ref-CR16">Stoakley R, Conway MJ, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of Human Factors and Computer Systems, 265–272</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Wickens, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Wickens CD, Prevett TT (1995) Exploring the dimensions of egocentricity in aircraft navigation displays. J Exp" /><p class="c-article-references__text" id="ref-CR17">Wickens CD, Prevett TT (1995) Exploring the dimensions of egocentricity in aircraft navigation displays. J Exper Psych Appl 1(2):110–135</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F1076-898X.1.2.110" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J%20Exper%20Psych%20Appl&amp;volume=1&amp;publication_year=1995&amp;author=Wickens%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wickens CD (1999) Frames of reference for navigation. In: Gopher D, Koriat A (eds) Attention and performance X" /><p class="c-article-references__text" id="ref-CR18">Wickens CD (1999) Frames of reference for navigation. In: Gopher D, Koriat A (eds) Attention and performance XVII, MIT Press, Cambridge, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang H (2002) Multiple perspectives for collaborative navigation in CVE. In: Extended abstracts for the ACM Co" /><p class="c-article-references__text" id="ref-CR19">Yang H (2002) Multiple perspectives for collaborative navigation in CVE. In: Extended abstracts for the ACM Conference on Human Factors in Computer Systems, Springer, Berlin Heidelberg New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang H, Olson GM (2002) Exploring collaborative navigation: the effect of perspectives on group performance. I" /><p class="c-article-references__text" id="ref-CR20">Yang H, Olson GM (2002) Exploring collaborative navigation: the effect of perspectives on group performance. In: Proceedings of the 4th International Conference on Collaborative Virtual Environments, Bonn, Germany, 30 September-2 October 2002</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0123-3-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Virginia Polytechnic Institute and State University, Virginia Tech, Blacksburg, VA 24061, USA</p><p class="c-article-author-affiliation__authors-list">Wendy A. Schafer &amp; Doug A. Bowman</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Wendy_A_-Schafer"><span class="c-article-authors-search__title u-h3 js-search-name">Wendy A. Schafer</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Wendy A.+Schafer&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wendy A.+Schafer" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wendy A.+Schafer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Doug_A_-Bowman"><span class="c-article-authors-search__title u-h3 js-search-name">Doug A. Bowman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Doug A.+Bowman&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Doug A.+Bowman" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Doug A.+Bowman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0123-3/email/correspondent/c1/new">Wendy A. Schafer</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Evaluating%20the%20effects%20of%20frame%20of%20reference%20on%20spatial%20collaboration%20using%20desktop%20collaborative%20virtual%20environments&amp;author=Wendy%20A.%20Schafer%20et%20al&amp;contentID=10.1007%2Fs10055-004-0123-3&amp;publication=1359-4338&amp;publicationDate=2004-05-04&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Schafer, W.A., Bowman, D.A. Evaluating the effects of frame of reference on spatial collaboration using desktop collaborative virtual environments.
                    <i>Virtual Reality</i> <b>7, </b>164–174 (2004). https://doi.org/10.1007/s10055-004-0123-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0123-3.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-12-12">12 December 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-03-29">29 March 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-05-04">04 May 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-06">June 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0123-3" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0123-3</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Awareness</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative virtual environment (CVE)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multiple perspectives</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0123-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=123;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

