<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Dense 3D facial reconstruction from a single depth image in unconstrai"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="With the increasing demands of applications in virtual reality such as 3D films, virtual human&#8211;machine interactions and virtual agents, the analysis of 3D human face is considered to be more..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Dense 3D facial reconstruction from a single depth image in unconstrained environment"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-04-27"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="With the increasing demands of applications in virtual reality such as 3D films, virtual human&#8211;machine interactions and virtual agents, the analysis of 3D human face is considered to be more and more important as a fundamental step in these tasks. Due to information provided by the additional dimension, 3D facial reconstruction enables aforementioned tasks to be achieved with higher accuracy than those based on 2D facial analysis. The denser the 3D facial model is, the more information it could provide. However, most existing dense 3D facial reconstruction methods require complicated processing and high system cost. To this end, this paper presents a novel method that simplifies the process of dense 3D facial reconstruction by employing only one frame of depth data obtained with an off-the-shelf RGB-D sensor. The proposed method is composed of two main stages: (a) the acquisition of the initial 3D facial point cloud with automatically 3D facial region cropping, and (b) the generating of the dense facial point cloud with RBF-based adaptive 3D point interpolation. Experiments reported in this paper demonstrate the competitive results with real-world data."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-04-27"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="37"/>

    <meta name="prism.endingPage" content="46"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0311-6"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0311-6"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0311-6.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0311-6"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Dense 3D facial reconstruction from a single depth image in unconstrained environment"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2018/03"/>

    <meta name="citation_online_date" content="2017/04/27"/>

    <meta name="citation_firstpage" content="37"/>

    <meta name="citation_lastpage" content="46"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0311-6"/>

    <meta name="DOI" content="10.1007/s10055-017-0311-6"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0311-6"/>

    <meta name="description" content="With the increasing demands of applications in virtual reality such as 3D films, virtual human&#8211;machine interactions and virtual agents, the analysis "/>

    <meta name="dc.creator" content="Shu Zhang"/>

    <meta name="dc.creator" content="Hui Yu"/>

    <meta name="dc.creator" content="Ting Wang"/>

    <meta name="dc.creator" content="Lin Qi"/>

    <meta name="dc.creator" content="Junyu Dong"/>

    <meta name="dc.creator" content="Honghai Liu"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=J Electron Imaging; citation_title=Scattered data interpolation methods for electronic imaging systems: a survey; citation_author=I Amidror; citation_volume=11; citation_publication_date=2002; citation_pages=157-176; citation_doi=10.1117/1.1455013; citation_id=CR1"/>

    <meta name="citation_reference" content="Beis JS, Lowe DG (1997) Shape indexing using approximate nearest-neighbour search in high-dimensional spaces. In: Proceedings of IEEE computer society conference on computer vision and pattern recognition, 1997. IEEE, pp 1000&#8211;1006"/>

    <meta name="citation_reference" content="Bradley D, Heidrich W, Popa T, Sheffer (2010) A high resolution passive facial performance capture. In: ACM transactions on graphics (TOG), 2010, vol 4. ACM, p 41"/>

    <meta name="citation_reference" content="Brown RA (2015) Building kd Tree in O (knlog n) Time. J Comput Graph Tech 4:50&#8211;68"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Optimizing human model reconstruction from RGB-D images based on skin detection; citation_author=G Chen, J Li, J Zeng, B Wang, G Lu; citation_volume=20; citation_publication_date=2016; citation_pages=159-172; citation_doi=10.1007/s10055-016-0291-y; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Sensors for 3D imaging: metric evaluation and calibration of a CCD/CMOS time-of-flight camera; citation_author=F Chiabrando, R Chiabrando, D Piatti, F Rinaudo; citation_volume=9; citation_publication_date=2009; citation_pages=10080-10096; citation_doi=10.3390/s91210080; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Long baseline stereovision for automatic detection and ranging of moving objects in the night sky; citation_author=R Danescu, F Oniga, V Turcu, O Cristea; citation_volume=12; citation_publication_date=2012; citation_pages=12940-12963; citation_doi=10.3390/s121012940; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=A framework to design 3D interaction assistance in constraints-based virtual environments; citation_author=M Essabbah, G Bouyer, S Otmane, M Mallem; citation_volume=18; citation_publication_date=2014; citation_pages=219-234; citation_doi=10.1007/s10055-014-0247-z; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=SIAM J Sci Comput; citation_title=Stable evaluation of Gaussian radial basis function interpolants; citation_author=GE Fasshauer, MJ McCourt; citation_volume=34; citation_publication_date=2012; citation_pages=A737-A762; citation_doi=10.1137/110824784; citation_id=CR9"/>

    <meta name="citation_reference" content="Franke R, Nielson GM (1991) Scattered data interpolation and applications: a tutorial and survey. In: Hagen H, Roller D (eds) Geometric modeling. Computer Graphics&#8212;Systems and Applications. Springer, Berlin, pp 131&#8211;160"/>

    <meta name="citation_reference" content="Garcia E, Dugelay J-L (2001) Low cost 3D face acquisition and modeling. In: Proceedings of international conference on information technology: coding and computing. IEEE, pp 657&#8211;661"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Cybern; citation_title=Enhanced computer vision with microsoft kinect sensor: a review; citation_author=J Han, L Shao, D Xu, J Shotton; citation_volume=43; citation_publication_date=2013; citation_pages=1318-1334; citation_doi=10.1109/TSMCB.2012.2228851; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Theory Comput; citation_title=Approximate nearest neighbor: towards removing the curse of dimensionality; citation_author=S Har-Peled, P Indyk, R Motwani; citation_volume=8; citation_publication_date=2012; citation_pages=321-350; citation_doi=10.4086/toc.2012.v008a014; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_title=Multiple view geometry in computer vision; citation_publication_date=2003; citation_id=CR14; citation_author=R Hartley; citation_author=A Zisserman; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=Near laser-scan quality 3-D face reconstruction from a low-quality depth stream; citation_author=M Hernandez, J Choi, G Medioni; citation_volume=36; citation_publication_date=2015; citation_pages=61-69; citation_doi=10.1016/j.imavis.2014.12.004; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=A seamless solution for 3D real-time interaction: design and evaluation; citation_author=F Hernoux, O Christmann; citation_volume=19; citation_publication_date=2015; citation_pages=1-20; citation_doi=10.1007/s10055-014-0255-z; citation_id=CR16"/>

    <meta name="citation_reference" content="Hossain MS, Akbar M, Starkey JD (2007) Inexpensive construction of a 3D face model from stereo images. In: 10th international conference on computer and information technology. iccit 2007. IEEE, pp 1&#8211;6"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=3D face modeling using the multi-deformable method; citation_author=J Hwang, S Yu, J Kim, S Lee; citation_volume=12; citation_publication_date=2012; citation_pages=12870-12889; citation_doi=10.3390/s121012870; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn; citation_title=Single-view-based 3D facial reconstruction method robust against pose variations; citation_author=J Jo, H Choi, I-J Kim, J Kim; citation_volume=48; citation_publication_date=2015; citation_pages=73-85; citation_doi=10.1016/j.patcog.2014.07.013; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Real-time facial shape recovery from a single image under general, unknown lighting by rank relaxation; citation_author=M Lee, C-H Choi; citation_volume=120; citation_publication_date=2014; citation_pages=59-69; citation_doi=10.1016/j.cviu.2013.12.010; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=3D facial model exaggeration builder for small or large sized model manufacturing; citation_author=W-S Lee, A Soon, L Zhu; citation_volume=11; citation_publication_date=2007; citation_pages=229-239; citation_doi=10.1007/s10055-007-0071-9; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Extracting 3D facial animation parameters from multiview video clips; citation_author=I-C Lin, J-S Yeh, M Ouhyoung; citation_volume=22; citation_publication_date=2002; citation_pages=72-80; citation_doi=10.1109/MCG.2002.1046631; citation_id=CR22"/>

    <meta name="citation_reference" content="Mecca R, Wetzler A, Kimmel R, Bruckstein AM (2013) Direct shape recovery from photometric stereo with shadows. In: 2013 International conference on 3D vision-3DV 2013. IEEE, pp 382&#8211;389"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Structured light-based 3D reconstruction system for plants; citation_author=TT Nguyen, DC Slaughter, N Max, JN Maloof, N Sinha; citation_volume=15; citation_publication_date=2015; citation_pages=18587-18612; citation_doi=10.3390/s150818587; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE J Solid State Circuits; citation_title=A 0.18-m CMOS SoC for a 100-m-range 10-frame/s 200 96-pixel time-of-flight depth sensor; citation_author=C Niclass, M Soga, H Matsubara, M Ogawa, M Kagami; citation_volume=49; citation_publication_date=2014; citation_pages=315-330; citation_doi=10.1109/JSSC.2013.2284352; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_title=Scientific visualization; citation_publication_date=1997; citation_id=CR26; citation_author=G Nielson; citation_author=H Hagen; citation_author=H Muller; citation_publisher=Institute of Electrical and Electronics Engineers"/>

    <meta name="citation_reference" content="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR&#8217;06). IEEE, pp 2161&#8211;2168"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture; citation_author=C Qu, W-P Brinkman, Y Ling, P Wiggers, I Heynderickx; citation_volume=17; citation_publication_date=2013; citation_pages=307-321; citation_doi=10.1007/s10055-013-0231-z; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput IJC; citation_title=Performance issues on K-mean partitioning clustering algorithm; citation_author=PV Rao, SKM Rao; citation_volume=14; citation_publication_date=2014; citation_pages=41-51; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Interpret Multivar Data; citation_title=A brief description of natural neighbour interpolation; citation_author=R Sibson; citation_volume=21; citation_publication_date=1981; citation_pages=21-36; citation_id=CR30"/>

    <meta name="citation_reference" content="Silpa-Anan C, Hartley R (2008) Optimised KD-trees for fast image descriptor matching. In: IEEE conference on computer vision and pattern recognition, 2008. CVPR 2008. IEEE, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Sci Coll; citation_title=Facial feature detection using Haar classifiers ; citation_author=PI Wilson, J Fernandez; citation_volume=21; citation_publication_date=2006; citation_pages=127-133; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Perception-driven facial expression synthesis; citation_author=H Yu, OG Garrod, PG Schyns; citation_volume=36; citation_publication_date=2012; citation_pages=152-162; citation_doi=10.1016/j.cag.2011.12.002; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Multimed; citation_title=Microsoft kinect sensor and its effect; citation_author=Z Zhang; citation_volume=19; citation_publication_date=2012; citation_pages=4-10; citation_doi=10.1109/MMUL.2012.24; citation_id=CR34"/>

    <meta name="citation_reference" content="Zhu J, Wang L, Yang R, Davis J (2008) Fusion of time-of-flight depth and stereo for high accuracy depth maps. In: IEEE conference on computer vision and pattern recognition, 2008. CVPR 2008. IEEE, pp 1&#8211;8"/>

    <meta name="citation_author" content="Shu Zhang"/>

    <meta name="citation_author_institution" content="Ocean University of China, Qingdao, China"/>

    <meta name="citation_author_institution" content="University of Portsmouth, Portsmouth, UK"/>

    <meta name="citation_author" content="Hui Yu"/>

    <meta name="citation_author_institution" content="University of Portsmouth, Portsmouth, UK"/>

    <meta name="citation_author" content="Ting Wang"/>

    <meta name="citation_author_institution" content="Shandong University of Science and Technology, Qingdao, China"/>

    <meta name="citation_author" content="Lin Qi"/>

    <meta name="citation_author_institution" content="Ocean University of China, Qingdao, China"/>

    <meta name="citation_author" content="Junyu Dong"/>

    <meta name="citation_author_email" content="dongjunyu@ouc.edu.cn"/>

    <meta name="citation_author_institution" content="Ocean University of China, Qingdao, China"/>

    <meta name="citation_author" content="Honghai Liu"/>

    <meta name="citation_author_institution" content="University of Portsmouth, Portsmouth, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0311-6&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0311-6"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Dense 3D facial reconstruction from a single depth image in unconstrained environment"/>
        <meta property="og:description" content="With the increasing demands of applications in virtual reality such as 3D films, virtual human–machine interactions and virtual agents, the analysis of 3D human face is considered to be more and more important as a fundamental step in these tasks. Due to information provided by the additional dimension, 3D facial reconstruction enables aforementioned tasks to be achieved with higher accuracy than those based on 2D facial analysis. The denser the 3D facial model is, the more information it could provide. However, most existing dense 3D facial reconstruction methods require complicated processing and high system cost. To this end, this paper presents a novel method that simplifies the process of dense 3D facial reconstruction by employing only one frame of depth data obtained with an off-the-shelf RGB-D sensor. The proposed method is composed of two main stages: (a) the acquisition of the initial 3D facial point cloud with automatically 3D facial region cropping, and (b) the generating of the dense facial point cloud with RBF-based adaptive 3D point interpolation. Experiments reported in this paper demonstrate the competitive results with real-world data."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Dense 3D facial reconstruction from a single depth image in unconstrained environment | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0311-6","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual face, Three-dimensional image acquisition, Three-dimensional sensing, 3D interpolation","kwrd":["Virtual_face","Three-dimensional_image_acquisition","Three-dimensional_sensing","3D_interpolation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0311-6","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0311-6","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=311;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0311-6">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Dense 3D facial reconstruction from a single depth image in unconstrained environment
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0311-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0311-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-04-27" itemprop="datePublished">27 April 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Dense 3D facial reconstruction from a single depth image in unconstrained environment</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shu-Zhang" data-author-popup="auth-Shu-Zhang">Shu Zhang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ocean University of China" /><meta itemprop="address" content="0000 0001 2152 3263, grid.4422.0, Ocean University of China, Qingdao, 266100, China" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Portsmouth" /><meta itemprop="address" content="0000 0001 0728 6636, grid.4701.2, University of Portsmouth, Portsmouth, PO1 2DJ, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hui-Yu" data-author-popup="auth-Hui-Yu">Hui Yu</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Portsmouth" /><meta itemprop="address" content="0000 0001 0728 6636, grid.4701.2, University of Portsmouth, Portsmouth, PO1 2DJ, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ting-Wang" data-author-popup="auth-Ting-Wang">Ting Wang</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Shandong University of Science and Technology" /><meta itemprop="address" content="0000 0004 1799 3811, grid.412508.a, Shandong University of Science and Technology, Qingdao, 266590, China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Lin-Qi" data-author-popup="auth-Lin-Qi">Lin Qi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ocean University of China" /><meta itemprop="address" content="0000 0001 2152 3263, grid.4422.0, Ocean University of China, Qingdao, 266100, China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Junyu-Dong" data-author-popup="auth-Junyu-Dong" data-corresp-id="c1">Junyu Dong<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ocean University of China" /><meta itemprop="address" content="0000 0001 2152 3263, grid.4422.0, Ocean University of China, Qingdao, 266100, China" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Honghai-Liu" data-author-popup="auth-Honghai-Liu">Honghai Liu</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Portsmouth" /><meta itemprop="address" content="0000 0001 0728 6636, grid.4701.2, University of Portsmouth, Portsmouth, PO1 2DJ, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">37</span>–<span itemprop="pageEnd">46</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">563 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0311-6/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>With the increasing demands of applications in virtual reality such as 3D films, virtual human–machine interactions and virtual agents, the analysis of 3D human face is considered to be more and more important as a fundamental step in these tasks. Due to information provided by the additional dimension, 3D facial reconstruction enables aforementioned tasks to be achieved with higher accuracy than those based on 2D facial analysis. The denser the 3D facial model is, the more information it could provide. However, most existing dense 3D facial reconstruction methods require complicated processing and high system cost. To this end, this paper presents a novel method that simplifies the process of dense 3D facial reconstruction by employing only one frame of depth data obtained with an off-the-shelf RGB-D sensor. The proposed method is composed of two main stages: (a) the acquisition of the initial 3D facial point cloud with automatically 3D facial region cropping, and (b) the generating of the dense facial point cloud with RBF-based adaptive 3D point interpolation. Experiments reported in this paper demonstrate the competitive results with real-world data.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>With the thriving developments of the applications in virtual reality such as 3D films, virtual human–machine interaction (Essabbah et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Essabbah M, Bouyer G, Otmane S, Mallem M (2014) A framework to design 3D interaction assistance in constraints-based virtual environments. Virtual Real 18:219–234" href="/article/10.1007/s10055-017-0311-6#ref-CR8" id="ref-link-section-d55106e411">2014</a>; Hernoux and Christmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Hernoux F, Christmann O (2015) A seamless solution for 3D real-time interaction: design and evaluation. Virtual Real 19:1–20" href="/article/10.1007/s10055-017-0311-6#ref-CR16" id="ref-link-section-d55106e414">2015</a>) and virtual agents (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Zeng J, Wang B, Lu G (2016) Optimizing human model reconstruction from RGB-D images based on skin detection. Virtual Real 20:159–172" href="/article/10.1007/s10055-017-0311-6#ref-CR5" id="ref-link-section-d55106e417">2016</a>; Qu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Qu C, Brinkman W-P, Ling Y, Wiggers P, Heynderickx I (2013) Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture. Virtual Real 17:307–321" href="/article/10.1007/s10055-017-0311-6#ref-CR28" id="ref-link-section-d55106e420">2013</a>), 3D human face analysis is considered to be one of the most important researches (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Lee W-S, Soon A, Zhu L (2007) 3D facial model exaggeration builder for small or large sized model manufacturing. Virtual Real 11:229–239" href="/article/10.1007/s10055-017-0311-6#ref-CR21" id="ref-link-section-d55106e423">2007</a>) in this area. Compared to the analysis of 2D facial data, 3D facial analysis can achieve higher accuracy and efficiency than its equivalent 2D methods with the addition of another dimension. These advantages highly depend on the details of the 3D facial model. However, in practice, due to the limitations of the hardware and the structure of the scene such as depth shadowing, influence of the materials with reflection or refraction or infrared absorption in the scene, 3D information obtained from RGB-D data or many other reconstruction methods may often provide data that are insufficient for detailed facial analysis (Zhu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhu J, Wang L, Yang R, Davis J (2008) Fusion of time-of-flight depth and stereo for high accuracy depth maps. In: IEEE conference on computer vision and pattern recognition, 2008. CVPR 2008. IEEE, pp 1–8" href="/article/10.1007/s10055-017-0311-6#ref-CR35" id="ref-link-section-d55106e427">2008</a>). The resolutions of the 3D data are not good enough for detailed facial analysis. Moreover, high-resolution 3D face reconstruction frequently requires particularly complicated preparation in the 3D acquisition process, or the usage of expensive equipment.</p><p>To tackle these problems, this paper presents a novel dense 3D facial reconstruction method that only utilizes one frame data from an off-the-shelf RGB-D sensor, even the data are in low resolution. The reconstruction process is fully automatic without any manual intervention. The workflow of the proposed method is demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig1">1</a>. The process of the proposed method involves: (a) initial 3D point cloud acquisition; (b) automatically face region extraction from the initial point cloud; (c) filling of the holes in the initial point cloud; (d) highly dense 3D facial point cloud generating using initial point cloud as seeds. Experiments of the proposed method illustrate encouraging results with real-world data. The advantages of the proposed method in this paper are listed as follows.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Work flow of the proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Compared to most of the existing 3D facial reconstruction methods, the 3D recovery process of the proposed method is much simpler with minimal constraints. The only requirement for recovering the dense 3D facial point cloud using proposed method is one frame of the data from an off-the-shelf cheap RGB-D sensor without any additional setups.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>The system cost of the proposed method is much lower than other existing 3D facial reconstruction methods. The only sensor needed in our method is a cheap RGB-D sensor with possibly low data resolution.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Compared to most of the existing methods, the whole 3D facial reconstruction process of the proposed method is fully automatic without any manual operation.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>The output of the proposed method is a highly dense 3D facial point cloud with enriched facial details both in 3D structure and facial texture for further facial analysis.</p>
                    
                  </li>
                </ol>
<p>The rest of the paper is organized as follow: the related works are discussed in the Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec2">2</a>; the outline of the method is introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec3">3</a>; in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec4">4</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec5">5</a>, two stages of the proposed method are described elaborately; the experiments with real-world data are demonstrated and discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec6">6</a>; the paper is concluded in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec7">7</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>A number of 3D reconstruction methods exist in the literature, and they can be classified into two groups: (a) the methods only utilizing RGB cameras, and (b) the methods based on dedicated depth sensors. Generally, the methods in the first group apply computer vision algorithms to multiple visual images for 3D recovery, for example, multi-view geometry, shape from shading. The most commonly used method with multi-view geometry is the stereo vision (Danescu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Danescu R, Oniga F, Turcu V, Cristea O (2012) Long baseline stereovision for automatic detection and ranging of moving objects in the night sky. Sensors 12:12940–12963" href="/article/10.1007/s10055-017-0311-6#ref-CR7" id="ref-link-section-d55106e531">2012</a>). It utilizes two visual images, which are captured by two individual cameras placed with a fixed displacement, namely a baseline. A stereo rectification process is firstly conducted to reproject the image pair by two virtual cameras with the same focal length and no relative rotation to each other. Then the 3D structure can be achieved by analyzing the disparities from the matched 2D image features obtained with the epipolar constraints. Given more than two images, the method structure from motion (SfM) can recover the 3D structure of a scene and the poses of the camera (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-017-0311-6#ref-CR14" id="ref-link-section-d55106e534">2003</a>). Photometric stereo (PMS) is another 3D reconstruction method based on shape from shading. By capturing a scene multiple times with different directional illuminations, PMS can recover a continuous depth map (Mecca et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Mecca R, Wetzler A, Kimmel R, Bruckstein AM (2013) Direct shape recovery from photometric stereo with shadows. In: 2013 International conference on 3D vision-3DV 2013. IEEE, pp 382–389" href="/article/10.1007/s10055-017-0311-6#ref-CR23" id="ref-link-section-d55106e537">2013</a>). However, the camera should stay still when capturing these images to make sure that the contents in these images are not moving.</p><p>The methods in the second group rely on dedicated hardware to recover 3D data. These hardware utilize techniques such as structure light (Nguyen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Nguyen TT, Slaughter DC, Max N, Maloof JN, Sinha N (2015) Structured light-based 3D reconstruction system for plants. Sensors 15:18587–18612" href="/article/10.1007/s10055-017-0311-6#ref-CR24" id="ref-link-section-d55106e543">2015</a>) and time-of-flight (Chiabrando et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Chiabrando F, Chiabrando R, Piatti D, Rinaudo F (2009) Sensors for 3D imaging: metric evaluation and calibration of a CCD/CMOS time-of-flight camera. Sensors 9:10080–10096" href="/article/10.1007/s10055-017-0311-6#ref-CR6" id="ref-link-section-d55106e546">2009</a>) to obtain 3D information of the scene. Normally, the structure light-based methods are achieved by a structure light projector and a structure light receiver. A structured light projector projects a set of certain patterns onto the surfaces in the scene. The same pattern may change its appearance when projected onto the surfaces with different distances. By analyzing the pattern reflected from the scene, the 3D information of the scene can be calculated accordingly. Microsoft’s Kinect is the representative device (Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19:4–10" href="/article/10.1007/s10055-017-0311-6#ref-CR34" id="ref-link-section-d55106e549">2012</a>). The time-of-flight-based methods scan the scene with multiple laser beams. By calculating the travel time of the laser beams, the 3D information of the scene can be obtained. For example, Niclass et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Niclass C, Soga M, Matsubara H, Ogawa M, Kagami M (2014) A 0.18-m CMOS SoC for a 100-m-range 10-frame/s 200 96-pixel time-of-flight depth sensor. IEEE J Solid State Circuits 49:315–330" href="/article/10.1007/s10055-017-0311-6#ref-CR25" id="ref-link-section-d55106e552">2014</a>) introduced a depth sensor based on time-of-flight.</p><p>All these methods in the first group require capturing multiple images of the target either from different directions or using different illuminations. It is considered that these methods are neither able to generate 3D data with enough details for 3D facial analysis (e.g., stereo vision), nor be user-friendly enough for human-robot interaction (e.g., photometric stereo). The performance of the methods in the second group is relatively more stable and more reliable. However, higher price is required if highly detailed 3D results are expected. With the studies of the advantages and disadvantages of the 3D reconstruction methods in both aforementioned groups, this paper presents a novel method suitable for detailed 3D facial recovery that can achieve a high data resolution while cost less for the equipment setup and preparation. The proposed method is achieved by the combination of the computer vision algorithms and the initial data from the off-the-shelf RGB-D sensor with a low price, for example, Microsoft’s Kinect, Asus’s Xtion Pro Live, Primesense’s 3D Sensor and the Structure Sensor from Occipital, Inc.</p><p>There is a range of methods that can reconstruct 3D human faces (Yu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Yu H, Garrod OG, Schyns PG (2012) Perception-driven facial expression synthesis. Comput Graph 36:152–162" href="/article/10.1007/s10055-017-0311-6#ref-CR33" id="ref-link-section-d55106e561">2012</a>). However, most of them either require complicated and special system setup, or come up with low accuracy results. For example, Bradley et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bradley D, Heidrich W, Popa T, Sheffer (2010) A high resolution passive facial performance capture. In: ACM transactions on graphics (TOG), 2010, vol 4. ACM, p 41" href="/article/10.1007/s10055-017-0311-6#ref-CR3" id="ref-link-section-d55106e564">2010</a>) presented a passive 3D facial capturing system, which was achieved by multi-view stereo. The results were obtained with high resolution. However, the data acquisition device is composed of multiple cameras and multiple illuminations from all directions. The data were captured in a dark room to prevent the interference from the ambient light. Garcia et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Garcia E, Dugelay J-L (2001) Low cost 3D face acquisition and modeling. In: Proceedings of international conference on information technology: coding and computing. IEEE, pp 657–661" href="/article/10.1007/s10055-017-0311-6#ref-CR11" id="ref-link-section-d55106e567">2001</a>) also proposed a low-cost 3D face acquisition method. However, their method required to project a set of structure lights to form up a grid on the face directly, which was not user-friendly enough. The method presented by Hossain et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Hossain MS, Akbar M, Starkey JD (2007) Inexpensive construction of a 3D face model from stereo images. In: 10th international conference on computer and information technology. iccit 2007. IEEE, pp 1–6" href="/article/10.1007/s10055-017-0311-6#ref-CR17" id="ref-link-section-d55106e570">2007</a>) utilized the stereo vision for 3D facial reconstruction. However, a man-made rectangular board is required behind the user’s head, and four corners of this rectangle should be hand-coded with a red color before the process of stereo matching. Jo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Jo J, Choi H, Kim I-J, Kim J (2015) Single-view-based 3D facial reconstruction method robust against pose variations. Pattern Recogn 48:73–85" href="/article/10.1007/s10055-017-0311-6#ref-CR19" id="ref-link-section-d55106e573">2015</a>) presented a 3D face reconstruction method by combining the simplified 3DMM and SfM. However, their method required manually annotated facial feature points. Lee and Choi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Lee M, Choi C-H (2014) Real-time facial shape recovery from a single image under general, unknown lighting by rank relaxation. Comput Vis Image Underst 120:59–69" href="/article/10.1007/s10055-017-0311-6#ref-CR20" id="ref-link-section-d55106e577">2014</a>) proposed a 3D face estimation method from a 2D frontal face image with an approach of the rank constraint relaxing. However, before their method is applied, the input image needs to be manually cropped according to the region of the face. Moreover, the resolution of the result was also fixed. Hwang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hwang J, Yu S, Kim J, Lee S (2012) 3D face modeling using the multi-deformable method. Sensors 12:12870–12889" href="/article/10.1007/s10055-017-0311-6#ref-CR18" id="ref-link-section-d55106e580">2012</a>) proposed a 3D face modeling method with continuous facial surface and texture. However, their method required two mirrors placed at both side of the face, and two chessboards were also required for calibrating the images from the mirrors, which made the system much complicated.</p><p>Different from the aforementioned methods, the proposed 3D facial reconstruction method is fully automatic without any manual intervention. Moreover, a highly detailed 3D facial model can be obtained at a low system cost using our method, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig1">1</a>.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Overview of the proposed method</h2><div class="c-article-section__content" id="Sec3-content"><p>This paper presents a novel method that can reconstruct a highly dense 3D human face. Furthermore, it only requires one frame data from an off-the-shelf RGB-D sensor, even those cheap sensors with low resolutions. The process is fully automatic without any manual intervention. The proposed method is divided into two main stages: (a) initial 3D facial point cloud acquisition and (b) dense facial point cloud propagation. The output of the first stage is a sparse 3D point cloud with precise facial region; the output of the final stage is a dense 3D facial point cloud. Our method follows the outline below:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Extract one frame data from a RGB-D sensor, which contains an RGB image and a depth image. With the parameters of the sensor, registration between these two images is conducted to obtain 2D–3D correspondence between RGB image and depth image.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Facial recognition is carried out on the RGB image. With the obtained facial region on RGB image and the 2D–3D correspondence between RGB and depth data, the depth image is cropped to reserve the data that only belong to the facial area. To further refine the facial region on depth data, a <i>K</i>-means clustering algorithm is applied to remove the non-facial depth data. At the end of this step, an initial 3D point cloud can be obtained with a precise region of human face.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Approximate nearest neighbor (ANN) algorithm is applied to calculate the neighbor structure of the point cloud obtained in the previous step. With the neighborship of the cloud, an interpolation based on radial basis function (RBF) is employed to propagate a dense 3D facial point cloud from the initial point cloud. The detailed process is described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0311-6#Sec4">4</a>.</p>
                    
                  </li>
                </ol>
<p>The work flow of the proposed method is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig1">1</a>, which describes the two main stages of the process. The first contribution of this paper is that the generation of dense 3D facial point cloud is fully automatic. No manual intervention is needed during the process. We utilize a novel 3D facial region locating scheme to automatically crop the initial 3D point cloud for facial area. This automation is most suitable for the applications that require independent operations. The second contribution is that the highly dense facial point cloud can be obtained at a very low system cost. We achieve this by proposing an adaptive RBF-based 3D interpolation method to propagate the 3D points in the cloud. Therefore, the only hardware needed in the proposed method is a cheap RGB-D sensor, even those with low resolutions. For example, the resolution of the depth data from the Kinect is only 640 × 480. To achieve full automation of 3D facial reconstruction, we innovatively introduce an automatic 3D facial region cropping scheme and an adaptive RBF-based 3D interpolation process into the proposed method.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Acquisition of initial 3D facial point cloud</h2><div class="c-article-section__content" id="Sec4-content"><p>The first stage of the proposed method is to acquire initial 3D point cloud for human face. In this paper, we utilize the Microsoft Kinect (Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19:4–10" href="/article/10.1007/s10055-017-0311-6#ref-CR34" id="ref-link-section-d55106e654">2012</a>) as the input device to obtain the raw 3D data for demonstration. Kinect was originally designed for human machine interaction for video games. The 3D depth information captured by Kinect has also attracted the researchers in computer vision community. Similar with Xtion Pro Live, Primesense Sensor or the Structure Sensor, Kinect is also an RGB-D sensor that can simultaneously provide both RGB color and depth images. The RGB color image is captured by a built-in RGB digital camera inside the sensor. The depth data are achieved by an infrared laser projector and an infrared video camera mounted within the sensor.</p><p>There is a spatial shift between the RGB image and the depth image as the RGB camera and the infrared camera vary in spatial location (Han et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Han J, Shao L, Xu D, Shotton J (2013) Enhanced computer vision with microsoft kinect sensor: a review. IEEE Trans Cybern 43:1318–1334" href="/article/10.1007/s10055-017-0311-6#ref-CR12" id="ref-link-section-d55106e660">2013</a>). Thus, we apply a registration process to the RGB image and depth image before we can utilize them. This alignment process can be achieved by taking into account of the constant distance between the RGB camera and the infrared camera in the RGB-D device. With the knowledge of field of view (FOV) of the sensor, we can modify every pixel in the depth image accordingly to make them align with the pixel in RGB image. There is a built-in function in the Kinect SDK for the aforementioned registration process. However, we try to implement this process by our own to enhance the generalization and flexibility of the proposed method, for example, to enable the proposed method fully functional with the utilization of other RGB-D sensors rather than the Kinect.</p><p>The alignment is to retrieve the corresponding 2D coordinate in depth image for every pixel in the RGB image. Then coordinates of 3D points in the space can be obtained using (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ1">1</a>).</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\frac{{x_{p} }}{{u - u_{0} }} = \frac{{y_{p} }}{{v - v_{0} }} = \frac{{z_{p} }}{f}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where (<i>x</i>
                <sub>
                  <i>p</i>
                </sub>, <i>y</i>
                <sub>
                  <i>p</i>
                </sub>, <i>z</i>
                <sub>
                  <i>p</i>
                </sub>) is the coordinate of the 3D point in the space, (<i>u</i>, <i>v</i>) is the 2D coordinate of the point in the depth image, (<i>u</i>
                <sub>0</sub>, <i>v</i>
                <sub>0</sub>) is the principle point of the infrared camera, and <i>f</i> is the focal length of the infrared camera. With known value of <i>u</i>, <i>v</i>, <i>u</i>
                <sub>0</sub>, <i>v</i>
                <sub>0</sub>, <i>f</i> and <i>z</i>
                <sub>
                  <i>p</i>
                </sub>, the 3D coordinate of (<i>x</i>
                <sub>
                  <i>p</i>
                </sub>, <i>y</i>
                <sub>
                  <i>p</i>
                </sub>, <i>z</i>
                <sub>
                  <i>p</i>
                </sub>) can be retrieved.</p><p> After calculating the 3D points, the face detection is carried out on the aligned RGB image in order to filter out the 3D points to only reserve the points within the facial region, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig2">2</a>. We implement the facial detection process using Haar Cascade Classifier (Wilson and Fernandez <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Wilson PI, Fernandez J (2006) Facial feature detection using Haar classifiers. J Comput Sci Coll 21:127–133" href="/article/10.1007/s10055-017-0311-6#ref-CR32" id="ref-link-section-d55106e867">2006</a>). It trains the Haar feature classifiers for facial features such as eyes, mouth and nose with two sets of images at first. Then the facial region can be recognized in an RGB image with the trained classifiers. According to the registration between RGB image and 3D points, we only keep the 3D points inside the facial region, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig2">2</a>b.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>3D point cloud cropping by detected facial region. <b>a</b> The <i>green rectangle</i> is the facial region recognized in RGB image; <b>b</b> the 3D point cloud within the <i>green frame</i> is cropped to reserve the points that only belong to facial region detected in (<b>a</b>) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>Though facial detection can remove most of the 3D points that do not belong to facial region, it hardly provides a precise region of a human face. Only a general area of the face is obtained. However, as it can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig2">2</a>b, the 3D point cloud consists of several partitions, one of which is the real facial region. Therefore, to achieve a more accurate facial area in 3D point cloud, we apply a <i>K</i>-means clustering algorithm (Rao and Rao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Rao PV, Rao SKM (2014) Performance issues on K-mean partitioning clustering algorithm. Int J Comput IJC 14:41–51" href="/article/10.1007/s10055-017-0311-6#ref-CR29" id="ref-link-section-d55106e917">2014</a>) on the point cloud in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig2">2</a>b to divide it into several clusters, and only reserve the one that contains the center point of the facial region.</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\left\{ {\begin{array}{*{20}l} {\mathop {\arg \hbox{min} }\limits_{{\mathbf{S}}} \sum\limits_{n = 1}^{k} {\sum\limits_{{x \in S_{n} }}^{{}} {\left\| {x - \mu_{n} } \right\|^{2} } } } \hfill \\ {{\mathbf{S}} = \left\{ {S_{1} ,S_{2} ,S_{3} , \ldots ,S_{n} , \ldots ,S_{k} } \right\}} \hfill \\ \end{array} } \right.$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
<p>
                <i>K</i>-means algorithm achieves the clustering process by calculating the distance between points and centroids of the point groups, as shown in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ2">2</a>), where <i>μ</i>
                <sub>
                  <i>n</i>
                </sub> is the centroid coordinate of the cluster <i>S</i>
                <sub>
                  <i>n</i>
                </sub>. The initial centroids are selected randomly. After multiple iterations, a clustering solution <b>S</b> can be found when the similarity within a cluster is maximized and similarity inter-cluster is minimized. Since the facial detection has already provided a rough but tight facial region, we implement <i>K</i>-means algorithm with three initial clusters, and reserve only the one containing the center point of the detected facial region. The result is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig3">3</a>. The facial region is much more precise than the one in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig2">2</a>b, in which the 3D point cloud includes not only the facial points but also the points that belong to the background.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Initial 3D facial point cloud obtained by the first stage of the proposed method. <b>a</b>, <b>b</b> viewed from different angles. The facial region is achieved with more accuracy after applying <i>K</i>-means algorithm comparing to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig2">2</a>b</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Generation of dense facial point cloud</h2><div class="c-article-section__content" id="Sec5-content"><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig3">3</a>, the initial 3D facial point cloud is in low quality with missing data and holes on the surface. Moreover, the point cloud is not dense enough to provide the details of the human face, which is essential for 3D facial analysis in some computer vision tasks. Therefore, the second stage of the proposed method is to apply an interpolation process to fill up the depth blank. A denser point cloud is propagated from the initial 3D facial points, which results in a smoother surface.</p><p>There is a range of interpolation algorithms available. Amidror (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Amidror I (2002) Scattered data interpolation methods for electronic imaging systems: a survey. J Electron Imaging 11:157–176" href="/article/10.1007/s10055-017-0311-6#ref-CR1" id="ref-link-section-d55106e1204">2002</a>) made a literature review for scattered data interpolation methods. He classified the interpolation methods into following groups: (a) the triangulation-based methods, (b) the inverse distance-weighted methods, (c) the radial basis function-based methods and (d) the natural neighbor interpolation methods. The interpolation process based on the triangulation is achieved by finding the central point of each triangular represented by the original data (Nielson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Nielson G, Hagen H, Muller H (1997) Scientific visualization. Institute of Electrical and Electronics Engineers, New York" href="/article/10.1007/s10055-017-0311-6#ref-CR26" id="ref-link-section-d55106e1207">1997</a>). The calculation is local. However, it still requires to triangulate the given scattered point set as a preprocessing step. This process is sometimes computationally complex, especially for the dense interpolation which inserts more than one point within the original triangular. The inverse distance-weighted methods, also known as Shepard methods, calculate the interpolated point values according to all the points in the original point cloud with the inverse distance of the points as the weights. However, since the Shepard methods are only sensitive to the distance, the accuracy of the results is easily affected by the overweight of the data clusters. The natural neighbor interpolation methods are based on the Voronoi tessellation of the original point cloud (Sibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Sibson R (1981) A brief description of natural neighbour interpolation. Interpret Multivar Data 21:21–36" href="/article/10.1007/s10055-017-0311-6#ref-CR30" id="ref-link-section-d55106e1210">1981</a>).</p><p>In this paper, we choose the interpolation scheme based on the radial basis function (RBF) (Fasshauer and McCourt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Fasshauer GE, McCourt MJ (2012) Stable evaluation of Gaussian radial basis function interpolants. SIAM J Sci Comput 34:A737–A762" href="/article/10.1007/s10055-017-0311-6#ref-CR9" id="ref-link-section-d55106e1216">2012</a>). The RBF-based methods are considered to be some of the most elegant schemes and often work very well (Franke and Nielson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Franke R, Nielson GM (1991) Scattered data interpolation and applications: a tutorial and survey. In: Hagen H, Roller D (eds) Geometric modeling. Computer Graphics—Systems and Applications. Springer, Berlin, pp 131–160" href="/article/10.1007/s10055-017-0311-6#ref-CR10" id="ref-link-section-d55106e1219">1991</a>). We treat the 3D point cloud as a set of spatial variables (<i>X</i>, <i>Y</i>, <i>Z</i>), where (<i>X</i>, <i>Y</i>) is the 2D coordinate, and (<i>Z</i>) is composed of a set of scalar variables, which are seated on the grid formed up by the aforementioned 2D coordinate. Accordingly, the interpolation process is to find the interpolated (<i>Z</i>) value in (<i>X</i>, <i>Y</i>) coordinate system. We apply RBF framework to perform interpolation with Gaussian Kernel as the basis function. RBF model is built up by a set of basis functions with radial symmetry. It follows the form as in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ3">3</a>).</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$f(x) = \sum\limits_{i = 1}^{M} {(w_{i} \cdot \varphi (\left\| {x - c_{i} } \right\|))}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
<div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$F = \varPhi W$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>φ</i> is a basis function, <i>w</i>
                <sub>
                  <i>i</i>
                </sub> is the weight for each basis function, and <i>c</i>
                <sub>
                  <i>i</i>
                </sub> is the interpolation center, which coincide with the original 3D point in the initial 3D facial point cloud obtained in last section. The <i>c</i>
                <sub>
                  <i>i</i>
                </sub> can be referred as the seed. Basis functions can be achieved with several forms, such as Gaussian basis function, multi-quadric basis function, thin plate spline basis function, cubic basis function and poly-harmonic basis functions. A value for an interpolated point in the coordinate system can be provided by the basis function according to the values of its neighboring seed points, and the distances between this interpolated point and its neighboring seed points. Therefore, as shown in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ4">4</a>), a linear system for all seed points <i>c</i>
                <sub>
                  <i>i</i>
                </sub> can be found, where <i>F</i> is a vector of the interpolated values, <i>W</i> is a vector of the weights, and Φ is the interpolation matrix. The expansion of (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ4">4</a>) is shown in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ5">5</a>). Obviously, with the knowledge of <i>f</i>(<i>c</i>
                <sub>
                  <i>i</i>
                </sub>) for <i>i</i> = 1,…, <i>M</i>, which is the value of each seed point, the weights <i>W</i> can be derived from the system by (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ6">6</a>).</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\left[ {\begin{array}{*{20}c} {f(x_{1} )} \\ {f(x_{2} )} \\ \vdots \\ {f(x_{N} )} \\ \end{array} } \right] = \left[ {\begin{array}{*{20}c} {\varphi (\left\| {x_{1} - c_{1} } \right\|)} &amp; {\varphi (\left\| {x_{1} - c_{2} } \right\|)} &amp; \cdots &amp; {\varphi (\left\| {x_{1} - c_{M} } \right\|)} \\ {\varphi (\left\| {x_{2} - c_{1} } \right\|)} &amp; {\varphi (\left\| {x_{2} - c_{2} } \right\|)} &amp; \cdots &amp; {\varphi (\left\| {x_{2} - c_{M} } \right\|)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ {\varphi (\left\| {x_{N} - c_{1} } \right\|)} &amp; {\varphi (\left\| {x_{N} - c_{2} } \right\|)} &amp; \cdots &amp; {\varphi (\left\| {x_{N} - c_{M} } \right\|)} \\ \end{array} } \right]\left[ {\begin{array}{*{20}c} {w_{1} } \\ {w_{2} } \\ \vdots \\ {w_{M} } \\ \end{array} } \right]$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
<div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$W = \varPhi^{ - 1} F$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
<p>The Gaussian basis function is applied in our method as <i>φ</i> expressed in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ7">7</a>) since Gaussian basis function is localized and compact, which means the value of the interpolated point is only sensitive to the values of its neighboring seed points. A distance of about 6·<i>R</i>
                <sub>0</sub> will make the value almost equal zero with the Gaussian basis function, as shown in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-017-0311-6#Equ7">7</a>). This prevents interpolation from over-smooth which leads to losses of details on the 3D facial surface.</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\varphi (r) = e^{{\frac{{ - r^{2} }}{{2 \cdot \sigma^{2} }}}} = e^{{\frac{{ - r^{2} }}{{R_{0}^{2} }}}}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
<p>As a result, the choice of <i>R</i>
                <sub>0</sub> is crucial to our method. If <i>R</i>
                <sub>0</sub> is too small, the interpolated points will prone to ground zero since there are not enough seed points within valid influential distance. The value will degrade rapidly when apart from the position of the seed point. If <i>R</i>
                <sub>0</sub> is too large, the calculation process becomes more time-consuming, while interpolation performance does not improve too much. Our studies show that a better result emerges when <i>R</i>
                <sub>0</sub> is slightly larger than the average distance between original seed points in the coordinate system. Kd-tree is commonly used to find a neighborhood of discrete points (Brown <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Brown RA (2015) Building kd Tree in O (knlog n) Time. J Comput Graph Tech 4:50–68" href="/article/10.1007/s10055-017-0311-6#ref-CR4" id="ref-link-section-d55106e2264">2015</a>). However, with the increment of the dimensionality and data complexity, the effectiveness of the classic Kd-tree algorithm drops quickly. Therefore, we use approximate nearest neighbors (ANN) algorithm (Har-Peled et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Har-Peled S, Indyk P, Motwani R (2012) Approximate nearest neighbor: towards removing the curse of dimensionality. Theory Comput 8:321–350" href="/article/10.1007/s10055-017-0311-6#ref-CR13" id="ref-link-section-d55106e2267">2012</a>) instead of classic Kd-tree (exact nearest neighbors) algorithm to calculate the average distance of the points in initial 3D facial point cloud, and then form up the neighborhood of the points in point cloud for RBF-based interpolation. Several ANN algorithm implementations exist, such as best bin first (Beis and Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Beis JS, Lowe DG (1997) Shape indexing using approximate nearest-neighbour search in high-dimensional spaces. In: Proceedings of IEEE computer society conference on computer vision and pattern recognition, 1997. IEEE, pp 1000–1006" href="/article/10.1007/s10055-017-0311-6#ref-CR2" id="ref-link-section-d55106e2270">1997</a>), randomized Kd-trees (Silpa-Anan and Hartley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Silpa-Anan C, Hartley R (2008) Optimised KD-trees for fast image descriptor matching. In: IEEE conference on computer vision and pattern recognition, 2008. CVPR 2008. IEEE, pp 1–8" href="/article/10.1007/s10055-017-0311-6#ref-CR31" id="ref-link-section-d55106e2274">2008</a>), hierarchical <i>K</i>-means tree (Nister and Stewenius <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR’06). IEEE, pp 2161–2168" href="/article/10.1007/s10055-017-0311-6#ref-CR27" id="ref-link-section-d55106e2280">2006</a>). In this paper, randomized Kd-trees (RKT) are employed for nearest neighbor search to construct the neighborhood of the initial 3D facial point cloud.</p><p>RKT is an extension of classic Kd-tree algorithm. The classic Kd-tree is a binary tree with k dimensional data. Similar to binary search tree, a classic Kd-tree is built by dividing the points in the space into several partitions according to multiple dimensions once at a time. For instance, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig4">4</a>, the blue vertical line on the left and blue circle in the first layer of the Kd-tree on the right imply the division of the points along the first dimension. The green horizontal lines on the left and green circle in the second layer of the Kd-tree on the right imply the division of the points divided by previous division along the second dimension. In each layer of the Kd-tree, it chooses a dimension with max variance of the points and divides the points along this dimension. Any point in the space can be located in one of the leaf nodes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Illustration of the Kd-tree structure. The <i>left</i> part is the distribution of the points in 2D; the <i>right</i> part is the Kd-tree structure corresponding to the 2D points in the <i>left</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>RKT is a forest with multiple Kd-trees initialized by different parameters, e.g., with different rotations on the data. The searches on these trees with different structures can be independent. Different from the classic Kd-tree’s strategy of choosing the dimension with max variance of the points to subdivide the data, RKT randomly chooses from a few of dimensions that have high data variance. This strategy enables the RKT with higher efficiency both in querying and backtracking.</p><p>With obtained neighborhood of the points in the initial 3D facial point cloud, the average distance between seed points can be calculated easily. We use 6 times of average distance as the <i>R</i>
                <sub>0</sub> to perform RBF interpolation. The RGB colors of the interpolated points are the average color of its neighboring seed points. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig5">5</a>, a denser point cloud of 3D face can be obtained with full details of the surface. As it can be observed, most holes are filled up, and the surface is much smoother. More experiments with other subjects are presented in the next section.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Dense 3D facial point cloud obtained at the end of the final stage of the proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Six participants’ RGB image with recognized facial regions in <i>green rectangles</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Experiments and results</h2><div class="c-article-section__content" id="Sec6-content"><p>The proposed method is implemented using C++, which is compiled by Microsoft Visual Studio. Microsoft’s Kinect is used as demonstration to acquire the raw data. The initial depth image for the whole scene is under the resolution of VGA. There are totally six subjects in our experiments. Kinect only takes one single shot of each subject’s face, and the proposed method successfully reconstructs the dense 3D point cloud of his/her face.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig7">7</a>a, it can be seen that holes exist on the surface of the nose area in the initial 3D facial point cloud. After the final stage of our method, the majority of the holes in that area is filled up leading to a denser point cloud. The surface is much smoother without the loss of details, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig7">7</a>b. The six participants for experiments are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig6">6</a> with their facial region recognized in green rectangles. The dense 3D facial point clouds reconstructed for them are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig8">8</a>. As shown in the results, our method can extract human 3D face regions exactly from the complex scenes. The details of the participant’s face can be depicted in the reconstructed 3D facial model. The surface of the face is continuous, and almost all the data holes in the face are filled up. The performance of the proposed method is stable across six participants.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Demonstration of the interpolation performance in the nose area. <b>a</b> The initial 3D facial point cloud achieved by the first stage; <b>b</b> the final dense 3D facial model obtained by the second stage. The nose area both in <b>a</b>, <b>b</b> is magnified for comparison</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Dense 3D facial point cloud producing results. <b>a</b>–<b>f</b> six group of experiments with one subject in each group. In each group, <i>I</i>–<i>IV</i> are the initial 3D facial point cloud obtained after the first stage of proposed method, while <i>V</i>–<i>VIII</i> are the dense 3D facial point cloud obtained after final stage of the proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The density of the reconstructed point cloud is high enough to convey detailed information of a human face. The performance of the dense 3D point reconstruction is shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0311-6#Tab1">1</a>, which contains the numbers of the recovered 3D points in two stages of the proposed method for comparison. According to Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0311-6#Tab1">1</a>, with the proposed method, the second stage can generate the points almost 10 times more than the points in the 3D point cloud obtained in the first stage. This makes the surface of the 3D face much smoother.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Interpolation performance</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0311-6/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We also compare our method with other similar methods proposed in the literature for 3D facial modeling. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0311-6#Fig9">9</a>, the comparison is demonstrated for the percentages of the inaccurate points in the point clouds generated using our method and the method proposed by Hernandez et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Hernandez M, Choi J, Medioni G (2015) Near laser-scan quality 3-D face reconstruction from a low-quality depth stream. Image Vis Comput 36:61–69" href="/article/10.1007/s10055-017-0311-6#ref-CR15" id="ref-link-section-d55106e2665">2015</a>). As can be observed, almost no point in the point cloud has the error larger than ±5.5 mm in our method. The proposed method has a more stable performance with less errors in the reconstructed results. Moreover, over-smoothness in the results is successfully suppressed in our method, which is not the case for Hernandez’s approach. Other comparisons are demonstrated in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0311-6#Tab2">2</a>, which shows the competitive results of the proposed method.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0311-6/MediaObjects/10055_2017_311_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Comparisons of error distributions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0311-6/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Comparisons of means and SDs of the errors</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0311-6/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Conclusion</h2><div class="c-article-section__content" id="Sec7-content"><p>In this paper, a novel method that generates dense 3D facial point cloud is presented. The first contribution of the proposed method is that we simplify the process of dense 3D facial reconstruction, which is easily achieved with only one shot of an off-the-shelf RGB-D sensor aimed at a human face without any further manual intervention. It is most suitable for the VR tasks that require to be carried out by the system itself. Moreover, the reconstructed point cloud is dense with detailed information of human face along with facial texture, which makes virtual 3D facial analysis more accurate and more effective. The total number of the points in the cloud is around a hundred thousand. The second contribution is that the proposed method can be achieved at a very low cost without degradation of the performance. The only sensor used in our method is an inexpensive off-the-shelf RGB-D sensor, for example, the Microsoft’s Kinect as demonstrated in this paper. Furthermore, the low resolution of the inexpensive sensor does not affect the performance of the proposed method. Applications of the proposed method can be expected in many face-related tasks in VR applications.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Amidror, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Amidror I (2002) Scattered data interpolation methods for electronic imaging systems: a survey. J Electron Ima" /><p class="c-article-references__text" id="ref-CR1">Amidror I (2002) Scattered data interpolation methods for electronic imaging systems: a survey. J Electron Imaging 11:157–176</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1117%2F1.1455013" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scattered%20data%20interpolation%20methods%20for%20electronic%20imaging%20systems%3A%20a%20survey&amp;journal=J%20Electron%20Imaging&amp;volume=11&amp;pages=157-176&amp;publication_year=2002&amp;author=Amidror%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Beis JS, Lowe DG (1997) Shape indexing using approximate nearest-neighbour search in high-dimensional spaces. " /><p class="c-article-references__text" id="ref-CR2">Beis JS, Lowe DG (1997) Shape indexing using approximate nearest-neighbour search in high-dimensional spaces. In: Proceedings of IEEE computer society conference on computer vision and pattern recognition, 1997. IEEE, pp 1000–1006</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bradley D, Heidrich W, Popa T, Sheffer (2010) A high resolution passive facial performance capture. In: ACM tr" /><p class="c-article-references__text" id="ref-CR3">Bradley D, Heidrich W, Popa T, Sheffer (2010) A high resolution passive facial performance capture. In: ACM transactions on graphics (TOG), 2010, vol 4. ACM, p 41</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brown RA (2015) Building kd Tree in O (knlog n) Time. J Comput Graph Tech 4:50–68" /><p class="c-article-references__text" id="ref-CR4">Brown RA (2015) Building kd Tree in O (knlog n) Time. J Comput Graph Tech 4:50–68</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Chen, J. Li, J. Zeng, B. Wang, G. Lu, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Chen G, Li J, Zeng J, Wang B, Lu G (2016) Optimizing human model reconstruction from RGB-D images based on ski" /><p class="c-article-references__text" id="ref-CR5">Chen G, Li J, Zeng J, Wang B, Lu G (2016) Optimizing human model reconstruction from RGB-D images based on skin detection. Virtual Real 20:159–172</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-016-0291-y" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimizing%20human%20model%20reconstruction%20from%20RGB-D%20images%20based%20on%20skin%20detection&amp;journal=Virtual%20Real&amp;volume=20&amp;pages=159-172&amp;publication_year=2016&amp;author=Chen%2CG&amp;author=Li%2CJ&amp;author=Zeng%2CJ&amp;author=Wang%2CB&amp;author=Lu%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Chiabrando, R. Chiabrando, D. Piatti, F. Rinaudo, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Chiabrando F, Chiabrando R, Piatti D, Rinaudo F (2009) Sensors for 3D imaging: metric evaluation and calibrati" /><p class="c-article-references__text" id="ref-CR6">Chiabrando F, Chiabrando R, Piatti D, Rinaudo F (2009) Sensors for 3D imaging: metric evaluation and calibration of a CCD/CMOS time-of-flight camera. Sensors 9:10080–10096</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs91210080" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensors%20for%203D%20imaging%3A%20metric%20evaluation%20and%20calibration%20of%20a%20CCD%2FCMOS%20time-of-flight%20camera&amp;journal=Sensors&amp;volume=9&amp;pages=10080-10096&amp;publication_year=2009&amp;author=Chiabrando%2CF&amp;author=Chiabrando%2CR&amp;author=Piatti%2CD&amp;author=Rinaudo%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Danescu, F. Oniga, V. Turcu, O. Cristea, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Danescu R, Oniga F, Turcu V, Cristea O (2012) Long baseline stereovision for automatic detection and ranging o" /><p class="c-article-references__text" id="ref-CR7">Danescu R, Oniga F, Turcu V, Cristea O (2012) Long baseline stereovision for automatic detection and ranging of moving objects in the night sky. Sensors 12:12940–12963</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs121012940" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Long%20baseline%20stereovision%20for%20automatic%20detection%20and%20ranging%20of%20moving%20objects%20in%20the%20night%20sky&amp;journal=Sensors&amp;volume=12&amp;pages=12940-12963&amp;publication_year=2012&amp;author=Danescu%2CR&amp;author=Oniga%2CF&amp;author=Turcu%2CV&amp;author=Cristea%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Essabbah, G. Bouyer, S. Otmane, M. Mallem, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Essabbah M, Bouyer G, Otmane S, Mallem M (2014) A framework to design 3D interaction assistance in constraints" /><p class="c-article-references__text" id="ref-CR8">Essabbah M, Bouyer G, Otmane S, Mallem M (2014) A framework to design 3D interaction assistance in constraints-based virtual environments. Virtual Real 18:219–234</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-014-0247-z" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20framework%20to%20design%203D%20interaction%20assistance%20in%20constraints-based%20virtual%20environments&amp;journal=Virtual%20Real&amp;volume=18&amp;pages=219-234&amp;publication_year=2014&amp;author=Essabbah%2CM&amp;author=Bouyer%2CG&amp;author=Otmane%2CS&amp;author=Mallem%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GE. Fasshauer, MJ. McCourt, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Fasshauer GE, McCourt MJ (2012) Stable evaluation of Gaussian radial basis function interpolants. SIAM J Sci C" /><p class="c-article-references__text" id="ref-CR9">Fasshauer GE, McCourt MJ (2012) Stable evaluation of Gaussian radial basis function interpolants. SIAM J Sci Comput 34:A737–A762</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2914302" aria-label="View reference 9 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1137%2F110824784" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1252.65028" aria-label="View reference 9 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Stable%20evaluation%20of%20Gaussian%20radial%20basis%20function%20interpolants&amp;journal=SIAM%20J%20Sci%20Comput&amp;volume=34&amp;pages=A737-A762&amp;publication_year=2012&amp;author=Fasshauer%2CGE&amp;author=McCourt%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Franke R, Nielson GM (1991) Scattered data interpolation and applications: a tutorial and survey. In: Hagen H," /><p class="c-article-references__text" id="ref-CR10">Franke R, Nielson GM (1991) Scattered data interpolation and applications: a tutorial and survey. In: Hagen H, Roller D (eds) Geometric modeling. Computer Graphics—Systems and Applications. Springer, Berlin, pp 131–160</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Garcia E, Dugelay J-L (2001) Low cost 3D face acquisition and modeling. In: Proceedings of international confe" /><p class="c-article-references__text" id="ref-CR11">Garcia E, Dugelay J-L (2001) Low cost 3D face acquisition and modeling. In: Proceedings of international conference on information technology: coding and computing. IEEE, pp 657–661</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Han, L. Shao, D. Xu, J. Shotton, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Han J, Shao L, Xu D, Shotton J (2013) Enhanced computer vision with microsoft kinect sensor: a review. IEEE Tr" /><p class="c-article-references__text" id="ref-CR12">Han J, Shao L, Xu D, Shotton J (2013) Enhanced computer vision with microsoft kinect sensor: a review. IEEE Trans Cybern 43:1318–1334</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCB.2012.2228851" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Enhanced%20computer%20vision%20with%20microsoft%20kinect%20sensor%3A%20a%20review&amp;journal=IEEE%20Trans%20Cybern&amp;volume=43&amp;pages=1318-1334&amp;publication_year=2013&amp;author=Han%2CJ&amp;author=Shao%2CL&amp;author=Xu%2CD&amp;author=Shotton%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Har-Peled, P. Indyk, R. Motwani, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Har-Peled S, Indyk P, Motwani R (2012) Approximate nearest neighbor: towards removing the curse of dimensional" /><p class="c-article-references__text" id="ref-CR13">Har-Peled S, Indyk P, Motwani R (2012) Approximate nearest neighbor: towards removing the curse of dimensionality. Theory Comput 8:321–350</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2948494" aria-label="View reference 13 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4086%2Ftoc.2012.v008a014" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1278.68344" aria-label="View reference 13 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Approximate%20nearest%20neighbor%3A%20towards%20removing%20the%20curse%20of%20dimensionality&amp;journal=Theory%20Comput&amp;volume=8&amp;pages=321-350&amp;publication_year=2012&amp;author=Har-Peled%2CS&amp;author=Indyk%2CP&amp;author=Motwani%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Hartley, A. Zisserman, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR14">Hartley R, Zisserman A (2003) Multiple view geometry in computer vision. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple%20view%20geometry%20in%20computer%20vision&amp;publication_year=2003&amp;author=Hartley%2CR&amp;author=Zisserman%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Hernandez, J. Choi, G. Medioni, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Hernandez M, Choi J, Medioni G (2015) Near laser-scan quality 3-D face reconstruction from a low-quality depth" /><p class="c-article-references__text" id="ref-CR15">Hernandez M, Choi J, Medioni G (2015) Near laser-scan quality 3-D face reconstruction from a low-quality depth stream. Image Vis Comput 36:61–69</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.imavis.2014.12.004" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Near%20laser-scan%20quality%203-D%20face%20reconstruction%20from%20a%20low-quality%20depth%20stream&amp;journal=Image%20Vis%20Comput&amp;volume=36&amp;pages=61-69&amp;publication_year=2015&amp;author=Hernandez%2CM&amp;author=Choi%2CJ&amp;author=Medioni%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Hernoux, O. Christmann, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Hernoux F, Christmann O (2015) A seamless solution for 3D real-time interaction: design and evaluation. Virtua" /><p class="c-article-references__text" id="ref-CR16">Hernoux F, Christmann O (2015) A seamless solution for 3D real-time interaction: design and evaluation. Virtual Real 19:1–20</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-014-0255-z" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20seamless%20solution%20for%203D%20real-time%20interaction%3A%20design%20and%20evaluation&amp;journal=Virtual%20Real&amp;volume=19&amp;pages=1-20&amp;publication_year=2015&amp;author=Hernoux%2CF&amp;author=Christmann%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hossain MS, Akbar M, Starkey JD (2007) Inexpensive construction of a 3D face model from stereo images. In: 10t" /><p class="c-article-references__text" id="ref-CR17">Hossain MS, Akbar M, Starkey JD (2007) Inexpensive construction of a 3D face model from stereo images. In: 10th international conference on computer and information technology. iccit 2007. IEEE, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Hwang, S. Yu, J. Kim, S. Lee, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Hwang J, Yu S, Kim J, Lee S (2012) 3D face modeling using the multi-deformable method. Sensors 12:12870–12889" /><p class="c-article-references__text" id="ref-CR18">Hwang J, Yu S, Kim J, Lee S (2012) 3D face modeling using the multi-deformable method. Sensors 12:12870–12889</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs121012870" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20face%20modeling%20using%20the%20multi-deformable%20method&amp;journal=Sensors&amp;volume=12&amp;pages=12870-12889&amp;publication_year=2012&amp;author=Hwang%2CJ&amp;author=Yu%2CS&amp;author=Kim%2CJ&amp;author=Lee%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Jo, H. Choi, I-J. Kim, J. Kim, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Jo J, Choi H, Kim I-J, Kim J (2015) Single-view-based 3D facial reconstruction method robust against pose vari" /><p class="c-article-references__text" id="ref-CR19">Jo J, Choi H, Kim I-J, Kim J (2015) Single-view-based 3D facial reconstruction method robust against pose variations. Pattern Recogn 48:73–85</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2014.07.013" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Single-view-based%203D%20facial%20reconstruction%20method%20robust%20against%20pose%20variations&amp;journal=Pattern%20Recogn&amp;volume=48&amp;pages=73-85&amp;publication_year=2015&amp;author=Jo%2CJ&amp;author=Choi%2CH&amp;author=Kim%2CI-J&amp;author=Kim%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Lee, C-H. Choi, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Lee M, Choi C-H (2014) Real-time facial shape recovery from a single image under general, unknown lighting by " /><p class="c-article-references__text" id="ref-CR20">Lee M, Choi C-H (2014) Real-time facial shape recovery from a single image under general, unknown lighting by rank relaxation. Comput Vis Image Underst 120:59–69</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2013.12.010" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20facial%20shape%20recovery%20from%20a%20single%20image%20under%20general%2C%20unknown%20lighting%20by%20rank%20relaxation&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=120&amp;pages=59-69&amp;publication_year=2014&amp;author=Lee%2CM&amp;author=Choi%2CC-H">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W-S. Lee, A. Soon, L. Zhu, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Lee W-S, Soon A, Zhu L (2007) 3D facial model exaggeration builder for small or large sized model manufacturin" /><p class="c-article-references__text" id="ref-CR21">Lee W-S, Soon A, Zhu L (2007) 3D facial model exaggeration builder for small or large sized model manufacturing. Virtual Real 11:229–239</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-007-0071-9" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20facial%20model%20exaggeration%20builder%20for%20small%20or%20large%20sized%20model%20manufacturing&amp;journal=Virtual%20Real&amp;volume=11&amp;pages=229-239&amp;publication_year=2007&amp;author=Lee%2CW-S&amp;author=Soon%2CA&amp;author=Zhu%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I-C. Lin, J-S. Yeh, M. Ouhyoung, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Lin I-C, Yeh J-S, Ouhyoung M (2002) Extracting 3D facial animation parameters from multiview video clips. IEEE" /><p class="c-article-references__text" id="ref-CR22">Lin I-C, Yeh J-S, Ouhyoung M (2002) Extracting 3D facial animation parameters from multiview video clips. IEEE Comput Graph Appl 22:72–80</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2002.1046631" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Extracting%203D%20facial%20animation%20parameters%20from%20multiview%20video%20clips&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=22&amp;pages=72-80&amp;publication_year=2002&amp;author=Lin%2CI-C&amp;author=Yeh%2CJ-S&amp;author=Ouhyoung%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mecca R, Wetzler A, Kimmel R, Bruckstein AM (2013) Direct shape recovery from photometric stereo with shadows." /><p class="c-article-references__text" id="ref-CR23">Mecca R, Wetzler A, Kimmel R, Bruckstein AM (2013) Direct shape recovery from photometric stereo with shadows. In: 2013 International conference on 3D vision-3DV 2013. IEEE, pp 382–389</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TT. Nguyen, DC. Slaughter, N. Max, JN. Maloof, N. Sinha, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Nguyen TT, Slaughter DC, Max N, Maloof JN, Sinha N (2015) Structured light-based 3D reconstruction system for " /><p class="c-article-references__text" id="ref-CR24">Nguyen TT, Slaughter DC, Max N, Maloof JN, Sinha N (2015) Structured light-based 3D reconstruction system for plants. Sensors 15:18587–18612</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs150818587" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Structured%20light-based%203D%20reconstruction%20system%20for%20plants&amp;journal=Sensors&amp;volume=15&amp;pages=18587-18612&amp;publication_year=2015&amp;author=Nguyen%2CTT&amp;author=Slaughter%2CDC&amp;author=Max%2CN&amp;author=Maloof%2CJN&amp;author=Sinha%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Niclass, M. Soga, H. Matsubara, M. Ogawa, M. Kagami, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Niclass C, Soga M, Matsubara H, Ogawa M, Kagami M (2014) A 0.18-m CMOS SoC for a 100-m-range 10-frame/s 200 96" /><p class="c-article-references__text" id="ref-CR25">Niclass C, Soga M, Matsubara H, Ogawa M, Kagami M (2014) A 0.18-m CMOS SoC for a 100-m-range 10-frame/s 200 96-pixel time-of-flight depth sensor. IEEE J Solid State Circuits 49:315–330</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJSSC.2013.2284352" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%200.18-m%20CMOS%20SoC%20for%20a%20100-m-range%2010-frame%2Fs%20200%2096-pixel%20time-of-flight%20depth%20sensor&amp;journal=IEEE%20J%20Solid%20State%20Circuits&amp;volume=49&amp;pages=315-330&amp;publication_year=2014&amp;author=Niclass%2CC&amp;author=Soga%2CM&amp;author=Matsubara%2CH&amp;author=Ogawa%2CM&amp;author=Kagami%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Nielson, H. Hagen, H. Muller, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Nielson G, Hagen H, Muller H (1997) Scientific visualization. Institute of Electrical and Electronics Engineer" /><p class="c-article-references__text" id="ref-CR26">Nielson G, Hagen H, Muller H (1997) Scientific visualization. Institute of Electrical and Electronics Engineers, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scientific%20visualization&amp;publication_year=1997&amp;author=Nielson%2CG&amp;author=Hagen%2CH&amp;author=Muller%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: 2006 IEEE computer society confe" /><p class="c-article-references__text" id="ref-CR27">Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR’06). IEEE, pp 2161–2168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Qu, W-P. Brinkman, Y. Ling, P. Wiggers, I. Heynderickx, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Qu C, Brinkman W-P, Ling Y, Wiggers P, Heynderickx I (2013) Human perception of a conversational virtual human" /><p class="c-article-references__text" id="ref-CR28">Qu C, Brinkman W-P, Ling Y, Wiggers P, Heynderickx I (2013) Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture. Virtual Real 17:307–321</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-013-0231-z" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20perception%20of%20a%20conversational%20virtual%20human%3A%20an%20empirical%20study%20on%20the%20effect%20of%20emotion%20and%20culture&amp;journal=Virtual%20Real&amp;volume=17&amp;pages=307-321&amp;publication_year=2013&amp;author=Qu%2CC&amp;author=Brinkman%2CW-P&amp;author=Ling%2CY&amp;author=Wiggers%2CP&amp;author=Heynderickx%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PV. Rao, SKM. Rao, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Rao PV, Rao SKM (2014) Performance issues on K-mean partitioning clustering algorithm. Int J Comput IJC 14:41–" /><p class="c-article-references__text" id="ref-CR29">Rao PV, Rao SKM (2014) Performance issues on K-mean partitioning clustering algorithm. Int J Comput IJC 14:41–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance%20issues%20on%20K-mean%20partitioning%20clustering%20algorithm&amp;journal=Int%20J%20Comput%20IJC&amp;volume=14&amp;pages=41-51&amp;publication_year=2014&amp;author=Rao%2CPV&amp;author=Rao%2CSKM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Sibson, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Sibson R (1981) A brief description of natural neighbour interpolation. Interpret Multivar Data 21:21–36" /><p class="c-article-references__text" id="ref-CR30">Sibson R (1981) A brief description of natural neighbour interpolation. Interpret Multivar Data 21:21–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20brief%20description%20of%20natural%20neighbour%20interpolation&amp;journal=Interpret%20Multivar%20Data&amp;volume=21&amp;pages=21-36&amp;publication_year=1981&amp;author=Sibson%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Silpa-Anan C, Hartley R (2008) Optimised KD-trees for fast image descriptor matching. In: IEEE conference on c" /><p class="c-article-references__text" id="ref-CR31">Silpa-Anan C, Hartley R (2008) Optimised KD-trees for fast image descriptor matching. In: IEEE conference on computer vision and pattern recognition, 2008. CVPR 2008. IEEE, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PI. Wilson, J. Fernandez, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Wilson PI, Fernandez J (2006) Facial feature detection using Haar classifiers. J Comput Sci Coll 21:127–133" /><p class="c-article-references__text" id="ref-CR32">Wilson PI, Fernandez J (2006) Facial feature detection using Haar classifiers. J Comput Sci Coll 21:127–133</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20feature%20detection%20using%20Haar%20classifiers&amp;journal=J%20Comput%20Sci%20Coll&amp;volume=21&amp;pages=127-133&amp;publication_year=2006&amp;author=Wilson%2CPI&amp;author=Fernandez%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Yu, OG. Garrod, PG. Schyns, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Yu H, Garrod OG, Schyns PG (2012) Perception-driven facial expression synthesis. Comput Graph 36:152–162" /><p class="c-article-references__text" id="ref-CR33">Yu H, Garrod OG, Schyns PG (2012) Perception-driven facial expression synthesis. Comput Graph 36:152–162</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2011.12.002" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception-driven%20facial%20expression%20synthesis&amp;journal=Comput%20Graph&amp;volume=36&amp;pages=152-162&amp;publication_year=2012&amp;author=Yu%2CH&amp;author=Garrod%2COG&amp;author=Schyns%2CPG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhang, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19:4–10" /><p class="c-article-references__text" id="ref-CR34">Zhang Z (2012) Microsoft kinect sensor and its effect. IEEE Multimed 19:4–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMMUL.2012.24" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Microsoft%20kinect%20sensor%20and%20its%20effect&amp;journal=IEEE%20Multimed&amp;volume=19&amp;pages=4-10&amp;publication_year=2012&amp;author=Zhang%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhu J, Wang L, Yang R, Davis J (2008) Fusion of time-of-flight depth and stereo for high accuracy depth maps. " /><p class="c-article-references__text" id="ref-CR35">Zhu J, Wang L, Yang R, Davis J (2008) Fusion of time-of-flight depth and stereo for high accuracy depth maps. In: IEEE conference on computer vision and pattern recognition, 2008. CVPR 2008. IEEE, pp 1–8</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0311-6-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The Engineering and Physical Sciences Research Council Project (EPSRC), UK (No. EP/N025849/1); EU seventh framework programme under Grant Agreement No. 611391; National Natural Science Foundation of China (NSFC) (No. 41576011); and the International Science and Technology Cooperation Program of China (ISTCP) (No. 2014DFA10410).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Ocean University of China, Qingdao, 266100, China</p><p class="c-article-author-affiliation__authors-list">Shu Zhang, Lin Qi &amp; Junyu Dong</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">University of Portsmouth, Portsmouth, PO1 2DJ, UK</p><p class="c-article-author-affiliation__authors-list">Shu Zhang, Hui Yu &amp; Honghai Liu</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Shandong University of Science and Technology, Qingdao, 266590, China</p><p class="c-article-author-affiliation__authors-list">Ting Wang</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Shu-Zhang"><span class="c-article-authors-search__title u-h3 js-search-name">Shu Zhang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Shu+Zhang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shu+Zhang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shu+Zhang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Hui-Yu"><span class="c-article-authors-search__title u-h3 js-search-name">Hui Yu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hui+Yu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hui+Yu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hui+Yu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ting-Wang"><span class="c-article-authors-search__title u-h3 js-search-name">Ting Wang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ting+Wang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ting+Wang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ting+Wang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Lin-Qi"><span class="c-article-authors-search__title u-h3 js-search-name">Lin Qi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Lin+Qi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lin+Qi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lin+Qi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Junyu-Dong"><span class="c-article-authors-search__title u-h3 js-search-name">Junyu Dong</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Junyu+Dong&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Junyu+Dong" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Junyu+Dong%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Honghai-Liu"><span class="c-article-authors-search__title u-h3 js-search-name">Honghai Liu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Honghai+Liu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Honghai+Liu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Honghai+Liu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0311-6/email/correspondent/c1/new">Junyu Dong</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Dense%203D%20facial%20reconstruction%20from%20a%20single%20depth%20image%20in%20unconstrained%20environment&amp;author=Shu%20Zhang%20et%20al&amp;contentID=10.1007%2Fs10055-017-0311-6&amp;publication=1359-4338&amp;publicationDate=2017-04-27&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0311-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0311-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Zhang, S., Yu, H., Wang, T. <i>et al.</i> Dense 3D facial reconstruction from a single depth image in unconstrained environment.
                    <i>Virtual Reality</i> <b>22, </b>37–46 (2018). https://doi.org/10.1007/s10055-017-0311-6</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0311-6.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-10-06">06 October 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-03-28">28 March 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-04-27">27 April 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03">March 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0311-6" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0311-6</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual face</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Three-dimensional image acquisition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Three-dimensional sensing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D interpolation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0311-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=311;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

