<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Musical creativity in collaborative virtual environments"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individual parameters to higher level gestural..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/10/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Musical creativity in collaborative virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2006 10:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-09-07"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individual parameters to higher level gestural influence over whole systems. Musical performances in CVE also show a consistent re-emergence of a unique form of collaboration called &#8220;melding&#8221; in which individual virtuosity is subsumed to that of the group. Based on these observations, we hypothesized that CVE could be a medium for creating new forms of music, and developed the audiovisual augmented reality system (AVIARy) to explore higher level metaphors for composing spatial music in CVE. This paper describes the AVIARy system, the initial experiments with interaction metaphors, and the application of the system to develop and stage a collaborative musical performance at a sound art concert. The results from these experiments indicate that CVE can be a medium for new forms of musical creativity and distinctive forms of music."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-09-07"/>

    <meta name="prism.volume" content="10"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="149"/>

    <meta name="prism.endingPage" content="157"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0043-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0043-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0043-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0043-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Musical creativity in collaborative virtual environments"/>

    <meta name="citation_volume" content="10"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2006/10"/>

    <meta name="citation_online_date" content="2006/09/07"/>

    <meta name="citation_firstpage" content="149"/>

    <meta name="citation_lastpage" content="157"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0043-5"/>

    <meta name="DOI" content="10.1007/s10055-006-0043-5"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0043-5"/>

    <meta name="description" content="A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individu"/>

    <meta name="dc.creator" content="Stephen Barrass"/>

    <meta name="dc.creator" content="Tim Barrass"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Leonardo Music J; citation_title=Displaced Soundscapes: a survey of network systems for music and sonic art creation; citation_author=A Barbosa; citation_volume=13; citation_publication_date=2003; citation_pages=53-59; citation_doi=10.1162/096112104322750791; citation_id=CR1"/>

    <meta name="citation_reference" content="Bargar R, Choi I, Das S, Goudeseune C (1994) Model-based interactive sound for an immersive virtual environment. In: Proceedings of the international computer music conference, Aarhus "/>

    <meta name="citation_reference" content="Bencina R (2005) P5 Glove developments. 
                    http://www.audiomulch.com/simulus/p5glove 
                    
                  (visited 31 July 2006)"/>

    <meta name="citation_reference" content="Bencina R, Burk P (2003) PortAudio&#8211;an open source cross platform audio API. 
                    http://www.portaudio.com/
                    
                   (visited 31 July 2006)"/>

    <meta name="citation_reference" content="Berry R, Makino M, Hikawa N, Suzuki M (2003) The augmented composer project: the music table. In: Proceedings of the second IEEE and ACM international symposium on mixed and augmented reality, San Francisco "/>

    <meta name="citation_reference" content="citation_title=Graphics and graphic information processing; citation_publication_date=1981; citation_id=CR6; citation_author=J Bertin; citation_publisher=Walter de Gruter"/>

    <meta name="citation_reference" content="Billinghurst M, Kato H (1999) Collaborative mixed reality. In: Proceedings of the international symposium on mixed reality, Yokohama "/>

    <meta name="citation_reference" content="citation_journal_title=Comput Music J; citation_title=Music for an interactive network of microcomputers; citation_author=J Bischoff, R Gold, J Horton; citation_volume=2; citation_publication_date=1978; citation_pages=24-29; citation_id=CR8"/>

    <meta name="citation_reference" content="Borchers JO, Samminger W, M&#252;hlh&#228;user M (2001) Conducting a realistic electronic orchestra. In: Proceedings of the 14th annual symposium on user interface software and technology, Orlando"/>

    <meta name="citation_reference" content="Burke P (1998) JSyn&#8212;a real-time synthesis API for Java. In: Proceedings of the international computer music conference, Ann Arbor "/>

    <meta name="citation_reference" content="Carlile J, Hartmann B (2005) OROBORO: a collaborative controller with interpersonal haptic feedback. In: Proceedings of the conference on new interfaces for musical expression, Vancouver "/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The cave-audio visual experience automatic virtual environment; citation_author=C Cruz-Neira, DJ Sandin, TA DeFanti, RV Kenyon, JC Hart; citation_volume=35; citation_issue=6; citation_publication_date=1992; citation_pages=64-72; citation_doi=10.1145/129888.129892; citation_id=CR12"/>

    <meta name="citation_reference" content="Eckel G (1999) Application for the cyberstage spatial sound server. In: Proceedings of the AES 16th international conference: spatial sound reproduction, Rovaniemi "/>

    <meta name="citation_reference" content="Gadd A, Fels S (2002) MetaMuse: metaphors for expressive instruments. In: Proceedings of the conference on new interfaces for musical expression, Dublin "/>

    <meta name="citation_reference" content="Geiger C, Reimann C, Stocklein J, Paelke V (2002) JARToolkit-Java binding for AR toolkit. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt "/>

    <meta name="citation_reference" content="Hinckley K, Pausch R, Goble J.C, Kassell NF (1994) A three-dimensional user interface for neurosurgical visualization. In: Proceedings of the SPIE conference on medical imaging"/>

    <meta name="citation_reference" content="Hypersense Complex (2003) 
                    http://www.arrowtheory.com/hypersense/
                    
                   (visited 31 July 2006)"/>

    <meta name="citation_reference" content="citation_title=The beehive java3D audio device; citation_publication_date=2003; citation_id=CR18; citation_author=T Jacob; citation_author=S Barrass; citation_publisher=The Australian meeting on visualisation and virtual reality"/>

    <meta name="citation_reference" content="Jorda S, Kaltenbrunner M, Geiger G, and Bencina R (2005) The reacTable. In: Proceedings of the international computer music conference, Barcelona"/>

    <meta name="citation_reference" content="Jung B, Hwang J, Lee S, Kim GJ, Kim H (2000) Incorporating co-presence in distributed virtual music environment. In: Proceedings of the ACM symposium on virtual reality software and technology, Seoul "/>

    <meta name="citation_reference" content="Kaper HG, Tipei S, Wiebel E (1997) High-performance computing, music composition, and the sonification of scientific data, technical report ANL/MCS-P690-0997"/>

    <meta name="citation_reference" content="citation_journal_title=J Commun; citation_title=An insider&#8217;s view of the future of virtual reality; citation_author=J Lanier, F Biocca; citation_volume=42; citation_issue=4; citation_publication_date=1992; citation_pages=150; citation_doi=10.1111/j.1460-2466.1992.tb00816.x; citation_id=CR22"/>

    <meta name="citation_reference" content="Machover T, Chung J (1989) Hyperinstruments: musically intelligent and interactive performance and creativity systems. In: Proceedings of the international computer music conference, Columbus "/>

    <meta name="citation_reference" content="Massie TH, Salisbury JK (1994) The PHANToM haptic interface: a device for probing virtual objects. ASME winter annual meeting"/>

    <meta name="citation_reference" content="M&#228;ki-Patola T (2005) User interface comparison for virtual drums. In: Proceedings of the international conference on new interfaces for musical expression, Vancouver"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput; citation_title=A computer music system that follows a human conductor&#8221;; citation_author=H Morita, S Hashimoto, S Ohteru; citation_volume=24; citation_issue=7; citation_publication_date=1991; citation_pages=44-53; citation_id=CR26"/>

    <meta name="citation_reference" content="Morse (2004) AVIARy: audio visualisation interactive augmented reality, the australian meeting on visualisation and virtual reality, Brisbane. 
                    http://www.petermorse.com.au/PMcomhtml/aviary.html 
                    
                  (visited 31 July 2006)"/>

    <meta name="citation_reference" content="M&#228;ki-Patola T, Kanerva A, Laitinen J, Takala T (2005) Experiments with virtual reality instruments. In: Proceedings of the international conference on new interfaces for musical expression, Vancouver"/>

    <meta name="citation_reference" content="O&#8217;Modhrain S (2000) Playing by feel: incorporating haptic feedback into computer-based musical instruments. PhD Thesis, Stanford University"/>

    <meta name="citation_reference" content="Poupyrev I, Berry R, Kurumisawa J, Nakao K, Billinghurst M, Airola C, Kato H, Yonezawa T, Baldwin L (2000) Augmented groove: collaborative jamming in augmented reality. SIGGRAPH&#8217;2000 conference abstracts and applications. ACM"/>

    <meta name="citation_reference" content="Pulkki V (2001) Spatial sound generation and perception by amplitude panning techniques. PhD Thesis, Helsinki University of Technology, Espoo"/>

    <meta name="citation_reference" content="Ruspini D, Khatib O (1998) Acoustic cues for haptic rendering systems. In: Proceedings of the PHANTOM users group, Dedham"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE MultiMedia; citation_title=Conducting a virtual orchestra; citation_author=S Schertenleib, M Gutierrez, V Vexo, D Thalmann; citation_volume=11; citation_issue=3; citation_publication_date=2004; citation_pages=40-49; citation_doi=10.1109/MMUL.2004.5; citation_id=CR33"/>

    <meta name="citation_reference" content="Sensable (2004) HapticSound demonstration. ACM SIGGRAPH conference on graphics and interaction techniques. 
                    http://www.sensable.com/popup_haptic_application_videos.asp 
                    
                  (visited 31 July 2006)"/>

    <meta name="citation_reference" content="Sheridan J, Sood G, Jacob T, Gardner H, Barrass S (2004) SoundStudio4D-a VR interface for gestural composition of spatial soundscapes. In: Proceedings of the international conference on auditory display, Sydney "/>

    <meta name="citation_reference" content="Sood G, Barrass S (2003) Groovy tubes: an interface for designing sound effects in space and time. In: Proceedings of the australian conference on computer human interaction, Brisbane"/>

    <meta name="citation_reference" content="Wellner P (1992) The DigitalDesk calculator: tactile manipulation on a desktop display. In: Proceedings of the international conference on user interface and software technology, Hilton Head "/>

    <meta name="citation_reference" content="Yu J, Machover T (1996) The Palette, the brain opera. 
                    http://www.brainop.media.mit.edu/online/net-music/net-instrument/LabeledSharle.html
                    
                   (visited 31 July 2006)"/>

    <meta name="citation_author" content="Stephen Barrass"/>

    <meta name="citation_author_email" content="Stephen.Barrass@canberra.edu.au"/>

    <meta name="citation_author_institution" content="Sonic Communications Research Group, University of Canberra, Canberra, Australia"/>

    <meta name="citation_author" content="Tim Barrass"/>

    <meta name="citation_author_institution" content="Sonic Communications Research Group, University of Canberra, Canberra, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0043-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/10/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0043-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Musical creativity in collaborative virtual environments"/>
        <meta property="og:description" content="A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individual parameters to higher level gestural influence over whole systems. Musical performances in CVE also show a consistent re-emergence of a unique form of collaboration called “melding” in which individual virtuosity is subsumed to that of the group. Based on these observations, we hypothesized that CVE could be a medium for creating new forms of music, and developed the audiovisual augmented reality system (AVIARy) to explore higher level metaphors for composing spatial music in CVE. This paper describes the AVIARy system, the initial experiments with interaction metaphors, and the application of the system to develop and stage a collaborative musical performance at a sound art concert. The results from these experiments indicate that CVE can be a medium for new forms of musical creativity and distinctive forms of music."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Musical creativity in collaborative virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0043-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"CVE, Creativity, Music, Sound art, Interaction, Performance","kwrd":["CVE","Creativity","Music","Sound_art","Interaction","Performance"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0043-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0043-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=43;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0043-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Musical creativity in collaborative virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0043-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0043-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-09-07" itemprop="datePublished">07 September 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Musical creativity in collaborative virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stephen-Barrass" data-author-popup="auth-Stephen-Barrass" data-corresp-id="c1">Stephen Barrass<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Canberra" /><meta itemprop="address" content="grid.1039.b, 0000000403857472, Sonic Communications Research Group, University of Canberra, Canberra, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tim-Barrass" data-author-popup="auth-Tim-Barrass">Tim Barrass</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Canberra" /><meta itemprop="address" content="grid.1039.b, 0000000403857472, Sonic Communications Research Group, University of Canberra, Canberra, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 10</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">149</span>–<span itemprop="pageEnd">157</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">282 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0043-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>A review of musical creativity in collaborative virtual environments (CVE) shows recurring interaction metaphors that tend from precise control of individual parameters to higher level gestural influence over whole systems. Musical performances in CVE also show a consistent re-emergence of a unique form of collaboration called “melding” in which individual virtuosity is subsumed to that of the group. Based on these observations, we hypothesized that CVE could be a medium for creating new forms of music, and developed the audiovisual augmented reality system (AVIARy) to explore higher level metaphors for composing spatial music in CVE. This paper describes the AVIARy system, the initial experiments with interaction metaphors, and the application of the system to develop and stage a collaborative musical performance at a sound art concert. The results from these experiments indicate that CVE can be a medium for new forms of musical creativity and distinctive forms of music.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>People have been happily making beautiful music together long before anyone ever heard of collaborative virtual environments (CVE). But people are also curious, and avante-garde musicians and audiences have always been drawn to new technologies. In the past it seems it was quite usual that the premiere of any “new” kind of music was met with outrage and even physical hostility. However the fast past of technological change has unleashed a bit-torrent of mobile phone music, data music, glitch and an unending stream of fractionation and diversification of what music is. Strangely in this world of noisy technology churn, CVE remains quiet, dominated by visually oriented scientific, business, and industry applications, rather than the entertainment, cultural and creative applications that generate musical vibrations.</p><p>In this paper we will explore the musical creativity at the intersection of collaborative and VE technologies. What does this music sound like? Is it any different to other music? Who is making it? Who is listening? What benefits can CVEs bring for musical creativity? What is needed to make this quiet medium into one that is used to make music?</p><p>We touch on these issues in various ways through the rest of this paper, offering some illumination rather than definitive answers at this stage. The first section is a review of musical creativity at the intersection of collaborative and VE technologies, analysed in terms of levels of interaction. The second section describes the AVIARy CVE system developed specifically to prototype new metaphors for local, intermediate and global level interaction with spatial sounds. The third and final section describes the application of the system to develop and rehearse a collaborative musical performance for a public sound art concert, followed by observations of what was learnt from this experience.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Background</h2><div class="c-article-section__content" id="Sec2-content"><p>In the mid-1980s Jaron Lanier coined the term virtual reality (VR) and popularised the concept by performing music with “virtual instruments” wearing a head mounted display (HMD) and a VR glove with flex sensors in the fingers (Lanier and Biocca <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Lanier J, Biocca F (1992) An insider’s view of the future of virtual reality. J Commun 42(4):150" href="/article/10.1007/s10055-006-0043-5#ref-CR22" id="ref-link-section-d110111e301">1992</a>). Although musicians had been experimenting with “gestural” interfaces since the Theremin in the early 1920s, the concept of virtual instruments invoked a synaesthetic immersion in a world where anything that could be imagined could be realised. When Nintendo marketed the PowerGlove for computer games in 1989, it was not a popular success in the games market but found favour instead among VR enthusiasts who experimented with “gestural” interfaces for playing air-guitars, air-pianos, and air-drums. Although modeled on familiar instruments the latency between gesture and sound, and the lack of physical affordances and proprioceptive feedback did not allow for the same kind of musical control (Mäki-Patola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Mäki-Patola T (2005) User interface comparison for virtual drums. In: Proceedings of the international conference on new interfaces for musical expression, Vancouver" href="/article/10.1007/s10055-006-0043-5#ref-CR25" id="ref-link-section-d110111e304">2005</a>). However “gestural” interfaces were found to be suited to theatrical performances (such as an air-guitar contest) and public exhibitions where the familiar actions make them easy for players and audiences to understand (Mäki-Patola et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Mäki-Patola T, Kanerva A, Laitinen J, Takala T (2005) Experiments with virtual reality instruments. In: Proceedings of the international conference on new interfaces for musical expression, Vancouver" href="/article/10.1007/s10055-006-0043-5#ref-CR28" id="ref-link-section-d110111e307">2005</a>). In the late 1980s Tod Machover performed a collaborative musical piece called bug-mudra in which he used a glove to mix and manipulate the input from two guitarists and a percussionist using “conducting” gestures (Machover and Chung <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Machover T, Chung J (1989) Hyperinstruments: musically intelligent and interactive performance and creativity systems. In: Proceedings of the international computer music conference, Columbus " href="/article/10.1007/s10055-006-0043-5#ref-CR23" id="ref-link-section-d110111e310">1989</a>). The non-tactile musical metaphor was repeated when the glove was used to “conduct” a computer music system, also repeating the control of multiple parameters through gesture (Morita et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Morita H, Hashimoto S, Ohteru S (1991) A computer music system that follows a human conductor”. IEEE Comput 24(7):44–53" href="/article/10.1007/s10055-006-0043-5#ref-CR26" id="ref-link-section-d110111e313">1991</a>). In 2001, essential reality released the more advanced P5 Glove targeting PC-based games. Again, the glove was not popular with the intended market, but this time the fire sales fanned a culture of “gestural” glove music among computer laptop musicians reaching for higher level interfaces to multi-parameter synthesis algorithms than the sliders and checkboxes in the desktop graphical user interface (GUI) (Bencina <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bencina R, Burk P (2003) PortAudio–an open source cross platform audio API. &#xA;                    http://www.portaudio.com/&#xA;                    &#xA;                   (visited 31 July 2006)" href="/article/10.1007/s10055-006-0043-5#ref-CR4" id="ref-link-section-d110111e317">2005</a>). This “gestural” metaphor typically combined the general motion of the glove with more precise individual finger-based control of particular parameters. The number of parameters that could be simultaneously manipulated was dramatically increased when the glove music trio Hypersense Complex networked to become a six-gloved musician playing a single multi-parameter synthesis algorithm (Hypersense Complex <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hypersense Complex (2003) &#xA;                    http://www.arrowtheory.com/hypersense/&#xA;                    &#xA;                   (visited 31 July 2006)" href="/article/10.1007/s10055-006-0043-5#ref-CR17" id="ref-link-section-d110111e320">2003</a>). These performances identified a form of collaboration where the musical contribution of any performer is not distinguishable in the overall sound, akin to timbral blending in orchestral performances, and in contrast to the individuality of jazz and rock. In this paper we call this form of collaboration, where individual virtuosity is subsumed to the group, “melding”.</p><p>In 1992 the invention of the CAVE automatic virtual environment (CAVE) allowed a group of people to share the same virtual environment projected as stereographic imagery on the walls of a room (Cruz-Neira et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The cave-audio visual experience automatic virtual environment. Commun ACM 35(6):64–72" href="/article/10.1007/s10055-006-0043-5#ref-CR12" id="ref-link-section-d110111e326">1992</a>). By 1994 the Virtual Environments group at the NCSA included a six person audio team developing a sound server to render sounds co-located with the 3D graphic objects in the CAVE. This team included a composer-in-residence who performed by “probing” a sonification and visualisation of a chaotic algorithm with the six degree of freedom (6DOF) stylus (Bargar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Bargar R, Choi I, Das S, Goudeseune C (1994) Model-based interactive sound for an immersive virtual environment. In: Proceedings of the international computer music conference, Aarhus " href="/article/10.1007/s10055-006-0043-5#ref-CR2" id="ref-link-section-d110111e329">1994</a>). By 1996 there was a CAVE based Environment for Music Composition, called M4CAVE, which included software for music composition, score editing, digital sound synthesis, and sound visualization (Kaper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Kaper HG, Tipei S, Wiebel E (1997) High-performance computing, music composition, and the sonification of scientific data, technical report ANL/MCS-P690-0997" href="/article/10.1007/s10055-006-0043-5#ref-CR21" id="ref-link-section-d110111e332">1997</a>). At CeBIT 1998 a VE sound art installation called SoundSpheres enabled the general public to “probe” floating spheres with the stylus to produce a shared musical experience, with the intention to “shape some of the yet unstructured vocabulary of musical expression and experience in cyberspace” (Eckel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Eckel G (1999) Application for the cyberstage spatial sound server. In: Proceedings of the AES 16th international conference: spatial sound reproduction, Rovaniemi " href="/article/10.1007/s10055-006-0043-5#ref-CR13" id="ref-link-section-d110111e335">1999</a>). As with the glove, the stylus also suggested a “conducting” metaphor and began to be used in experiments with distributed music rehearsals in VEs (Schertenleib et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Schertenleib S, Gutierrez M, Vexo V, Thalmann D (2004) Conducting a virtual orchestra. IEEE MultiMedia 11(3):40–49" href="/article/10.1007/s10055-006-0043-5#ref-CR33" id="ref-link-section-d110111e338">2004</a>), and in public installations such as the Virtual Conductor at the Haus of Music in Vienna (Borchers JO et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Borchers JO, Samminger W, Mühlhäuser M (2001) Conducting a realistic electronic orchestra. In: Proceedings of the 14th annual symposium on user interface software and technology, Orlando" href="/article/10.1007/s10055-006-0043-5#ref-CR9" id="ref-link-section-d110111e342">2001</a>).</p><p>The affordances of the VE stylus are very similar to a pen and the SoundStudio4D system explores “3D drawing” as a metaphor for composing spatial soundscapes in VEs, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig1">1</a>. The pen is used to draw paths of sounds that can be branched to compose arbitrarily complex scenes, and time is visualised by changes in hue so that simultaneous regions have the same colour (Sheridan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sheridan J, Sood G, Jacob T, Gardner H, Barrass S (2004) SoundStudio4D-a VR interface for gestural composition of spatial soundscapes. In: Proceedings of the international conference on auditory display, Sydney " href="/article/10.1007/s10055-006-0043-5#ref-CR35" id="ref-link-section-d110111e351">2004</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>“3D drawing” as a metaphor for composing spatial soundscenes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The paths can pass through tubes that are visualisations of special effect parameters at specific points in space and time. For example the width of the tube, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig2">2</a>, controlled the resonance frequency of a band-pass filter (Sood and Barrass <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sood G, Barrass S (2003) Groovy tubes: an interface for designing sound effects in space and time. In: Proceedings of the australian conference on computer human interaction, Brisbane" href="/article/10.1007/s10055-006-0043-5#ref-CR36" id="ref-link-section-d110111e378">2003</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>“3D drawing” of sound effect parameters</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>An evaluation of the “3D drawing” interface showed consistently increasing errors with distance of the path away from the user. These errors may have been due to the leveraging effect of the stylus ray over distance and/or increasing errors in visual depth perception in stereographic VE. The conclusions from this experiment were that “3D drawing” is not suited to the kind of precise re-tracing and editing operations tested in this experiment (Sheridan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sheridan J, Sood G, Jacob T, Gardner H, Barrass S (2004) SoundStudio4D-a VR interface for gestural composition of spatial soundscapes. In: Proceedings of the international conference on auditory display, Sydney " href="/article/10.1007/s10055-006-0043-5#ref-CR35" id="ref-link-section-d110111e403">2004</a>). On the other hand, in informal experiments it was observed that it was possible to use “3D drawing” to compose complex 3D spatial soundscenes in only a few minutes that would likely take many hours to reproduce with 2d desktop tools. The “3D drawing” interface seemed to enable people to think about spatial sound more easily, and to produce soundscapes they would not otherwise have imagined.</p><p>In 1993 researchers at Xerox PARC began to explore how real objects could be used to interface to virtual operations in another kind of virtual environment called augmented reality (AR). The DigitalDesk allowed a user seated at an office desk to interact with the computer by writing on sheets of paper and moving them around the desktop. The paper was scanned by an overhead camera and the results of operations (e.g. the sum of numbers) were projected back onto the desktop (Wellner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Wellner P (1992) The DigitalDesk calculator: tactile manipulation on a desktop display. In: Proceedings of the international conference on user interface and software technology, Hilton Head " href="/article/10.1007/s10055-006-0043-5#ref-CR37" id="ref-link-section-d110111e409">1992</a>). People around the table could work together without having to pass around a special tool like a stylus or mouse. The AR concept was developed further with fiducial markers that could allow many objects to be tracked in 6DOF by the video camera without tethering cables. The open source Augmented Reality ARtoolkit (Billinghurst and Kato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Billinghurst M, Kato H (1999) Collaborative mixed reality. In: Proceedings of the international symposium on mixed reality, Yokohama " href="/article/10.1007/s10055-006-0043-5#ref-CR7" id="ref-link-section-d110111e412">1999</a>) has been used in musical experiments such as Augmented Groove which has a DJ “scratching” metaphor with tracked LP records (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Poupyrev I, Berry R, Kurumisawa J, Nakao K, Billinghurst M, Airola C, Kato H, Yonezawa T, Baldwin L (2000) Augmented groove: collaborative jamming in augmented reality. SIGGRAPH’2000 conference abstracts and applications. ACM" href="/article/10.1007/s10055-006-0043-5#ref-CR30" id="ref-link-section-d110111e415">2000</a>), and the MusicTable where cards that produce musical notes can be arranged to compose looping tunes on a “tabletop” (Berry et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Berry R, Makino M, Hikawa N, Suzuki M (2003) The augmented composer project: the music table. In: Proceedings of the second IEEE and ACM international symposium on mixed and augmented reality, San Francisco " href="/article/10.1007/s10055-006-0043-5#ref-CR5" id="ref-link-section-d110111e418">2003</a>). The “tabletop” metaphor has proliferated rapidly in the computer music community in variations such as the reacTable designed specifically for music performance (Jorda et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Jorda S, Kaltenbrunner M, Geiger G, and Bencina R (2005) The reacTable. In: Proceedings of the international computer music conference, Barcelona" href="/article/10.1007/s10055-006-0043-5#ref-CR19" id="ref-link-section-d110111e421">2005</a>).</p><p>The tangible interface to VR is taken further with “props” such as a tracked scalpel that is used to select MRI brain slices by positioning it relative to a doll’s head (Hinckley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Hinckley K, Pausch R, Goble J.C, Kassell NF (1994) A three-dimensional user interface for neurosurgical visualization. In: Proceedings of the SPIE conference on medical imaging" href="/article/10.1007/s10055-006-0043-5#ref-CR16" id="ref-link-section-d110111e427">1994</a>). Props convey specific interaction metaphors and physical affordances for specific tasks. For example an acoustic instrument, such as a clarinet, can be used as a prop to control a music synthesis algorithm. A prop can increase the “expressiveness” of a musical performance by allowing performers and audiences to share a common metaphor (Gadd and Fels <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Gadd A, Fels S (2002) MetaMuse: metaphors for expressive instruments. In: Proceedings of the conference on new interfaces for musical expression, Dublin " href="/article/10.1007/s10055-006-0043-5#ref-CR14" id="ref-link-section-d110111e430">2002</a>). This hypothesis was investigated with a props-based interface to a multi-parameter granular synthesis algorithm. The prop was a watering can that could be picked up and tilted to pour virtual droplets that were rendered by the granular synthesizer when they hit a virtual surface. Informal testing showed users could understand and control the synthesis algorithm with this interface. However, technical shortcomings that caused deviations from the metaphor caused more confusion than expected. Metaphors draw on shared common knowledge, and failing to fulfill expectations may lead to lower expressiveness that is worse than no metaphor at all (Gadd and Fels <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Gadd A, Fels S (2002) MetaMuse: metaphors for expressive instruments. In: Proceedings of the conference on new interfaces for musical expression, Dublin " href="/article/10.1007/s10055-006-0043-5#ref-CR14" id="ref-link-section-d110111e433">2002</a>).</p><p>In 1994 researchers at MIT attached a stylus to a robot arm to make virtual objects more tangible through force feedback that allows the user to feel a virtual surface (Massie and Salisbury <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Massie TH, Salisbury JK (1994) The PHANToM haptic interface: a device for probing virtual objects. ASME winter annual meeting" href="/article/10.1007/s10055-006-0043-5#ref-CR24" id="ref-link-section-d110111e439">1994</a>). By 1998 contact sounds were used to give auditory substance to the virtual surface felt with the Phantom (Ruspini and Khatib <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Ruspini D, Khatib O (1998) Acoustic cues for haptic rendering systems. In: Proceedings of the PHANTOM users group, Dedham" href="/article/10.1007/s10055-006-0043-5#ref-CR32" id="ref-link-section-d110111e442">1998</a>). By 2000 haptic interfaces were used in a “virtual instrument” metaphor to re-substantiate digital simulations of pianos, clarinets, percussion and other instruments (O’Modhrain <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="O’Modhrain S (2000) Playing by feel: incorporating haptic feedback into computer-based musical instruments. PhD Thesis, Stanford University" href="/article/10.1007/s10055-006-0043-5#ref-CR29" id="ref-link-section-d110111e445">2000</a>). By 2004 the haptic interface was used in a “scratching” metaphor where music was composed by moving the stylus around a groove in a virtual LP record (Sensable <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sensable (2004) HapticSound demonstration. ACM SIGGRAPH conference on graphics and interaction techniques. &#xA;                    http://www.sensable.com/popup_haptic_application_videos.asp &#xA;                    &#xA;                  (visited 31 July 2006)" href="/article/10.1007/s10055-006-0043-5#ref-CR34" id="ref-link-section-d110111e448">2004</a>). By 2005 the haptic interface was used in a form of melding called “haptic mirroring” where the movement of one musician’s hand on one haptic-music instrument induced the movement of the hand of a remote collaborator interacting with another networked version of the instrument (Carlile and Hartmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Carlile J, Hartmann B (2005) OROBORO: a collaborative controller with interpersonal haptic feedback. In: Proceedings of the conference on new interfaces for musical expression, Vancouver " href="/article/10.1007/s10055-006-0043-5#ref-CR11" id="ref-link-section-d110111e451">2005</a>).</p><p>Networked computer music has a long history and there are many examples of large internet music collaborations. In 1977, Rich Gold and Jim Horton linked their personal computers together via the serial ports to perform the first networked computer music. They went on to form a group called the league of automatic music composers who played regular performances in which their computers would send interrupts and data to each other’s music making programs (Bischoff et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="Bischoff J, Gold R, Horton J (1978) Music for an interactive network of microcomputers. Comput Music J 2:24–29" href="/article/10.1007/s10055-006-0043-5#ref-CR8" id="ref-link-section-d110111e457">1978</a>). The League considered humans and computers to be equal contributors to the music, and videos of their performances seem to show a “melding” not only between the human collaborators but also with the machines. Collaborative network music leapt from the local to the global scale in 1996 when Tod Machover staged the Brain Opera under the premise that “anyone who wanted to express ideas, experiences, and feelings in music and sounds was able help create the Brain Opera and participate in the live performances.” This vision was realised by making a Java applet called the Palette available that could be used to contribute sounds to the concert performance over the internet (Yu and and Machover <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Yu J, Machover T (1996) The Palette, the brain opera. &#xA;                    http://www.brainop.media.mit.edu/online/net-music/net-instrument/LabeledSharle.html&#xA;                    &#xA;                   (visited 31 July 2006)" href="/article/10.1007/s10055-006-0043-5#ref-CR38" id="ref-link-section-d110111e460">1996</a>). The interface consisted of a square with four sliders on each side and a painting area in the middle. The activity and nature of the music can be generally influenced but the user has no control over individual notes. The contributions of internet participants were anonymous and the musical algorithm had an asynchronous aesthetic where precise musical timing was not an issue. There are now many networked music environments where a community can spontaneously improvise an online concert by manipulating or transforming sounds and musical structures, and listen to pieces created collectively. Melding is a common characteristic in these concerts where the overall outcome is more important than individual virtuosity. Delays in the internet, accepted as a natural characteristic of the medium, and the lack of body cues that are vital in conventional music (Barbosa <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Barbosa A (2003) Displaced Soundscapes: a survey of network systems for music and sonic art creation. Leonardo Music J 13:53–59" href="/article/10.1007/s10055-006-0043-5#ref-CR1" id="ref-link-section-d110111e463">2003</a>), shape network music towards aleatoric, minimalist and soundscape genres that do not rely on sychronisation between players, or distinctive identification of the players. In the late 1990s VE researchers hypothesized that body language cues could be reintroduced in a networked CVE. However, an experiment with spatial metaphors, avatars, and sound spatialisation cues found the co-located control group still performed better than the distributed musicians (Jung et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Jung B, Hwang J, Lee S, Kim GJ, Kim H (2000) Incorporating co-presence in distributed virtual music environment. In: Proceedings of the ACM symposium on virtual reality software and technology, Seoul " href="/article/10.1007/s10055-006-0043-5#ref-CR20" id="ref-link-section-d110111e466">2000</a>). These observations indicate that CVE may be suited to the exploration of different forms of music rather than reproduction of existing forms.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Interaction metaphors and levels of interaction</h2><div class="c-article-section__content" id="Sec3-content"><p>The history of musical creativity in CVE reveals interaction patterns such as “probing”, “gesturing” and “conducting”. These three metaphors trend from local precise control of individual parameters, to the simultaneous control of a handful of parameters, to the sweeping control over a whole system. These levels of control are similar to the local, intermediate and global levels in Bertin’s theory of graphic semiotics (Bertin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Bertin J (1981) Graphics and graphic information processing. Walter de Gruter, Berlin" href="/article/10.1007/s10055-006-0043-5#ref-CR6" id="ref-link-section-d110111e477">1981</a>). Bertin proposes that while most graphic displays allow access to information about local elements, a good display allows the user to understand the overall system, as well as intermediate relations between elements, and to query individual elements as well.</p><p>Following from Bertin, we classify interaction metaphors as </p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Local–individual interaction with specific parameters of a specific object.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Intermediate–interaction with relations between objects.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Global–interaction with the system as a whole.</p>
                    
                  </li>
                </ol><p>Some existing interfaces for CVE music have been classified according to level of interaction and interaction pattern in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0043-5#Tab1">1</a>.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Classification of examples by level of interaction and interaction pattern</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0043-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">AVIARy system and experiments</h2><div class="c-article-section__content" id="Sec4-content"><p>Most of the examples so far fall in the category of local interaction. Those that fall into global interaction, however, do not allow access to intermediate or local level elements. Our experiments with “3D drawing” suggest that a VE might enable creativity by allowing the composer to interact more freely and flexibly at the intermediate and global levels through gestural metaphors.</p><p>These observations led us to explore the question of whether new interaction metaphors in a CVE could enable new forms of musical creativity. Tabletops are the most commonly used CVEs in concerts, perhaps because the AR interface allows untethered interaction, and ad-hoc flexibility in the number of collaborating musicians. However the ARtoolkit system does not have sound spatialisation and synthesis capability. In order to build the necessary CVE interface we integrated ARtoolkit with SoundStudio4D to create the audiovisual augmented reality system (AVIARy). The AVIARy acronym was chosen for the vision of “shepherding, moulding, and weaving flocks, swarms, and clouds to produce massively emergent auditory gestalts and musical surprises” (Morse <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Morse (2004) AVIARy: audio visualisation interactive augmented reality, the australian meeting on visualisation and virtual reality, Brisbane. &#xA;                    http://www.petermorse.com.au/PMcomhtml/aviary.html &#xA;                    &#xA;                  (visited 31 July 2006)" href="/article/10.1007/s10055-006-0043-5#ref-CR27" id="ref-link-section-d110111e740">2004</a>). AVIARy has a Java interface for programming interaction, 3D graphics and spatial audio components. The system integrates visual tracking using jARtoolkit (Geiger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Geiger C, Reimann C, Stocklein J, Paelke V (2002) JARToolkit-Java binding for AR toolkit. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt " href="/article/10.1007/s10055-006-0043-5#ref-CR15" id="ref-link-section-d110111e743">2002</a>) and sound synthesis using jSyn (Burke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Burke P (1998) JSyn—a real-time synthesis API for Java. In: Proceedings of the international computer music conference, Ann Arbor " href="/article/10.1007/s10055-006-0043-5#ref-CR10" id="ref-link-section-d110111e746">1998</a>), with the underlying SoundStudio4D system (Jacob and Barrass <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Jacob T, Barrass S (2003) The beehive java3D audio device. The Australian meeting on visualisation and virtual reality, Melbourne" href="/article/10.1007/s10055-006-0043-5#ref-CR18" id="ref-link-section-d110111e749">2003</a>) built on a multi-speaker JavaVBAPDevice3D class that integrates the VBAP spatial audio algorithm (Pulkki <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Pulkki V (2001) Spatial sound generation and perception by amplitude panning techniques. PhD Thesis, Helsinki University of Technology, Espoo" href="/article/10.1007/s10055-006-0043-5#ref-CR31" id="ref-link-section-d110111e752">2001</a>) with PortAudio (Bencina and Burk <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Bencina R (2005) P5 Glove developments. &#xA;                    http://www.audiomulch.com/simulus/p5glove &#xA;                    &#xA;                  (visited 31 July 2006)" href="/article/10.1007/s10055-006-0043-5#ref-CR3" id="ref-link-section-d110111e756">2003</a>) and ASIO drivers to a multi-channel soundcard, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig3">3</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>AVIARy system diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The first experiments with the system were reference prototypes of local, intermediate and global interaction.</p><p>The local interaction prototype consisted of 16 marked cards placed on a table with a web-cam above facing down. The centre of the table specified the origin of a polar coordinate system for angular, radial and height placement of sounds in a surround sound system. Anyone around the table could move sounds around the room by moving the cards around the tabletop. The system was able to render 16 synthesised buzzes in real-time in the 4.1 surround system. This was a “direct manipulation” metaphor, where the location of a card directly specifies the location of a sound.</p><p>The intermediate prototype used cards to control nine parameters of an FM synthesis algorithm, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig4">4</a>. The parameters were visualized as lines between six network nodes. Multiple parameters could be changed by moving the 3D location of a node with a card. Multiple cards could be used simultaneously to change multiple nodes. This metaphor does not provide access to the parameters at a local level, but it can be interacted with at the global level by three collaborators each controlling two nodes apiece. We called this a “weaving” metaphor.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Intermediate “weaving” metaphor</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The network consisted of four oscillator nodes, two delay nodes and one monitor node. Nodes could be connected or disconnected with a gesture. The frequencies of the oscillator nodes were set by rotating the cards around local up-down axes, and the influence of one node’s output on another’s frequency decreased with the distance between them. The delay time of any signal sent to a delay node increased with the distance traveled by the signal. The monitor could be connected to listen to any combination of nodes within the network, using a standard distance formula to set the mix of sound from each node to the monitor location.</p><p>The global prototype is a swarm of 16 spatialised buzzes that are controlled using cards that act as attractors, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig5">5</a>. Multiple cards can be used to create various formations, separate sub-groups or influence individual buzzes using position and orientation of the attractors. This “shepherding” metaphor allows any number of collaborators to interact at global, intermediate and local levels as they choose.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Global “shepherding” interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Collaborative musical creativity with AVIARy</h2><div class="c-article-section__content" id="Sec5-content"><p>The prototypes show AVIARy can be used to build new interface metaphors at all levels of interaction with spatial sounds. The next question was whether these interface metaphors could be used in a musical collaboration. To explore this question we invited a group of three creative performers with no prior experience with the system to compose music with the “direct manipulation” prototype. The sounds were cut-up samples recorded during an orchestra rehearsal. Samples were randomly assigned to markers, and any participant could pick up and manipulate any marker at any time. During this open-ended session the participants explored the characteristics of the system by trying various circling, crossover, and randomly chaotic movements to hear the effect. Movements needed to be slow enough for the visual tracking system to keep pace. The cards needed to be kept inside the invisible pyramidal field-of-view of the camera that narrowed considerably above the tabletop, in order to prevent sounds from flickering in and out of existence at the edges. Flickering also occurred when a hand or shadow obscured a marker beneath it. The scaling of the space of the tabletop to the room size space of the speaker array meant small hand movements could cause sudden jumps around the speakers. Crossing the polar axis in the centre caused 180° flips in direction. However, with practice, the participants felt that the boundaries of interaction and relations between gesture space and sound space could be learnt, like with any other instrument.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Creative concept</h2><div class="c-article-section__content" id="Sec6-content"><p>At the end of the session the participants were enthusiastic about the potential of the system for performance. In an ensuing discussion about performance opportunities one participant suggested an upcoming sound art festival, and the others agreed that they would also like to be involved. A plan was made to meet again to begin the development of a sound art performance using the AVIARy system. At the next meeting the group revisited the earlier prototype and began to brainstorm creative possibilities. The activity with four people seated around a table picking up the cards from the middle and moving them around was reminiscent of a dinner party. Someone asked “can the cards be replaced by biscuits that could be tracked in the same way?” This was not such a silly question given that the visual tracking system could theoretically follow a fiducial marker printed on any flat surface. If this was possible then each biscuit could have a sound attached to it and the performance could be staged as a dinner party where the performers could literally “eat the music”.</p><p>In order to follow this creative concept it was decided to find out whether fiducial markers could actually be placed on biscuits. As there was no information available on this topic it was necessary to carry out our own experiments. The fiducial markers require a dark border with a light interior pattern at a 1:2:1 ratio of border to interior dimensions. Each different marker must have a uniquely different and recognizable interior pattern. In the experiments we were aiming to track 16 markers, which would allow for the use of 16 different sounds in a performance. Various biscuits and breads were tested as the base overlaid with dark squares made of seaweed, Vegemite, icing, beetroot, and liquorice, and interior patterns made by cut-outs or overlays of cheese, white icing, cornflour, mayonaisse, etc., as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig6">6</a>. Different materials allowed different degrees of precision with cutting and painting, and the bonding between different substances proved to be a major issue. Some combinations were too low resolution to allow 16 unique patterns to be recognized by the system. A taste test of each experiment proved that cornflour would not be acceptable. The food needed to be prepared the day before the performance so tests while “fresh” were followed by a “stale” test 24 h later. The fresh tests showed shininess, colour variations and texture had effects on contrast thresholds and recognition under different lighting angles. The stale test showed problems with re-recognition due to changes in shape, pattern and colour caused by dehydration, slippage, colour blending and chemical interactions. An ice-cream wafer with the dark border square made of cocoa mixture and the interior pattern of white icing sugar proved the best solution overall in terms of precision, bonding, shininess, colour consistency, recognition, the stale test and taste. There were several iterations of development of the marker patterns to improve the recognition, differentiation and tracking under different lighting angles.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Experiments with visual tracking of food using ARtoolkit</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              </div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Experimental performance</h2><div class="c-article-section__content" id="Sec7-content"><p>In the next rehearsal the idea of a “dinner party” was developed into four sections, titled entrees, mains, dessert and a toast, that provided a framework for sequencing movements and directing musical progress. Rehearsals continued over the period of a month during which graphic screens for the four transitions were designed and incorporated, theatrical improvisations on the compositional structure were explored, sonic materials and mappings were further developed, and the food markers were tested in performance settings. The performers practiced flamboyant gestures to draw attention to the relation between motions of the biscuits and the motions of the sounds, developed coordinated theatrical and auditory movements for each section, and practiced keeping the markers within the visual boundaries of the system and from overlapping. A video of the tabletop was projected up behind to reinforce the connection between gestures on the tabletop and auditory gestures in sound space. The staging set up with four people seated around a small round table lit with an overhead light with the camera hidden in the lightshade was tested.</p><p>The performance was staged at the National Gallery of Australia in July 2005 as part of a national sound arts festival called liquid architecture (LA) in a line up that showcased a slice through the history of music technology wth DJ Olive on turntables, Nat on mixer feedback, Shannon O’Neill on laptop, Tq on electronic organ with dub mixing by Sam Karmel, and the consumers on AVIARy. The consumers improvised a spatial soundscape together for 15 min within the framework of an entree, main course, dessert and salutary toast by moving the food around the table in coordinated formations, or at other times in more individually and chaotic ways. A screen behind showed a down-view of the tabletop with images of audience faces superimposed on markers, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0043-5#Fig7">7</a>. Movements of markers around the table caused the music to move around the eight speaker sound system surrounding the audience. As each marker was theatrically eaten the associated music was removed from the soundscape and the face image disappeared from the screen.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0043-5/MediaObjects/10055_2006_43_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Edible audience performance at the liquid architecture sound art concert</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0043-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The experimental performance on stage at a sound art concert was a valuable experience that will influence the future development of the AVIARy system, and its adaptation for future performances. The stage set-up, sound system and lighting in the venue were vastly different from the rehearsal room, and the provided vastly different interaction and audiovisual feedback for the performers. The concert hall was at least an order of magnitude grander in scale than the rehearsal room, causing an order of magnitude in the spatial leveraging between the gestural space of the tabletop and the sound space around the audience that amplified sudden movements, flips, and discontinuities. The staging under a spotlight in the dark produced strong shadows that were misrecognised as markers by the visual tracking system, and generated additional unexpected and uncontrolled sounds that flickered on and off. Most significantly, the performers on stage were no longer immersed at the centre of the surround sound space. Separated up front on stage, they compensated for a lack of auditory feedback by focusing on theatrical rather than musical improvisation. The heightened theatrical emphasis of the performance, and the reduced connectivity between gestures and sounds, led to an audience perception that the performers were acting a scene to a backing soundtrack, rather than improvising the soundscape live. The metaphor of “a dinner party” and “eating the music” may have been too great an imaginative leap relative to the other performances by individuals each clearly “playing” an instrument of some kind. The large video screen dominating the centre stage drew attention from the performers off to the side under low lighting, and may have further signified a pre-produced soundtrack to accompany the video to an audience unfamiliar with VE technologies. The vertical projection of gestures on the screen may have interfered with the perception of correspondences between the horizontal tabletop and the horizontal sound space.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Conclusions</h2><div class="c-article-section__content" id="Sec8-content"><p>The review of musical creativity in CVE was purposefully inclusive of a range of collaborative and VE technologies that individually might not be considered CVE in the strictest sense, but which have potential to be integrated into a CVE. The main criterion was that there should be examples of the technology in some form of collaborative musical activity, and that the collaboration should be in real-time. Through these examples we identified repeating interaction metaphors, and a trend from local to global level interaction. These observations were used to propose a framework for describing the examples in terms of interaction patterns and levels of interaction. The framework was used to design three new metaphors with specific levels of interaction, local “direct manipulation”, intermediate “weaving” and global “shepherding”. The metaphors were prototyped using the AVIARy CVE system that integrates VR and AR interfaces with interactive spatial sound synthesis in a common Java API. The creative potential of the system was tested with four participants who identified technical characteristics of the system as an interactive medium that included recognition speed, pyramidal field-of-view, the polar coordinates, and overlapping markers. They also helped to identify the performance characteristics of synchronized, contrapuntal, and chaotic movements. The enthusiasm of the participants to continue to develop the system for a public performance shows that the AVIARy CVE could inspire technical confidence and creative energy in people who had no technical investment in its development. The system proved to be technically robust in rehearsals, and flexible to allow rapid iteration and development of ideas, and the integration of graphic, photographic and other media elements. The novel use of tracked musical food items and the concept of “eating the music” demonstrates that a CVE can inspire musical creativity. Staging the performance at a live concert identified the need to consider the characteristics of the performance space during concept development and rehearsals, the need to provide a performer-centric sound feedback system, and the need to consider the significations of the various media components relative to the performers and the other acts in the concert.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Barbosa, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Barbosa A (2003) Displaced Soundscapes: a survey of network systems for music and sonic art creation. Leonardo" /><p class="c-article-references__text" id="ref-CR1">Barbosa A (2003) Displaced Soundscapes: a survey of network systems for music and sonic art creation. Leonardo Music J 13:53–59</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2031143" aria-label="View reference 1 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F096112104322750791" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Displaced%20Soundscapes%3A%20a%20survey%20of%20network%20systems%20for%20music%20and%20sonic%20art%20creation&amp;journal=Leonardo%20Music%20J&amp;volume=13&amp;pages=53-59&amp;publication_year=2003&amp;author=Barbosa%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bargar R, Choi I, Das S, Goudeseune C (1994) Model-based interactive sound for an immersive virtual environmen" /><p class="c-article-references__text" id="ref-CR2">Bargar R, Choi I, Das S, Goudeseune C (1994) Model-based interactive sound for an immersive virtual environment. In: Proceedings of the international computer music conference, Aarhus </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bencina R (2005) P5 Glove developments. http://www.audiomulch.com/simulus/p5glove (visited 31 July 2006)" /><p class="c-article-references__text" id="ref-CR3">Bencina R (2005) P5 Glove developments. <a href="http://www.audiomulch.com/simulus/p5glove ">http://www.audiomulch.com/simulus/p5glove </a>(visited 31 July 2006)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bencina R, Burk P (2003) PortAudio–an open source cross platform audio API. http://www.portaudio.com/ (visited" /><p class="c-article-references__text" id="ref-CR4">Bencina R, Burk P (2003) PortAudio–an open source cross platform audio API. <a href="http://www.portaudio.com/">http://www.portaudio.com/</a> (visited 31 July 2006)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Berry R, Makino M, Hikawa N, Suzuki M (2003) The augmented composer project: the music table. In: Proceedings " /><p class="c-article-references__text" id="ref-CR5">Berry R, Makino M, Hikawa N, Suzuki M (2003) The augmented composer project: the music table. In: Proceedings of the second IEEE and ACM international symposium on mixed and augmented reality, San Francisco </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Bertin, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Bertin J (1981) Graphics and graphic information processing. Walter de Gruter, Berlin" /><p class="c-article-references__text" id="ref-CR6">Bertin J (1981) Graphics and graphic information processing. Walter de Gruter, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Graphics%20and%20graphic%20information%20processing&amp;publication_year=1981&amp;author=Bertin%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Kato H (1999) Collaborative mixed reality. In: Proceedings of the international symposium on m" /><p class="c-article-references__text" id="ref-CR7">Billinghurst M, Kato H (1999) Collaborative mixed reality. In: Proceedings of the international symposium on mixed reality, Yokohama </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Bischoff, R. Gold, J. Horton, " /><meta itemprop="datePublished" content="1978" /><meta itemprop="headline" content="Bischoff J, Gold R, Horton J (1978) Music for an interactive network of microcomputers. Comput Music J 2:24–29" /><p class="c-article-references__text" id="ref-CR8">Bischoff J, Gold R, Horton J (1978) Music for an interactive network of microcomputers. Comput Music J 2:24–29</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Music%20for%20an%20interactive%20network%20of%20microcomputers&amp;journal=Comput%20Music%20J&amp;volume=2&amp;pages=24-29&amp;publication_year=1978&amp;author=Bischoff%2CJ&amp;author=Gold%2CR&amp;author=Horton%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Borchers JO, Samminger W, Mühlhäuser M (2001) Conducting a realistic electronic orchestra. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR9">Borchers JO, Samminger W, Mühlhäuser M (2001) Conducting a realistic electronic orchestra. In: Proceedings of the 14th annual symposium on user interface software and technology, Orlando</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burke P (1998) JSyn—a real-time synthesis API for Java. In: Proceedings of the international computer music co" /><p class="c-article-references__text" id="ref-CR10">Burke P (1998) JSyn—a real-time synthesis API for Java. In: Proceedings of the international computer music conference, Ann Arbor </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Carlile J, Hartmann B (2005) OROBORO: a collaborative controller with interpersonal haptic feedback. In: Proce" /><p class="c-article-references__text" id="ref-CR11">Carlile J, Hartmann B (2005) OROBORO: a collaborative controller with interpersonal haptic feedback. In: Proceedings of the conference on new interfaces for musical expression, Vancouver </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Cruz-Neira, DJ. Sandin, TA. DeFanti, RV. Kenyon, JC. Hart, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The cave-audio visual experience automatic virt" /><p class="c-article-references__text" id="ref-CR12">Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The cave-audio visual experience automatic virtual environment. Commun ACM 35(6):64–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F129888.129892" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20cave-audio%20visual%20experience%20automatic%20virtual%20environment&amp;journal=Commun%20ACM&amp;volume=35&amp;issue=6&amp;pages=64-72&amp;publication_year=1992&amp;author=Cruz-Neira%2CC&amp;author=Sandin%2CDJ&amp;author=DeFanti%2CTA&amp;author=Kenyon%2CRV&amp;author=Hart%2CJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Eckel G (1999) Application for the cyberstage spatial sound server. In: Proceedings of the AES 16th internatio" /><p class="c-article-references__text" id="ref-CR13">Eckel G (1999) Application for the cyberstage spatial sound server. In: Proceedings of the AES 16th international conference: spatial sound reproduction, Rovaniemi </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gadd A, Fels S (2002) MetaMuse: metaphors for expressive instruments. In: Proceedings of the conference on new" /><p class="c-article-references__text" id="ref-CR14">Gadd A, Fels S (2002) MetaMuse: metaphors for expressive instruments. In: Proceedings of the conference on new interfaces for musical expression, Dublin </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geiger C, Reimann C, Stocklein J, Paelke V (2002) JARToolkit-Java binding for AR toolkit. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR15">Geiger C, Reimann C, Stocklein J, Paelke V (2002) JARToolkit-Java binding for AR toolkit. In: Proceedings of the first IEEE international augmented reality toolkit workshop, Darmstadt </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hinckley K, Pausch R, Goble J.C, Kassell NF (1994) A three-dimensional user interface for neurosurgical visual" /><p class="c-article-references__text" id="ref-CR16">Hinckley K, Pausch R, Goble J.C, Kassell NF (1994) A three-dimensional user interface for neurosurgical visualization. In: Proceedings of the SPIE conference on medical imaging</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hypersense Complex (2003) http://www.arrowtheory.com/hypersense/ (visited 31 July 2006)" /><p class="c-article-references__text" id="ref-CR17">Hypersense Complex (2003) <a href="http://www.arrowtheory.com/hypersense/">http://www.arrowtheory.com/hypersense/</a> (visited 31 July 2006)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="T. Jacob, S. Barrass, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Jacob T, Barrass S (2003) The beehive java3D audio device. The Australian meeting on visualisation and virtual" /><p class="c-article-references__text" id="ref-CR18">Jacob T, Barrass S (2003) The beehive java3D audio device. The Australian meeting on visualisation and virtual reality, Melbourne</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20beehive%20java3D%20audio%20device&amp;publication_year=2003&amp;author=Jacob%2CT&amp;author=Barrass%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jorda S, Kaltenbrunner M, Geiger G, and Bencina R (2005) The reacTable. In: Proceedings of the international c" /><p class="c-article-references__text" id="ref-CR19">Jorda S, Kaltenbrunner M, Geiger G, and Bencina R (2005) The reacTable. In: Proceedings of the international computer music conference, Barcelona</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jung B, Hwang J, Lee S, Kim GJ, Kim H (2000) Incorporating co-presence in distributed virtual music environmen" /><p class="c-article-references__text" id="ref-CR20">Jung B, Hwang J, Lee S, Kim GJ, Kim H (2000) Incorporating co-presence in distributed virtual music environment. In: Proceedings of the ACM symposium on virtual reality software and technology, Seoul </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaper HG, Tipei S, Wiebel E (1997) High-performance computing, music composition, and the sonification of scie" /><p class="c-article-references__text" id="ref-CR21">Kaper HG, Tipei S, Wiebel E (1997) High-performance computing, music composition, and the sonification of scientific data, technical report ANL/MCS-P690-0997</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Lanier, F. Biocca, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Lanier J, Biocca F (1992) An insider’s view of the future of virtual reality. J Commun 42(4):150" /><p class="c-article-references__text" id="ref-CR22">Lanier J, Biocca F (1992) An insider’s view of the future of virtual reality. J Commun 42(4):150</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1460-2466.1992.tb00816.x" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20insider%E2%80%99s%20view%20of%20the%20future%20of%20virtual%20reality&amp;journal=J%20Commun&amp;volume=42&amp;issue=4&amp;publication_year=1992&amp;author=Lanier%2CJ&amp;author=Biocca%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Machover T, Chung J (1989) Hyperinstruments: musically intelligent and interactive performance and creativity " /><p class="c-article-references__text" id="ref-CR23">Machover T, Chung J (1989) Hyperinstruments: musically intelligent and interactive performance and creativity systems. In: Proceedings of the international computer music conference, Columbus </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Massie TH, Salisbury JK (1994) The PHANToM haptic interface: a device for probing virtual objects. ASME winter" /><p class="c-article-references__text" id="ref-CR24">Massie TH, Salisbury JK (1994) The PHANToM haptic interface: a device for probing virtual objects. ASME winter annual meeting</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mäki-Patola T (2005) User interface comparison for virtual drums. In: Proceedings of the international confere" /><p class="c-article-references__text" id="ref-CR25">Mäki-Patola T (2005) User interface comparison for virtual drums. In: Proceedings of the international conference on new interfaces for musical expression, Vancouver</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Morita, S. Hashimoto, S. Ohteru, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Morita H, Hashimoto S, Ohteru S (1991) A computer music system that follows a human conductor”. IEEE Comput 24" /><p class="c-article-references__text" id="ref-CR26">Morita H, Hashimoto S, Ohteru S (1991) A computer music system that follows a human conductor”. IEEE Comput 24(7):44–53</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20computer%20music%20system%20that%20follows%20a%20human%20conductor%E2%80%9D&amp;journal=IEEE%20Comput&amp;volume=24&amp;issue=7&amp;pages=44-53&amp;publication_year=1991&amp;author=Morita%2CH&amp;author=Hashimoto%2CS&amp;author=Ohteru%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Morse (2004) AVIARy: audio visualisation interactive augmented reality, the australian meeting on visualisatio" /><p class="c-article-references__text" id="ref-CR27">Morse (2004) AVIARy: audio visualisation interactive augmented reality, the australian meeting on visualisation and virtual reality, Brisbane. <a href="http://www.petermorse.com.au/PMcomhtml/aviary.html ">http://www.petermorse.com.au/PMcomhtml/aviary.html </a>(visited 31 July 2006)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mäki-Patola T, Kanerva A, Laitinen J, Takala T (2005) Experiments with virtual reality instruments. In: Procee" /><p class="c-article-references__text" id="ref-CR28">Mäki-Patola T, Kanerva A, Laitinen J, Takala T (2005) Experiments with virtual reality instruments. In: Proceedings of the international conference on new interfaces for musical expression, Vancouver</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="O’Modhrain S (2000) Playing by feel: incorporating haptic feedback into computer-based musical instruments. Ph" /><p class="c-article-references__text" id="ref-CR29">O’Modhrain S (2000) Playing by feel: incorporating haptic feedback into computer-based musical instruments. PhD Thesis, Stanford University</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Berry R, Kurumisawa J, Nakao K, Billinghurst M, Airola C, Kato H, Yonezawa T, Baldwin L (2000) Aug" /><p class="c-article-references__text" id="ref-CR30">Poupyrev I, Berry R, Kurumisawa J, Nakao K, Billinghurst M, Airola C, Kato H, Yonezawa T, Baldwin L (2000) Augmented groove: collaborative jamming in augmented reality. SIGGRAPH’2000 conference abstracts and applications. ACM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pulkki V (2001) Spatial sound generation and perception by amplitude panning techniques. PhD Thesis, Helsinki " /><p class="c-article-references__text" id="ref-CR31">Pulkki V (2001) Spatial sound generation and perception by amplitude panning techniques. PhD Thesis, Helsinki University of Technology, Espoo</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ruspini D, Khatib O (1998) Acoustic cues for haptic rendering systems. In: Proceedings of the PHANTOM users gr" /><p class="c-article-references__text" id="ref-CR32">Ruspini D, Khatib O (1998) Acoustic cues for haptic rendering systems. In: Proceedings of the PHANTOM users group, Dedham</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Schertenleib, M. Gutierrez, V. Vexo, D. Thalmann, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Schertenleib S, Gutierrez M, Vexo V, Thalmann D (2004) Conducting a virtual orchestra. IEEE MultiMedia 11(3):4" /><p class="c-article-references__text" id="ref-CR33">Schertenleib S, Gutierrez M, Vexo V, Thalmann D (2004) Conducting a virtual orchestra. IEEE MultiMedia 11(3):40–49</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMMUL.2004.5" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Conducting%20a%20virtual%20orchestra&amp;journal=IEEE%20MultiMedia&amp;volume=11&amp;issue=3&amp;pages=40-49&amp;publication_year=2004&amp;author=Schertenleib%2CS&amp;author=Gutierrez%2CM&amp;author=Vexo%2CV&amp;author=Thalmann%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sensable (2004) HapticSound demonstration. ACM SIGGRAPH conference on graphics and interaction techniques. htt" /><p class="c-article-references__text" id="ref-CR34">Sensable (2004) HapticSound demonstration. ACM SIGGRAPH conference on graphics and interaction techniques. <a href="http://www.sensable.com/popup_haptic_application_videos.asp ">http://www.sensable.com/popup_haptic_application_videos.asp </a>(visited 31 July 2006)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sheridan J, Sood G, Jacob T, Gardner H, Barrass S (2004) SoundStudio4D-a VR interface for gestural composition" /><p class="c-article-references__text" id="ref-CR35">Sheridan J, Sood G, Jacob T, Gardner H, Barrass S (2004) SoundStudio4D-a VR interface for gestural composition of spatial soundscapes. In: Proceedings of the international conference on auditory display, Sydney </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sood G, Barrass S (2003) Groovy tubes: an interface for designing sound effects in space and time. In: Proceed" /><p class="c-article-references__text" id="ref-CR36">Sood G, Barrass S (2003) Groovy tubes: an interface for designing sound effects in space and time. In: Proceedings of the australian conference on computer human interaction, Brisbane</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wellner P (1992) The DigitalDesk calculator: tactile manipulation on a desktop display. In: Proceedings of the" /><p class="c-article-references__text" id="ref-CR37">Wellner P (1992) The DigitalDesk calculator: tactile manipulation on a desktop display. In: Proceedings of the international conference on user interface and software technology, Hilton Head </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yu J, Machover T (1996) The Palette, the brain opera. http://www.brainop.media.mit.edu/online/net-music/net-in" /><p class="c-article-references__text" id="ref-CR38">Yu J, Machover T (1996) The Palette, the brain opera. <a href="http://www.brainop.media.mit.edu/online/net-music/net-instrument/LabeledSharle.html">http://www.brainop.media.mit.edu/online/net-music/net-instrument/LabeledSharle.html</a> (visited 31 July 2006)</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0043-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The consumers were A/Prof. Stephen Barrass (direction), Tim Barrass (programming), Dr. Alistair Riddell (sound), Anita Fitton (graphics), Onaclov (theatre), Dr. Peter Morse (semiotics). We would especially like to thank Jun/Prof. Steffi Graf and Dr. Leonie Schäfer for their encouragement, incredible patience and invaluable help with this article.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Sonic Communications Research Group, University of Canberra, Canberra, Australia</p><p class="c-article-author-affiliation__authors-list">Stephen Barrass &amp; Tim Barrass</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Stephen-Barrass"><span class="c-article-authors-search__title u-h3 js-search-name">Stephen Barrass</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Stephen+Barrass&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Stephen+Barrass" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stephen+Barrass%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tim-Barrass"><span class="c-article-authors-search__title u-h3 js-search-name">Tim Barrass</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tim+Barrass&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tim+Barrass" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tim+Barrass%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0043-5/email/correspondent/c1/new">Stephen Barrass</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Musical%20creativity%20in%20collaborative%20virtual%20environments&amp;author=Stephen%20Barrass%20et%20al&amp;contentID=10.1007%2Fs10055-006-0043-5&amp;publication=1359-4338&amp;publicationDate=2006-09-07&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Barrass, S., Barrass, T. Musical creativity in collaborative virtual environments.
                    <i>Virtual Reality</i> <b>10, </b>149–157 (2006). https://doi.org/10.1007/s10055-006-0043-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0043-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-03-06">06 March 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-07-07">07 July 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-09-07">07 September 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-10">October 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0043-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0043-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">CVE</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Creativity</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Music</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Sound art</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Performance</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0043-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=43;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

