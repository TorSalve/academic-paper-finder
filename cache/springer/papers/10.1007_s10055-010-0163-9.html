<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Two-handed tangible interaction techniques for composing augmented blo"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Modeling tools typically have their own interaction methods for combining virtual objects. For realistic composition in 3D space, many researchers from the fields of virtual and augmented reality..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Two-handed tangible interaction techniques for composing augmented blocks"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-06-08"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Modeling tools typically have their own interaction methods for combining virtual objects. For realistic composition in 3D space, many researchers from the fields of virtual and augmented reality have been trying to develop intuitive interactive techniques using novel interfaces. However, many modeling applications require a long learning time for novice users because of unmanageable interfaces. In this paper, we propose two-handed tangible augmented reality interaction techniques that provide an easy-to-learn and natural combination method using simple augmented blocks. We have designed a novel interface called the cubical user interface, which has two tangible cubes that are tracked by marker tracking. Using the interface, we suggest two types of interactions based on familiar metaphors from real object assembly. The first, the screw-driving method, recognizes the user&#8217;s rotation gestures and allows them to screw virtual objects together. The second, the block-assembly method, adds objects based on their direction and position relative to predefined structures. We evaluate the proposed methods in detail with a user experiment that compares the different methods."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-06-08"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="133"/>

    <meta name="prism.endingPage" content="146"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0163-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0163-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0163-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0163-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Two-handed tangible interaction techniques for composing augmented blocks"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2011/06"/>

    <meta name="citation_online_date" content="2010/06/08"/>

    <meta name="citation_firstpage" content="133"/>

    <meta name="citation_lastpage" content="146"/>

    <meta name="citation_article_type" content="SI: Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0163-9"/>

    <meta name="DOI" content="10.1007/s10055-010-0163-9"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0163-9"/>

    <meta name="description" content="Modeling tools typically have their own interaction methods for combining virtual objects. For realistic composition in 3D space, many researchers from the"/>

    <meta name="dc.creator" content="Hyeongmook Lee"/>

    <meta name="dc.creator" content="Mark Billinghurst"/>

    <meta name="dc.creator" content="Woontack Woo"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Math Stat; citation_title=Asymptotic theory of certain &#8216;goodness of fit&#8217; criteria based on stochastic processes; citation_author=TW Anderson, DA Darling; citation_volume=23; citation_publication_date=1952; citation_pages=193-212; citation_doi=10.1214/aoms/1177729437; citation_id=CR1"/>

    <meta name="citation_reference" content="ARToolKit, 
                    http://www.hitl.washington.edu/artoolkit
                    
                  
                        "/>

    <meta name="citation_reference" content="Butterworth J, Davidson A, Hench S, Olano M (1992) 3DM: a three dimensional modeler using a head mounted display. In: International symposium on interactive 3D graphics 1992, pp&#160;135&#8211;138"/>

    <meta name="citation_reference" content="Camarata K, Do EY, Johnson BR, Gross MD (2002) Navigational blocks: navigating information space with tangible media. In: Proceeding of the 7th international conference on intelligent user interface, 2002, pp&#160;31&#8211;38"/>

    <meta name="citation_reference" content="Connacher H, Jayaram S, Lyons K (1995) Virtual assembly design environment. In: Proceedings of 1995 computers in engineering conference, Boston, MA"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Interact Des Manuf; citation_title=Tangible user interface integration in engineering; citation_author=N Couture, G Riviere, J Legardeur; citation_volume=2; citation_issue=3; citation_publication_date=2008; citation_pages=175-182; citation_doi=10.1007/s12008-008-0046-4; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graphics; citation_title=Virtual reality as a tool for verification of assembly and maintenance processes; citation_author=AG Sa, G Zachmann; citation_volume=23; citation_publication_date=1999; citation_pages=389-403; citation_doi=10.1016/S0097-8493(99)00047-3; citation_id=CR6"/>

    <meta name="citation_reference" content="Depaulis F, Couture N, Legardeur J, Garreau L (2005) A reusable methodology based on filters in order to define relevant tangible parts for a TUI. In: Proceedings of electronic imaging science and technology, stereoscopic displays and virtual reality systems XII electronic imaging 2005, vol 5664, pp&#160;530&#8211;539"/>

    <meta name="citation_reference" content="Fiala P, Adamo-Villani N (2005) ARpm: an augmented reality interface for polygonal model-ing. In: Proceeding of the 4th international symposium on augmented reality, 2005, pp&#160;196&#8211;197"/>

    <meta name="citation_reference" content="citation_journal_title=Yearb Phys Anthropol; citation_title=Evolution of sex differences in spatial ability; citation_author=S Gaulin; citation_volume=35; citation_publication_date=1992; citation_pages=125-151; citation_doi=10.1002/ajpa.1330350606; citation_id=CR9"/>

    <meta name="citation_reference" content="Gribnau MW, Hennessey JM (1998) Comparing single-and two-handed 3D input for a 3D object assembly task. In: Conference on human factors in computing systems, 1998"/>

    <meta name="citation_reference" content="citation_journal_title=J Motor Behav; citation_title=Symmetric division of labor in human skilled bimanual action: the kinematic chain as a model; citation_author=Y Guiard; citation_volume=19; citation_issue=4; citation_publication_date=1987; citation_pages=486-517; citation_id=CR11"/>

    <meta name="citation_reference" content="Irawati S, Green S, Billinghurst M, Duenser A (2006) An evaluation of an augmented reality multimodal interface using speech and paddle Gestures. Lecture notes in computer science Springer Berlin Heidelberg, 2006, vol 4282, pp&#160;272&#8211;283"/>

    <meta name="citation_reference" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM conference on human factors in computing systems, 1997, pp&#160;234&#8211;241"/>

    <meta name="citation_reference" content="Jayaram S, Connacher H, Lyons K (1997) Virtual assembly using virtual reality techniques. Computer-Aided Design, 1997, vol 29(8)"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Poupyrev I (2001) Tangible augmented reality. In: SIGGRAPH 2001 Course, Notes 21, 2001"/>

    <meta name="citation_reference" content="Kato H, Tachibana K, Tanabe M, Nakajima T (2003) A city-planning system based on augmented reality with a tangible interface. In: Proceedings of the 2nd international symposium on mixed and augmented reality, 2003, pp&#160;340&#8211;341"/>

    <meta name="citation_reference" content="Kiyokawa K, Takemura H, Katayama Y, Iwasa H, Yokoya N (1996) VLEGO: a simple two-handed modeling environment based on toy blocks. ACM VRST, 1996, pp&#160;27&#8211;34"/>

    <meta name="citation_reference" content="LDD Lego Digital Designer, 
                    http://ldd.lego.com
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=J Visual Lang Comput; citation_title=Immersive authoring of tangible augmented reality content: a user study; citation_author=GA Lee, GJ Kim; citation_volume=20; citation_issue=2; citation_publication_date=2009; citation_pages=61-79; citation_doi=10.1016/j.jvlc.2008.07.001; citation_id=CR18"/>

    <meta name="citation_reference" content="Lersithichai S, Seegmiller M (2002) CUBIK: a bi-directional tangible modeling interface. In: Conference on human factors in computing systems, 2002, pp&#160;756&#8211;757"/>

    <meta name="citation_reference" content="Olkin I, Ghurye SG, Hoeffding W, Madow WG, Mann HB (1960) Robust tests for equality of variances. In: Contributions to probability and statistics, Chap. 25, Stanford University Press, Stanford, CA"/>

    <meta name="citation_reference" content="OSG Open Scene Graph: 
                    http://www.openscenegraph.org/projects/osg
                    
                  
                        "/>

    <meta name="citation_reference" content="Park JY, Lee JW (2004) Tangible augmented reality modeling. Lecture Note in Computer Science Springer Berlin Heidelberg, 2004, pp&#160;254&#8211;259"/>

    <meta name="citation_reference" content="Park Y, Lepetit V, Woo W (2008) Multiple 3D object tracking for augmented reality. In: Proceedings of the 7th international symposium on mixed and augmented reality, 2008 (ISMAR 2008), pp&#160;117&#8211;120"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput; citation_title=Developing a generic augmented-reality interface; citation_author=I Poupyrev, DS Tan, M Billinghurst, H Kato, H Regenbrecht, N Tetsutani; citation_volume=35; citation_issue=3; citation_publication_date=2002; citation_pages=44-50; citation_id=CR23"/>

    <meta name="citation_reference" content="Reuter P, Rivi&#232;re G, Couture N, Sorraing N, Espinasse L, Vergnieux R (2007) ArcheoTUI&#8212;a tangible user interface for the virtual reassembly of fractured archeological objects. In: VAST2007: Proceedings of the 8th EuroGraphics international symposium on virtual reality, archaeology and cultural heritage, 2007, pp&#160;15&#8211;22"/>

    <meta name="citation_reference" content="Wang X, Kotranza A, Quarles J, Lok B, Allen D (2005) A pipeline for rapidly incorporating real objects into a mixed environment. In: Proceedings of the 4th international symposium on mixed and augmented reality, 2008 (ISMAR 2008), pp&#160;170&#8211;173"/>

    <meta name="citation_reference" content="Watanabe R, Itoh Y, Asai M, Kitamura Y, Kishino F, Kikuchi H (2004) The soul of Active Cube: implementing a flexible, multimodal, three-dimensional spatial tangible interface. In: Proceedings of the international conference on advances in computer entertainment technology, 2004, pp&#160;173&#8211;180"/>

    <meta name="citation_reference" content="Yonemoto S, Yotsumoto T, Taniguchi R (2007) Virtual object manipulation using physical blocks. In: Proceedings of the 11th international conference information visualization, 2007, pp&#160;781&#8211;785"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=wIzQubesTM&#8211;A novel tangible interface for interactive storytelling in mixed reality; citation_author=ZY Zhou, AD Cheok, H Tedjokusumo, GS Omer; citation_volume=7; citation_issue=4; citation_publication_date=2008; citation_pages=9-15; citation_id=CR32"/>

    <meta name="citation_author" content="Hyeongmook Lee"/>

    <meta name="citation_author_email" content="hmooklee@gist.ac.kr"/>

    <meta name="citation_author_institution" content="GIST U-VR Lab, Gwangju, South Korea"/>

    <meta name="citation_author" content="Mark Billinghurst"/>

    <meta name="citation_author_email" content="mark.billinghurst@hitlabnz.org"/>

    <meta name="citation_author_institution" content="The HIT Lab NZ, University of Canterbury, Christchurch, New Zealand"/>

    <meta name="citation_author" content="Woontack Woo"/>

    <meta name="citation_author_email" content="wwoo@gist.ac.kr"/>

    <meta name="citation_author_institution" content="GIST U-VR Lab, Gwangju, South Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0163-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0163-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Two-handed tangible interaction techniques for composing augmented blocks"/>
        <meta property="og:description" content="Modeling tools typically have their own interaction methods for combining virtual objects. For realistic composition in 3D space, many researchers from the fields of virtual and augmented reality have been trying to develop intuitive interactive techniques using novel interfaces. However, many modeling applications require a long learning time for novice users because of unmanageable interfaces. In this paper, we propose two-handed tangible augmented reality interaction techniques that provide an easy-to-learn and natural combination method using simple augmented blocks. We have designed a novel interface called the cubical user interface, which has two tangible cubes that are tracked by marker tracking. Using the interface, we suggest two types of interactions based on familiar metaphors from real object assembly. The first, the screw-driving method, recognizes the user’s rotation gestures and allows them to screw virtual objects together. The second, the block-assembly method, adds objects based on their direction and position relative to predefined structures. We evaluate the proposed methods in detail with a user experiment that compares the different methods."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Two-handed tangible interaction techniques for composing augmented blocks | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0163-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Two-handed interaction, Tangible interaction, Augmented reality, 3D model assembly, Multi-modal feedback","kwrd":["Two-handed_interaction","Tangible_interaction","Augmented_reality","3D_model_assembly","Multi-modal_feedback"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0163-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0163-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=163;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0163-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Two-handed tangible interaction techniques for composing augmented blocks
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0163-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0163-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-06-08" itemprop="datePublished">08 June 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Two-handed tangible interaction techniques for composing augmented blocks</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hyeongmook-Lee" data-author-popup="auth-Hyeongmook-Lee">Hyeongmook Lee</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="GIST U-VR Lab" /><meta itemprop="address" content="grid.61221.36, 0000000110339831, GIST U-VR Lab, Gwangju, 500-712, South Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mark-Billinghurst" data-author-popup="auth-Mark-Billinghurst">Mark Billinghurst</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Canterbury" /><meta itemprop="address" content="grid.21006.35, 0000000121791970, The HIT Lab NZ, University of Canterbury, Christchurch, New Zealand" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Woontack-Woo" data-author-popup="auth-Woontack-Woo" data-corresp-id="c1">Woontack Woo<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="GIST U-VR Lab" /><meta itemprop="address" content="grid.61221.36, 0000000110339831, GIST U-VR Lab, Gwangju, 500-712, South Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">133</span>–<span itemprop="pageEnd">146</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">449 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0163-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Modeling tools typically have their own interaction methods for combining virtual objects. For realistic composition in 3D space, many researchers from the fields of virtual and augmented reality have been trying to develop intuitive interactive techniques using novel interfaces. However, many modeling applications require a long learning time for novice users because of unmanageable interfaces. In this paper, we propose two-handed tangible augmented reality interaction techniques that provide an easy-to-learn and natural combination method using simple augmented blocks. We have designed a novel interface called the cubical user interface, which has two tangible cubes that are tracked by marker tracking. Using the interface, we suggest two types of interactions based on familiar metaphors from real object assembly. The first, the screw-driving method, recognizes the user’s rotation gestures and allows them to screw virtual objects together. The second, the block-assembly method, adds objects based on their direction and position relative to predefined structures. We evaluate the proposed methods in detail with a user experiment that compares the different methods.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Although augmented reality (AR) has been studied for a long time, there is still a need for new interaction techniques in AR applications. In particular, there has been little work done on interaction methods for AR modeling applications. In the real world, small physical objects are often combined to form a large object or scene. For instance, tiny toy blocks can be easily combined together to make an original creation using two-handed interaction. However, the limited number of real blocks or space for construction may make it impossible to build very large objects. In a virtual environment (VE), there is no limit on the virtual blocks or construction space, but the object manipulation may be more difficult. In our work, we are trying to bridge the gap between the real and virtual world and develop an interaction technique that is as easy to use as moving real blocks, with the flexibility of virtual content.</p><p>In this paper, we propose a two-handed tangible AR interaction method for combining augmented toy blocks into new virtual objects. We are only focusing on simple virtual toy block composition that is based on familiar interaction metaphors from the real toy assembly and so should be easy to learn. We present a novel manipulation interface called the cubical user interface (CUI), which has two tangible cubes that are tracked by a marker tracking method. The CUI supports physical object combination through a screwing operation using built-in magnets and button input by wireless communication modules. Using the CUI, we have designed two types of tangible interactions. The first, the screw-driving (SD) method, recognizes the users’ rotating gestures and allows them to screw virtual objects together. The second, the block-assembly (BA) method, adds objects based on their direction and position relative to predefined structures.</p><p>The proposed interaction techniques have the following advantages. Two-handed operations can give better performance than using a single hand in 3D object composition (Gribnau and Hennessey <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Gribnau MW, Hennessey JM (1998) Comparing single-and two-handed 3D input for a 3D object assembly task. In: Conference on human factors in computing systems, 1998" href="/article/10.1007/s10055-010-0163-9#ref-CR10" id="ref-link-section-d102177e344">1998</a>). Through the SD method, the user can freely position a virtual block for combining objects in 3D space. The BA method supports a fast composition strategy and effective visual guidance using information about the virtual blocks. Finally, the tangible cubes provide natural force feedback in the composing operation.</p><p>The remainder of this paper is organized as follows. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0163-9#Sec2">2</a>, we review related work from immersive virtual reality (VR) and tangible AR interactions for virtual object composition. Then in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0163-9#Sec5">3</a>, we describe the proposed two-handed CUI tangible interface and the SD method and the BA method. Next, we show the implementation aspects and results for an augmented toy block composition task. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0163-9#Sec13">5</a>, we present a usability study that evaluates the proposed interactions by comparing different input methods. Finally, we end the paper with a discussion of some directions for future research and conclusions.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Virtual object composition in immersive VR</h3><p>Many VR researchers have attempted to transfer effective GUI-based modeling and object composition tools into immersive 3D space. Industrial researchers have tried to develop modeling and planning systems for engineers (Jayaram et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Jayaram S, Connacher H, Lyons K (1997) Virtual assembly using virtual reality techniques. Computer-Aided Design, 1997, vol 29(8)" href="/article/10.1007/s10055-010-0163-9#ref-CR14" id="ref-link-section-d102177e371">1997</a>; De Sa and Zachmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="De Sa AG, Zachmann G (1999) Virtual reality as a tool for verification of assembly and maintenance processes. Comput Graphics 23:389–403" href="/article/10.1007/s10055-010-0163-9#ref-CR6" id="ref-link-section-d102177e374">1999</a>). For example, VADE (Connacher et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Connacher H, Jayaram S, Lyons K (1995) Virtual assembly design environment. In: Proceedings of 1995 computers in engineering conference, Boston, MA" href="/article/10.1007/s10055-010-0163-9#ref-CR4" id="ref-link-section-d102177e377">1995</a>) is an immersive 3D modeling environment for the engineer, which supports 3D space modeling with a glove interface. Research like this has found that accuracy in collision detection and avoidance and intuitive interactions are critical problems that need to be addressed.</p><p>There is relevant earlier research related to the virtual object modeling and object composing techniques. “3DM” (Butterworth et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Butterworth J, Davidson A, Hench S, Olano M (1992) 3DM: a three dimensional modeler using a head mounted display. In: International symposium on interactive 3D graphics 1992, pp 135–138" href="/article/10.1007/s10055-010-0163-9#ref-CR2" id="ref-link-section-d102177e383">1992</a>) is an immersive 3D surface modeling application that allows model manipulation from both CAD- and GUI-based drawing program. A head mounted display (HMD) device shows the virtual space, and the system supports one-handed modeling interaction with a wired six degree of freedom two button mouse. “VLEGO” (Kiyokawa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Kiyokawa K, Takemura H, Katayama Y, Iwasa H, Yokoya N (1996) VLEGO: a simple two-handed modeling environment based on toy blocks. ACM VRST, 1996, pp 27–34" href="/article/10.1007/s10055-010-0163-9#ref-CR17" id="ref-link-section-d102177e386">1996</a>) is an immersive two-handed 3D modeler, which uses virtual toy block assembly and flexible two-handed interaction for object composition. The user can manipulate virtual blocks by holding two wired 3D input devices, providing various bimanual interactions such as parallel block selection, composition, separation, and cooperative orientation. The system also automatically supports collision detection and avoidance between object primitives and adjusts their positions. ArcheoTUI (Reuter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Reuter P, Rivière G, Couture N, Sorraing N, Espinasse L, Vergnieux R (2007) ArcheoTUI—a tangible user interface for the virtual reassembly of fractured archeological objects. In: VAST2007: Proceedings of the 8th EuroGraphics international symposium on virtual reality, archaeology and cultural heritage, 2007, pp 15–22" href="/article/10.1007/s10055-010-0163-9#ref-CR24" id="ref-link-section-d102177e389">2007</a>) is a two-handed and foot-based tangible user interface for assembling fragments of fractured archeological objects in VR environment. In this system, a handheld prop tracked by an optical tracker allows the user to translate and rotate the virtual fragments, and foot pedal input is used to clutch the movements of the hands.</p><p>In general, immersive VR approaches enable free positioning in 3D space thanks to using HMDs and input devices such as gloves, and 3D mice. However, the user has to rely on virtual control points in the immersive space, which means a lack of real-world spatial context for providing depth perception in the composition task. Therefore, user experience or gender differences could cause a big difference in performance (Gaulin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Gaulin S (1992) Evolution of sex differences in spatial ability. Yearb Phys Anthropol 35:125–151" href="/article/10.1007/s10055-010-0163-9#ref-CR9" id="ref-link-section-d102177e395">1992</a>). Furthermore, the virtual input device may not provide any natural force feedback related to the composition interaction during the operation.</p><h3 class="c-article__sub-heading" id="Sec4">Virtual object composition in tangible AR</h3><p>In AR interfaces, there is an opportunity to use real objects to interact with virtual content. In this context, Ishii and Ullmer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM conference on human factors in computing systems, 1997, pp 234–241" href="/article/10.1007/s10055-010-0163-9#ref-CR13" id="ref-link-section-d102177e406">1997</a>) have developed the concept of tangible user interface (TUI) where real-world objects are used as the computer input and output devices. Many researchers have explored how to use TUI techniques for object composition (Camarata et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Camarata K, Do EY, Johnson BR, Gross MD (2002) Navigational blocks: navigating information space with tangible media. In: Proceeding of the 7th international conference on intelligent user interface, 2002, pp 31–38" href="/article/10.1007/s10055-010-0163-9#ref-CR3" id="ref-link-section-d102177e409">2002</a>; Lersithichai and Seegmiller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lersithichai S, Seegmiller M (2002) CUBIK: a bi-directional tangible modeling interface. In: Conference on human factors in computing systems, 2002, pp 756–757" href="/article/10.1007/s10055-010-0163-9#ref-CR19" id="ref-link-section-d102177e412">2002</a>). For example, the “Active Cube” (Watanabe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Watanabe R, Itoh Y, Asai M, Kitamura Y, Kishino F, Kikuchi H (2004) The soul of Active Cube: implementing a flexible, multimodal, three-dimensional spatial tangible interface. In: Proceedings of the international conference on advances in computer entertainment technology, 2004, pp 173–180" href="/article/10.1007/s10055-010-0163-9#ref-CR26" id="ref-link-section-d102177e415">2004</a>) is a multimodal tangible user interface that allows users to construct 3D graphics on a monitor using physical cubes. Multiple cubes can be used for real-time 3D modeling by networking between the cubes when they are physically connected.</p><p>However, TUI-based virtual object composition has some disadvantages. In case of one-to-one mapping between the physical and virtual object (Watanabe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Watanabe R, Itoh Y, Asai M, Kitamura Y, Kishino F, Kikuchi H (2004) The soul of Active Cube: implementing a flexible, multimodal, three-dimensional spatial tangible interface. In: Proceedings of the international conference on advances in computer entertainment technology, 2004, pp 173–180" href="/article/10.1007/s10055-010-0163-9#ref-CR26" id="ref-link-section-d102177e421">2004</a>), the weight of the real objects and space they take up are going to increase for each additional tangible object. When the user is manipulating real objects and seeing the output on a screen, then there is a separation between the input and output space that may be confusing.</p><p>Tangible AR is a metaphor that combines TUI input techniques with AR output to overcome some of these limitations (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kato H, Billinghurst M, Poupyrev I (2001) Tangible augmented reality. In: SIGGRAPH 2001 Course, Notes 21, 2001" href="/article/10.1007/s10055-010-0163-9#ref-CR15" id="ref-link-section-d102177e427">2001</a>). This has been used in a number of AR applications. For example, in the “Tiles” system (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan DS, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2002) Developing a generic augmented-reality interface. IEEE Comput 35(3):44–50" href="/article/10.1007/s10055-010-0163-9#ref-CR23" id="ref-link-section-d102177e430">2002</a>), multiple planar magnetic tiles have single fiducial markers attached to them and the user composes virtual objects when the tiles are move close together. Zhou et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhou ZY, Cheok AD, Tedjokusumo H, Omer GS (2008) wIzQubesTM–A novel tangible interface for interactive storytelling in mixed reality. Int J Virtual Real 7(4):9–15" href="/article/10.1007/s10055-010-0163-9#ref-CR32" id="ref-link-section-d102177e433">2008</a>) also uses a tangible AR technique based on a cube to support virtual data composition for interactive storytelling.</p><p>Various tangible AR modeling applications have been developed. “TARM” (Park and Lee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Park JY, Lee JW (2004) Tangible augmented reality modeling. Lecture Note in Computer Science Springer Berlin Heidelberg, 2004, pp 254–259" href="/article/10.1007/s10055-010-0163-9#ref-CR21" id="ref-link-section-d102177e439">2004</a>) is a tangible AR modeling system that allows the user to generate a new 3D model using physical block manipulation. In the main modeling process of TARM, four real blocks represent four virtual primitives such as cube, cone, cylinder, and sphere. One virtual primitive block is overlaid onto a single marker attached to a physical block. A collision detection method is also provided between a marker and the target virtual object for object combination. Therefore, this interface is an intuitive way for a novice user to perform object composition. However, the use of a single marker means that it does not support free orientation of the virtual block. Another tangible AR modeling system, “ARpm” (Fiala and Adamo-Villani <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Fiala P, Adamo-Villani N (2005) ARpm: an augmented reality interface for polygonal model-ing. In: Proceeding of the 4th international symposium on augmented reality, 2005, pp 196–197" href="/article/10.1007/s10055-010-0163-9#ref-CR8" id="ref-link-section-d102177e442">2005</a>), used the 3D Studio Max modeling functions in the real world with a fiducial marker-attached wireless pointer device. ARpm enables users to use the complex 3D polygonal modeling provided by 3D Studio Max. However, the selection of a small spot where the user wants to do the modeling is still hard work in AR space, and the adjustment of the movement volume is also difficult in 3D space. In the engineering field, Wang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wang X, Kotranza A, Quarles J, Lok B, Allen D (2005) A pipeline for rapidly incorporating real objects into a mixed environment. In: Proceedings of the 4th international symposium on mixed and augmented reality, 2008 (ISMAR 2008), pp 170–173" href="/article/10.1007/s10055-010-0163-9#ref-CR25" id="ref-link-section-d102177e445">2005</a>) have shown natural tangible interaction in assembly between virtual objects in augmented environment. However, this required heavy installation and preparation for execution including 3D model creation using laser scanners and setting up a color marker system.</p><p>For effective virtual object composition in tangible AR, the system has to provide special interaction techniques for composition such as familiar assembly metaphors from the real world. However, most AR applications use similar interaction techniques with selection or manipulation (Irawati et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Irawati S, Green S, Billinghurst M, Duenser A (2006) An evaluation of an augmented reality multimodal interface using speech and paddle Gestures. Lecture notes in computer science Springer Berlin Heidelberg, 2006, vol 4282, pp 272–283" href="/article/10.1007/s10055-010-0163-9#ref-CR12" id="ref-link-section-d102177e452">2006</a>; Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kato H, Tachibana K, Tanabe M, Nakajima T (2003) A city-planning system based on augmented reality with a tangible interface. In: Proceedings of the 2nd international symposium on mixed and augmented reality, 2003, pp 340–341" href="/article/10.1007/s10055-010-0163-9#ref-CR16" id="ref-link-section-d102177e455">2003</a>; Yonemoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Yonemoto S, Yotsumoto T, Taniguchi R (2007) Virtual object manipulation using physical blocks. In: Proceedings of the 11th international conference information visualization, 2007, pp 781–785" href="/article/10.1007/s10055-010-0163-9#ref-CR31" id="ref-link-section-d102177e458">2007</a>). Effective input methods are a critical issue for the fast overall performance. Furthermore, user studies for evaluating the input methods should be conducted. Unfortunately, there are few virtual object compositing interaction techniques so far and almost no examples of user studies with such systems.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">CUI-based tangible interactions</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6">Cubical user interface</h3><p>We consider the design of a novel interface for virtual block composition usable by a novice. Most other tangible AR interfaces use tools such as paddles, props, and cups which have been designed by considering external properties such as shape, size, and weight (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kato H, Tachibana K, Tanabe M, Nakajima T (2003) A city-planning system based on augmented reality with a tangible interface. In: Proceedings of the 2nd international symposium on mixed and augmented reality, 2003, pp 340–341" href="/article/10.1007/s10055-010-0163-9#ref-CR16" id="ref-link-section-d102177e474">2003</a>; Irawati et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Irawati S, Green S, Billinghurst M, Duenser A (2006) An evaluation of an augmented reality multimodal interface using speech and paddle Gestures. Lecture notes in computer science Springer Berlin Heidelberg, 2006, vol 4282, pp 272–283" href="/article/10.1007/s10055-010-0163-9#ref-CR12" id="ref-link-section-d102177e477">2006</a>; Lee and Kim <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee GA, Kim GJ (2009) Immersive authoring of tangible augmented reality content: a user study. J Visual Lang Comput 20(2):61–79" href="/article/10.1007/s10055-010-0163-9#ref-CR18" id="ref-link-section-d102177e480">2009</a>). However, they have less attention paid to the internal properties for natural user interaction by novices, such as robust manipulator tracking, two-handed tool-based inter-operation, and fast wireless input methods.</p><p><i>Robust manipulator tracking.</i> In general, a manipulator that has tracking markers on it could be occluded by an inexperienced users’ hand or be over-rotated so that the camera can no long see it. This shows that the tracking manipulator is an important factor in interaction and single marker-based interaction may not be so useful. In a multi-marker approach, one coordinate frame is created from relative pose information from a set of different markers making the tracking more robust.</p><p><i>Two</i>-<i>handed tools-based inter</i>-<i>operation</i>. Using two-handed tools supports two types of bimanual interactions: symmetric and asymmetric (Guiard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Guiard Y (1987) Symmetric division of labor in human skilled bimanual action: the kinematic chain as a model. J Motor Behav 19(4):486–517" href="/article/10.1007/s10055-010-0163-9#ref-CR11" id="ref-link-section-d102177e499">1987</a>). Bimanual symmetric means each hand performs identical actions either synchronously or asynchronously. Bimanual asymmetric indicates that the actions of both hands are different but closely coordinated to achieve the same task. In a selection of two virtual blocks which are going to be combined together, a bimanual symmetric approach using two tools could save time. A bimanual asymmetric approach enables a practical two-handed-based composing strategy (e.g., hammering, screw-driving, gluing, assembling) which could be more useful in virtual composition.</p><p><i>Wireless fast and stable input</i>. Various tangible AR interaction methods have used marker-based input using a time delay for selection techniques. This input method has an inverse relationship between speed and stability. For instance, it can create erroneous input events by unexpected marker occlusions with a short time delay. If a long time delay is specified, stable input is guaranteed but each event takes more time and can cause a large delay due to repeated input like in an assembly task. In order to support both speed and stability in user input, we use a wireless button which has the additional benefit of providing force feedback.</p><p>After considering possible external and internal properties, we arrived at the design requirements for a two-handed tangible tool for composing augmented blocks shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0163-9#Tab1">1</a>. The first three characteristics are concerned with external properties. In the tangible user interface design process, the most relevant shape to achieve the user task has different answers according to the application domain of the target system (Depaulis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Depaulis F, Couture N, Legardeur J, Garreau L (2005) A reusable methodology based on filters in order to define relevant tangible parts for a TUI. In: Proceedings of electronic imaging science and technology, stereoscopic displays and virtual reality systems XII electronic imaging 2005, vol 5664, pp 530–539" href="/article/10.1007/s10055-010-0163-9#ref-CR7" id="ref-link-section-d102177e514">2005</a>). We decided on a cube shape because it is a primitive that supports multiple combinations and simple assembly structure. Using a multi-marker approach with one tracking marker on each face, a cube provides robust tracking and free rotation of augmented blocks. In addition, the six perpendicular sides of the cube meet the requirements of using a simple block for composing virtual objects. The remaining two characteristics are related to providing an input mechanism for when a user makes any selection or decision situation. In order to guarantee continuity, we used multiple wireless buttons. These give a faster and more stable response than the traditional occlusion-based input. In our case, we place a button on every cube corner for easy accessibility. Our approach is similar to Couture et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Couture N, Riviere G, Legardeur J (2008) Tangible user interface integration in engineering. Int J Interact Des Manuf 2(3):175–182" href="/article/10.1007/s10055-010-0163-9#ref-CR5" id="ref-link-section-d102177e517">2008</a>) who have introduced the ESKUA cube-shaped tool and TUI for CAD object assembling; however, their system is not designed for AR interaction and does not satisfy the same internal (fast and stable input) and external (accessibility) design requirements.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Five issues of designing guideline in two-handed tangible tool</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0163-9/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In order to conduct two-handed composing interactions in an AR environment, we built up an interface based on using two tangible cubes and defined it as the cubical user interface (CUI). The CUI is targeted at construction of AR scene where users are able to easily create virtual block combinations using a simplified tangible configuration with natural interaction. The CUI consists of three main components: (1) a camera, (2) a stage, and (3) tangible cubes. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig1">1</a> shows the configuration of the CUI, based on a fiducial marker tracking method. The first component is a camera for capturing the real scene. The stage is for placing available virtual blocks and putting the interim and final results on. Finally, two tangible cubes are used to select virtual blocks from the stage and handle them in the real world. In CUI, the camera and other components each have a coordinate frame generated from a fiducial pattern. Those independent coordinates are used to calculate relative information such as distance, position, and rotation between objects.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The configuration of the cubical user interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec7">The screw-driving method</h3><p>In addition to defining objects in the interface, we need to specify how objects are able to interact with one another. Rather than allowing users to combine objects in any manner, we provide nature combination points on each virtual object where they can be joined. Then, we use a screw-driving (SD) technique to connect objects at these points. This is based on the real-world condition where two or more real objects are joined together using a screw and screwdriver. If a person turns the screwdriver clockwise, the two target objects are fastened, while a counterclockwise rotation loosens two targets that are already screwed together. The screw-driving (SD) method uses this same metaphor to join virtual objects. The screw-driving technique can also support axis change by the help of additional button input so 3D positioning is also possible.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig2">2</a> shows the flowchart of the SD method. We divide the two tangible cubes into the main cube which is held in the non-dominant hand and the subsidiary cube which is held in the dominant hand. In advance, the initial position between tangible cubes (including the selected virtual block) is set by user using the push buttons on the cubes. This preliminary connection makes a total of 36 (6 × 6) cases which have slightly different formations between the two selected blocks. After that, the augmented 3D model is moved along the axis of the main cube according to rotating gesture of subsidiary cube. Using this movement, the amount of the change in position is scaled depending on the size of moving block. When the cube is rotated clockwise, the virtual object toward the main cube and when it is counterclockwise the object moves away from the main cube. The faster the rotation speed is, the faster the cube moves.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A flowchart of the screw-driving method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To detect the users’ rotation gesture, the relative pose between multi-markers is compared. After storing the relative rotation of initial position following a button press, the SD method checks the difference between the stored and the current relative rotation. Equation <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0163-9#Equ1">1</a> shows how we find the initial rotation from the start position by matrix multiplication and <i>R</i>
                           <sub>
                    <i>t</i>
                  </sub> function. <b>M</b>
                           <sub>marker i</sub> is the pose matrix of one of the tangible cubes and <i>R</i>
                           <sub>
                    <i>t</i>
                  </sub> is the rotation value only from pose matrix. Here, we use quaternions not Euler angles to represent the rotation between two markers and so avoid the problem of gimbal lock. In Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0163-9#Equ2">2</a>, we obtain the combined rotation between the initial and current rotation by multiplication of the two quaternions. The difference in outcome between Eqs. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0163-9#Equ1">1</a> and <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0163-9#Equ2">2</a> shows the variation in rotation. In order to reduce the accumulated error, a round off method around the activated angle is required in the rotation measurement. Clockwise or right rotation, which acts as fastening a screw, defines the positive direction and counterclockwise or left rotation, which acts as loosening, defines the negative direction. Based on this principle, the moving virtual block event is activated according to the rotation direction of the users’ hand.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{initRot}}_{t - 1} = R_{t - 1} \left( {{\mathbf{M}}_{\text{marker2}} \cdot {\mathbf{M}}_{\text{marker1}}^{ - 1} } \right) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                           <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{combRot}}_{t} = {\mathbf{initRot}}_{t - 1} * R_{t} \left( {{\mathbf{M}}_{\text{marker1}} \cdot {\mathbf{M}}_{\text{marker2}}^{ - 1} } \right). $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                        <p>In order to allow positioning in 3D space, the SD method can change the rotation axis using active button input. We designate the three axes (<i>X</i>, <i>Y</i>, and <i>Z</i>) with blue, green, and red colors, and the default combinable axis is initially set to the <i>X</i>-axis. In this context, the left two axes are preserved and the applicable arrow is superimposed on the buttons as well. By pushing the augmented button that matches the defined axes color, the moving axis is changed and the previous axes and color are saved to the pushed button. This process is appropriate for working with the non-dominant hand and supporting fast composition. In addition, in the middle of the composition, the guide arrow indicating the active axis changes according to the situation of augmented buttons and users’ input at that time. This helps to keep the user informed about the direction used for 3D positioning with the various axis changes.</p><h3 class="c-article__sub-heading" id="Sec8">The block-assembly method</h3><p>If we know about the model structure of the component model objects, we can anticipate the product of the composition process, and so we do not expect to spend much time in combining virtual blocks. For instance, when we assemble real toy blocks, the connection points are often already fixed, so we just need to look at the coupling position and direction between blocks and fit them together. In addition, thanks to pre-existing knowledge of given models, the user may get a manual showing how the blocks should be assembled. The block-assembly (BA) method is an attempt to apply these ideas to the composing interaction.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig3">3</a> shows the flowchart of the BA method. As with the SD method, the BA method is used on the two virtual objects that are selected by the tangible objects. In this context, if the selected objects are not in the combinable set, it does not allow the user to progress to the next assembly step. So user has the chance to choose different blocks that are part of the feasible set.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A flowchart of the block-assembly method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To confirm the feasibility condition, a predefined structure is required. This structure table is given from the assembly system, and it includes the hierarchical, relative position, and relative rotation information among the virtual blocks. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig4">4</a>a shows examples of the predefined structure. Each model has one id and six directional pointers representing the combinable spots with static rotation and translation values. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig4">4</a>b, we depict the tree view using partial models of a virtual toy car. The tree is generated from the set of bold lines in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig4">4</a>a. A connected line means that two models satisfy the feasible condition, and a directed line describes the hierarchy. In addition, every connected model has relative rotation and translation values as well. In case of M<sub>3</sub>, the wheel model could have two rotation and translation sets, so it is the users’ decision as to how to use the tangible cube for combining the wheel with the body part. Based on this strategy, the BA method creates a runtime tree structure when the selected block set are feasible and updates it during the composition process.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The representation of the predefined structure (<b>a</b>) and tree view in partial model of virtual toy car (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Based on the predefined structure, the BA method provides two types of additional guidance for the user: preliminary guidance and interim guidance. The preliminary guidance draws a virtual animated arrow toward the combinable side (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig5">5</a>). When a new block is selected, the BA method traverses the runtime tree structure and then finds the available combinable sides of objects. In contrast, the interim guidance returns a color sign as the visual feedback. In the toy block composition case, three conditions are possible when an initial composition happens. One is a correct position and orientation, another is a correct position but an incorrect orientation, and the other is an incorrect position. The color sign feedback shows which condition the combined object is in. From a composition point of view, the position is a more important factor than orientation. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0163-9#Tab2">2</a> shows the linking policy between the color sign system and the combined status of virtual objects.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>An example of the preliminary guidance for the combinable points</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Linking policy for interim guidance of BA method</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0163-9/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In order to judge the position and orientation of the subsidiary cube relative to the main cube, we calculate the relative translation vector (<b>Vec(T</b>
                           <sub>
                    <b>x</b>
                  </sub>
                           <b>,T</b>
                           <sub>
                    <b>y</b>
                  </sub>
                           <b>,T</b>
                           <sub>
                    <b>z</b>
                  </sub>
                           <b>)</b>) from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-010-0163-9#Equ3">3</a>. <b>T()</b> denotes the translation value from the pose matrix. By a combination of three elements in the relative translation vector, six combination cases of the subsidiary cube have to be defined by the system in advance. Then, the system knows which side of the main cube the subsidiary cube is approaching from. For the users’ decision, the button input is used but it is activated on the green sign only.</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ Vec\left( {{\mathbf{T}}_{{\mathbf{x}}} ,{\mathbf{T}}_{{\mathbf{y}}} ,{\mathbf{T}}_{{\mathbf{z}}} } \right) = T\left( {{\mathbf{M}}_{\text{marker1}} \cdot {\mathbf{M}}_{\text{marker2}}^{{ - {\mathbf{1}}}} } \right). $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                        </div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Implementation</h2><div class="c-article-section__content" id="Sec9-content"><h3 class="c-article__sub-heading" id="Sec10">Tangible cube</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig6">6</a> shows one prototype of the constructed tangible cube. It was made in acrylic and is 70 × 70 × 70 mm in size. To enable physical connection, five magnets were attached on each inner surface of the cube. One strong magnet was located at the center of the surface and four sub-magnets at the corners, which supports rotating gestures with force feedback, and provides additional magnetic connection for each 90 degree rotation. According to the placement of the sub-magnets (numbers, power, distance, etc), the system can provide the different magnetic force feedback but this is required to be synchronized with the spin recognition process by the marker tracking. For wireless input, we designed a simple circuit with a Bluetooth module and buttons on the microprocessor chipboard and then put it into the tangible cube with a charging battery. Buttons were attached to every corner for fast input and easy accessibility. Each button has assigned to the individual port on the chipboard so it allows the developer to use them for various purposes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Description of tangible cube: <b>a</b> handy size, <b>b</b> multiple magnets, <b>c</b> multiple buttons</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>For visual tracking of the cube and overlay of the virtual model, we attached multiple markers of 4 × 4 cm on each face and used a multi-marker method which has one base origin. In addition, we let the center of the cube correspond to the center of virtual model as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig7">7</a>. We added a semi-transparent virtual cube of identical size over the tangible cube for better depth perception. In this implementation, we used the ARToolkit tracking library to track the marker (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference ARToolKit" title="ARToolKit, &#xA;                    http://www.hitl.washington.edu/artoolkit&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0163-9#ref-CR28" id="ref-link-section-d102177e1109">ARToolKit</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>An augmented car model covering the tangible cube</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec11">The SD method in the augmented block composition</h3><p>In implemented SD method, the moving augmented block event is activated when the relative rotation between the tangible cubes is more than 90 degrees. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig8">8</a> shows successive images of model movement for the composition between an augmented spindle and wheel. The default direction of movement was set toward the <i>X</i>-axis, the blue arrow shown at the left in the images. These static axes are displayed all the time with the direction of arrow pointing toward the positive direction. Therefore, the wheel is screwed onto the spindle by clockwise (or positive directional) rotations of the subsidiary cube. If a counterclockwise rotation is repeated, the wheel is moved further away from the spindle in this context.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>3D model movement by clockwise rotation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>There are three visual hints about the current target object and the moving axis. First is the superimposed animated arrow of active axis around the main cube. Second is virtual buttons with directional arrow at the corner. The last is the active color of the bounding box of the 3D model. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig9">9</a> shows an example of an axis change to the <i>Z</i>-axis with visual hints. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig9">9</a>a, we know at a glance that the present target block is a separated wheel and the moving axis is the <i>X</i>-axis. In order to change the composition axis to the <i>Z</i>-axis, the user pushes the corner button which is the most accessible. When this happens, the previous <i>X</i>-axis is stored to the pushed button with a blue arrow and the color of bounding box is changed to the desired axis as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig9">9</a>b. Using the direction policy of the static axes shown at the lower left in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig9">9</a>c, a positive directional rotation moves the car cover block up and a negative directional rotation moves the block down.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The moving axis changed from the <i>x</i>-axis to the <i>z</i>-axis by pushing a button with visual hints (<b>a</b>–<b>c</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec12">The BA method in augmented block composition</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig10">10</a> shows the implemented BA method based on the predefined structure of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig4">4</a>. The user attaches the wheel block to the subsidiary cube and combines it with a spindle on the main cube because the spindle is higher in the object hierarchy than wheel. The two augmented blocks on the tangible cubes satisfy the feasibility condition so the BA method checks the relative position and orientation in real time. When the correct position and orientation is found, pushing a white button is enough to complete the composition on the main cube and runtime scene tree structure is updated.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Sequential steps for the block-assembly method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig11">11</a> shows preliminary guidance using an animated virtual arrow and interim guidance with a virtual color sign box. The guidance is related to the runtime tree structure. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig11">11</a>a, a spindle has two possible cases for wheel composition but there is only one case existing in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig11">11</a>b. The main cube has the role as the stored stage so the direction of the arrows faces inside. On the other hand, the subsidiary cube has an opposite directional arrow which faces outside. Therefore, if the two kinds of arrows are shown at the same time, it shows all the possible connections between selected blocks to user. In spite of the preliminary guidance, when the users neglect the policy of block hierarchy in the middle of the composition, the interim guidance directly notices the users’ mistake and provides the appropriate color feedback on the block (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig11">11</a>c–e). It provides position and direction feedback using semi-transparent color cubes but it is not so useful when the combinable position is hard for the user to guess. So in the implemented BA method, the preliminary and interim guidance strategies work in cooperation with each other.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Visual guidance: <b>a</b>, <b>b</b> preliminary guidance, <b>c</b>–<b>e</b> interim guidance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Experimental results</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading" id="Sec14">Pilot study with comparison task</h3><p>In this section, we report on a pilot user study to evaluate our two tangible AR composition techniques and the CUI. In this study, we ask users to assemble virtual toy blocks using the CUI- and GUI-based modeling approaches. We chose two different targets to compare with the SD and BA methods. The SD method mainly offers selection and connection steps of virtual objects so we tried to find the proper comparative modeling tool. However, most existing GUI-based tools including commercial software were designed with a different purpose and so comparison with one of them did not seem to be suitable. Therefore, we developed our own GUI-based composition tool using Open Scene Graph (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference OSG" title="OSG Open Scene Graph: &#xA;                    http://www.openscenegraph.org/projects/osg&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0163-9#ref-CR30" id="ref-link-section-d102177e1312">OSG</a>). Using a rotation and translation manipulator, a user can select and position virtual objects in virtual 3D space using keyboard input and mouse events (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig12">12</a>a). For the BA method, we compared performance against the Lego Digital Designer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference LDD" title="LDD Lego Digital Designer, &#xA;                    http://ldd.lego.com&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0163-9#ref-CR29" id="ref-link-section-d102177e1318">LDD</a>) because the LDD knows information about model constraints and supports composition based on fixed combinable points and not free positioning by the user. The LDD also provides a wireframe box–based guide like the interim guidance used in the BA method (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig12">12</a>b).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>GUI-based assembly tools: <b>a</b> OSG-based positioning tool, <b>b</b> block-assembly tool (LDD)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The OSG-based tool and LDD software were installed on a standard desktop PC, and the two tangible AR methods were set up with a front USB camera and a large display as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig13">13</a>. Eleven subjects (8 men, 3 women) participated in the pilot study, ranging in age from 24 to 31 years old. Of the subjects, nine had previous experience with VR/AR technology. However, only two of them had relevant experience with object composition task in some 3D modeling tools.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Experimental setup of CUI composition pilot study</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The experimental task for the subjects was to combine a set of virtual blocks. We considered the characteristics of each interaction method on the choice of the target model. Particularly, we are not focusing on the completed assembly system but introducing tangible interactions based on AR tracking technology. Therefore, the experimental task was not very complex, and we arranged each scenario to check that the interaction properties would work well and to explore the possibility of each interaction method. Therefore, to compare between the SD (CUI)- and OSG-based tools (GUI), the goal of task was to conduct the assembly of a virtual car (two wheels and cover model addition to the unfinished initial block) by positioning with axis change. To comparison between the BA (CUI) and LDD tools, the setup included the use of multiple combinable points and the assembly goal was to generate the cars wheel using a spindle and two wheel blocks.</p><p>The experimental procedure had four steps (1) preliminary information gathering about the subjects, (2) training on how to use the interfaces, (3) task execution with the model assembly, (4) post-interview and subjective surveys. Subjects were trained on the four composing methods until they felt comfortable with the techniques and then they were told to complete the assembly tasks as quickly as possible. During the task execution, the subject’s performance time was measured. In the case of comparing the OSG-based and SD methods, we added the visual feedback for when a selected block is arranged at the right position, indicating that the user can stop moving the block. We performed the experiment in two parts: comparing the OSG-based and SD methods and then comparing the LDD and BA methods. In each part, the subjects performed the task twice in a counterbalanced order, once with the GUI-based tool (either OSG-based or LDD) and once with the CUI. After the completion of each part, we interviewed the subjects and collected their opinions about the ease of use of the interface and their performance.</p><h3 class="c-article__sub-heading" id="Sec15">Performance analysis</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig14">14</a> shows the measured execution time of the participants across the different composing methods. Significant performance time differences were tested for using two sample <i>t</i> tests with the two-sided alternative. Before the <i>t</i> test was conducted, the normality was examined using the Anderson–Darling test (Anderson and Darling <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1952" title="Anderson TW, Darling DA (1952) Asymptotic theory of certain ‘goodness of fit’ criteria based on stochastic processes. Ann Math Stat 23:193–212" href="/article/10.1007/s10055-010-0163-9#ref-CR1" id="ref-link-section-d102177e1394">1952</a>) and then Levene’s test (Olkin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1960" title="Olkin I, Ghurye SG, Hoeffding W, Madow WG, Mann HB (1960) Robust tests for equality of variances. In: Contributions to probability and statistics, Chap. 25, Stanford University Press, Stanford, CA" href="/article/10.1007/s10055-010-0163-9#ref-CR20" id="ref-link-section-d102177e1397">1960</a>) was used to check if the data sets had equal variance. The mean performance time for each of the two comparison groups is shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0163-9#Tab3">3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>The measured performance time of participants: <b>a</b> OSG-based versus SD <b>b</b> LDD versus BA</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Task performance of four composing methods</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0163-9/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the G1 group, subjects spent approximately one and a quarter times longer completing the given task with the GUI interface than with the CUI method. Using a <i>t</i> test analysis, we find that the difference is statistically significant (<i>t</i> = 2.36, <i>df</i> = 20, <i>p</i> = 0.03 &lt; 0.05), so the SD method is faster than the GUI interface in 3D positioning for this particular virtual toy block composition task.</p><p>When we observed the subjects during the execution, the input modes of the two methods were not very different because they both used similar button input to start and end the virtual block selection and composition steps. However, subjects did show differences in the composing process. Our OSG-based tool, like many traditional GUI interfaces, provides a virtual camera that enables the user to change viewpoint. However, through the block composition task in virtual 3D space, we observed that most users had trouble in moving the camera to provide a suitable object view. This caused uncertain view changes and positioning so it led to delays in the task execution. In contrast, the CUI interface provided 3D translation and rotation of the block with users’ seamless cube movements by multi-marker tracking. Therefore, most of subjects had no trouble to find the initial combination position. Also, with the augmented active axis arrow, the SD method allowed a virtual block to be easily and comfortably translated in one dimension at a time while screw-driving.</p><p>In case of G2, a <i>t</i> test showed that the performance times were not statistically significantly different (<i>t</i> = −1.96, <i>df</i> = 20, <i>p</i> = 0.06 &gt; 0.05). By looking at the G2 group execution, we observed that most participants exhibited a tendency to depend on the supporting guidance rather than just combining the virtual blocks.</p><h3 class="c-article__sub-heading" id="Sec16">Lessons learnt from the pilot study and public demonstrations</h3><p>In addition to the performance data described earlier, we have collected qualitative evaluation data on two-handed tangible interaction techniques for augmented blocks through the pilot study and public demonstrations of the technology (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig15">15</a>). After the comparison experiments, we collected valuable comments about the proposed techniques from post-experiment interviews of the pilot study. We also obtained additional informal comments at several demonstrations through the person who had experiences of our two techniques at the same time. Furthermore, we demonstrated a prototype of SD method-based AR block-assembly system for applicable use and listened carefully to user opinions related to composing method (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig15">15</a>). Based on this feedback, we summarize what we have learned for enhancing our CUI-based composing interaction techniques.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>The toy block-assembly system demonstration on International Symposium on Mixed and Augmented Reality 2008: <b>a</b> the experience by a participant <b>b</b> an assembly for a virtual car</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p><i>Two</i>-<i>handed cubical interface</i>. Our composing environment uses two cube interface objects which enable that virtual block selection using a single cube and virtual block combination using interaction between both hands. In addition, the support for free cube rotation can be used to check the shape or interim composition step of the virtual objects. At this point, users said that using the environment was truly similar to real block assembly so it was familiar and easy to use. Every corner button facilitates accurate and fast input and steps through the suitable modes for the sequential assembly process. Finally, the built-in magnets offer realistic force feedback in combination, separation, and rotation gestures. Most of the users were satisfied with the force feedback system and agreed that it was necessity for AR applications to have realistic force feedback in interactive AR assembly applications. However, some users said that the solid acrylic material was not so good and that cubes were too big for small hands. Therefore, we may need to conduct further work to find a better esthetic design to satisfy users.</p><p><i>Composing interaction</i>. This paper uses familiar composing interactions of screw-driving and a block-assembly metaphor for natural 3D object composition. After an explanation, most of the users who were well acquainted with the screw-driving principle learned the SD method without difficulty and they were able to easily complete the augmented block composition. In case of the BA method, people were familiar with how toy blocks are assembled and so every user finished the composition task quickly. In order to improve the users’ satisfaction in the final AR block-assembly application, it would be good to combine characteristics of the two methods. For instance, using the BA and SD methods in consecutive order enables an application with fast composition and free object placement. This mixed method will be possible to apply to the decision method in case of multiple connection points in one surface.</p><p><i>Robust tracking</i>. In our tangible object-based AR interaction, the tracking patterns on the cube were often occluded by the users’ fingers causing the tracking to fail. So a more robust tracking manner is required. For example, we could use 3D object tracking methods (Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Park Y, Lepetit V, Woo W (2008) Multiple 3D object tracking for augmented reality. In: Proceedings of the 7th international symposium on mixed and augmented reality, 2008 (ISMAR 2008), pp 117–120" href="/article/10.1007/s10055-010-0163-9#ref-CR22" id="ref-link-section-d102177e1679">2008</a>) and obtain more stable cube tracking results. In addition, when the user performs a fast continuous screw-driving gesture, then a blurred image may be generated and the SD method can fail to recognize the rotation. One way to overcome this would be to add an inertial tracker to the system to provide results that are independent of the computer vision and improve the stability of the rotation gesture recognition.</p><p><i>Visual assembly guidance</i>. Each technique provided different visual guidance for virtual block composition, and most of users said that the visual guidance methods were very useful for completing the combination task. However, some participants said that in order to use the moving axis guide in the SD method they had to recall the axis color and so it took more concentration. In the BA method, the preliminary guidance pointed out the reasonable combinable candidates and the interim guidance gave significant help when assembling confusing blocks like bisymmetric models (e.g., wheel, robot arm). This shows that good visual assembly guidance tools should help reducing users’ confusion in the assembly process.</p><h3 class="c-article__sub-heading" id="Sec17">A feasible application for the user-generated character creation</h3><p>Based on proposed tangible AR-based composing concept, one possible new application is for the creation of various virtual characters for a novice user. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig16">16</a> shows the example of user-generated characters from component objects. Besides the composing interaction, several tangible interactions can be implemented using the relative distance between cube, button input, and cube rotation. In addition, cubes can support a variety of property changes such as scale, color, orientation for creating a user-generated character.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig16_HTML.jpg?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0163-9/MediaObjects/10055_2010_163_Fig16_HTML.jpg" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>An example of the various user-generated characters by the tangible block interactions: <b>a</b> the source blocks for the robot <b>b</b> big body robot <b>c</b> a big head robot <b>d</b> a long head robot <b>e</b> a robot raising a hand</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0163-9/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Conclusions and future work</h2><div class="c-article-section__content" id="Sec18-content"><p>In this paper, we explored two-handed tangible interaction techniques for composing augmented reality blocks. We developed the cubical user interface (CUI) to provide a familiar composing environment for general users and to enable block unit composition using both hands. The CUI are a pair of real cubes with magnets and switches inside and AR tracking markers on their surface. Based on this interface, we designed and implemented two types of natural composing interactions using a real assembly metaphor. First, the SD method recognizes the users’ rotating gestures and allows them to screw virtual objects together. The user can also freely position a virtual block for combining objects in 3D space. Second, the BA method adds objects based on their direction and position relative to predefined structures. It supports a fast composition strategy and effective visual guidance using information about the virtual blocks. In a pilot study, we analyzed the performance of each method and then verified that they are effective when compared to GUI-based methods for simple virtual block composition tasks. In addition, we summarized user feedback including the strengths and weaknesses of the methods and areas for improvement.</p><p>In the future, we will aim to develop an effective virtual block-assembly system together with extended assembly functions for the general user and conduct a more rigorous user evaluation. We developed a first prototype of the assembly system and gave various users the chance to experience it as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig15">15</a>. Although a systematic evaluation was not performed, participants were able to create their own product using two cubes and a limited number of source blocks, and they immersed themselves in the assembly task (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0163-9#Fig16">16</a>). These user-generated objects are typical of those used in AR applications and show how the system could be extended to create a tangible AR character authoring system for a novice user.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TW. Anderson, DA. Darling, " /><meta itemprop="datePublished" content="1952" /><meta itemprop="headline" content="Anderson TW, Darling DA (1952) Asymptotic theory of certain ‘goodness of fit’ criteria based on stochastic pro" /><p class="c-article-references__text" id="ref-CR1">Anderson TW, Darling DA (1952) Asymptotic theory of certain ‘goodness of fit’ criteria based on stochastic processes. Ann Math Stat 23:193–212</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=50238" aria-label="View reference 1 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0048.11301" aria-label="View reference 1 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1214%2Faoms%2F1177729437" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Asymptotic%20theory%20of%20certain%20%E2%80%98goodness%20of%20fit%E2%80%99%20criteria%20based%20on%20stochastic%20processes&amp;journal=Ann%20Math%20Stat&amp;volume=23&amp;pages=193-212&amp;publication_year=1952&amp;author=Anderson%2CTW&amp;author=Darling%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ARToolKit, http://www.hitl.washington.edu/artoolkit&#xA;                        " /><p class="c-article-references__text" id="ref-CR28">ARToolKit, <a href="http://www.hitl.washington.edu/artoolkit">http://www.hitl.washington.edu/artoolkit</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Butterworth J, Davidson A, Hench S, Olano M (1992) 3DM: a three dimensional modeler using a head mounted displ" /><p class="c-article-references__text" id="ref-CR2">Butterworth J, Davidson A, Hench S, Olano M (1992) 3DM: a three dimensional modeler using a head mounted display. In: International symposium on interactive 3D graphics 1992, pp 135–138</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Camarata K, Do EY, Johnson BR, Gross MD (2002) Navigational blocks: navigating information space with tangible" /><p class="c-article-references__text" id="ref-CR3">Camarata K, Do EY, Johnson BR, Gross MD (2002) Navigational blocks: navigating information space with tangible media. In: Proceeding of the 7th international conference on intelligent user interface, 2002, pp 31–38</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Connacher H, Jayaram S, Lyons K (1995) Virtual assembly design environment. In: Proceedings of 1995 computers " /><p class="c-article-references__text" id="ref-CR4">Connacher H, Jayaram S, Lyons K (1995) Virtual assembly design environment. In: Proceedings of 1995 computers in engineering conference, Boston, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Couture, G. Riviere, J. Legardeur, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Couture N, Riviere G, Legardeur J (2008) Tangible user interface integration in engineering. Int J Interact De" /><p class="c-article-references__text" id="ref-CR5">Couture N, Riviere G, Legardeur J (2008) Tangible user interface integration in engineering. Int J Interact Des Manuf 2(3):175–182</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs12008-008-0046-4" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tangible%20user%20interface%20integration%20in%20engineering&amp;journal=Int%20J%20Interact%20Des%20Manuf&amp;volume=2&amp;issue=3&amp;pages=175-182&amp;publication_year=2008&amp;author=Couture%2CN&amp;author=Riviere%2CG&amp;author=Legardeur%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AG. Sa, G. Zachmann, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="De Sa AG, Zachmann G (1999) Virtual reality as a tool for verification of assembly and maintenance processes. " /><p class="c-article-references__text" id="ref-CR6">De Sa AG, Zachmann G (1999) Virtual reality as a tool for verification of assembly and maintenance processes. Comput Graphics 23:389–403</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2899%2900047-3" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20as%20a%20tool%20for%20verification%20of%20assembly%20and%20maintenance%20processes&amp;journal=Comput%20Graphics&amp;volume=23&amp;pages=389-403&amp;publication_year=1999&amp;author=Sa%2CAG&amp;author=Zachmann%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Depaulis F, Couture N, Legardeur J, Garreau L (2005) A reusable methodology based on filters in order to defin" /><p class="c-article-references__text" id="ref-CR7">Depaulis F, Couture N, Legardeur J, Garreau L (2005) A reusable methodology based on filters in order to define relevant tangible parts for a TUI. In: Proceedings of electronic imaging science and technology, stereoscopic displays and virtual reality systems XII electronic imaging 2005, vol 5664, pp 530–539</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fiala P, Adamo-Villani N (2005) ARpm: an augmented reality interface for polygonal model-ing. In: Proceeding o" /><p class="c-article-references__text" id="ref-CR8">Fiala P, Adamo-Villani N (2005) ARpm: an augmented reality interface for polygonal model-ing. In: Proceeding of the 4th international symposium on augmented reality, 2005, pp 196–197</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Gaulin, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Gaulin S (1992) Evolution of sex differences in spatial ability. Yearb Phys Anthropol 35:125–151" /><p class="c-article-references__text" id="ref-CR9">Gaulin S (1992) Evolution of sex differences in spatial ability. Yearb Phys Anthropol 35:125–151</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fajpa.1330350606" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evolution%20of%20sex%20differences%20in%20spatial%20ability&amp;journal=Yearb%20Phys%20Anthropol&amp;volume=35&amp;pages=125-151&amp;publication_year=1992&amp;author=Gaulin%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gribnau MW, Hennessey JM (1998) Comparing single-and two-handed 3D input for a 3D object assembly task. In: Co" /><p class="c-article-references__text" id="ref-CR10">Gribnau MW, Hennessey JM (1998) Comparing single-and two-handed 3D input for a 3D object assembly task. In: Conference on human factors in computing systems, 1998</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Guiard, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Guiard Y (1987) Symmetric division of labor in human skilled bimanual action: the kinematic chain as a model. " /><p class="c-article-references__text" id="ref-CR11">Guiard Y (1987) Symmetric division of labor in human skilled bimanual action: the kinematic chain as a model. J Motor Behav 19(4):486–517</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Symmetric%20division%20of%20labor%20in%20human%20skilled%20bimanual%20action%3A%20the%20kinematic%20chain%20as%20a%20model&amp;journal=J%20Motor%20Behav&amp;volume=19&amp;issue=4&amp;pages=486-517&amp;publication_year=1987&amp;author=Guiard%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Irawati S, Green S, Billinghurst M, Duenser A (2006) An evaluation of an augmented reality multimodal interfac" /><p class="c-article-references__text" id="ref-CR12">Irawati S, Green S, Billinghurst M, Duenser A (2006) An evaluation of an augmented reality multimodal interface using speech and paddle Gestures. Lecture notes in computer science Springer Berlin Heidelberg, 2006, vol 4282, pp 272–283</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Procee" /><p class="c-article-references__text" id="ref-CR13">Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM conference on human factors in computing systems, 1997, pp 234–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jayaram S, Connacher H, Lyons K (1997) Virtual assembly using virtual reality techniques. Computer-Aided Desig" /><p class="c-article-references__text" id="ref-CR14">Jayaram S, Connacher H, Lyons K (1997) Virtual assembly using virtual reality techniques. Computer-Aided Design, 1997, vol 29(8)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I (2001) Tangible augmented reality. In: SIGGRAPH 2001 Course, Notes 21, 2001" /><p class="c-article-references__text" id="ref-CR15">Kato H, Billinghurst M, Poupyrev I (2001) Tangible augmented reality. In: SIGGRAPH 2001 Course, Notes 21, 2001</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Tachibana K, Tanabe M, Nakajima T (2003) A city-planning system based on augmented reality with a tang" /><p class="c-article-references__text" id="ref-CR16">Kato H, Tachibana K, Tanabe M, Nakajima T (2003) A city-planning system based on augmented reality with a tangible interface. In: Proceedings of the 2nd international symposium on mixed and augmented reality, 2003, pp 340–341</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kiyokawa K, Takemura H, Katayama Y, Iwasa H, Yokoya N (1996) VLEGO: a simple two-handed modeling environment b" /><p class="c-article-references__text" id="ref-CR17">Kiyokawa K, Takemura H, Katayama Y, Iwasa H, Yokoya N (1996) VLEGO: a simple two-handed modeling environment based on toy blocks. ACM VRST, 1996, pp 27–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LDD Lego Digital Designer, http://ldd.lego.com&#xA;                        " /><p class="c-article-references__text" id="ref-CR29">LDD Lego Digital Designer, <a href="http://ldd.lego.com">http://ldd.lego.com</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GA. Lee, GJ. Kim, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Lee GA, Kim GJ (2009) Immersive authoring of tangible augmented reality content: a user study. J Visual Lang C" /><p class="c-article-references__text" id="ref-CR18">Lee GA, Kim GJ (2009) Immersive authoring of tangible augmented reality content: a user study. J Visual Lang Comput 20(2):61–79</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jvlc.2008.07.001" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20authoring%20of%20tangible%20augmented%20reality%20content%3A%20a%20user%20study&amp;journal=J%20Visual%20Lang%20Comput&amp;volume=20&amp;issue=2&amp;pages=61-79&amp;publication_year=2009&amp;author=Lee%2CGA&amp;author=Kim%2CGJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lersithichai S, Seegmiller M (2002) CUBIK: a bi-directional tangible modeling interface. In: Conference on hum" /><p class="c-article-references__text" id="ref-CR19">Lersithichai S, Seegmiller M (2002) CUBIK: a bi-directional tangible modeling interface. In: Conference on human factors in computing systems, 2002, pp 756–757</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Olkin I, Ghurye SG, Hoeffding W, Madow WG, Mann HB (1960) Robust tests for equality of variances. In: Contribu" /><p class="c-article-references__text" id="ref-CR20">Olkin I, Ghurye SG, Hoeffding W, Madow WG, Mann HB (1960) Robust tests for equality of variances. In: Contributions to probability and statistics, Chap. 25, Stanford University Press, Stanford, CA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="OSG Open Scene Graph: http://www.openscenegraph.org/projects/osg&#xA;                        " /><p class="c-article-references__text" id="ref-CR30">OSG Open Scene Graph: <a href="http://www.openscenegraph.org/projects/osg">http://www.openscenegraph.org/projects/osg</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park JY, Lee JW (2004) Tangible augmented reality modeling. Lecture Note in Computer Science Springer Berlin H" /><p class="c-article-references__text" id="ref-CR21">Park JY, Lee JW (2004) Tangible augmented reality modeling. Lecture Note in Computer Science Springer Berlin Heidelberg, 2004, pp 254–259</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park Y, Lepetit V, Woo W (2008) Multiple 3D object tracking for augmented reality. In: Proceedings of the 7th " /><p class="c-article-references__text" id="ref-CR22">Park Y, Lepetit V, Woo W (2008) Multiple 3D object tracking for augmented reality. In: Proceedings of the 7th international symposium on mixed and augmented reality, 2008 (ISMAR 2008), pp 117–120</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Poupyrev, DS. Tan, M. Billinghurst, H. Kato, H. Regenbrecht, N. Tetsutani, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Poupyrev I, Tan DS, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2002) Developing a generic augmented-r" /><p class="c-article-references__text" id="ref-CR23">Poupyrev I, Tan DS, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2002) Developing a generic augmented-reality interface. IEEE Comput 35(3):44–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developing%20a%20generic%20augmented-reality%20interface&amp;journal=IEEE%20Comput&amp;volume=35&amp;issue=3&amp;pages=44-50&amp;publication_year=2002&amp;author=Poupyrev%2CI&amp;author=Tan%2CDS&amp;author=Billinghurst%2CM&amp;author=Kato%2CH&amp;author=Regenbrecht%2CH&amp;author=Tetsutani%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reuter P, Rivière G, Couture N, Sorraing N, Espinasse L, Vergnieux R (2007) ArcheoTUI—a tangible user interfac" /><p class="c-article-references__text" id="ref-CR24">Reuter P, Rivière G, Couture N, Sorraing N, Espinasse L, Vergnieux R (2007) ArcheoTUI—a tangible user interface for the virtual reassembly of fractured archeological objects. In: VAST2007: Proceedings of the 8th EuroGraphics international symposium on virtual reality, archaeology and cultural heritage, 2007, pp 15–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang X, Kotranza A, Quarles J, Lok B, Allen D (2005) A pipeline for rapidly incorporating real objects into a " /><p class="c-article-references__text" id="ref-CR25">Wang X, Kotranza A, Quarles J, Lok B, Allen D (2005) A pipeline for rapidly incorporating real objects into a mixed environment. In: Proceedings of the 4th international symposium on mixed and augmented reality, 2008 (ISMAR 2008), pp 170–173</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Watanabe R, Itoh Y, Asai M, Kitamura Y, Kishino F, Kikuchi H (2004) The soul of Active Cube: implementing a fl" /><p class="c-article-references__text" id="ref-CR26">Watanabe R, Itoh Y, Asai M, Kitamura Y, Kishino F, Kikuchi H (2004) The soul of Active Cube: implementing a flexible, multimodal, three-dimensional spatial tangible interface. In: Proceedings of the international conference on advances in computer entertainment technology, 2004, pp 173–180</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yonemoto S, Yotsumoto T, Taniguchi R (2007) Virtual object manipulation using physical blocks. In: Proceedings" /><p class="c-article-references__text" id="ref-CR31">Yonemoto S, Yotsumoto T, Taniguchi R (2007) Virtual object manipulation using physical blocks. In: Proceedings of the 11th international conference information visualization, 2007, pp 781–785</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ZY. Zhou, AD. Cheok, H. Tedjokusumo, GS. Omer, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Zhou ZY, Cheok AD, Tedjokusumo H, Omer GS (2008) wIzQubesTM–A novel tangible interface for interactive storyte" /><p class="c-article-references__text" id="ref-CR32">Zhou ZY, Cheok AD, Tedjokusumo H, Omer GS (2008) wIzQubesTM–A novel tangible interface for interactive storytelling in mixed reality. Int J Virtual Real 7(4):9–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=wIzQubesTM%E2%80%93A%20novel%20tangible%20interface%20for%20interactive%20storytelling%20in%20mixed%20reality&amp;journal=Int%20J%20Virtual%20Real&amp;volume=7&amp;issue=4&amp;pages=9-15&amp;publication_year=2008&amp;author=Zhou%2CZY&amp;author=Cheok%2CAD&amp;author=Tedjokusumo%2CH&amp;author=Omer%2CGS">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0163-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">GIST U-VR Lab, Gwangju, 500-712, South Korea</p><p class="c-article-author-affiliation__authors-list">Hyeongmook Lee &amp; Woontack Woo</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">The HIT Lab NZ, University of Canterbury, Christchurch, New Zealand</p><p class="c-article-author-affiliation__authors-list">Mark Billinghurst</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Hyeongmook-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Hyeongmook Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hyeongmook+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hyeongmook+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hyeongmook+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mark-Billinghurst"><span class="c-article-authors-search__title u-h3 js-search-name">Mark Billinghurst</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mark+Billinghurst&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mark+Billinghurst" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mark+Billinghurst%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Woontack-Woo"><span class="c-article-authors-search__title u-h3 js-search-name">Woontack Woo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Woontack+Woo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Woontack+Woo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Woontack+Woo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0163-9/email/correspondent/c1/new">Woontack Woo</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>This research is supported by Ministry of Culture, Sports and Tourism (MCST) and Korea Creative Content Agency (KOCCA) in the Culture Technology (CT) Research &amp; Development Program 2009.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Two-handed%20tangible%20interaction%20techniques%20for%20composing%20augmented%20blocks&amp;author=Hyeongmook%20Lee%20et%20al&amp;contentID=10.1007%2Fs10055-010-0163-9&amp;publication=1359-4338&amp;publicationDate=2010-06-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lee, H., Billinghurst, M. &amp; Woo, W. Two-handed tangible interaction techniques for composing augmented blocks.
                    <i>Virtual Reality</i> <b>15, </b>133–146 (2011). https://doi.org/10.1007/s10055-010-0163-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0163-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-28">28 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-05-19">19 May 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-06-08">08 June 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06">June 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0163-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0163-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Two-handed interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tangible interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D model assembly</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multi-modal feedback</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0163-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=163;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

