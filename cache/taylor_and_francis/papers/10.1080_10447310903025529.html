<!DOCTYPE html>
<html lang="en" class="pb-page" data-request-id="5064a78a-c134-4a10-936c-c46ca56b0ba0"><head data-pb-dropzone="head"><meta name="pbContext" content=";page:string:Article/Chapter View;ctype:string:Journal Content;issue:issue:10.1080/hihc20.v025.i08;journal:journal:hihc20;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;article:article:10.1080/10447310903025529;subPage:string:Full Text;requestedJournal:journal:hihc20" />
<link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><meta name="citation_journal_title" content="Intl. Journal of Human–Computer Interaction" /><meta name="dc.Title" content="Vision for Performance in Virtual Environments: The Role of Feedback Timing" /><meta name="dc.Creator" content=" Andrea   H.    Mason " /><meta name="dc.Creator" content=" Brandon   J.    Bernardin " /><meta name="dc.Description" content="This work explores how people use visual feedback when performing simple reach-to-grasp movements in a tabletop virtual environment. In particular we investigated whether visual feedback is require..." /><meta name="Description" content="This work explores how people use visual feedback when performing simple reach-to-grasp movements in a tabletop virtual environment. In particular we investigated whether visual feedback is require..." /><meta name="dc.Publisher" content=" Taylor &amp; Francis Group " /><meta name="dc.Date" scheme="WTN8601" content="3 Dec 2009" /><meta name="dc.Type" content="research-article" /><meta name="dc.Format" content="text/HTML" /><meta name="dc.Identifier" scheme="publisher-id" content="402725" /><meta name="dc.Identifier" scheme="doi" content="10.1080/10447310903025529" /><meta name="dc.Identifier" scheme="coden" content="Intl. Journal of Human–Computer Interaction, Vol. 25, No. 8, November-December 2009: pp. 785–805" /><meta name="dc.Source" content="http://dx.doi.org/10.1080/10447310903025529" /><meta name="dc.Language" content="en" /><meta name="dc.Coverage" content="world" /><meta name="dc.Rights" content="Copyright Taylor and Francis Group, LLC" />
<link rel="meta" type="application/atom+xml" href="https://doi.org/10.1080%2F10447310903025529" />
<link rel="meta" type="application/rdf+json" href="https://doi.org/10.1080%2F10447310903025529" />
<link rel="meta" type="application/unixref+xml" href="https://doi.org/10.1080%2F10447310903025529" />
<title>Full article: Vision for Performance in Virtual Environments: The Role of Feedback Timing</title>
<meta charset="UTF-8">
<meta name="robots" content="noarchive" />
<meta name="pb-robots-disabled">

<meta property="og:title" content="Vision for Performance in Virtual Environments: The Role of Feedback Timing" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.tandfonline.com/doi/abs/10.1080/10447310903025529" />
<meta property="og:image" content="https://www.tandfonline.com/doi/cover-img/10.1080/hihc20.v025.i08" />
<meta property="og:site_name" content="Taylor & Francis" />
<meta property="og:description" content="(2009). Vision for Performance in Virtual Environments: The Role of Feedback Timing. International Journal of Human&#x2013;Computer Interaction: Vol. 25, No. 8, pp. 785-805." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@tandfonline">
<meta name="viewport" content="width=device-width,initial-scale=1" />
<script>var tandfData = {"search":{"hasOpenAccess":true}};</script>
<link rel="stylesheet" type="text/css" href="/wro/j2y2~product.css"><link rel="stylesheet" type="text/css" href="/pb/css/t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" id="pb-css" data-pb-css-id="t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" />
<link href="//www.trendmd.com" rel="preconnect" />
<link href="//app.wizdom.ai" rel="preconnect" />
<link href="//connect.facebook.net" rel="preconnect" />
<link href="//go.taylorandfrancis.com" rel="preconnect" />
<link href="//pi.pardot.com" rel="preconnect" />
<link href="//static.hotjar.com" rel="preconnect" />
<link href="//cdn.pbgrd.com" rel="preconnect" />
<link href="//f1-eu.readspeaker.com" rel="preconnect" />
<link href="//www.googleadservices.com" rel="preconnect" />
<link href="https://ajax.googleapis.com" rel="preconnect" />
<link href="https://m.addthis.com" rel="preconnect" />
<link href="https://wl.figshare.com" rel="preconnect" />
<link href="https://pagead2.googlesyndication.com" rel="preconnect" />
<link href="https://www.googletagmanager.com" rel="preconnect" />
<link href="https://www.google-analytics.com" rel="preconnect" />
<script type="text/javascript" src="/wro/j2y2~loadinview.js"></script>
<script type="text/javascript" src="/wro/j2y2~product.js"></script>
<script type="text/javascript">
        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};
    </script>
<script type="application/javascript" src="//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr" id="read-speaker" async></script>
<script type="text/javascript" src="//cdn.pbgrd.com/core-tandf.js" async defer></script>
<script data-ad-client="ca-pub-5143040550582507" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script>

<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:864760,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>
<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script async src="https://www.colwiz.com/pubsol/widget/34000f34a146a2017e2b5acad48d6b07.js"></script>
<link href="//qa.colwiz.com" rel="preconnect" />
<script src="//scholar.google.com/scholar_js/casa.js" async></script>
</head>
<body class="pb-ui">

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<div class="skipContent off-screen">
<a href="#top-content-scroll" class="skipToContent" title="Skip to Main Content" tabIndex="0">Skip to Main Content</a>
</div>
<script type="text/javascript">
    if (true) {
        var skipToContent = document.getElementsByClassName("skipToContent");
        if (skipToContent != null) {
            skipToContent[0].onclick = function skipElement() {
                var element = document.getElementById('top-content-scroll');
                if (element == null || element === undefined) {
                    element = document.getElementsByClassName('top-content-scroll').item(0);
                }
                element.setAttribute('tabindex', '0');
                element.focus();
            }
        }
    }
    document.addEventListener("DOMContentLoaded",function(e){
        if(document.getElementsByClassName("mediaThumbnailContainer").length > 0){
            TandfUtils.appendScript(document.body,"/wro/j2y2~jwplayer.js","jwplayer_src",true,true);
        }
    });
</script>
<div id="pb-page-content" data-ng-non-bindable>
<div data-pb-dropzone="main" data-pb-dropzone-name="Main">
<div class="widget pageHeader none  widget-none  widget-compact-all" id="a4d4fdd3-c594-4d68-9f06-b69b8b37ed56">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><header class="page-header">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="036fa949-dc25-4ffe-9df0-d7daefee281b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-xs  ">
<div class="col-xs-1-6 header-index">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal" id="e817489e-2520-418b-a731-b62e247e74df">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/" title="Taylor and Francis Online">
<img src="/pb-assets/Global/tfo_logo-1444989687640.png" alt="Taylor and Francis Online" />
</a></div>
</div>
</div>
<div class="widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal" id="b3fe8380-8b88-4558-b004-6485d3aea155">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/">
<img src="/pb-assets/Global/tfo_logo_sm-1459688573210.png" />
</a></div>
</div>
</div>
</div>
</div>
<div class="col-xs-5-6 ">
<div class="contents" data-pb-dropzone="contents1">

<div class="widget layout-inline-content alignRight  widget-none  widget-compact-all" id="a8a37801-55c7-4566-bdef-e4e738967e38">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget layout-inline-content none customLoginBar widget-none" id="fbe90803-b9c8-4bef-9365-cb53cc4bfa0e">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumInstitutionBanner none bannerWidth widget-none" id="3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="institution-image-text hidden-xs hidden-sm disable-click">Access provided by<strong> Copenhagen University Library</strong>
</div>
<div class="institution-image logout-institution-image">
</div></div>
</div>
</div>
<div class="widget literatumNavigationLoginBar none  widget-none  widget-compact-all" id="1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="loginBar not-logged-in">
<span class="icon-user"></span>
<a href="/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F10447310903025529" class="sign-in-link">
Log in
</a>
<span class="loginSeprator">&nbsp;|&nbsp;</span>
<a href="/action/registration?redirectUri=%2F" class="register-link">
Register
</a>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget eCommerceCartIndicatorWidget none literatumCartLink widget-none" id="9de10bb5-08af-48bc-b9f6-3f6433229f3e">
<div class="wrapped ">
<div class="widget-body body body-none "><a href="/action/showCart?FlowID=1" class="cartLabel">
<span class="hidden-xs hidden-sm visible-tl-inline-block">Cart</span>
<span class="cartItems" data-id="cart-size" role="status">
</span>
</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</header></div>
</div>
</div>
<div class="widget pageBody none  widget-none  widget-compact-all" id="35d9ca18-265e-4501-9038-4105e95a4b7d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<div class="page-body pagefulltext">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all" id="1728e801-36cd-4288-9f53-392bad29506a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-5-12 search_container ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget quickSearchWidget none search-customize-width widget-none  widget-compact-all" id="d46e3260-1f5c-4802-821a-28a03a699c82">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="quickSearchFormContainer ">
<form action="/action/doSearch" name="quickSearch" class="quickSearchForm " title="Quick Search" method="get" onsubmit="appendSearchFilters(this)" aria-label="Quick Search"><span class="simpleSearchBoxContainer">
<input name="AllField" class="searchText main-search-field autocomplete" value="" type="search" id="searchText" title="Type search term here" aria-label="Search" placeholder="Enter keywords, authors, DOI, ORCID etc" autocomplete="off" data-history-items-conf="3" data-publication-titles-conf="3" data-publication-items-conf="3" data-topics-conf="3" data-contributors-conf="3" data-fuzzy-suggester="false" data-auto-complete-target="title-auto-complete" />
</span>
<span class="searchDropDownDivRight">
<label for="searchInSelector" class="visuallyhidden">Search in:</label>
<select id="searchInSelector" name="SeriesKey" class="js__searchInSelector">
<option value="hihc20" id="thisJournal" data-search-in="thisJournal">
This Journal
</option>
<option value="" data-search-in="default">
Anywhere
</option>
</select>
</span>
<span class="quick-search-btn">
<input class="mainSearchButton searchButtons pointer" title="Search" role="button" type="submit" value="" aria-label="Search" />
</span></form>
</div>
<div class="advancedSearchLinkDropZone" data-pb-dropzone="advancedSearchLinkDropZone">
<div class="widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all" id="323e2a31-1c81-4995-bd17-8e149458c214">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a href="/search/advanced" class="advSearchArticle">Advanced search</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
 </div>
<div class="col-md-7-12 serNav_container">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumSeriesNavigation none  widget-none" id="7730bfe1-9fca-4cf4-a6d6-2a0148105437">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="issueSerialNavigation journal">
<div class="cover">
<img data-src='{"type":"image" , "src":"/na101/home/literatum/publisher/tandf/journals/covergifs/hihc20/cover.jpg"}' src="//:0" alt="Publication Cover" width="120" height="156" />
</div>
<div class="info ">
<div class="title-container">
<span class="titleHeading">Journal</span>
<h1>
<a href="/toc/hihc20/current">
International Journal of Human&#x2013;Computer Interaction
</a>
</h1>
<h2>
Volume 25, 2009 - <a href="/toc/hihc20/25/8" class="nav-toc-list">Issue 8</a>
</h2>
</div>
<div class="seriesNavDropZone" data-pb-dropzone="seriesNavDropZone">
<div class="widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal" id="753455df-1eeb-47ca-bdc9-e19022075973">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="serial-action">
<a href="http://www.editorialmanager.com/ijhc" class="green submitAnArticle"><span>Submit an article</span></a>
<a href="/toc/hihc20/current" class="jHomepage"><span>Journal homepage</span></a>
</div></div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
 </div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none" id="e42aea8f-434a-4d39-aaef-f56af3ff00dc">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all" id="6aacf107-e82d-494d-a14c-0c00bba52560">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="accessLogo">
<div>
<img class="accessIconLocation" data-src='{"type":"image" , "src":"/pb-assets/3rdPartyLogos/accessFull-1452596451717.png"}' src="//:0" alt="Full access" />
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none publicationContentHeader widget-none  widget-compact-all" id="63f402e4-3498-4709-8d7d-ee8e69f93467">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical" id="5afd8b6d-7e09-43ff-8ad6-afa3764e543c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="articleMetricsContainer">
<div class="content compactView">

<div class="section">
<div class="value">
87
</div>
<div class="title">
Views
</div>
</div>
<div class="section">
<div class="value">
4
</div>
<div class="title">
CrossRef citations to date
</div>
</div>
<div class="section score">
<div class="altmetric-score true">
<div class="value" data="10.1080/10447310903025529"></div>
<div class="title">
Altmetric
</div>
</div>
</div>
<div class="altmetric-Key hidden" data="be0ef6915d1b2200a248b7195d01ef22"></div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-2-3 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all" id="fa57727f-b942-4eb8-9ed2-ecfe11ac03f5">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="read-speaker-container" style="display: block">
<div id="readspeaker_button1" class="rs_skip rsbtn rs_preserve">
<a href="//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F10447310903025529" rel="nofollow" class="rsbtn_play" accesskey="L" title="Listen to this page using ReadSpeaker webReader" style="border-radius: 0 11.4px 11.4px 2px;">
<span class="rsbtn_left rsimg rspart"><span class="rsbtn_text"><span>Listen</span></span></span>
<span class="rsbtn_right rsimg rsplay rspart"></span>
</a>
</div>
</div>
<div class="toc-heading">
<h3>
Articles
</h3>
</div>
<h1><span class="NLM_article-title hlFld-title">Vision for Performance in Virtual Environments: The Role of Feedback Timing</span></h1><span class="sub-title"><h2></h2></span><div class="literatumAuthors"><div class="publicationContentAuthors"><div class="hlFld-ContribAuthor"><span class="NLM_contrib-group"><span class="contribDegrees corresponding "><a class="entryAuthor" href="/author/Mason%2C+Andrea+H"> Andrea H. Mason <span class="overlay"> University of Wisconsin–Madison , <span class="corr-sec"><span class="heading">Correspondence</span><span class="corr-email"><i class="fa fa-envelope" style="color: #10147E; padding-right: 7px" aria-hidden="true"></i><span data-mailto="mailto:amason@education.wisc.edu">amason@education.wisc.edu</span></span><br /></span></span></a> &amp; </span><span class="contribDegrees "><a class="entryAuthor" href="/author/Bernardin%2C+Brandon+J"> Brandon J. Bernardin <span class="overlay"> University of Wisconsin–Madison , </span></a></span></span></div></div></div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="5f562208-b1d5-4e5a-81c7-356431240f04">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md gutterless ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget layout-inline-content none  widget-none  widget-compact-all" id="87ac5840-18fa-4a14-8eca-065b90ede3d7">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumContentItemPageRange none  widget-none  widget-compact-all" id="45057865-d60c-414c-bc81-646debb621b0">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><span class="contentItemPageRange">Pages 785-805
</span></div>
</div>
</div>
<div class="widget literatumContentItemHistory none  widget-none  widget-compact-all" id="32bf868e-52ce-411a-9dc3-717743aad997">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div>Published online: 03 Dec 2009</div></div>
</div>
</div>
<div class="widget literatumArticleToolsWidget none  widget-none  widget-compact-all" id="ed673666-7b5d-470e-bd33-c5c679d996cb">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="articleTools">
<ul class="linkList blockLinks separators centered">
<li class="downloadCitations">
<a href="/action/showCitFormats?doi=10.1080%2F10447310903025529"><i class="fa fa-quote-left" aria-hidden="true"></i>Download citation</a>
</li>
<li class="dx-doi">
<a href="https://doi.org/10.1080/10447310903025529"><i class="fa fa-external-link-square" style="margin: 0 0.25rem 0 0" aria-hidden="true"></i>https://doi.org/10.1080/10447310903025529</a>
</li>
</ul>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents2">
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none publicationContentBody widget-none" id="f4a74f7a-9ba2-4605-86b1-8094cb1f01de">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget sectionsNavigation none  widget-none" id="f15bd2de-bb18-4067-8ab9-03ea3be30bf7">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="sections-nav"><span class="title">In this article<a href="#" class="close" tabindex="-1"><span aria-describedby="close-description"><span class="off-screen" id="close-description">Close</span></span></a></span><ul class="sections-list"><li><span class="sub-art-heading"><a href="#_i1">1. INTRODUCTION</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i7">2. METHOD</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i17">3. RESULTS</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i23">4. DISCUSSION</a></span><ul class="sub-art-titles"></ul></li><li><a href="#references-Section">References</a></li></ul></div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none rs_readArea widget-none  widget-compact-all" id="9751b4f9-64b9-44c0-955b-f75246902839">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all" id="d29f04e9-776c-4996-a0d8-931023161e00">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
        "HTML-CSS": {scale: 70, linebreaks: {automatic: true, width: "container"}},
        SVG: {linebreaks: {automatic: true, width: "25%"}},
        menuSettings: {zoom: "Click"},

        /* This is necessary to lazy loading. */
        skipStartupTypeset: true
    });
</script>
<div class="articleMeta ja">
<div class="tocHeading">
<h2>Articles</h2>
</div>
<div class="hlFld-Title">
<div class="publicationContentTitle">
<h1 class="chaptertitle">
Vision for Performance in Virtual Environments: The Role of Feedback Timing
</h1>
</div>
</div>
<div class="copyrightStatement">
</div>
<div class="articleMetaDrop publicationContentDropZone" data-pb-dropzone="articleMetaDropZone">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone1" data-pb-dropzone="articleMetaDropZone1">
</div>
<div class="copyrightline">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone2" data-pb-dropzone="articleMetaDropZone2">
</div>
</div>
<div class="publication-tabs ja publication-tabs-dropdown">
<div class="tabs tabs-widget">
<ul class="tab-nav" role="tablist">
<li class="active" role="tab">
<a href="/doi/full/10.1080/10447310903025529?scroll=top&amp;needAccess=true" class="show-full">
<i class="fa fa-file-text" aria-hidden="true"></i>
<span class="nav-data">
Full Article
</span>
</a>
</li>
<li role="tab">
<a href="/doi/figure/10.1080/10447310903025529?scroll=top&amp;needAccess=true" class="show-figure">
<i class="fa fa-image" aria-hidden="true"></i>
<span class="nav-data">Figures & data</span>
</a>
</li>
<li role="tab">
<a href="/doi/ref/10.1080/10447310903025529?scroll=top" class="show-references">
<i class="fa fa-book" aria-hidden="true"></i>
<span class="nav-data">References</span>
</a>
</li>
<li class="citedbyTab " role="tab">
<a href="/doi/citedby/10.1080/10447310903025529?scroll=top&amp;needAccess=true">
<i class="fa fa-quote-left" aria-hidden="true"></i>
<span class="nav-data">
Citations
</span>
</a>
</li>
<li class="off-screen"></li>
<li role="tab" class="metrics-tab">
<a href="#metrics-content" class="show-metrics">
<i class="fa fa-bar-chart" aria-hidden="true"></i>
<span class="nav-data">Metrics</span>
</a>
</li>
<li role="tab" class="permissions-tab ">
<a href="/doi/abs/10.1080/10447310903025529?tab=permissions&amp;scroll=top" class="show-permissions">
<i class="fa fa-print" aria-hidden="true"></i>
<span class="nav-data">
Reprints & Permissions</span></a>
</li>
<li class="pdf-tab " role="button">
<a href="/doi/pdf/10.1080/10447310903025529?needAccess=true" class="show-pdf" target="_blank">
<span class="nav-data">
PDF
</span>
</a>
</li>
</ul>
<div class="tab-content ">
<a id="top-content-scroll"></a>
<div class="tab tab-pane active">
<article class="article">
<p class="fulltext"></p><div class="hlFld-Abstract"><p class="fulltext"></p><div class="sectionInfo abstractSectionHeading"><h2 id="abstract" class="section-heading-2">Abstract</h2></div><div class="abstractSection abstractInFull"> <p>This work explores how people use visual feedback when performing simple reach-to-grasp movements in a tabletop virtual environment. In particular we investigated whether visual feedback is required for the entire reach or whether minimal feedback can be effectively used. Twelve participants performed reach-to-grasp movements toward targets at two locations. Visual feedback about the index finger and thumb was provided in four conditions: vision available throughout the movement, vision available up to peak wrist velocity, vision available until movement initiation, or vision absent throughout the movement. It was hypothesized that vision available until movement onset would be an advantage over a no vision situation yet not attain the performance observed when vision was available up to peak velocity. Results indicated that movement time was longest in the no vision condition but similar for the three conditions where vision was available. However, deceleration time and peak aperture measures suggest grasping is more difficult when vision is not available for at least the first third of the movement. These results suggest that designers of virtual environments can manipulate the availability of visual feedback of one's hand without compromising interactivity. This may be applied, for example, when detailed rendering of other aspects of the environmental layout is more important, when motion lag is a problem or when hand/object concealment is an issue.</p> </div></div><div class="hlFld-Fulltext"><div id="S0001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1"> <h2 id="_i1" class="section-heading-2">1. INTRODUCTION</h2> <p>In recent years, the field of Virtual Reality has grown significantly. These environments give users the capability to interact and train in locations and situations that would otherwise be too costly, too dangerous, or physically impossible to experience in the real world. Dangerous scenarios like flight training and complex procedures like surgical training have been implemented as virtual simulations providing users with a safe, efficient, and customizable method of training (e.g., <span class="ref-lnk lazy-ref"><a data-rid="CIT0031" data-refLink="_i29" href="#">Medical Robotics Group, 2002</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0035" data-refLink="_i29" href="#">University of Michigan 3D Lab, 2007</a></span>). Virtual reconstruction of architecture and archeological sites allow users to be transported to realistic walk-throughs (e.g., <span class="ref-lnk lazy-ref"><a data-rid="CIT0004" data-refLink="_i29" href="#">Brown University Computer Graphics Group, 2007</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0036" data-refLink="_i29" href="#">The University of North Carolina at Chapel Hill, n.d.</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0035" data-refLink="_i29" href="#">University of Michigan 3D Lab, 2007</a></span>). However, along with these technological advances, challenges to the widespread implementation of virtual environments have also surfaced. In particular, the difficulty of implementing graphical effects with sufficient realism can lead to various distortions in perception (i.e., swimming of the visual world, lack of motion parallax, misperception of depth) and degraded performance (i.e., disturbed perceptual-motor coordination) when users try to perform simple and complex tasks in these environments (<span class="ref-lnk lazy-ref"><a data-rid="CIT0001" data-refLink="_i29" href="#">Allison, Harris, Jenkin, Jasiobedzka, &amp; Zacher, 2001</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0003" data-refLink="_i29" href="#">Biocca &amp; Rolland, 1998</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0010" data-refLink="_i29" href="#">DiZio &amp; Lackner, 1997</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0021" data-refLink="_i29" href="#">Jung, Adelstein, &amp; Ellis, 2000</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0038" data-refLink="_i29" href="#">Ware &amp; Arsenault, 2004</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0039" data-refLink="_i29" href="#">Ware &amp; Balakrishnan, 1994</a></span>). This is particularly true for presentation of accurate feedback about hand movements. It is our belief that virtual environments will only reach their full potential once we have a basic understanding of the human user. In particular, understanding what feedback humans require when interacting, when they require that feedback and how to provide the feedback in an effective manner is key to the optimal design and evaluation of virtual environments. Therefore, a systematic study of the use of sensory feedback in virtual environments is required so that we can make evidence-based decisions regarding how best to present sensory feedback to users as they interact in virtual environments.</p> <div id="S2001" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i2">1.1. Review of Related Literature</h3> <p>In the following sections we review the scientific literature from both real-world and virtual reality experiments and applications that have motivated the current work.</p> <div id="S3001" class="NLM_sec NLM_sec_level_3"> <h4 class="section-heading-4" id="_i3"></h4> <div id="S4001" class="NLM_sec NLM_sec_level_4"> <h5 class="section-heading-5" id="_i4">Visual feedback about self-movement in both natural and virtual environments</h5> <p>When considering the provision of visual feedback in virtual environments, we can distinguish between two types of information. The first type of feedback relates to visual information about the characteristics of the targets to be manipulated, surrounding objects and the workspace within which subjects are interacting. It has been observed that in aiming and reach-to-grasp movements visual information about the target, obtained prior to movement onset, is used to create a motor plan for effective execution of the transport and grasp movements (<span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-refLink="_i29" href="#">Elliott, 1988</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0040" data-refLink="_i29" href="#">Westwood, Heath, &amp; Roy, 2000</a></span>). The second type of feedback deals with visual information about the characteristics and movements of one's own limbs and body within the environment. In the current study, we are concerned with this second type of feedback.</p> <p>Since the seminal work of <span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-refLink="_i29" href="#">Woodworth (1899</a></span>), who introduced the terms <i>initial adjustment</i> (i.e., open-loop ballistic movement toward the target) and <i>current control</i> (closed-loop error-correction phase), researchers have been interested in the role that vision about oneself and the environment plays in the control of coordinated movements. In natural environments, it has been shown that when humans perform simple aiming or reach-to-grasp movements with full vision of their limbs, they perform these tasks efficiently and accurately (<span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-refLink="_i29" href="#">Carlton, 1992</a></span>). With no visual feedback, movements tend to become slower and less accurate than when vision is present (<span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-refLink="_i29" href="#">Berthier, Clifton, Gullapalli, McCall, &amp; Robin, 1996</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-refLink="_i29" href="#">Gentilucci, Toni, Chieffi, &amp; Pavesi, 1994</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-refLink="_i29" href="#">Jakobson &amp; Goodale, 1991</a></span>). Research has also been conducted to determine whether intermittent feedback can be useful for task performance. In particular, <span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-refLink="_i29" href="#">Elliott (1988</a></span>) has shown that a short-lived visual representation of the movement and environment may provide a reasonable, although not perfect, substitute for continuous visual feedback.</p> <p>In virtual environments, feedback must be synthetically created to be presented to users. As such, it is important to investigate whether the same relationships seen in the natural environment between vision and performance are also present in virtual environments. Furthermore, it is appealing to determine whether performance can be optimized with decreased fidelity of sensory information or by reducing the amount of time that the feedback is made available. To date, studies on the role of visual feedback in virtual environments have shown some similarities to those performed in natural environments. For example, Fitts's (<span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-refLink="_i29" href="#">Fitts &amp; Peterson, 1964</a></span>) type aiming movements (<span class="ref-lnk lazy-ref"><a data-rid="CIT0016" data-refLink="_i29" href="#">Graham &amp; MacKenzie, 1995</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-refLink="_i29" href="#">Graham &amp; MacKenzie, 1996</a></span>), transportation and orientation tasks (<span class="ref-lnk lazy-ref"><a data-rid="CIT0037" data-refLink="_i29" href="#">Wang &amp; MacKenzie, 2000</a></span>), simple reach-to-grasp movements (<span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-refLink="_i29" href="#">Mason, Walji, Lee, &amp; MacKenzie, 2001</a></span>) and collaborative movements (<span class="ref-lnk lazy-ref"><a data-rid="CIT0028" data-refLink="_i29" href="#">Mason &amp; MacKenzie, 2002</a></span>, 2004) have been studied. Evidence from these studies has indicated that a representation of one's self is advantageous for optimal performance. In a recent report, <span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-refLink="_i29" href="#">Buchman, Nilson, and Billinghurst (2005</a></span>) developed a system that allows designers to vary the transparency of a user's hand as they manipulate blocks in a static environment while wearing a head-mounted display. Their goal was to provide a solution to the problem of the hand concealing important objects and information as people interact in a tabletop virtual workspace. Of interest is that anecdotal reports made by the participants indicated that they felt a reduced control over their hands when transparency was too high. Thus, these results provide additional evidence that some form of graphical feedback about self-movement is advantageous when performing even simple tasks in virtual environments.</p> <p>However, it remains unclear how the timing of the representation influences performance. In other words, does the representation of the hand have to be visible throughout the entire movement for performance to be optimal? In a recent study, <span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i29" href="#">Mason and Bernardin (2007</a></span>) represented the hand as two small planar circles located at the tips of the index finger and thumb. They manipulated the timing of visual feedback by allowing feedback up to peak velocity or presenting it after peak velocity. Results indicated that visual feedback provided up to peak velocity resulted in performance that was similar to when the crude representation was available throughout the entire trial. In the current experiment, we wanted to further scale back the availability of graphical feedback to determine how feedback available only prior to the initiation of the movement compared to feedback available up to peak velocity or throughout the movement.</p> </div> <div id="S4002" class="NLM_sec NLM_sec_level_4"> <h5 class="section-heading-5" id="_i5">The role of proprioception in the performance of reach-to-grasp movements</h5> <p>Why would visual feedback available prior to movement initiation be beneficial to the performance of a reach-to-grasp task? At times, humans are able to make accurate aiming and reach-to-grasp movements even in the absence of vision. For example, in the morning people are capable of reaching out and turning off an alarm clock without even opening their eyes. This is possible for two reasons: First, we have performed this action so many times that we are aware of the general location of the target. Second, proprioceptors, which are sensory receptors located in our muscles, tendons, joints, and inner ear, give us general knowledge of where our limbs are located in space relative to our body. Even though movements are possible with only the availability of proprioceptive feedback (i.e., without vision), it has been observed that these movements are typically slower and less accurate than movements performed with full vision of the hand (<span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-refLink="_i29" href="#">Berthier et al., 1996</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-refLink="_i29" href="#">Gentilucci et al., 1994</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-refLink="_i29" href="#">Jakobson &amp; Goodale, 1991</a></span>). The reason that we see decreases in performance in the absence of vision is that proprioception alone is generally not accurate enough to give us the fine position detail needed to complete tasks such as reaching to grasp an object with precision (<span class="ref-lnk lazy-ref"><a data-rid="CIT0023" data-refLink="_i29" href="#">Klatzky &amp; Lederman, 2002</a></span>). However, <span class="ref-lnk lazy-ref"><a data-rid="CIT0033" data-refLink="_i29" href="#">Rossetti, Stelmach, Desmurget, Prablanc, and Jeannerod (1994</a></span>) have provided evidence that visual information can serve to calibrate proprioceptive knowledge of one's position when the visual feedback is provided before movement onset. Specifically, seeing one's hand before the movement begins allows for the sensory feedback from the visual system to be compared and calibrated with the sensory information from the proprioceptive system. Thus, it is possible that providing visual feedback of one's position at the beginning of a movement, but removing that feedback once the movement has been initiated might provide performance advantages over a situation where no visual feedback, and thus no calibration mechanism is available.</p> </div> </div> </div> <div id="S2002" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i6">1.2. Purpose</h3> <p>Preliminary evidence regarding the ability to calibrate proprioception with vision in a desktop virtual environment has been provided (<span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i29" href="#">Mason, 2007</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i29" href="#">Mason &amp; Bernardin, 2007</a></span>). However, no direct comparison has been made between the effectiveness of performance when vision is removed at movement onset compared to extending this representation up to peak velocity. Thus, the purpose of the current study was to directly compare how beneficial the additional visual information received up to peak velocity was compared to removing feedback at movement onset. We were also interested in comparing the two partial feedback conditions with the no vision (NV) and full vision (FV) conditions to determine whether partial vision was most similar to NV or FV. We hypothesized that vision available until movement onset would produce better performance when compared to an NV situation yet not reach the performance observed when vision was available up to peak velocity, especially for the far target distance. The rationale for our expectations was that having vision of self prior to movement onset would allow for calibration of the proprioception system and action plan formation based on current position information; conversely, in an NV situation, calibration of the proprioception system would not be possible and action plan formation would be generated based on proprioceptive feedback and memory of previous trials. We also hypothesized that target distance would influence how much visual feedback was necessary during a movement. When participants moved to the near target, we hypothesized that visual feedback prior to movement initiation would be sufficient because at the near distance variability in movement production and errors would be lower, thus requiring less visual feedback for error correction. However, as participants moved farther to grasp the object, the action plan would be less accurate and variability in movement performance would be greater, thus necessitating visual feedback to correct for performance errors.</p> </div> </div><div id="S0002" class="NLM_sec NLM_sec_level_1"> <h2 id="_i7" class="section-heading-2">2. METHOD</h2> <div id="S2003" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i8">2.1. Participants</h3> <p>Twelve undergraduate and graduate university students volunteered in a single, 30-min experimental session. All participants were right-handed and had normal or corrected-to-normal vision. The mean age of the participants was 25 ± 4.7 (7 male, 5 female). Participants provided informed consent before participating, and ethical approval was obtained from the University of Wisconsin–Madison Social and Behavioral Institutional Review Board.</p> </div> <div id="S2004" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i9">2.2. Experimental Apparatus</h3> <p>The data were collected in the Wisconsin Collaborative Virtual Environment. Shown in <a href="#F0001">Figure 1</a>, a graphic image of a target cube produced by a Linux-based scene-rendering PC was displayed on a downward-facing monitor. A half-silvered mirror was placed parallel to the computer screen, midway between the screen and the table surface. The image on the computer screen was thus reflected in the mirror and appeared to the participant to be located in a workspace on the table surface.<div class="figure figureViewer" id="F0001"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 1</b> Experimental apparatus.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0001image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production/images/medium/hihc_a_402725_o_f0001g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0001"> <p> <b>FIGURE 1</b> Experimental apparatus.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0001"></div> </p> <p>The images presented to the participant's left and right eyes were alternately displayed on the monitor and were synchronized with the CrystalEyes™ goggles (Real D, Beverly Hills CA) worn by the participant. The participant thus obtained a stereoscopic view of the images being projected onto the mirror. Three light-emitting diodes (LEDs) were fixed to the goggles. A VisualEyez 3000 motion capture system (PTI PhoeniX Technologies, Inc., Burnaby, BC) tracked the three-dimensional position of the LEDs on the goggles at a sampling rate of 120 Hz. This information was processed by the scene rendering PC with a 10-msec lag, to provide the participant with a near real-time, head-coupled view of the virtual environment on the work surface. Participants were not aware of the lag.</p> <p>Two LEDs were positioned on the participant's right thumb and index finger, respectively (see <a href="#F0002">Figure 2</a>). In some conditions, this information was used to present the participant with a crude graphical representation of his or her movement as he or she reached for and grasped the target cubes. The graphical representation of hand movement was in the form of yellow spheres (3 mm diameter) superimposed on the LEDs located on the tips of the participant's index finger and thumb. A third LED was positioned on the participant's wrist. Information from all three hand LEDs was stored for data analysis purposes.<div class="figure figureViewer" id="F0002"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 2</b> LED placement.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0002image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production/images/medium/hihc_a_402725_o_f0002g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0002"> <p> <b>FIGURE 2</b> LED placement.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0002"></div> </p> <p>Finally, three LEDs were positioned on the top surface of the wooden target cube, which measured 33 cm<sup>3</sup> (see <a href="#F0002">Figure 2</a>). The tracked position of LEDs on the physical target cube was used to generate the superimposed graphical object, which had identical dimensions to its physical counterpart.</p> </div> <div id="S2005" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i12">2.3. Experimental Design</h3> <p>In the current experiment we manipulated both target distance and the availability of visual feedback about hand movement. Participants reached for targets located at a near distance of 10 cm or a far distance of 20 cm from the start position. The target cube was rotated to a 45-degree angle such that a corner of the cube was always facing the participants. This target positioning made for a comfortable reach-to-grasp movement. At each target distance, participants were presented with four visual feedback conditions. In the first feedback condition, participants did not receive a graphical representation of their hand throughout the trial (NV). In the second feedback condition, a graphical representation of the hand was available prior to movement initiation but was extinguished as soon as the participants lifted their hand off of the start position (vision extinguished at movement onset [VEX]). In the third feedback condition, a graphical representation of the hand was available through movement initiation, but was extinguished when peak velocity of the wrist was attained (vision up to peak velocity [UPV]). It is important to note that because we used raw data to estimate peak velocity in real time, vision could only be extinguished on average approximately 50 msec after the true peak velocity value. In the final feedback condition, participants received a graphical representation of their hand throughout the entire movement (FV). In all four feedback conditions, graphical information about the size and location of the target was always available. Participants were prevented from seeing the real workspace via the placement of a shield below the mirror. Thus participants did not have visual feedback of their moving limb or the surrounding environment. These manipulations resulted in a balanced design of 2 (distances) × 4 (visual feedback conditions)<i>.</i> Participants performed 10 trials in each condition, and trials were presented in a blocked order for a total of 80 trials (see <a class="ref showTableEventRef" data-ID="T0001">Table 1</a>). Conditions were presented in a random order for each participant to minimize the effects of learning.<div id="tableViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="tableView"><div class="tableCaption"><div class="short-legend"><h3> <b>Table 1: Summary of Experimental Trials</b> </h3></div></div><div class="tableDownloadOption" data-hasCSVLnk="true" id="T0001-table-wrapper"><a id="CSVdownloadButton" class="downloadButton btn btn-sm" href="/action/downloadTable?id=T0001&amp;doi=10.1080%2F10447310903025529&amp;downloadType=CSV">CSV</a><a data-id="T0001" class="downloadButton btn btn-sm displaySizeTable" href="#">Display Table</a></div></div> </p> </div> <div id="S2006" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i14">2.4. Procedure</h3> <p>At the beginning of the experiment, the participant was seated in a height-adjustable chair in front of the tabletop virtual environment. The participant was then asked to put on the CrystalEyes goggles. Next, we ran a small series of four to eight practice trials to familiarize the participant with the virtual environment and the four visual conditions. After the participant was comfortable with the task, we began with data collection. Participants began each trial with the pads of the index finger and thumb lightly touching and resting on a haptic “rest” position (Velcro tab on the table surface). The appearance of a green light in the upper right-hand corner of the work surface was the participant's cue to move from the “rest” position to the start position, which was represented by a second Velcro tab, located approximately 3 in. to the left of the rest mark. No graphic representation of either mark was available; participants found these marks based on touch alone. However, in the VEX, UPV, and FV conditions, participants could see representations of their fingers moving from the rest position to the start position. Once the fingers remained stationary on the start position for 1 sec, the light turned from green to red, immediately followed by the appearance of the target cube. Participants were instructed to reach for, grasp, and lift the target cube at a comfortable pace as soon as it became visible. Once they had acquired and lifted the cube, they held it until the experimenter gave a verbal “stop” command. Before each trial, the target was repositioned by the experimenter, and participants returned their grasping hand to the rest position.</p> </div> <div id="S2007" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i15">2.5. Data Analysis</h3> <p>In the current experiment, we employed traditional kinematic measures of human performance to evaluate the temporal and spatial aspects of the movements being performed under the manipulated conditions. Simple timing measures such as movement time provide a general description of upper limb movements. However, in motor control studies, more complex three-dimensional kinematic measures such as displacement profiles, movement velocity, deceleration time, and the formation of the grasp aperture (distance between the index finger and thumb for a precision pinch grip) have also been used to characterize object acquisition movements (C. L. <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-refLink="_i29" href="#">MacKenzie &amp; Iberall, 1994</a></span>). In particular, time to peak velocity has been used as a measure of the open-loop processes occurring during target acquisition tasks and is thought to be reflective of motor planning. On the other hand, percent time from peak velocity has typically been used as a measure of closed-loop control, where a longer time spent decelerating toward the target is equated with a greater reliance on feedback. These temporal measures combined with movement time and peak wrist velocity allow us to completely describe a target acquisition task in terms of open and closed loop control.</p> <p>For target acquisition tasks that involve grasping objects a measure of the opening and closing of the hand is also required. Grasp aperture (resultant distance between the index finger and thumb as the movement unfolds) has typically been used to quantify grasp formation. In human performance literature, larger apertures have been associated with more complex tasks that demand greater attentional resources (<span class="ref-lnk lazy-ref"><a data-rid="CIT0041" data-refLink="_i29" href="#">Wing, Turton, &amp; Fraser, 1986</a></span>). It is believed that a larger aperture is used as a compensatory strategy to avoid missing or hitting the target. The timing of peak aperture can also give us an indication of the use of feedback when grasping the object. Specifically, a longer time after peak aperture is an indication of increased reliance on feedback when closing the hand down on the target.</p> <p>To extract the dependent measures just described, the position data recorded by the Visualeyez motion analysis system (PTI PhoeniX Technologies, Inc., Burnaby, BC) were first interpolated over no more than four missing frames, rotated into a meaningful coordinate system (x = forward movements, y = side to side movements, z = up and down movements) and smoothed with a 7 Hz low-pass second-order bidirectional Butterworth filter. A customized computer program (KinSys, Eh?Soft, Madison, WI) was used to determine the start of movement based on a criterion wrist velocity of 5 mm/sec. The end of movement was determined as the point when the block first began to lift off the table, signifying that the reach-to-grasp movement had been completed (see <a href="#F0003">Figure 3</a>). End of movement minus start of movement was used to calculate movement time (MT). The position data were differentiated and peak resultant velocity (PV) and the timing of the peak (TPV) were extracted. Percent time from peak velocity was defined as (MT – TPV)/MT × 100. The resultant distance between the index finger and thumb was calculated and both peak aperture and the timing of peak aperture (TPA) were extracted. Percent time from peak aperture was defined as (MT – TPA)/MT × 100.<div class="figure figureViewer" id="F0003"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 3</b> Plots of a typical wrist velocity and block lift profile. <i>Note.</i> Start of movement was chosen at the point when wrist velocity reached the 5 mm/sec threshold. End of movement was determined as the point when block lift velocity surpassed the same threshold.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0003image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production/images/medium/hihc_a_402725_o_f0003g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0003"> <p> <b>FIGURE 3</b> Plots of a typical wrist velocity and block lift profile. <i>Note.</i> Start of movement was chosen at the point when wrist velocity reached the 5 mm/sec threshold. End of movement was determined as the point when block lift velocity surpassed the same threshold.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0003"></div> </p> <p>Separate mean values were calculated for each participant and dependent measure using the 10 trials performed in each condition. For each dependent measure, these means were analyzed using separate 2 (distance: near, far) × 4 (visual condition: NV, VEX, UPV, FV) repeated measures analyses of variance and an a priori alpha level of <i>p</i> &lt; .05. Post hoc analysis using Tukey's Honestly Significant Difference was performed on significant effects. Means and standard errors are reported in the figures and text for significant analysis of variance results.</p> </div> </div><div id="S0003" class="NLM_sec NLM_sec_level_1"> <h2 id="_i17" class="section-heading-2">3. RESULTS</h2> <p>No significant interactions between distance and visual condition were found for any of the kinematic performance measures. Thus, participants moved similarly for each visual condition whether they were reaching for the target at the near or far distance. We did however find several main effects of distance and visual condition and these are reported in the text and figures next.</p> <div id="S2008" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i18">3.1. Reach Kinematics</h3> <p>Main effects were found for target distance, F(1, 11) = 42.1, <i>p</i> &lt; .001, and visual condition, F(3, 33) = 6.2, <i>p</i> &lt; .05. As expected, as distance increased, participants took significantly longer to reach for the target (near = 970 ± 79 msec, far = 1184 ± 89 msec). For the main effect of visual condition, movement time was longest when vision was absent throughout the entire movement (NV). Of greater interest was that movement time was significantly faster when participants were given vision that was VEX or UPV. Furthermore movement times in VEX and UPV conditions were not significantly different than the FV condition (see <a href="#F0004">Figure 4</a>).<div class="figure figureViewer" id="F0004"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 4</b> Main effect of visual condition on movement time. <i>Note.</i> Compared to no vision (NV), performance improves in the vision extinguished at movement onset (VEX) and vision up to peak velocity (UPV) and full vision (FV) conditions, which are similar.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0004image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production/images/medium/hihc_a_402725_o_f0004g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0004"> <p> <b>FIGURE 4</b> Main effect of visual condition on movement time. <i>Note.</i> Compared to no vision (NV), performance improves in the vision extinguished at movement onset (VEX) and vision up to peak velocity (UPV) and full vision (FV) conditions, which are similar.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0004"></div> </p> <p>A main effect of target distance, F(1, 11) = 17.8, <i>p</i> &lt; .001, was found for time to peak velocity. As expected, participants took longer to reach peak velocity when reaching to targets located at the farther distance. The main effect of visual condition was not significant.</p> <p>The significant main effect of target distance, F(1, 11) = 295.8, <i>p</i> &lt; .001, on peak velocity indicated that participants attained a greater peak velocity when reaching for targets located at the far distance (468.8 ± 24 mm/sec) compared to the near distance (287.4 ± 18 mm/sec). The main effect of visual condition failed to reach significance.</p> <p>For percent time from peak velocity, main effects were found for target distance, F(1, 11) = 42.7, <i>p</i> &lt; .001, and visual condition, F(3, 33) = 4.9, <i>p</i> &lt; .05. Participants spent a greater percentage of time decelerating toward the target when it was located at the far distance (65.1 ± 1.3%) than when it was located at the near distance (59.6 ± 1.9%). As shown in <a href="#F0005">Figure 5</a>, participants also spent the most time decelerating in the NV condition. Deceleration time was significantly shorter for the UPV and FV conditions when compared to NV; however, when vision was extinguished at movement onset, deceleration time was not significantly shorter than in the NV condition.<div class="figure figureViewer" id="F0005"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 5</b> Main effect of visual feedback on percent time from peak velocity. <i>Note.</i> Deceleration time was shortest for the full vision (FV) and vision up to peak velocity (UPV) conditions. Compared to the no vision (NV) condition, vision extinguished at movement onset (VEX) was not statistically shorter.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0005image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production/images/medium/hihc_a_402725_o_f0005g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0005"> <p> <b>FIGURE 5</b> Main effect of visual feedback on percent time from peak velocity. <i>Note.</i> Deceleration time was shortest for the full vision (FV) and vision up to peak velocity (UPV) conditions. Compared to the no vision (NV) condition, vision extinguished at movement onset (VEX) was not statistically shorter.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0005"></div> </p> </div> <div id="S2009" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i21">3.2. Grasp Kinematics</h3> <p>There were no drops or misses when participants reached-to-grasp the target block for any of the visual conditions or target distances. However, corrective actions during the grasp were occasionally seen, defined by double peaks in the kinematic aperture profile. Participants either closed their hand too early or misjudged the correct aperture length on approximately 8 ± 7 trials out of the 80 total, with 51% of corrective actions made during the NV condition.</p> <p>A main effect of target distance, F(1, 11) = 46.3, <i>p</i> &lt; .001, was found for the timing of peak aperture. Much like the results for time to peak velocity, participants took a longer time reaching peak aperture when grasping objects located at the far target distance than objects at the close distance. The main effect of visual condition failed to reach statistical significance.</p> <p>Main effects on peak aperture were found for both target distance, F(1, 11) = 45.0, <i>p</i> &lt; .001, and visual condition, F(3, 33) = 20.0, <i>p</i> &lt; .001. Peak aperture was greater (77.8 + 2.5 mm) for the far target distance than for the near target distance (73.7 ± 2.4 mm). When considering the main effect of visual condition, aperture was greatest in the NV condition (see <a href="#F0006">Figure 6</a>). Of particular interest, however, is that aperture was significantly smaller in the VEX condition when compared to the NV condition. Further, aperture was again significantly smaller for the UPV and FV conditions when compared to the VEX condition.<div class="figure figureViewer" id="F0006"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision for Performance in Virtual Environments: The Role of Feedback Timing</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Mason%2C+Andrea+H"><span class="hlFld-ContribAuthor"><a href="/author/Mason%2C+Andrea+H"><span class="NLM_given-names">Andrea H.</span> Mason</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Bernardin%2C+Brandon+J"><span class="hlFld-ContribAuthor"><a href="/author/Bernardin%2C+Brandon+J"><span class="NLM_given-names">Brandon J.</span> Bernardin</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447310903025529">https://doi.org/10.1080/10447310903025529</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>03 December 2009</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 6</b> Main effect of visual condition on peak aperature (PA). <i>Note.</i> Aperture is largest in the no vision (NV) condition, significantly smaller when vision is afforded prior to movement onset (VEX), but smallest in the vision up to peak velocity (UPV) and full vision (FV) conditions.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0006image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production/images/medium/hihc_a_402725_o_f0006g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0006"> <p> <b>FIGURE 6</b> Main effect of visual condition on peak aperature (PA). <i>Note.</i> Aperture is largest in the no vision (NV) condition, significantly smaller when vision is afforded prior to movement onset (VEX), but smallest in the vision up to peak velocity (UPV) and full vision (FV) conditions.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0006"></div> </p> <p>A main effect of visual condition, F(3, 33) = 12.2, <i>p</i> &lt; .005, was found for percent time from peak aperture. Percent time from peak aperture was greatest in the NV condition (42.2 ± 1.8%) and was significantly smaller for the three other visual conditions (VEX = 38.8 ± 2.0%, UPV = 35.9 ± 1.4%, FV = 35.9 ± 1.4%). The main effect of target distance failed to reach significance.</p> </div> </div><div id="S0004" class="NLM_sec NLM_sec_level_1"> <h2 id="_i23" class="section-heading-2">4. DISCUSSION</h2> <p>In the current experiment we studied how the availability and timing of a graphical representation of the hand affected reaching movements to acquire an object in a desktop virtual environment. For natural (Rosetti et al., 1994) and virtual (<span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i29" href="#">Mason, 2007</a></span>) environments, it has been shown that visual feedback available prior to the initiation of a movement can serve to “calibrate” proprioceptive knowledge of one's position. Specifically, seeing one's hand before the movement begins allows for the sensory feedback from the visual system to be compared and contrasted with sensory information from the proprioceptive system resulting in decreased aiming variability and improved movement times. In the current experiment, we were interested in extending this work by comparing the condition where participants had visual feedback prior to movement initiation with the condition where participants had vision up to peak velocity to determine whether the additional visual information would result in superior performance. This comparison is of particular interest because studies in natural environments have led researchers to conclude that the transport and aperture characteristics of a reaching movement up to the point of peak reaching velocity are preplanned and open-loop (<span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-refLink="_i29" href="#">Khan &amp; Franks, 2000</a></span>). Furthermore, it is known that the visual information available as participants decelerate toward a target is used in a closed-loop fashion to guide the hand to the target (<span class="ref-lnk lazy-ref"><a data-rid="CIT0009" data-refLink="_i29" href="#">Churchill, Hopkins, Ronnqvist, &amp; Vogt, 2000</a></span>). Thus, it was unclear to what extent the additional visual information received in the acceleration phase (i.e. UPV condition) would improve performance over visual feedback available prior to movement onset, particularly as distance to the target was manipulated. Finally we were interested in comparing the two partial feedback conditions with the NV and FV conditions to determine for each of the movement characteristics whether partial vision was most similar to NV or FV.</p> <p>Contrary to our hypothesis, our results indicated that the role of visual feedback was not influenced by target distance. One possible explanation for the lack of a Vision × Distance interaction is that the farther target distance was simply not far enough for the additional visual feedback available in the UPV condition to play a role in movement performance. However, it is important to note that we placed the far target at the very edge of the available workspace in the virtual environment and this workspace is representative of a comfortable working range for seated performance. Thus, we can conclude that visual feedback about limb movement is used similarly regardless of target distance in a desktop virtual environment. It is important to note however that in the current experiment, objects were always presented along the midline. Follow-up studies should be conducted where target angle is manipulated. Results would allow us to determine whether vision and target location interact as participants are required to perform on the right and left sides of the body.</p> <p>As expected, movement time, percent time from peak velocity, and peak aperture performance measures were superior when vision of the hand was available throughout the movement. Previous work by <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-refLink="_i29" href="#">Gentilucci et al. (1994</a></span>) and <span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-refLink="_i29" href="#">Carlton (1992</a></span>) has shown that movements in natural environments are quicker and more accurate when people have continuous vision of their moving limb. Our results provide evidence that this concept also applies to a virtual environment and replicates (<span class="ref-lnk lazy-ref"><a data-rid="CIT0016" data-refLink="_i29" href="#">Graham &amp; MacKenzie, 1995</a></span>, 1996; <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i29" href="#">Mason, 2007</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i29" href="#">Mason &amp; Bernardin, 2007</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-refLink="_i29" href="#">Mason et al., 2001</a></span>). However, extending the results found in the previous studies, in the current experiment we have shown that interactivity with objects in a virtual environment can be accomplished with almost the same ease when vision of the moving limb is available only prior to movement initiation (for movement time) and up to peak wrist velocity (for both transport measures and aperture formation). We compare these two partial vision conditions in the following paragraphs.</p> <div id="S2010" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i24">4.1. Using Vision to Calibrate Proprioception</h3> <p>The results of the current experiment indicated that for movement time and percent time from peak aperture, having visual feedback that was extinguished at movement onset was similar to having vision up to peak velocity or throughout the entire movement. In particular, these results indicate that visual feedback available only prior to movement initiation can be exploited to create a movement plan that allows the participant to effectively complete the task in a timely manner. This movement plan appears to contain a temporal representation for overall movement duration as well as hand closure time and allows for superior performance when compared to the NV condition. Furthermore, because percent time from peak aperture, which occurs during the deceleration phase of the movement, was similar in the two partial feedback conditions, we also have evidence that visual feedback available prior to movement initiation can be used to calibrate the proprioceptive system. This calibrated proprioceptive information can be used to effectively guide the hand to the target when visual feedback is no longer available.</p> </div> <div id="S2011" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i25">4.2. Using Vision for Online Corrections</h3> <p>In contrast to the similar results seen for MT and %TFPA in the UPV and VEX conditions, differences were found for percent time from peak velocity (%TFPV) between the two partial vision conditions. In particular, having additional feedback in the UPV condition allowed participants to improve performance compared to the NV condition. In contrast, %TFPV was similar in both the NV and VEX conditions. These results indicate that the additional early visual information available in the UPV condition was used to alter the motor plan for the hand transport movement. Specifically, participants were able to make use of the more recent position data about hand location available in the UPV condition to update the movement plan they had generated before movement initiation. This updated plan facilitated the task of homing in on the target when vision was subsequently removed after peak velocity.</p> <p>Differences were also found between UPV and VEX for peak aperture. Recall that peak aperture has typically been used as a measure of task difficulty or complexity, with larger apertures being associated with greater task difficulty when the target objects are the same size. Although, as shown in <a href="#F0006">Figure 6</a>, peak aperture was smaller during the VEX condition when compared to the NV condition, apertures were still larger in the VEX condition when compared to the UPV or FV conditions. These results indicate that although the visual information available about hand position before movement onset in the VEX condition allowed for some improvement in aperture formation, compensations for the lack of feedback after movement onset were made to ensure an accurate grasp. The additional visual feedback in the UPV condition was useful in allowing aperture scaling to be more appropriate for the target size and in fact resulted in performance that was as good as the FV condition. Thus, movement complexity was significantly reduced when visual feedback was made available up to peak velocity.</p> </div> <div id="S2012" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i26">4.3. Reducing the Length of Time that Visual Feedback is Available</h3> <p>One goal of the current experiment was to determine how much we could reduce visual feedback while still eliciting performance that was as good as the FV condition. Of particular interest is that although we saw decrements in performance when vision was extinguished at movement onset, the UPV condition was similar to the FV condition for all movement characteristics measured in the current experiment. Thus, removing vision for the deceleration phase of the task did not have a deleterious effect on performance. To better understand why the removal of visual feedback after peak velocity did not have a negative impact on performance we considered the proportion of the total movement time and movement distance spent accelerating versus decelerating. Given that the average movement time for the UPV condition was approximately 1,050 msec and the average time to peak velocity was 370 msec, participants had vision of the moving limb for approximately 35% of the overall movement time. Furthermore, inspection of the data revealed that participants were approximately 40% of the distance toward the target when vision of the hand was removed. These numbers indicate that during the initial third of the total movement where participants had access to visual feedback about the limb, they were capable of effectively using that information to update their initial motor plan with current position data. This updated motor plan could then be used to fine-tune wrist deceleration and peak aperture such that it equaled the performance measures seen in the full vision condition. Since the time of <span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-refLink="_i29" href="#">Woodworth (1899</a></span>), studies in natural environments have indicated that the time after peak velocity (deceleration phase) is when vision becomes critical for fine-tuning the movement and homing-in on the target (Carlton, 1994; <span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-refLink="_i29" href="#">Woodworth, 1899</a></span>). However, the results presented here indicate that in our virtual environment, when visual feedback was only available early in the task participants used that information to produce a movement that was equivalent to when vision was available during the deceleration phase. In other words, it appears that the role of visual feedback may be different in natural compared to our virtual environment. For natural environments, visual information available primarily during the deceleration phase is thought to be used for feedback-based online control and error correction (see Carlton, 1994, for a review). However, for the virtual environments used here, it appears that visual information (provided via a graphic representation of the hand) is instead used in a feedforward manner for error correction. When vision is available for the first third of a reach-to-grasp movement, we use the most current visual feedback received to perform error correction even once vision is extinguished. In addition, our results indicate that unlike natural environments, when visual feedback is available throughout the movement in a virtual environment, we do not make use of the information available during the deceleration phase to further improve performance. These results suggest, rather, that vision is more important early in the movement, during the acceleration phase, so that we can successfully predict how to make corrective movements during the deceleration phase. Perhaps we do not see an advantage to feedback available after peak velocity in the FV condition because (a) the impoverished graphic representation of the hand does not contain the visual aspects necessary for improved performance (i.e., wrist position, joint angle information), (b) the small motion lag in the visual representation of the hand makes the use of this information difficult in the FV condition, or (c) visual information about the surrounding environment is necessary to adequately use information available after peak velocity. These hypotheses merit further investigation.</p> <p>Thus, in combination, our results suggest that when grasping objects in a virtual environment, vision available before movement initiation can be used to calibrate the proprioceptive system and specify hand closure on the object. This limited visual information is also sufficient to plan a reach-to-grasp movement with movement times that are similar to when full vision is available. Finally, visual feedback available before movement onset can also be used in a limited way to fine-tune aperture characteristics such that they are slightly more representative of object size when compared to a no vision condition. When additional visual information is provided, even if it only spans the first third of total movement time, this information can be used to fine-tune aperture and hand transport characteristics so that movement performance is identical to when a crude representation of the hand is provided throughout the movement.</p> </div> <div id="S2013" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i27">4.4. Implications for Human–Computer Interaction</h3> <p>Feedback is an essential component of any learning or performance environment, and providing too much or too little feedback can have significant impacts on performance. Thus, determining how and when to provide that feedback is the key to creating a successful virtual environment.</p> <p>The results of the current study can provide some guidelines for designers of virtual environments to follow when planning for the representation of the user in a virtual environment. First, even though the graphic feedback provided in the current experiment was crude, performance advantages were found in each of the kinematic measures over a no feedback condition. Thus, our results indicate that in order for humans to work effectively in virtual environments, some form of graphic feedback about one's own movements should be provided in these systems.</p> <p>Second, providing feedback of the moving limb up to peak velocity is sufficient to elicit performance that is similar to the full vision condition. Therefore, providing vision of the hand only up to peak velocity allows a user to plan his movements and execute those movements without much difficulty. Thus, when it is impossible or undesirable to provide feedback throughout an entire movement, a simple representation up to peak velocity will suffice. Specifically, it may be desirable to manipulate/limit when graphic feedback is presented to the user when the following occurs:</p> <ol class="NLM_list NLM_list-list_type-order"> <li> <p class="inline">Occlusion of important information by the hand is a significant problem (see <span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-refLink="_i29" href="#">Buchman et al., 2005</a></span>).</p> </li> <li> <p class="inline">Detailed rendering of other aspects of the environmental layout is more important.</p> </li> <li> <p class="inline">Continuously tracking the hand is difficult because of LED occlusion or Lag.</p> </li> </ol> <p>Finally, a simple representation of the hand at movement initiation is preferable to no representation. When we allowed users vision of the hand until movement onset, aperture and movement time performance showed a significant improvement over the no vision condition. Thus, even with this limited availability of feedback, users can plan directional movement and object manipulation such that the task goal is efficiently achieved. This type of visual feedback presentation may be useful when some interactivity is desired; however, overall movement duration is more important than precision manipulation (i.e., architectural walk-throughs).</p> </div> <div id="S2014" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i28">4.5. Limitations and Future Work</h3> <p>We acknowledge that these results mainly apply to a fish-tank virtual environment with tangible interfaces (e.g., our virtual images were superimposed over physical objects). Manipulation of virtual objects without physical counterparts could possibly render different results given the same visual conditions. As shown by <span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-refLink="_i29" href="#">Mason et al. (2001</a></span>), movements are typically slower when grasping a virtual object without haptic feedback and this increase in movement time may alter a participant's reliance on visual feedback. Hence, having visual feedback for the entire movement might significantly improve performance over partial feedback conditions in a completely virtual environment. Follow-up work in our lab will investigate differences in the role of visual feedback timing for grasping tangible compared to virtual objects. However, with increases in both research and implementation of synthetic haptic feedback devices in virtual environments, it is important to consider that our results will have wider applicability.</p> <p>A second issue that merits further consideration in our future experiments relates to the number of target positions used in the current experiment. Consistent with many studies investigating the role of vision for movement control we used a small number of target locations in the current study to maintain a reasonable number of trials for our repeated measures design (e.g., see <span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i29" href="#">Chua &amp; Elliott, 1993</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0007" data-refLink="_i29" href="#">Carnahan, Goodale, &amp; Marteniuk, 1993</a></span>; Elliott, Chua, Pollock, &amp; Lyons, 1995; Goodale, Pelisson, &amp; Prablanc, 1986; <span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-refLink="_i29" href="#">Helsen, Elliott, Starkes, &amp; Ricker, 1998</a></span>). Although it is possible that the limited number of target distances used in the current experiment and the fact that these distances were presented in a blocked order allowed participants to learn the position of the targets thus ignoring available visual feedback, we feel that our results speak to the contrary. If participants were simply using the same plan over and over again without making use of the visual feedback available during each trial, we would expect no differences between the conditions. Instead, we saw significant differences when visual information was available only for a short amount of time at the beginning of the movement when compared to no vision. We believe that this result speaks to the fact that this early visual information was being used in the control of the movement. In fact <span class="ref-lnk lazy-ref"><a data-rid="CIT0012" data-refLink="_i29" href="#">Elliott et al. (1995</a></span>), using only one target position and size for 120 trials while varying the amount of visual feedback given to two experimental groups, also found that participants used all sensory information available to them despite the repetitive nature of the task. However, to ensure the generalizability of our results, follow-up work in our lab will investigate a larger range of target positions and sizes with randomized presentation order.</p> <p>The purpose of the current study was to investigate the role of visual feedback in simple reach to grasp movements in a fish-tank virtual environment. However, extensions of this paradigm to other tasks and environmental contexts suggest future work that will provide further insight into the role of visual feedback for performance in a variety of virtual environments. The following examples describe three tasks/environmental contexts for which we feel our results may have interesting applications.</p> <ol class="NLM_list NLM_list-list_type-order"> <li> <p class="inline">The current study looked at unimanual reach to grasp movements; however, a task such as object transfer from hand to hand could provide important information about the role of vision during object manipulation after contact. Although no studies have directly investigated this type of task, <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i29" href="#">Mason and MacKenzie (2004</a></span>) showed that removing visual feedback about self-movement in a between-person object passing task dramatically affected both the receiver's movement to grasp the object and the time to transfer the object between partners. Although these authors investigated transferring objects between people and used a simple vision/no vision paradigm these results suggest that vision of self may be important during object transfer between hands. Follow-up experiments in our lab will test whether the advantage of having vision available for only the first third of the movement extends beyond simple reach to grasp tasks to other tasks such as object transfer between hands and between people.</p> </li> <li> <p class="inline">A second topic for future research regards the interaction between visual feedback and the correspondence ratio between physical and virtual spaces. Specifically, in the current experiment there was a 1:1 correspondence ratio between physical movements made by the participant and the resulting movements in the virtual space. This intuitive type of correspondence is typical of desktop virtual environment applications such as flight/driving simulators, data visualization, and therapeutic/rehabilitation tools and thus the direct applications of our results are numerous. However, other applications such as minimally invasive surgery and interaction in large virtual worlds have humans working in an environment where there is not a 1:1 correspondence ratio between the physical and virtual spaces. Using a correspondence ratio that is less than 1 (i.e., small physical motions result in large displacements of the virtual representation) allows the user to cover the large workspace of a Cave Automatic Virtual Environment (CAVE) without requiring a large physical interaction space. In a recent review, <span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-refLink="_i29" href="#">Healey (2008</a></span>) suggested that adaptive sensorimotor skills must be obtained via practice in order to work in a virtual reality environment where there is a difference in mapping between perception and action (as in the case of a non-1:1 correspondence ratio). Furthermore, <span class="ref-lnk lazy-ref"><a data-rid="CIT0032" data-refLink="_i29" href="#">Pennel, Ferrel, Coello, and Orliaguet (2002)</a></span> suggested that it takes approximately 7 to 12 trials to adjust and compensate to disparities in perception and action. However, these conclusions rely on the fact that visual feedback regarding actions is available throughout the task. If, however, we were to further manipulate the availability or timing of visual feedback (as with our UPV and VEX conditions) in an environment where the correspondence between physical and virtual movements is not 1:1 it is unclear how participants would adapt to the altered correspondence ratio. Specifically, participants may take longer to adapt to the novel perception/action mapping with less visual feedback. Furthermore, the benefits of the UPV and VEX feedback conditions may diminish or disappear entirely in this type of environment. In contrast, it is possible that participants may alter their reliance on visual feedback and make greater use of calibrated proprioceptive feedback allowing for efficient and effective sensorimotor adaptation. These hypotheses merit further consideration.</p> </li> <li> <p class="inline">Finally, the lag between hand movements and the visual feedback regarding those hand movements was minimal (10 msec) in the current experiment and should not have negatively impacted performance in the FV, UPV, and VEX conditions. In fact, <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-refLink="_i29" href="#">MacKenzie and Ware (1993</a></span>) have shown that the threshold at which visual lag starts to affect performance is approximately 75 msec and that movement times and error rates increase linearly with lag and task difficulty. However, the results of the current experiment suggest an interesting application for the UPV (and perhaps the VEX) conditions. In particular, providing participants with minimal visual feedback in situations where this feedback is misleading (i.e., when lag is greater than 75 msec) may allow participants to improve performance when compared to a full vision condition. Specifically, by forcing participants to rely on proprioceptive feedback instead of misleading visual feedback, performance may be improved. Follow-up experiments in our lab are aimed at investigating the interaction between visual feedback timing and lag to determine whether there are performance advantages when visual feedback is limited to the first third of the movement and, if advantages are present, how long those advantages persist.</p> </li> </ol> </div> </div><div class="ack"> <p>This work is supported by the National Science Foundation under Grant No. IIS – 0346871. We also thank Jared Markiewitz, Scott Mason and Nicholas Penwarden for software, data collection, and data analysis support.</p> </div></div><script type="text/javascript">
                        window.figureViewer={doi:'10.1080/10447310903025529',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production',figures:[{i:'F0001',g:[{m:'hihc_a_402725_o_f0001g.gif',l:'hihc_a_402725_o_f0001g.jpeg',size:'112 KB'}]}
                            ,{i:'F0002',g:[{m:'hihc_a_402725_o_f0002g.gif',l:'hihc_a_402725_o_f0002g.jpeg',size:'46 KB'}]}
                            ,{i:'F0003',g:[{m:'hihc_a_402725_o_f0003g.gif',l:'hihc_a_402725_o_f0003g.jpeg',size:'58 KB'}]}
                            ,{i:'F0004',g:[{m:'hihc_a_402725_o_f0004g.gif',l:'hihc_a_402725_o_f0004g.jpeg',size:'41 KB'}]}
                            ,{i:'F0005',g:[{m:'hihc_a_402725_o_f0005g.gif',l:'hihc_a_402725_o_f0005g.jpeg',size:'49 KB'}]}
                            ,{i:'F0006',g:[{m:'hihc_a_402725_o_f0006g.gif',l:'hihc_a_402725_o_f0006g.jpeg',size:'37 KB'}]}
                            ]}</script><script type="text/javascript">window.tableViewer={doi:'10.1080/10447310903025529',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2009/hihc20.v025.i08/10447310903025529/production',tables:[{i:'T0001'}]}</script><script type="text/javascript">window.tableIDIndexMap = {"id":-1};window.tableIDIndexMap['T0001'] = 1; </script><div id="table-content-T0001" class="hidden"><table class="table frame_topbot"><div class="caption"> <b>Table 1: Summary of Experimental Trials</b> </div><colgroup><col /><col /><col /><col /><col /><col /></colgroup><thead valign="bottom"><tr valign="top"><th valign="bottom"> </th><th colspan="4" align="center" valign="bottom" class="rowsep1 align_center">Experimental Conditions</th><th valign="bottom" class="last"> </th></tr><tr valign="top" class="rowsep1"><th align="left" valign="bottom" class="rowsep1 align_left">Target Distance</th><th align="center" valign="bottom" class="rowsep1 align_center">NV</th><th align="center" valign="bottom" class="rowsep1 align_center">VEX</th><th align="center" valign="bottom" class="rowsep1 align_center">UPV</th><th align="center" valign="bottom" class="rowsep1 align_center">FV</th><th align="center" valign="bottom" class="rowsep1 align_center last">Total Trials</th></tr></thead><tbody><tr valign="top"><td align="left" class=" align_left">10 cm</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char last">40</td></tr><tr valign="top"><td align="left" class=" align_left">20 cm</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char">10</td><td align="char" char="." class=" align_char last">40</td></tr><tr valign="top"><td align="left" class=" align_left">Total trials</td><td align="char" char="." class=" align_char">20</td><td align="char" char="." class=" align_char">20</td><td align="char" char="." class=" align_char">20</td><td align="char" char="." class=" align_char">20</td><td align="char" char="." class=" align_char last">80</td></tr><tr valign="top" class="last"><td colspan="6" class="last"> <i>Note.</i> Participants performed 80 trials, 10 for each combination of distance and visual condition. NV = no vision; VEX = vision extinguished at movement onset; UPV = vision up to peak velocity; FV = full vision.</td></tr></tbody></table></div><ul class="references numeric-ordered-list" id="references-Section"><h2 id="figures">REFERENCES</h2><li id="CIT0001"><span><span class="hlFld-ContribAuthor">Allison, <span class="NLM_given-names">R. S.</span></span>, <span class="hlFld-ContribAuthor">Harris, <span class="NLM_given-names">L. R.</span></span>, <span class="hlFld-ContribAuthor">Jenkin, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Jasiobedzka, <span class="NLM_given-names">U.</span></span> and <span class="hlFld-ContribAuthor">Zacher, <span class="NLM_given-names">J. E.</span></span> <span class="NLM_year">2001</span>. <span class="NLM_article-title">Tolerance of temporal delay in virtual environments</span>. <i>Proceedings of Virtual Reality</i>, : <span class="NLM_fpage">247</span><span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2001&pages=247&author=R.+S.+Allison&author=L.+R.+Harris&author=M.+Jenkin&author=U.+Jasiobedzka&author=J.+E.+Zacher&title=Tolerance+of+temporal+delay+in+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0001&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DAllison%26aufirst%3DR.%2520S.%26date%3D2001%26atitle%3DTolerance%2520of%2520temporal%2520delay%2520in%2520virtual%2520environments%26jtitle%3DProceedings%2520of%2520Virtual%2520Reality%26spage%3D247" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0002"><span><span class="hlFld-ContribAuthor">Berthier, <span class="NLM_given-names">N. E.</span></span>, <span class="hlFld-ContribAuthor">Clifton, <span class="NLM_given-names">R. K.</span></span>, <span class="hlFld-ContribAuthor">Gullapalli, <span class="NLM_given-names">V.</span></span>, <span class="hlFld-ContribAuthor">McCall, <span class="NLM_given-names">D. D.</span></span> and <span class="hlFld-ContribAuthor">Robin, <span class="NLM_given-names">D. J.</span></span> <span class="NLM_year">1996</span>. <span class="NLM_article-title">Visual information and object size in the control of reaching</span>. <i>Journal of Motor Behavior</i>, 28: <span class="NLM_fpage">187</span>–<span class="NLM_lpage">197</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=20&amp;doi=10.1080%2F10447310903025529&amp;key=10.1080%2F00222895.1996.9941744&amp;tollfreelink=144713_1415795_8af23fe2056ac07649643664f24d7600671aaf08e16cc84fe4027182db4f13b8">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0002&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1996VD56400001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1996&pages=187-197&author=N.+E.+Berthier&author=R.+K.+Clifton&author=V.+Gullapalli&author=D.+D.+McCall&author=D.+J.+Robin&title=Visual+information+and+object+size+in+the+control+of+reaching" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=16384&amp;doi=10.1080%2F00222895.1996.9941744&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F00222895.1996.9941744%26sid%3Dliteratum%253Atandf%26aulast%3DBerthier%26aufirst%3DN.%2520E.%26date%3D1996%26atitle%3DVisual%2520information%2520and%2520object%2520size%2520in%2520the%2520control%2520of%2520reaching%26jtitle%3DJournal%2520of%2520Motor%2520Behavior%26volume%3D28%26spage%3D187%26epage%3D197" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0003"><span><span class="hlFld-ContribAuthor">Biocca, <span class="NLM_given-names">F.</span></span> and <span class="hlFld-ContribAuthor">Rolland, <span class="NLM_given-names">J.</span></span> <span class="NLM_year">1998</span>. <span class="NLM_article-title">Virtual eyes can rearrange your body: adaptation to virtual eye location in see-thru head-mounted displays</span>. <i>Presence-Teleoperators and Virtual Environments</i>, 7: <span class="NLM_fpage">262</span>–<span class="NLM_lpage">277</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1998&pages=262-277&author=F.+Biocca&author=J.+Rolland&title=Virtual+eyes+can+rearrange+your+body%3A+adaptation+to+virtual+eye+location+in+see-thru+head-mounted+displays" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0003&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBiocca%26aufirst%3DF.%26date%3D1998%26atitle%3DVirtual%2520eyes%2520can%2520rearrange%2520your%2520body%253A%2520adaptation%2520to%2520virtual%2520eye%2520location%2520in%2520see-thru%2520head-mounted%2520displays%26jtitle%3DPresence-Teleoperators%2520and%2520Virtual%2520Environments%26volume%3D7%26spage%3D262%26epage%3D277" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0004"><span> Brown University Computer Graphics Group <a class="ext-link" href="http://graphics.cs.brown.edu/reseach/cave/home.html" target="_blank">http://graphics.cs.brown.edu/reseach/cave/home.html</a> (Accessed: <span class="NLM_date-in-citation">August 2007</span>). <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+Brown+University+Computer+Graphics+Group+http%3A%2F%2Fgraphics.cs.brown.edu%2Freseach%2Fcave%2Fhome.html+%28Accessed%3A+August+2007%29.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0004&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Fgraphics.cs.brown.edu%2Freseach%2Fcave%2Fhome.html" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0005"><span><span class="hlFld-ContribAuthor">Buchman, <span class="NLM_given-names">V.</span></span>, <span class="hlFld-ContribAuthor">Nilson, <span class="NLM_given-names">T.</span></span> and <span class="hlFld-ContribAuthor">Billinghurst, <span class="NLM_given-names">M.</span></span> <span class="NLM_year">2005</span>. <span class="NLM_article-title">Interaction with partially transparent hands and objects</span>. <i>Proceedings of the Australasian User Interface Conference</i>, : <span class="NLM_fpage">17</span>–<span class="NLM_lpage">20</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&pages=17-20&author=V.+Buchman&author=T.+Nilson&author=M.+Billinghurst&title=Interaction+with+partially+transparent+hands+and+objects" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0005&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBuchman%26aufirst%3DV.%26date%3D2005%26atitle%3DInteraction%2520with%2520partially%2520transparent%2520hands%2520and%2520objects%26jtitle%3DProceedings%2520of%2520the%2520Australasian%2520User%2520Interface%2520Conference%26spage%3D17%26epage%3D20" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0006"><span><span class="hlFld-ContribAuthor">Carlton, <span class="NLM_given-names">L. G.</span></span> <span class="NLM_year">1992</span>. “<span class="NLM_article-title">Visual processing time and the control of movement</span>”. In <i>Vision and motor control</i>, Edited by: <span class="hlFld-ContribAuthor">Proteau, <span class="NLM_given-names">L.</span></span> and <span class="hlFld-ContribAuthor">Elliott, <span class="NLM_given-names">D.</span></span> <span class="NLM_fpage">3</span>–<span class="NLM_lpage">31</span>. <span class="NLM_publisher-loc">New York</span>: <span class="NLM_publisher-name">North-Holland</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0006&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1016%2FS0166-4115%2808%2962008-7" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1992&pages=3-31&author=L.+G.+Carlton&title=+Vision+and+motor+control+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0006&amp;dbid=16384&amp;doi=10.1016%2FS0166-4115%2808%2962008-7&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252FS0166-4115%252808%252962008-7%26sid%3Dliteratum%253Atandf%26aulast%3DCarlton%26aufirst%3DL.%2520G.%26date%3D1992%26atitle%3DVisual%2520processing%2520time%2520and%2520the%2520control%2520of%2520movement%26btitle%3DVision%2520and%2520motor%2520control%26aulast%3DProteau%26aufirst%3DL.%26spage%3D3%26epage%3D31%26pub%3DNorth-Holland" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0007"><span><span class="hlFld-ContribAuthor">Carnahan, <span class="NLM_given-names">H.</span></span>, <span class="hlFld-ContribAuthor">Goodale, <span class="NLM_given-names">M.</span></span> and <span class="hlFld-ContribAuthor">Marteniuk, <span class="NLM_given-names">R.</span></span> <span class="NLM_year">1993</span>. <span class="NLM_article-title">Grasping versus pointing and the differential use of visual feedback</span>. <i>Human Movement Science</i>, 12: <span class="NLM_fpage">219</span>–<span class="NLM_lpage">234</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0007&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1016%2F0167-9457%2893%2990016-I" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0007&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1993LC82900001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1993&pages=219-234&author=H.+Carnahan&author=M.+Goodale&author=R.+Marteniuk&title=Grasping+versus+pointing+and+the+differential+use+of+visual+feedback" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0007&amp;dbid=16384&amp;doi=10.1016%2F0167-9457%2893%2990016-I&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252F0167-9457%252893%252990016-I%26sid%3Dliteratum%253Atandf%26aulast%3DCarnahan%26aufirst%3DH.%26date%3D1993%26atitle%3DGrasping%2520versus%2520pointing%2520and%2520the%2520differential%2520use%2520of%2520visual%2520feedback%26jtitle%3DHuman%2520Movement%2520Science%26volume%3D12%26spage%3D219%26epage%3D234" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0008"><span><span class="hlFld-ContribAuthor">Chua, <span class="NLM_given-names">R.</span></span> and <span class="hlFld-ContribAuthor">Elliott, <span class="NLM_given-names">D.</span></span> <span class="NLM_year">1993</span>. <span class="NLM_article-title">Visual regulation of manual aiming</span>. <i>Human Movement Science</i>, 12: <span class="NLM_fpage">365</span>–<span class="NLM_lpage">401</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0008&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1016%2F0167-9457%2893%2990026-L" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0008&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1993LP86800002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1993&pages=365-401&author=R.+Chua&author=D.+Elliott&title=Visual+regulation+of+manual+aiming" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0008&amp;dbid=16384&amp;doi=10.1016%2F0167-9457%2893%2990026-L&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252F0167-9457%252893%252990026-L%26sid%3Dliteratum%253Atandf%26aulast%3DChua%26aufirst%3DR.%26date%3D1993%26atitle%3DVisual%2520regulation%2520of%2520manual%2520aiming%26jtitle%3DHuman%2520Movement%2520Science%26volume%3D12%26spage%3D365%26epage%3D401" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0009"><span><span class="hlFld-ContribAuthor">Churchill, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Hopkins, <span class="NLM_given-names">B.</span></span>, <span class="hlFld-ContribAuthor">Ronnqvist, <span class="NLM_given-names">L.</span></span> and <span class="hlFld-ContribAuthor">Vogt, <span class="NLM_given-names">S.</span></span> <span class="NLM_year">2000</span>. <span class="NLM_article-title">Vision of the hand and environmental context in human prehension</span>. <i>Experimental Brain Research</i>, 134: <span class="NLM_fpage">81</span>–<span class="NLM_lpage">89</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0009&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1007%2Fs002210000444" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0009&amp;dbid=8&amp;doi=10.1080%2F10447310903025529&amp;key=11026729" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0009&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=000089092900010" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=81-89&author=A.+Churchill&author=B.+Hopkins&author=L.+Ronnqvist&author=S.+Vogt&title=Vision+of+the+hand+and+environmental+context+in+human+prehension" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0009&amp;dbid=16384&amp;doi=10.1007%2Fs002210000444&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1007%252Fs002210000444%26sid%3Dliteratum%253Atandf%26aulast%3DChurchill%26aufirst%3DA.%26date%3D2000%26atitle%3DVision%2520of%2520the%2520hand%2520and%2520environmental%2520context%2520in%2520human%2520prehension%26jtitle%3DExperimental%2520Brain%2520Research%26volume%3D134%26spage%3D81%26epage%3D89" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0010"><span><span class="hlFld-ContribAuthor">DiZio, <span class="NLM_given-names">P.</span></span> and <span class="hlFld-ContribAuthor">Lackner, <span class="NLM_given-names">J. R.</span></span> <span class="NLM_year">1997</span>. “<span class="NLM_article-title">Circumventing side effects of immersive virtual environments</span>”. In <i>Advances in human factors/ergonomics. Vol. 21. Design of computing systems</i>, Edited by: <span class="hlFld-ContribAuthor">Smith, <span class="NLM_given-names">M. J.</span></span>, <span class="hlFld-ContribAuthor">Salvendy, <span class="NLM_given-names">G.</span></span> and <span class="hlFld-ContribAuthor">Koubek, <span class="NLM_given-names">R. J.</span></span> <span class="NLM_fpage">893</span>–<span class="NLM_lpage">897</span>. <span class="NLM_publisher-loc">Amsterdam</span>: <span class="NLM_publisher-name">Elsevier</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1997&pages=893-897&author=P.+DiZio&author=J.+R.+Lackner&title=+Advances+in+human+factors%2Fergonomics.+Vol.+21.+Design+of+computing+systems+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0010&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DDiZio%26aufirst%3DP.%26date%3D1997%26atitle%3DCircumventing%2520side%2520effects%2520of%2520immersive%2520virtual%2520environments%26btitle%3DAdvances%2520in%2520human%2520factors%252Fergonomics.%2520Vol.%252021.%2520Design%2520of%2520computing%2520systems%26aulast%3DSmith%26aufirst%3DM.%2520J.%26spage%3D893%26epage%3D897%26pub%3DElsevier" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0011"><span><span class="hlFld-ContribAuthor">Elliott, <span class="NLM_given-names">D.</span></span> <span class="NLM_year">1988</span>. <span class="NLM_article-title">The influence of visual target and limb information on manual aiming</span>. <i>Canadian Journal of Psychology</i>, 42: <span class="NLM_fpage">57</span>–<span class="NLM_lpage">68</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0011&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1037%2Fh0084172" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0011&amp;dbid=8&amp;doi=10.1080%2F10447310903025529&amp;key=3167704" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0011&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1988M771700005" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1988&pages=57-68&author=D.+Elliott&title=The+influence+of+visual+target+and+limb+information+on+manual+aiming" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0011&amp;dbid=16384&amp;doi=10.1037%2Fh0084172&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1037%252Fh0084172%26sid%3Dliteratum%253Atandf%26aulast%3DElliott%26aufirst%3DD.%26date%3D1988%26atitle%3DThe%2520influence%2520of%2520visual%2520target%2520and%2520limb%2520information%2520on%2520manual%2520aiming%26jtitle%3DCanadian%2520Journal%2520of%2520Psychology%26volume%3D42%26spage%3D57%26epage%3D68" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0012"><span><span class="hlFld-ContribAuthor">Elliott, <span class="NLM_given-names">D.</span></span>, <span class="hlFld-ContribAuthor">Chua, <span class="NLM_given-names">R.</span></span>, <span class="hlFld-ContribAuthor">Pollock, <span class="NLM_given-names">B.</span></span> and <span class="hlFld-ContribAuthor">Lyons, <span class="NLM_given-names">J.</span></span> <span class="NLM_year">1995</span>. <span class="NLM_article-title">Optimizing the use of vision in manual aiming: The role of practice</span>. <i>Quarterly Journal of Experimental Psychology: Section A</i>, 48: <span class="NLM_fpage">72</span>–<span class="NLM_lpage">83</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0012&amp;dbid=20&amp;doi=10.1080%2F10447310903025529&amp;key=10.1080%2F14640749508401376&amp;tollfreelink=144713_1415795_0087af67e81c3dc0ff657c077e603b9d11bddb9e0fb1bbc7ee9e925b6d9b753b">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0012&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1995QN37700005" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1995&pages=72-83&author=D.+Elliott&author=R.+Chua&author=B.+Pollock&author=J.+Lyons&title=Optimizing+the+use+of+vision+in+manual+aiming%3A+The+role+of+practice" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0012&amp;dbid=16384&amp;doi=10.1080%2F14640749508401376&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F14640749508401376%26sid%3Dliteratum%253Atandf%26aulast%3DElliott%26aufirst%3DD.%26date%3D1995%26atitle%3DOptimizing%2520the%2520use%2520of%2520vision%2520in%2520manual%2520aiming%253A%2520The%2520role%2520of%2520practice%26jtitle%3DQuarterly%2520Journal%2520of%2520Experimental%2520Psychology%253A%2520Section%2520A%26volume%3D48%26spage%3D72%26epage%3D83" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0013"><span><span class="hlFld-ContribAuthor">Fitts, <span class="NLM_given-names">P. M.</span></span> and <span class="hlFld-ContribAuthor">Peterson, <span class="NLM_given-names">J. P.</span></span> <span class="NLM_year">1964</span>. <span class="NLM_article-title">Information capacity of discrete motor response</span>. <i>Journal of Experimental Psychology</i>, 6: <span class="NLM_fpage">103</span>–<span class="NLM_lpage">112</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1964&pages=103-112&author=P.+M.+Fitts&author=J.+P.+Peterson&title=Information+capacity+of+discrete+motor+response" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0013&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DFitts%26aufirst%3DP.%2520M.%26date%3D1964%26atitle%3DInformation%2520capacity%2520of%2520discrete%2520motor%2520response%26jtitle%3DJournal%2520of%2520Experimental%2520Psychology%26volume%3D6%26spage%3D103%26epage%3D112" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0014"><span><span class="hlFld-ContribAuthor">Gentilucci, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Toni, <span class="NLM_given-names">I.</span></span>, <span class="hlFld-ContribAuthor">Chieffi, <span class="NLM_given-names">S.</span></span> and <span class="hlFld-ContribAuthor">Pavesi, <span class="NLM_given-names">G.</span></span> <span class="NLM_year">1994</span>. <span class="NLM_article-title">The role of proprioception in the control of prehension movements: a kinematic study in a peripherally deafferented patient and in normal subjects</span>. <i>Experimental Brain Research</i>, 83: <span class="NLM_fpage">447</span>–<span class="NLM_lpage">482</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1994&pages=447-482&author=M.+Gentilucci&author=I.+Toni&author=S.+Chieffi&author=G.+Pavesi&title=The+role+of+proprioception+in+the+control+of+prehension+movements%3A+a+kinematic+study+in+a+peripherally+deafferented+patient+and+in+normal+subjects" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0014&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DGentilucci%26aufirst%3DM.%26date%3D1994%26atitle%3DThe%2520role%2520of%2520proprioception%2520in%2520the%2520control%2520of%2520prehension%2520movements%253A%2520a%2520kinematic%2520study%2520in%2520a%2520peripherally%2520deafferented%2520patient%2520and%2520in%2520normal%2520subjects%26jtitle%3DExperimental%2520Brain%2520Research%26volume%3D83%26spage%3D447%26epage%3D482" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0015"><span><span class="hlFld-ContribAuthor">Goodale, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Pelisson, <span class="NLM_given-names">D.</span></span> and <span class="hlFld-ContribAuthor">Prablanc, <span class="NLM_given-names">C.</span></span> <span class="NLM_year">7986</span>. <span class="NLM_article-title">Large adjustments in visually guided reaching do not depend on vision of the hand or perception of target displacement</span>. <i>Nature</i>, 320: <span class="NLM_fpage">748</span>–<span class="NLM_lpage">750</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=7986&pages=748-750&author=M.+Goodale&author=D.+Pelisson&author=C.+Prablanc&title=Large+adjustments+in+visually+guided+reaching+do+not+depend+on+vision+of+the+hand+or+perception+of+target+displacement" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0015&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DGoodale%26aufirst%3DM.%26date%3D7986%26atitle%3DLarge%2520adjustments%2520in%2520visually%2520guided%2520reaching%2520do%2520not%2520depend%2520on%2520vision%2520of%2520the%2520hand%2520or%2520perception%2520of%2520target%2520displacement%26jtitle%3DNature%26volume%3D320%26spage%3D748%26epage%3D750" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0016"><span><span class="hlFld-ContribAuthor">Graham, <span class="NLM_given-names">E. D.</span></span> and <span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> <span class="NLM_year">1995</span>. <span class="NLM_article-title">Pointing on a computer display</span>. <i>Proceedings of ACM Computer-Human Interaction Conference</i>, : <span class="NLM_fpage">314</span>–<span class="NLM_lpage">315</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1995&pages=314-315&author=E.+D.+Graham&author=C.+L.+MacKenzie&title=Pointing+on+a+computer+display" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0016&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DGraham%26aufirst%3DE.%2520D.%26date%3D1995%26atitle%3DPointing%2520on%2520a%2520computer%2520display%26jtitle%3DProceedings%2520of%2520ACM%2520Computer-Human%2520Interaction%2520Conference%26spage%3D314%26epage%3D315" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0017"><span><span class="hlFld-ContribAuthor">Graham, <span class="NLM_given-names">E. D.</span></span> and <span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> <span class="NLM_year">1996</span>. <span class="NLM_article-title">Physical versus virtual pointing</span>. <i>Proceedings of ACM Computer-Human Interaction Conference</i>, : <span class="NLM_fpage">292</span>–<span class="NLM_lpage">299</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1996&pages=292-299&author=E.+D.+Graham&author=C.+L.+MacKenzie&title=Physical+versus+virtual+pointing" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0017&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DGraham%26aufirst%3DE.%2520D.%26date%3D1996%26atitle%3DPhysical%2520versus%2520virtual%2520pointing%26jtitle%3DProceedings%2520of%2520ACM%2520Computer-Human%2520Interaction%2520Conference%26spage%3D292%26epage%3D299" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0018"><span><span class="hlFld-ContribAuthor">Healey, <span class="NLM_given-names">A.</span></span> <span class="NLM_year">2008</span>. <span class="NLM_article-title">Speculation on the neuropsychology of teleoperation: Implications for presence research and minimally invasive surgery</span>. <i>Presence</i>, 17(2): <span class="NLM_fpage">199</span>–<span class="NLM_lpage">211</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=199-211&issue=2&author=A.+Healey&title=Speculation+on+the+neuropsychology+of+teleoperation%3A+Implications+for+presence+research+and+minimally+invasive+surgery" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0018&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DHealey%26aufirst%3DA.%26date%3D2008%26atitle%3DSpeculation%2520on%2520the%2520neuropsychology%2520of%2520teleoperation%253A%2520Implications%2520for%2520presence%2520research%2520and%2520minimally%2520invasive%2520surgery%26jtitle%3DPresence%26volume%3D17%26issue%3D2%26spage%3D199%26epage%3D211" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0019"><span><span class="hlFld-ContribAuthor">Helsen, <span class="NLM_given-names">W.</span></span>, <span class="hlFld-ContribAuthor">Elliott, <span class="NLM_given-names">D.</span></span>, <span class="hlFld-ContribAuthor">Starkes, <span class="NLM_given-names">J.</span></span> and <span class="hlFld-ContribAuthor">Ricker, <span class="NLM_given-names">K.</span></span> <span class="NLM_year">1998</span>. <span class="NLM_article-title">Temporal and spatial coupling of point of gaze and hand movements in aiming</span>. <i>Journal of Motor Behavior</i>, 30: <span class="NLM_fpage">249</span>–<span class="NLM_lpage">259</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0019&amp;dbid=20&amp;doi=10.1080%2F10447310903025529&amp;key=10.1080%2F00222899809601340&amp;tollfreelink=144713_1415795_593275d42860c8eec71c1c79e136013a2aa34e988414b72fee721452a6fb6536">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0019&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=000076168900005" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1998&pages=249-259&author=W.+Helsen&author=D.+Elliott&author=J.+Starkes&author=K.+Ricker&title=Temporal+and+spatial+coupling+of+point+of+gaze+and+hand+movements+in+aiming" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0019&amp;dbid=16384&amp;doi=10.1080%2F00222899809601340&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F00222899809601340%26sid%3Dliteratum%253Atandf%26aulast%3DHelsen%26aufirst%3DW.%26date%3D1998%26atitle%3DTemporal%2520and%2520spatial%2520coupling%2520of%2520point%2520of%2520gaze%2520and%2520hand%2520movements%2520in%2520aiming%26jtitle%3DJournal%2520of%2520Motor%2520Behavior%26volume%3D30%26spage%3D249%26epage%3D259" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0020"><span><span class="hlFld-ContribAuthor">Jakobson, <span class="NLM_given-names">L. S.</span></span> and <span class="hlFld-ContribAuthor">Goodale, <span class="NLM_given-names">M. A.</span></span> <span class="NLM_year">1991</span>. <span class="NLM_article-title">Factors affecting higher-order movement planning: A kinematic analysis of human prehension</span>. <i>Experimental Brain Research</i>, 86: <span class="NLM_fpage">199</span>–<span class="NLM_lpage">208</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1007%2FBF00231054" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0020&amp;dbid=8&amp;doi=10.1080%2F10447310903025529&amp;key=1756790" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0020&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1991GC83200021" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1991&pages=199-208&author=L.+S.+Jakobson&author=M.+A.+Goodale&title=Factors+affecting+higher-order+movement+planning%3A+A+kinematic+analysis+of+human+prehension" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16384&amp;doi=10.1007%2FBF00231054&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1007%252FBF00231054%26sid%3Dliteratum%253Atandf%26aulast%3DJakobson%26aufirst%3DL.%2520S.%26date%3D1991%26atitle%3DFactors%2520affecting%2520higher-order%2520movement%2520planning%253A%2520A%2520kinematic%2520analysis%2520of%2520human%2520prehension%26jtitle%3DExperimental%2520Brain%2520Research%26volume%3D86%26spage%3D199%26epage%3D208" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0021"><span><span class="hlFld-ContribAuthor">Jung, <span class="NLM_given-names">J. Y.</span></span>, <span class="hlFld-ContribAuthor">Adelstein, <span class="NLM_given-names">B. D.</span></span> and <span class="hlFld-ContribAuthor">Ellis, <span class="NLM_given-names">S. R.</span></span> <span class="NLM_year">2000</span>. <span class="NLM_article-title">Predictive compensator optimization for head tracking lag in virtual environments</span>. <i>Proceedings of the International Ergonomics Association/Human Factors and Engineering Society</i>, : <span class="NLM_fpage">499</span>–<span class="NLM_lpage">502</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=499-502&author=J.+Y.+Jung&author=B.+D.+Adelstein&author=S.+R.+Ellis&title=Predictive+compensator+optimization+for+head+tracking+lag+in+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0021&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DJung%26aufirst%3DJ.%2520Y.%26date%3D2000%26atitle%3DPredictive%2520compensator%2520optimization%2520for%2520head%2520tracking%2520lag%2520in%2520virtual%2520environments%26jtitle%3DProceedings%2520of%2520the%2520International%2520Ergonomics%2520Association%252FHuman%2520Factors%2520and%2520Engineering%2520Society%26spage%3D499%26epage%3D502" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0022"><span><span class="hlFld-ContribAuthor">Khan, <span class="NLM_given-names">M. A.</span></span> and <span class="hlFld-ContribAuthor">Franks, <span class="NLM_given-names">I. M.</span></span> <span class="NLM_year">2000</span>. <span class="NLM_article-title">The effect of practice on component submovements is dependent on the availability of visual feedback</span>. <i>Journal of Motor Behavior</i>, 32: <span class="NLM_fpage">227</span>–<span class="NLM_lpage">240</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=20&amp;doi=10.1080%2F10447310903025529&amp;key=10.1080%2F00222890009601374&amp;tollfreelink=144713_1415795_5793dae0da457ba3e89d8f096ecd590cbf3a4c56c96bd4cdc923d59c15aac7d5">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0022&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=000088951300002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=227-240&author=M.+A.+Khan&author=I.+M.+Franks&title=The+effect+of+practice+on+component+submovements+is+dependent+on+the+availability+of+visual+feedback" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=16384&amp;doi=10.1080%2F00222890009601374&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F00222890009601374%26sid%3Dliteratum%253Atandf%26aulast%3DKhan%26aufirst%3DM.%2520A.%26date%3D2000%26atitle%3DThe%2520effect%2520of%2520practice%2520on%2520component%2520submovements%2520is%2520dependent%2520on%2520the%2520availability%2520of%2520visual%2520feedback%26jtitle%3DJournal%2520of%2520Motor%2520Behavior%26volume%3D32%26spage%3D227%26epage%3D240" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0023"><span><span class="hlFld-ContribAuthor">Klatzky, <span class="NLM_given-names">R. L.</span></span> and <span class="hlFld-ContribAuthor">Lederman, <span class="NLM_given-names">S. J.</span></span> <span class="NLM_year">2002</span>. “<span class="NLM_article-title">Experimental psychology</span>”. In <i>Handbook of psychology</i>, Edited by: <span class="hlFld-ContribAuthor">Weiner, <span class="NLM_given-names">I. B.</span></span> <span class="NLM_fpage">147</span>–<span class="NLM_lpage">176</span>. <span class="NLM_publisher-loc">New York</span>: <span class="NLM_publisher-name">Wiley</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=147-176&author=R.+L.+Klatzky&author=S.+J.+Lederman&title=+Handbook+of+psychology+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0023&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DKlatzky%26aufirst%3DR.%2520L.%26date%3D2002%26atitle%3DExperimental%2520psychology%26btitle%3DHandbook%2520of%2520psychology%26aulast%3DWeiner%26aufirst%3DI.%2520B.%26spage%3D147%26epage%3D176%26pub%3DWiley" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0024"><span><span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> and <span class="hlFld-ContribAuthor">Iberall, <span class="NLM_given-names">T.</span></span> <span class="NLM_year">1994</span>. <i>The grasping hand</i>, <span class="NLM_publisher-loc">New York</span>: <span class="NLM_publisher-name">North-Holland</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1994&author=C.+L.+MacKenzie&author=T.+Iberall&title=+The+grasping+hand+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0024&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMacKenzie%26aufirst%3DC.%2520L.%26date%3D1994%26btitle%3DThe%2520grasping%2520hand%26pub%3DNorth-Holland" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0025"><span><span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">S.</span></span> and <span class="hlFld-ContribAuthor">Ware, <span class="NLM_given-names">C.</span></span> <span class="NLM_year">1993</span>. <span class="NLM_article-title">Lag as a determinant of human performance in interactive systems</span>. <i>Proceedings of INTERCHI</i>, : <span class="NLM_fpage">488</span>–<span class="NLM_lpage">493</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1993&pages=488-493&author=S.+MacKenzie&author=C.+Ware&title=Lag+as+a+determinant+of+human+performance+in+interactive+systems" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0025&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMacKenzie%26aufirst%3DS.%26date%3D1993%26atitle%3DLag%2520as%2520a%2520determinant%2520of%2520human%2520performance%2520in%2520interactive%2520systems%26jtitle%3DProceedings%2520of%2520INTERCHI%26spage%3D488%26epage%3D493" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0026"><span><span class="hlFld-ContribAuthor">Mason, <span class="NLM_given-names">A. H.</span></span> <span class="NLM_year">2007</span>. <span class="NLM_article-title">An experimental study on the role of graphical information about hand movement when interacting with objects in virtual reality environments</span>. <i>Interacting with Computers</i>, 19: <span class="NLM_fpage">370</span>–<span class="NLM_lpage">381</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0026&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1016%2Fj.intcom.2006.11.002" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=370-381&author=A.+H.+Mason&title=An+experimental+study+on+the+role+of+graphical+information+about+hand+movement+when+interacting+with+objects+in+virtual+reality+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0026&amp;dbid=16384&amp;doi=10.1016%2Fj.intcom.2006.11.002&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.intcom.2006.11.002%26sid%3Dliteratum%253Atandf%26aulast%3DMason%26aufirst%3DA.%2520H.%26date%3D2007%26atitle%3DAn%2520experimental%2520study%2520on%2520the%2520role%2520of%2520graphical%2520information%2520about%2520hand%2520movement%2520when%2520interacting%2520with%2520objects%2520in%2520virtual%2520reality%2520environments%26jtitle%3DInteracting%2520with%2520Computers%26volume%3D19%26spage%3D370%26epage%3D381" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0027"><span><span class="hlFld-ContribAuthor">Mason, <span class="NLM_given-names">A. H.</span></span> and <span class="hlFld-ContribAuthor">Bernardin, <span class="NLM_given-names">B. J.</span></span> <span class="NLM_year">2007</span>. <span class="NLM_article-title">The role of early or late graphical feedback about oneself for interaction in virtual environments</span>. <i>Proceedings of the Laval Virtual Reality International Conference</i>, : <span class="NLM_fpage">121</span>–<span class="NLM_lpage">130</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=121-130&author=A.+H.+Mason&author=B.+J.+Bernardin&title=The+role+of+early+or+late+graphical+feedback+about+oneself+for+interaction+in+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0027&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMason%26aufirst%3DA.%2520H.%26date%3D2007%26atitle%3DThe%2520role%2520of%2520early%2520or%2520late%2520graphical%2520feedback%2520about%2520oneself%2520for%2520interaction%2520in%2520virtual%2520environments%26jtitle%3DProceedings%2520of%2520the%2520Laval%2520Virtual%2520Reality%2520International%2520Conference%26spage%3D121%26epage%3D130" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0028"><span><span class="hlFld-ContribAuthor">Mason, <span class="NLM_given-names">A. H.</span></span> and <span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> <span class="NLM_year">2002</span>. <span class="NLM_article-title">The effects of visual information about self-movement on grip forces when receiving objects in an augmented environment</span>. <i>Proceedings of IEEE Virtual Reality and Haptics Symposium</i>, : <span class="NLM_fpage">105</span>–<span class="NLM_lpage">112</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=105-112&author=A.+H.+Mason&author=C.+L.+MacKenzie&title=The+effects+of+visual+information+about+self-movement+on+grip+forces+when+receiving+objects+in+an+augmented+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0028&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMason%26aufirst%3DA.%2520H.%26date%3D2002%26atitle%3DThe%2520effects%2520of%2520visual%2520information%2520about%2520self-movement%2520on%2520grip%2520forces%2520when%2520receiving%2520objects%2520in%2520an%2520augmented%2520environment%26jtitle%3DProceedings%2520of%2520IEEE%2520Virtual%2520Reality%2520and%2520Haptics%2520Symposium%26spage%3D105%26epage%3D112" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0029"><span><span class="hlFld-ContribAuthor">Mason, <span class="NLM_given-names">A. H.</span></span> and <span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> <span class="NLM_year">2004</span>. <span class="NLM_article-title">The role of graphical feedback about self-movement when receiving objects in an augmented environment</span>. <i>Presence Teleoperators and Virtual Environments</i>, 13: <span class="NLM_fpage">507</span>–<span class="NLM_lpage">519</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0029&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1162%2F1054746042545319" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=507-519&author=A.+H.+Mason&author=C.+L.+MacKenzie&title=The+role+of+graphical+feedback+about+self-movement+when+receiving+objects+in+an+augmented+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0029&amp;dbid=16384&amp;doi=10.1162%2F1054746042545319&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1162%252F1054746042545319%26sid%3Dliteratum%253Atandf%26aulast%3DMason%26aufirst%3DA.%2520H.%26date%3D2004%26atitle%3DThe%2520role%2520of%2520graphical%2520feedback%2520about%2520self-movement%2520when%2520receiving%2520objects%2520in%2520an%2520augmented%2520environment%26jtitle%3DPresence%2520Teleoperators%2520and%2520Virtual%2520Environments%26volume%3D13%26spage%3D507%26epage%3D519" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0030"><span><span class="hlFld-ContribAuthor">Mason, <span class="NLM_given-names">A. H.</span></span>, <span class="hlFld-ContribAuthor">Walji, <span class="NLM_given-names">M. A.</span></span>, <span class="hlFld-ContribAuthor">Lee, <span class="NLM_given-names">E. J.</span></span> and <span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> <span class="NLM_year">2001</span>. <span class="NLM_article-title">Reaching movements to virtual reality and graphic objects in virtual environments</span>. <i>Proceedings of ACM Computer-Human Interaction</i>, : <span class="NLM_fpage">426</span>–<span class="NLM_lpage">433</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2001&pages=426-433&author=A.+H.+Mason&author=M.+A.+Walji&author=E.+J.+Lee&author=C.+L.+MacKenzie&title=Reaching+movements+to+virtual+reality+and+graphic+objects+in+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0030&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMason%26aufirst%3DA.%2520H.%26date%3D2001%26atitle%3DReaching%2520movements%2520to%2520virtual%2520reality%2520and%2520graphic%2520objects%2520in%2520virtual%2520environments%26jtitle%3DProceedings%2520of%2520ACM%2520Computer-Human%2520Interaction%26spage%3D426%26epage%3D433" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0031"><span> Medical Robotics Group. (2002). <i>Publications</i> <a class="ext-link" href="http://robotics.eecs.berkeley.edu/medical/publications.html" target="_blank">http://robotics.eecs.berkeley.edu/medical/publications.html</a> (Accessed: <span class="NLM_date-in-citation">August 2007</span>). <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+Medical+Robotics+Group.+%282002%29.+Publications+http%3A%2F%2Frobotics.eecs.berkeley.edu%2Fmedical%2Fpublications.html+%28Accessed%3A+August+2007%29.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0031&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Frobotics.eecs.berkeley.edu%2Fmedical%2Fpublications.html" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0032"><span><span class="hlFld-ContribAuthor">Pennel, <span class="NLM_given-names">I.</span></span>, <span class="hlFld-ContribAuthor">Ferrel, <span class="NLM_given-names">C.</span></span>, <span class="hlFld-ContribAuthor">Coello, <span class="NLM_given-names">Y.</span></span> and <span class="hlFld-ContribAuthor">Orliaguet, <span class="NLM_given-names">J. P.</span></span> <span class="NLM_year">2002</span>. <span class="NLM_article-title">Sensorimotor control in teleoperation: Theoretical and ergonomic considerations</span>. <i>Le Travail Humain</i>, 65(1): <span class="NLM_fpage">29</span>–<span class="NLM_lpage">58</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0032&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.3917%2Fth.651.0029" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=29-58&issue=1&author=I.+Pennel&author=C.+Ferrel&author=Y.+Coello&author=J.+P.+Orliaguet&title=Sensorimotor+control+in+teleoperation%3A+Theoretical+and+ergonomic+considerations" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0032&amp;dbid=16384&amp;doi=10.3917%2Fth.651.0029&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.3917%252Fth.651.0029%26sid%3Dliteratum%253Atandf%26aulast%3DPennel%26aufirst%3DI.%26date%3D2002%26atitle%3DSensorimotor%2520control%2520in%2520teleoperation%253A%2520Theoretical%2520and%2520ergonomic%2520considerations%26jtitle%3DLe%2520Travail%2520Humain%26volume%3D65%26issue%3D1%26spage%3D29%26epage%3D58" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0033"><span><span class="hlFld-ContribAuthor">Rossetti, <span class="NLM_given-names">Y.</span></span>, <span class="hlFld-ContribAuthor">Stelmach, <span class="NLM_given-names">G. E.</span></span>, <span class="hlFld-ContribAuthor">Desmurget, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Prablanc, <span class="NLM_given-names">C.</span></span> and <span class="hlFld-ContribAuthor">Jeannerod, <span class="NLM_given-names">M.</span></span> <span class="NLM_year">1994</span>. <span class="NLM_article-title">Influence of viewing the static hand prior to movement onset on pointing kinematics and accuracy</span>. <i>Experimental Brain Research</i>, 101: <span class="NLM_fpage">323</span>–<span class="NLM_lpage">330</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1994&pages=323-330&author=Y.+Rossetti&author=G.+E.+Stelmach&author=M.+Desmurget&author=C.+Prablanc&author=M.+Jeannerod&title=Influence+of+viewing+the+static+hand+prior+to+movement+onset+on+pointing+kinematics+and+accuracy" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0033&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DRossetti%26aufirst%3DY.%26date%3D1994%26atitle%3DInfluence%2520of%2520viewing%2520the%2520static%2520hand%2520prior%2520to%2520movement%2520onset%2520on%2520pointing%2520kinematics%2520and%2520accuracy%26jtitle%3DExperimental%2520Brain%2520Research%26volume%3D101%26spage%3D323%26epage%3D330" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0035"><span> University of Michigan 3D Lab. (2007). <i>Virtual reality CAVE</i> <a class="ext-link" href="http://um3d.dc.umich.edu/hardware/CAVE/index.html" target="_blank">http://um3d.dc.umich.edu/hardware/CAVE/index.html</a> (Accessed: <span class="NLM_date-in-citation">July 2007</span>). <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+University+of+Michigan+3D+Lab.+%282007%29.+Virtual+reality+CAVE+http%3A%2F%2Fum3d.dc.umich.edu%2Fhardware%2FCAVE%2Findex.html+%28Accessed%3A+July+2007%29.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0035&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Fum3d.dc.umich.edu%2Fhardware%2FCAVE%2Findex.html" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0036"><span> The University of North Carolina at Chapel Hill. (n.d.). <i>Effective virtual environments</i> <a class="ext-link" href="http://wwwx.cs.unc.edu/~eve/index.html" target="_blank">http://wwwx.cs.unc.edu/~eve/index.html</a> (Accessed: <span class="NLM_date-in-citation">August 2007</span>). <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+The+University+of+North+Carolina+at+Chapel+Hill.+%28n.d.%29.+Effective+virtual+environments+http%3A%2F%2Fwwwx.cs.unc.edu%2F%7Eeve%2Findex.html+%28Accessed%3A+August+2007%29.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0036&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Fwwwx.cs.unc.edu%2F%7Eeve%2Findex.html" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0037"><span><span class="hlFld-ContribAuthor">Wang, <span class="NLM_given-names">Y.</span></span> and <span class="hlFld-ContribAuthor">MacKenzie, <span class="NLM_given-names">C. L.</span></span> <span class="NLM_year">2000</span>. <span class="NLM_article-title">The role of contextual haptic and visual constraints on object manipulation in virtual environments</span>. <i>Proceedings of ACM Computer-Human Interaction</i>, : <span class="NLM_fpage">532</span>–<span class="NLM_lpage">538</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=532-538&author=Y.+Wang&author=C.+L.+MacKenzie&title=The+role+of+contextual+haptic+and+visual+constraints+on+object+manipulation+in+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0037&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DWang%26aufirst%3DY.%26date%3D2000%26atitle%3DThe%2520role%2520of%2520contextual%2520haptic%2520and%2520visual%2520constraints%2520on%2520object%2520manipulation%2520in%2520virtual%2520environments%26jtitle%3DProceedings%2520of%2520ACM%2520Computer-Human%2520Interaction%26spage%3D532%26epage%3D538" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0038"><span><span class="hlFld-ContribAuthor">Ware, <span class="NLM_given-names">C.</span></span> and <span class="hlFld-ContribAuthor">Arsenault, <span class="NLM_given-names">R.</span></span> <span class="NLM_year">2004</span>. <span class="NLM_article-title">Frames of reference in virtual object rotation</span>. <i>Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization</i>, : <span class="NLM_fpage">135</span>–<span class="NLM_lpage">141</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=135-141&author=C.+Ware&author=R.+Arsenault&title=Frames+of+reference+in+virtual+object+rotation" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0038&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DWare%26aufirst%3DC.%26date%3D2004%26atitle%3DFrames%2520of%2520reference%2520in%2520virtual%2520object%2520rotation%26jtitle%3DProceedings%2520of%2520the%25201st%2520Symposium%2520on%2520Applied%2520Perception%2520in%2520Graphics%2520and%2520Visualization%26spage%3D135%26epage%3D141" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0039"><span><span class="hlFld-ContribAuthor">Ware, <span class="NLM_given-names">C.</span></span> and <span class="hlFld-ContribAuthor">Balakrishnan, <span class="NLM_given-names">R.</span></span> <span class="NLM_year">1994</span>. <span class="NLM_article-title">Reaching for objects in VR displays: Lag and frame rate</span>. <i>ACM Transactions on Computer-Human Interaction</i>, 1(4): <span class="NLM_fpage">331</span>–<span class="NLM_lpage">356</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0039&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1145%2F198425.198426" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1994&pages=331-356&issue=4&author=C.+Ware&author=R.+Balakrishnan&title=Reaching+for+objects+in+VR+displays%3A+Lag+and+frame+rate" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0039&amp;dbid=16384&amp;doi=10.1145%2F198425.198426&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1145%252F198425.198426%26sid%3Dliteratum%253Atandf%26aulast%3DWare%26aufirst%3DC.%26date%3D1994%26atitle%3DReaching%2520for%2520objects%2520in%2520VR%2520displays%253A%2520Lag%2520and%2520frame%2520rate%26jtitle%3DACM%2520Transactions%2520on%2520Computer-Human%2520Interaction%26volume%3D1%26issue%3D4%26spage%3D331%26epage%3D356" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0040"><span><span class="hlFld-ContribAuthor">Westwood, <span class="NLM_given-names">D. A.</span></span>, <span class="hlFld-ContribAuthor">Heath, <span class="NLM_given-names">M.</span></span> and <span class="hlFld-ContribAuthor">Roy, <span class="NLM_given-names">E. A.</span></span> <span class="NLM_year">2000</span>. <span class="NLM_article-title">The effect of a pictorial illusion on closed-loop and open-loop prehension</span>. <i>Experimental Brain Research</i>, 134: <span class="NLM_fpage">456</span>–<span class="NLM_lpage">463</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0040&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1007%2Fs002210000489" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0040&amp;dbid=8&amp;doi=10.1080%2F10447310903025529&amp;key=11081827" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0040&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=000090006000006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=456-463&author=D.+A.+Westwood&author=M.+Heath&author=E.+A.+Roy&title=The+effect+of+a+pictorial+illusion+on+closed-loop+and+open-loop+prehension" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0040&amp;dbid=16384&amp;doi=10.1007%2Fs002210000489&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1007%252Fs002210000489%26sid%3Dliteratum%253Atandf%26aulast%3DWestwood%26aufirst%3DD.%2520A.%26date%3D2000%26atitle%3DThe%2520effect%2520of%2520a%2520pictorial%2520illusion%2520on%2520closed-loop%2520and%2520open-loop%2520prehension%26jtitle%3DExperimental%2520Brain%2520Research%26volume%3D134%26spage%3D456%26epage%3D463" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0041"><span><span class="hlFld-ContribAuthor">Wing, <span class="NLM_given-names">A. M.</span></span>, <span class="hlFld-ContribAuthor">Turton, <span class="NLM_given-names">A.</span></span> and <span class="hlFld-ContribAuthor">Fraser, <span class="NLM_given-names">C.</span></span> <span class="NLM_year">1986</span>. <span class="NLM_article-title">Grasp size and accuracy of approach in reaching</span>. <i>Journal of Motor Behavior</i>, 18: <span class="NLM_fpage">245</span>–<span class="NLM_lpage">260</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0041&amp;dbid=20&amp;doi=10.1080%2F10447310903025529&amp;key=10.1080%2F00222895.1986.10735380&amp;tollfreelink=144713_1415795_d85a781c04a83abd243750c1079aff823eca06f801d25fd434ac2bb85d0fbfb7">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0041&amp;dbid=128&amp;doi=10.1080%2F10447310903025529&amp;key=A1986E686300001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1986&pages=245-260&author=A.+M.+Wing&author=A.+Turton&author=C.+Fraser&title=Grasp+size+and+accuracy+of+approach+in+reaching" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0041&amp;dbid=16384&amp;doi=10.1080%2F00222895.1986.10735380&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F00222895.1986.10735380%26sid%3Dliteratum%253Atandf%26aulast%3DWing%26aufirst%3DA.%2520M.%26date%3D1986%26atitle%3DGrasp%2520size%2520and%2520accuracy%2520of%2520approach%2520in%2520reaching%26jtitle%3DJournal%2520of%2520Motor%2520Behavior%26volume%3D18%26spage%3D245%26epage%3D260" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0042"><span><span class="hlFld-ContribAuthor">Woodworth, <span class="NLM_given-names">R. S.</span></span> <span class="NLM_year">1899</span>. <span class="NLM_article-title">The accuracy of voluntary movement</span>. <i>The Psychological Review: Monograph Supplements</i>, 3: <span class="NLM_fpage">1</span>–<span class="NLM_lpage">114</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0042&amp;dbid=16&amp;doi=10.1080%2F10447310903025529&amp;key=10.1037%2Fh0092992" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1899&pages=1-114&author=R.+S.+Woodworth&title=The+accuracy+of+voluntary+movement" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0042&amp;dbid=16384&amp;doi=10.1037%2Fh0092992&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1037%252Fh0092992%26sid%3Dliteratum%253Atandf%26aulast%3DWoodworth%26aufirst%3DR.%2520S.%26date%3D1899%26atitle%3DThe%2520accuracy%2520of%2520voluntary%2520movement%26jtitle%3DThe%2520Psychological%2520Review%253A%2520Monograph%2520Supplements%26volume%3D3%26spage%3D1%26epage%3D114" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li></ul><div class="response"><div class="sub-article-title"></div></div>
</article>
</div>
<div class="tab tab-pane" id="relatedContent">
</div>
<div class="tab tab-pane " id="metrics-content">
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics" data-pb-dropzone="publicationContentDropZoneMetrics">
<div class="widget literatumArticleMetricsWidget none  widget-none" id="00886058-9b49-4cdf-9f1e-deb78b7818c3">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="articleMetricsContainer">
<div class="content fullView">
<h2>
Article Metrics
</h2>
<div class="section views">
<div class="title">
Views
</div>
<div class="circle">
<span class="value">87</span>
</div>
</div>
<div class="section citations">
<div class="title">
Citations
</div>
<a href="/doi/citedby/10.1080/10447310903025529" class="circle crossRef " target="_blank">
<span>
Crossref
</span>
<span class="value">4</span>
</a>
<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=CitingArticles&KeyUT=000272798500004" target="_blank" class="circle webOfScience ">
<span>
Web of Science
</span>
<span class="value">3</span>
</a>
<a href="http://www.scopus.com/inward/citedby.url?partnerID=HzOxMe3b&scp=77949367418" target="_blank" class="circle scopus ">
<span>
Scopus
</span>
<span class="value">4</span>
</a>
</div>
<div class="section altmetric-container">
<div class="title">
Altmetric
</div>
<script type="text/javascript">
    TandfUtils.appendScript(document.head, 'https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js', 'altmetric-embed-src', true, true);
</script>
<div class="metrics-badge linkToPopup">
<div class='altmetric-embed' data-badge-type='medium-donut' data-badge-details='right' data-condensed='true' data-template="tandf" data-hide-no-mentions="false" data-doi="10.1080/10447310903025529">
</div>
</div>
</div>
<div class="altmetricsPopup"></div>
</div>
</div>
<div class="metricsLinks">
<a href="/article-metrics">Article metrics information</a>
<br>
<a href="/article-citations-disclaimer">Disclaimer for citing articles</a>
</div></div>
</div>
</div>
</div>
</div>
<div class="access__limit" data-pb-dropzone="accessLimitPage">
</div>
</div>
</div>
</div>
<input id="viewLargeImageCaption" type="hidden" value="View Large Image" /></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-4 ">
<div class="contents" data-pb-dropzone="contents2">
<div class="widget general-bookmark-share none  widget-none  widget-compact-all" id="c8494935-e102-4ff5-9395-4ffa44a77f1c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<ul>
<li>
<div class="addthis_toolbox addthis_20x20_style">
<div class="custom_images">
<a class="addthis_button_twitter">
<span class="at-icon-twitter"></span>
</a>
<a class="addthis_button_facebook">
<span class="at-icon-facebook"></span>
</a>
<a class="addthis_button_email">
<span class="at-icon-email"></span>
</a>
<a class="addthis_button_none">
<span class="at-icon-none"></span>
</a>
<a class="addthis_button_compact"><span class="at-icon-wrapper"></span>
<span aria-describedby="shareOptions-description">
<span class="off-screen" id="shareOptions-description">More Share Options</span>
</span>
</a>
</div>
</div>
</li>
</ul>
<script type="text/javascript">
    
    var script = document.createElement('script');
    script.type='text/javascript';
    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';
    script.async = true;
    $('head').append(script)
</script>
</div>
</div>
</div>

<div class="widget general-html none  widget-none" id="16111d74-c554-42b2-a277-f2727ad2b285">
<div class="wrapped ">
<div class="widget-body body body-none ">&nbsp;</div>
</div>
</div>
<div class="widget general-html none  widget-none  widget-compact-all" id="649c3793-a589-4dfb-8de2-9b6cbc4dc15b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script defer src='//js.trendmd.com/trendmd.min.js' data-trendmdconfig='{"element":"#trendmd-suggestions"}'></script></div>
</div>
</div>
<div class="widget graphQueryWidget none  widget-none  widget-compact-all" id="6583d550-25db-458c-9c81-291f5c88b6ee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="trendmd-suggestions"></div></div>
</div>
</div>
<div class="widget pbOptimizerWidget none  widget-none  widget-compact-all" id="25efeb89-6948-4246-800a-5e246009698d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div data-optimizer data-widget-id="25efeb89-6948-4246-800a-5e246009698d" id="widget-25efeb89-6948-4246-800a-5e246009698d" data-observer>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget pageFooter none  widget-none  widget-compact-all" id="d97c173f-d838-4de1-bbd7-ed69f0d36a91">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><footer class="page-footer">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all" id="1f15adc0-4a59-4d27-93fe-8cbb14a5108a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-1-1 fit-padding">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget topicalIndex none  widget-none  widget-compact-all" id="9298c7a6-6903-4607-8380-4c83e2b7142f">
<div class="wrapped ">
<h1 class="widget-header header-none  header-compact-all">Browse journals by subject</h1>
<div class="widget-body body body-none  body-compact-all"><div class="topicalIndexBrowsingTips" data-pb-dropzone="topicalIndexBrowsingTips">
<div class="widget general-html none  widget-none  widget-compact-all" id="1ec3bad2-243b-45a9-a59a-5aceb80fc5a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a class="nav-top" href="#top">Back to top <span class="fa fa-angle-up"></span></a></div>
</div>
</div>
</div>
<div class="container">
<ul>
<li>
<a href="/topic/allsubjects/as?target=topic&amp;ConceptID=4251">Area Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ar?target=topic&amp;ConceptID=4250">Arts</a>
</li>
<li>
<a href="/topic/allsubjects/be?target=topic&amp;ConceptID=4252">Behavioral Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/bs?target=topic&amp;ConceptID=4253">Bioscience</a>
</li>
<li>
<a href="/topic/allsubjects/bu?target=topic&amp;ConceptID=4254">Built Environment</a>
</li>
<li>
<a href="/topic/allsubjects/cs?target=topic&amp;ConceptID=4256">Communication Studies</a>
</li>
<li>
<a href="/topic/allsubjects/cm?target=topic&amp;ConceptID=4255">Computer Science</a>
</li>
<li>
<a href="/topic/allsubjects/ds?target=topic&amp;ConceptID=4257">Development Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ea?target=topic&amp;ConceptID=4258">Earth Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/eb?target=topic&amp;ConceptID=4259">Economics, Finance, Business & Industry</a>
</li>
<li>
<a href="/topic/allsubjects/ed?target=topic&amp;ConceptID=4261">Education</a>
</li>
<li>
<a href="/topic/allsubjects/ec?target=topic&amp;ConceptID=4260">Engineering & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ag?target=topic&amp;ConceptID=4248">Environment & Agriculture</a>
</li>
<li>
<a href="/topic/allsubjects/es?target=topic&amp;ConceptID=4262">Environment and Sustainability</a>
</li>
<li>
<a href="/topic/allsubjects/fs?target=topic&amp;ConceptID=4263">Food Science & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ge?target=topic&amp;ConceptID=4264">Geography</a>
</li>
<li>
<a href="/topic/allsubjects/hs?target=topic&amp;ConceptID=4266">Health and Social Care</a>
</li>
<li>
<a href="/topic/allsubjects/hu?target=topic&amp;ConceptID=4267">Humanities</a>
</li>
<li>
<a href="/topic/allsubjects/if?target=topic&amp;ConceptID=4268">Information Science</a>
</li>
<li>
<a href="/topic/allsubjects/la?target=topic&amp;ConceptID=4269">Language & Literature</a>
</li>
<li>
<a href="/topic/allsubjects/lw?target=topic&amp;ConceptID=4270">Law</a>
</li>
<li>
<a href="/topic/allsubjects/ma?target=topic&amp;ConceptID=4271">Mathematics & Statistics</a>
</li>
<li>
<a href="/topic/allsubjects/me?target=topic&amp;ConceptID=4272">Medicine, Dentistry, Nursing & Allied Health</a>
</li>
<li>
<a href="/topic/allsubjects/ah?target=topic&amp;ConceptID=4249">Museum and Heritage Studies</a>
</li>
<li>
<a href="/topic/allsubjects/pc?target=topic&amp;ConceptID=4273">Physical Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/pi?target=topic&amp;ConceptID=4274">Politics & International Relations</a>
</li>
<li>
<a href="/topic/allsubjects/sn?target=topic&amp;ConceptID=4278">Social Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/sl?target=topic&amp;ConceptID=4277">Sports and Leisure</a>
</li>
<li>
<a href="/topic/allsubjects/sp?target=topic&amp;ConceptID=4279">Tourism, Hospitality and Events</a>
</li>
<li>
<a href="/topic/allsubjects/us?target=topic&amp;ConceptID=4280">Urban Studies</a>
</li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none footer-links widget-none  widget-compact-horizontal" id="64a44adf-45ed-4da3-be26-ef25beb9dbee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="6918e9df-910a-4206-9bd0-1a02bc17f740">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="aa9510dd-52ed-4b74-8211-fb510cd9468e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Information for</h3>
<ul>
<li><a href="http://authorservices.taylorandfrancis.com/">Authors</a></li>
<li><a href="http://editorresources.taylorandfrancisgroup.com/">Editors</a></li>
<li><a href="/page/librarians">Librarians</a></li>
<li><a href="/societies">Societies</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Open access</h3>
<ul>
<li><a href="/openaccess">Overview</a></li>
<li><a href="/openaccess/openjournals">Open journals</a></li>
<li><a href="/openaccess/openselect">Open Select</a></li>
<li><a href="https://www.cogentoa.com/">Cogent OA</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="fc564559-f496-499c-87c7-d851f371f061">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="f3fb3d36-db42-4373-9d0e-432958bf2fbc">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Help and info</h3>
<ul>
<li><a href="https://help.tandfonline.com">Help & contact</a></li>
<li><a href="https://newsroom.taylorandfrancisgroup.com/">Newsroom</a></li>
<li><a href="https://taylorandfrancis.com/partnership/commercial/">Commercial services</a></li>
<li><a href="/action/showPublications?pubType=journal">All journals</a></li>
<li><a href="https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot">Books</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="914433f6-0ea6-4a47-9781-07564061be86">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-social-label">
<h3>Keep up to date</h3>
</div>
<div class="font-size-correction-sml">Register to receive personalised research and resources by email</div>
<div class="bs">
<div class="pull-left links font-size-correction">
<a class="font-size-correction-link" href="https://taylorandfrancis.formstack.com/forms/tfoguest_signup"><i class="fa fa-envelope-square" title="Register to receive personalised research and resources by email"></i>Sign me up</a>
</div></div></div>
</div>
</div>
<div class="widget literatumSocialLinks none  widget-none  widget-compact-all" id="3b6a5e53-cd62-452f-adc1-92e187a0849d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="bs">
<div class="pull-left links">
<a href="http://facebook.com/TaylorandFrancisGroup">
<i class="icon-facebook" title="Taylor and Francis Group Facebook page" aria-hidden="true" role="button"></i>
<span aria-describedby="fb-description">
<span class="off-screen" id="fb-description">Taylor and Francis Group Facebook page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="https://twitter.com/tandfonline">
<i class="fa fa-twitter-square" title="Taylor and Francis Group Twitter page" aria-hidden="true" role="button"></i>
<span aria-describedby="twitter-description">
<span class="off-screen" id="twitter-description">Taylor and Francis Group Twitter page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://linkedin.com/company/taylor-&-francis-group">
<i class="fa fa-linkedin-square" title="Taylor and Francis Group LinkedIn page" aria-hidden="true" role="button"></i>
<span aria-describedby="linkedin-description">
<span class="off-screen" id="linkedin-description">Taylor and Francis Group Linkedin page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
<div class="pull-left links">
<a href="https://www.youtube.com/user/TaylorandFrancis">
<i class="fa fa-youtube-square" title="Taylor and Francis Group YouTube page" aria-hidden="true" role="button"></i>
<span aria-describedby="youtube-description">
<span class="off-screen" id="youtube-description">Taylor and Francis Group Youtube page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://www.weibo.com/tandfchina">
<i class="fa fa-weibo" title="Taylor and Francis Group Weibo page" aria-hidden="true" role="button"></i>
<span aria-describedby="weibo-description">
<span class="off-screen" id="weibo-description">Taylor and Francis Group Weibo page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>

<div class="widget responsive-layout none  widget-none  widget-compact-horizontal" id="8d803f96-081d-4768-ab7d-280a77af723b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-sm  ">
<div class="col-sm-3-4 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none footer-info-container widget-none  widget-compact-vertical" id="b247ecb9-84c9-4762-b270-20f8be1f0ae4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="informa-group-info">
<span>Copyright © 2020 Informa UK Limited</span>
<span><a href="https://informa.com/privacy-policy/">Privacy policy</a></span>
<span><a href="/cookies">Cookies</a></span>
<span><a href="/terms-and-conditions">Terms & conditions</a></span>
<span><a href="/accessibility">Accessibility</a></span>
<p>Registered in England & Wales No. 3099067<br />
5 Howick Place | London | SW1P 1WG</p>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-4 footer_tandf_logo">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-image none  widget-none  widget-compact-vertical" id="b6bde365-079b-454f-94f6-1841291656a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><a href="http://taylorandfrancis.com/" title="Taylor and Francis Group">
<img src="/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png" alt="Taylor and Francis Group" />
</a></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget cookiePolicy none  widget-none  widget-compact-all" id="cea739ac-da2c-4d77-9cf1-cb3e0da7e31e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="banner">
<a href="#" class="btn">Accept</a>
<p class="message">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href="/cookies">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>
</div></div>
</div>
</div>
</div>
</footer></div>
</div>
</div>
</div>
</div>
</body>
</html>
