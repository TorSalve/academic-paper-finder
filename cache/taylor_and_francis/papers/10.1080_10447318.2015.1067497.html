<!DOCTYPE html>
<html lang="en" class="pb-page" data-request-id="c21be540-4927-4c3a-9013-6f77f5934019"><head data-pb-dropzone="head"><meta name="pbContext" content=";page:string:Article/Chapter View;ctype:string:Journal Content;issue:issue:10.1080/hihc20.v031.i12;journal:journal:hihc20;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text;requestedJournal:journal:hihc20;article:article:10.1080/10447318.2015.1067497" />
<link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><meta name="citation_journal_title" content="International Journal of Human-Computer Interaction" /><meta name="dc.Title" content="Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution" /><meta name="dc.Creator" content="Andrzej  Grabowski" /><meta name="dc.Description" content="Engaging the sense of touch in virtual environments is a challenging and important issue. Currently, the most common way to address this issue is the use of a set of actuators that restrict hand mo..." /><meta name="Description" content="Engaging the sense of touch in virtual environments is a challenging and important issue. Currently, the most common way to address this issue is the use of a set of actuators that restrict hand mo..." /><meta name="dc.Publisher" content="Taylor &amp; Francis" /><meta name="dc.Date" scheme="WTN8601" content="25 Nov 2015" /><meta name="dc.Type" content="research-article" /><meta name="dc.Format" content="text/HTML" /><meta name="dc.Identifier" scheme="publisher-id" content="1067497" /><meta name="dc.Identifier" scheme="doi" content="10.1080/10447318.2015.1067497" /><meta name="dc.Source" content="https://doi.org/10.1080/10447318.2015.1067497" /><meta name="dc.Language" content="en" /><meta name="dc.Coverage" content="world" /><meta name="dc.Rights" content="Copyright Â© Taylor &amp; Francis Group, LLC" />
<link rel="meta" type="application/atom+xml" href="https://doi.org/10.1080%2F10447318.2015.1067497" />
<link rel="meta" type="application/rdf+json" href="https://doi.org/10.1080%2F10447318.2015.1067497" />
<link rel="meta" type="application/unixref+xml" href="https://doi.org/10.1080%2F10447318.2015.1067497" />
<title>Full article: Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</title>
<meta charset="UTF-8">
<meta name="robots" content="noarchive" />
<meta name="pb-robots-disabled">

<meta property="og:title" content="Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.tandfonline.com/doi/abs/10.1080/10447318.2015.1067497" />
<meta property="og:image" content="https://www.tandfonline.com/doi/cover-img/10.1080/hihc20.v031.i12" />
<meta property="og:site_name" content="Taylor & Francis" />
<meta property="og:description" content="(2015). Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution. International Journal of Human&#x2013;Computer Interaction: Vol. 31, No. 12, pp. 861-868." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@tandfonline">
<meta name="viewport" content="width=device-width,initial-scale=1" />
<script>var tandfData = {"search":{"hasOpenAccess":true}};</script>
<link rel="stylesheet" type="text/css" href="/wro/j2y2~product.css"><link rel="stylesheet" type="text/css" href="/pb/css/t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" id="pb-css" data-pb-css-id="t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" />
<link href="//www.trendmd.com" rel="preconnect" />
<link href="//app.wizdom.ai" rel="preconnect" />
<link href="//connect.facebook.net" rel="preconnect" />
<link href="//go.taylorandfrancis.com" rel="preconnect" />
<link href="//pi.pardot.com" rel="preconnect" />
<link href="//static.hotjar.com" rel="preconnect" />
<link href="//cdn.pbgrd.com" rel="preconnect" />
<link href="//f1-eu.readspeaker.com" rel="preconnect" />
<link href="//www.googleadservices.com" rel="preconnect" />
<link href="https://ajax.googleapis.com" rel="preconnect" />
<link href="https://m.addthis.com" rel="preconnect" />
<link href="https://wl.figshare.com" rel="preconnect" />
<link href="https://pagead2.googlesyndication.com" rel="preconnect" />
<link href="https://www.googletagmanager.com" rel="preconnect" />
<link href="https://www.google-analytics.com" rel="preconnect" />
<script type="text/javascript" src="/wro/j2y2~loadinview.js"></script>
<script type="text/javascript" src="/wro/j2y2~product.js"></script>
<script type="text/javascript">
        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};
    </script>
<script type="application/javascript" src="//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr" id="read-speaker" async></script>
<script type="text/javascript" src="//cdn.pbgrd.com/core-tandf.js" async defer></script>
<script data-ad-client="ca-pub-5143040550582507" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script>

<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:864760,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>
<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script async src="https://www.colwiz.com/pubsol/widget/34000f34a146a2017e2b5acad48d6b07.js"></script>
<link href="//qa.colwiz.com" rel="preconnect" />
<script src="//scholar.google.com/scholar_js/casa.js" async></script>
</head>
<body class="pb-ui">

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<div class="skipContent off-screen">
<a href="#top-content-scroll" class="skipToContent" title="Skip to Main Content" tabIndex="0">Skip to Main Content</a>
</div>
<script type="text/javascript">
    if (true) {
        var skipToContent = document.getElementsByClassName("skipToContent");
        if (skipToContent != null) {
            skipToContent[0].onclick = function skipElement() {
                var element = document.getElementById('top-content-scroll');
                if (element == null || element === undefined) {
                    element = document.getElementsByClassName('top-content-scroll').item(0);
                }
                element.setAttribute('tabindex', '0');
                element.focus();
            }
        }
    }
    document.addEventListener("DOMContentLoaded",function(e){
        if(document.getElementsByClassName("mediaThumbnailContainer").length > 0){
            TandfUtils.appendScript(document.body,"/wro/j2y2~jwplayer.js","jwplayer_src",true,true);
        }
    });
</script>
<div id="pb-page-content" data-ng-non-bindable>
<div data-pb-dropzone="main" data-pb-dropzone-name="Main">
<div class="widget pageHeader none  widget-none  widget-compact-all" id="a4d4fdd3-c594-4d68-9f06-b69b8b37ed56">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><header class="page-header">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="036fa949-dc25-4ffe-9df0-d7daefee281b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-xs  ">
<div class="col-xs-1-6 header-index">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal" id="e817489e-2520-418b-a731-b62e247e74df">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/" title="Taylor and Francis Online">
<img src="/pb-assets/Global/tfo_logo-1444989687640.png" alt="Taylor and Francis Online" />
</a></div>
</div>
</div>
<div class="widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal" id="b3fe8380-8b88-4558-b004-6485d3aea155">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/">
<img src="/pb-assets/Global/tfo_logo_sm-1459688573210.png" />
</a></div>
</div>
</div>
</div>
</div>
<div class="col-xs-5-6 ">
<div class="contents" data-pb-dropzone="contents1">

<div class="widget layout-inline-content alignRight  widget-none  widget-compact-all" id="a8a37801-55c7-4566-bdef-e4e738967e38">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget layout-inline-content none customLoginBar widget-none" id="fbe90803-b9c8-4bef-9365-cb53cc4bfa0e">

<div class="wrapped ">
<div class="widget-body body body-none "><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumInstitutionBanner none bannerWidth widget-none" id="3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="institution-image-text hidden-xs hidden-sm disable-click">Access provided by<strong> Copenhagen University Library</strong>
</div>
<div class="institution-image logout-institution-image">
</div></div>
</div>
</div>
<div class="widget literatumNavigationLoginBar none  widget-none  widget-compact-all" id="1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="loginBar not-logged-in">
<span class="icon-user"></span>
<a href="/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F10447318.2015.1067497" class="sign-in-link">
Log in
</a>
<span class="loginSeprator">&nbsp;|&nbsp;</span>
<a href="/action/registration?redirectUri=%2F" class="register-link">
Register
</a>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget eCommerceCartIndicatorWidget none literatumCartLink widget-none" id="9de10bb5-08af-48bc-b9f6-3f6433229f3e">
<div class="wrapped ">
<div class="widget-body body body-none "><a href="/action/showCart?FlowID=1" class="cartLabel">
<span class="hidden-xs hidden-sm visible-tl-inline-block">Cart</span>
<span class="cartItems" data-id="cart-size" role="status">
</span>
</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</header></div>
</div>
</div>

<div class="widget pageBody none  widget-none  widget-compact-all" id="35d9ca18-265e-4501-9038-4105e95a4b7d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<div class="page-body pagefulltext">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all" id="1728e801-36cd-4288-9f53-392bad29506a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-5-12 search_container ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget quickSearchWidget none search-customize-width widget-none  widget-compact-all" id="d46e3260-1f5c-4802-821a-28a03a699c82">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="quickSearchFormContainer ">
<form action="/action/doSearch" name="quickSearch" class="quickSearchForm " title="Quick Search" method="get" onsubmit="appendSearchFilters(this)" aria-label="Quick Search"><span class="simpleSearchBoxContainer">
<input name="AllField" class="searchText main-search-field autocomplete" value="" type="search" id="searchText" title="Type search term here" aria-label="Search" placeholder="Enter keywords, authors, DOI, ORCID etc" autocomplete="off" data-history-items-conf="3" data-publication-titles-conf="3" data-publication-items-conf="3" data-topics-conf="3" data-contributors-conf="3" data-fuzzy-suggester="false" data-auto-complete-target="title-auto-complete" />
</span>
<span class="searchDropDownDivRight">
<label for="searchInSelector" class="visuallyhidden">Search in:</label>
<select id="searchInSelector" name="SeriesKey" class="js__searchInSelector">
<option value="hihc20" id="thisJournal" data-search-in="thisJournal">
This Journal
</option>
<option value="" data-search-in="default">
Anywhere
</option>
</select>
</span>
<span class="quick-search-btn">
<input class="mainSearchButton searchButtons pointer" title="Search" role="button" type="submit" value="" aria-label="Search" />
</span></form>
</div>
<div class="advancedSearchLinkDropZone" data-pb-dropzone="advancedSearchLinkDropZone">
<div class="widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all" id="323e2a31-1c81-4995-bd17-8e149458c214">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a href="/search/advanced" class="advSearchArticle">Advanced search</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 serNav_container">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumSeriesNavigation none  widget-none" id="7730bfe1-9fca-4cf4-a6d6-2a0148105437">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="issueSerialNavigation journal">
<div class="cover">
<img data-src='{"type":"image" , "src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/hihc20.v031.i12/20151123-01/hihc20.v031.i12.cover.jpg"}' src="//:0" alt="Publication Cover" width="120" height="156" />
</div>
<div class="info ">
<div class="title-container">
<span class="titleHeading">Journal</span>
<h1>
<a href="/toc/hihc20/current">
International Journal of Human&#x2013;Computer Interaction
</a>
</h1>
<h2>
Volume 31, 2015 - <a href="/toc/hihc20/31/12" class="nav-toc-list">Issue 12</a>
</h2>
</div>
<div class="seriesNavDropZone" data-pb-dropzone="seriesNavDropZone">
<div class="widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal" id="753455df-1eeb-47ca-bdc9-e19022075973">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="serial-action">
<a href="http://www.editorialmanager.com/ijhc" class="green submitAnArticle"><span>Submit an article</span></a>
<a href="/toc/hihc20/current" class="jHomepage"><span>Journal homepage</span></a>
</div></div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>

<div class="widget responsive-layout none  widget-none" id="e42aea8f-434a-4d39-aaef-f56af3ff00dc">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all" id="6aacf107-e82d-494d-a14c-0c00bba52560">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="accessLogo">
<div>
<img class="accessIconLocation" data-src='{"type":"image" , "src":"/pb-assets/3rdPartyLogos/accessFull-1452596451717.png"}' src="//:0" alt="Full access" />
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none publicationContentHeader widget-none  widget-compact-all" id="63f402e4-3498-4709-8d7d-ee8e69f93467">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical" id="5afd8b6d-7e09-43ff-8ad6-afa3764e543c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="articleMetricsContainer">
<div class="content compactView">
<div class="section">
<div class="value">
225
</div>
<div class="title">
Views
</div>
</div>
<div class="section">
<div class="value">
0
</div>
<div class="title">
CrossRef citations to date
</div>
</div>
<div class="section score">
<div class="altmetric-score true">
<div class="value" data="10.1080/10447318.2015.1067497"></div>
<div class="title">
Altmetric
</div>
</div>
</div>
<div class="altmetric-Key hidden" data="be0ef6915d1b2200a248b7195d01ef22"></div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-2-3 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all" id="fa57727f-b942-4eb8-9ed2-ecfe11ac03f5">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="read-speaker-container" style="display: block">
<div id="readspeaker_button1" class="rs_skip rsbtn rs_preserve">
<a href="//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F10447318.2015.1067497" rel="nofollow" class="rsbtn_play" accesskey="L" title="Listen to this page using ReadSpeaker webReader" style="border-radius: 0 11.4px 11.4px 2px;">
<span class="rsbtn_left rsimg rspart"><span class="rsbtn_text"><span>Listen</span></span></span>
<span class="rsbtn_right rsimg rsplay rspart"></span>
</a>
</div>
</div>
<div class="toc-heading">
<h3>
Original Articles
</h3>
</div>
<h1><span class="NLM_article-title hlFld-title">Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</span></h1><span class="sub-title"><h2></h2></span><div class="literatumAuthors"><div class="publicationContentAuthors"><div class="hlFld-ContribAuthor"><span class="NLM_contrib-group"><span class="contribDegrees corresponding "><a class="entryAuthor" href="/author/Grabowski%2C+Andrzej">Andrzej Grabowski<span class="overlay"> Virtual Reality Laboratory, Central Institute for Labour Protection, National Research Institute, Warsaw, Poland<span class="corr-sec"><span class="heading">Correspondence</span><span class="corr-email"><i class="fa fa-envelope" style="color: #10147E; padding-right: 7px" aria-hidden="true"></i><span data-mailto="mailto:angra@ciop.pl">angra@ciop.pl</span></span><br /></span><div class="author-extra-info" tabindex="0" data-authorsInfo="{&quot;id&quot; : &quot;B0001&quot;, &quot;hasFull&quot; : &quot;true&quot;}">View further author information</div></span></a></span></span></div></div></div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="5f562208-b1d5-4e5a-81c7-356431240f04">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md gutterless ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget layout-inline-content none  widget-none  widget-compact-all" id="87ac5840-18fa-4a14-8eca-065b90ede3d7">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumContentItemPageRange none  widget-none  widget-compact-all" id="45057865-d60c-414c-bc81-646debb621b0">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><span class="contentItemPageRange">Pages 861-868
</span></div>
</div>
</div>
<div class="widget literatumContentItemHistory none  widget-none  widget-compact-all" id="32bf868e-52ce-411a-9dc3-717743aad997">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div>Accepted author version posted online: 22 Oct 2015</div><div>Published online:25 Nov 2015</div></div>
</div>
</div>
<div class="widget literatumArticleToolsWidget none  widget-none  widget-compact-all" id="ed673666-7b5d-470e-bd33-c5c679d996cb">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="articleTools">
<ul class="linkList blockLinks separators centered">
<li class="downloadCitations">
<a href="/action/showCitFormats?doi=10.1080%2F10447318.2015.1067497"><i class="fa fa-quote-left" aria-hidden="true"></i>Download citation</a>
</li>
<li class="dx-doi">
<a href="https://doi.org/10.1080/10447318.2015.1067497"><i class="fa fa-external-link-square" style="margin: 0 0.25rem 0 0" aria-hidden="true"></i>https://doi.org/10.1080/10447318.2015.1067497</a>
</li>
<li class="cross-mark">
<a id="crossMark" data-doi="10.1080/10447318.2015.1067497" data-target="crossmark">
<img data-src='{"type":"image" , "src":"/templates/jsp/images/CROSSMARK_Color_horizontal.svg"}' src="//:0" alt="CrossMark Logo" width="100" />
<span aria-describedby="crossMark-description"><span class="off-screen" id="crossMark-description">CrossMark</span></span>
</a>
</li>
</ul>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents2">

</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none publicationContentBody widget-none" id="f4a74f7a-9ba2-4605-86b1-8094cb1f01de">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget sectionsNavigation none  widget-none" id="f15bd2de-bb18-4067-8ab9-03ea3be30bf7">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="sections-nav"><span class="title">In this article<a href="#" class="close" tabindex="-1"><span aria-describedby="close-description"><span class="off-screen" id="close-description">Close</span></span></a></span><ul class="sections-list"><li><span class="sub-art-heading"><a href="#_i1">1. INTRODUCTION</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i2">2. OBJECTIVES</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i3">3. THE EXPERIMENT</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i10">4. RESULTS</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i18">5. Conclusions</a></span><ul class="sub-art-titles"></ul></li><li><a href="#references-Section">References</a></li></ul></div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none rs_readArea widget-none  widget-compact-all" id="9751b4f9-64b9-44c0-955b-f75246902839">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all" id="d29f04e9-776c-4996-a0d8-931023161e00">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
        "HTML-CSS": {scale: 70, linebreaks: {automatic: true, width: "container"}},
        SVG: {linebreaks: {automatic: true, width: "25%"}},
        menuSettings: {zoom: "Click"},

        /* This is necessary to lazy loading. */
        skipStartupTypeset: true
    });
</script>
<div class="articleMeta ja">
<div class="tocHeading">
<h2>Original Articles</h2>
</div>
<div class="hlFld-Title">
<div class="publicationContentTitle">
<h1 class="chaptertitle">
Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution
</h1>
</div>
</div>
<div class="copyrightStatement">
</div>
<div class="articleMetaDrop publicationContentDropZone" data-pb-dropzone="articleMetaDropZone">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone1" data-pb-dropzone="articleMetaDropZone1">
</div>
<div class="copyrightline">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone2" data-pb-dropzone="articleMetaDropZone2">
</div>
</div>
<div class="publication-tabs ja publication-tabs-dropdown">
<div class="tabs tabs-widget">
<ul class="tab-nav" role="tablist">
<li class="active" role="tab">
<a href="/doi/full/10.1080/10447318.2015.1067497?scroll=top&amp;needAccess=true" class="show-full">
<i class="fa fa-file-text" aria-hidden="true"></i>
<span class="nav-data">
Full Article
</span>
</a>
</li>
<li role="tab">
<a href="/doi/figure/10.1080/10447318.2015.1067497?scroll=top&amp;needAccess=true" class="show-figure">
<i class="fa fa-image" aria-hidden="true"></i>
<span class="nav-data">Figures & data</span>
</a>
</li>
<li role="tab">
<a href="/doi/ref/10.1080/10447318.2015.1067497?scroll=top" class="show-references">
<i class="fa fa-book" aria-hidden="true"></i>
<span class="nav-data">References</span>
</a>
</li>
<li class="citedbyTab " role="tab">
<a href="/doi/citedby/10.1080/10447318.2015.1067497?scroll=top&amp;needAccess=true">
<i class="fa fa-quote-left" aria-hidden="true"></i>
<span class="nav-data">
Citations
</span>
</a>
</li>
<li class="off-screen"></li>
<li role="tab" class="metrics-tab">
<a href="#metrics-content" class="show-metrics">
<i class="fa fa-bar-chart" aria-hidden="true"></i>
<span class="nav-data">Metrics</span>
</a>
</li>
<li role="tab" class="permissions-tab ">
<a href="/doi/abs/10.1080/10447318.2015.1067497?tab=permissions&amp;scroll=top" class="show-permissions">
<i class="fa fa-print" aria-hidden="true"></i>
<span class="nav-data">
Reprints & Permissions</span></a>
</li>
<li class="pdf-tab " role="button">
<a href="/doi/pdf/10.1080/10447318.2015.1067497?needAccess=true" class="show-pdf" target="_blank">
<span class="nav-data">
PDF
</span>
</a>
</li>
</ul>
<div class="tab-content ">
<a id="top-content-scroll"></a>
<div class="tab tab-pane active">
<article class="article">

<p class="fulltext"></p><div class="hlFld-Abstract"><p class="fulltext"></p><div class="sectionInfo abstractSectionHeading"><h2 id="abstract" class="section-heading-2">Abstract</h2></div><div class="abstractSection abstractInFull"><p>Engaging the sense of touch in virtual environments is a challenging and important issue. Currently, the most common way to address this issue is the use of a set of actuators that restrict hand movement. This article presents an alternative method to track physical surrogate representations of virtual objects using a vision-based system. Using real objects that can be manipulated by a person immersed in a virtual environment is an alternative to expensive robotized haptic systems. Moreover, this technique allows the user to move objects over long distances. Research conducted with 30 volunteers shows that the sense of touch can significantly reduce the execution time of tasks demanding high precision. Despite this decrease in execution time, an increase in precision has been observed. The conclusion of this article looks at the pros and cons of the use of physical surrogates for virtual objects, for example, less time needed for virtual reality-based training with the use of physical surrogates. </p></div></div><div class="hlFld-Fulltext"><div id="S0001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1"><h2 id="_i1" class="section-heading-2">1. INTRODUCTION</h2><p>Thanks to the amount of flexibility that virtual reality (VR) techniques present, they are being used with increasing frequency in various areas of science. Some examples of how VR systems are currently being put to use include treating stress-related disorders (Banosa et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-refLink="_i23" href="#">2011</a></span>), analyzing hand postures while grasping different objects (Vatavun, Ionut, &amp; Zait, <span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i23" href="#">2013</a></span>), studying how emotion can effect performance in the context of driving a motor vehicle (Cai &amp; Lin, <span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-refLink="_i23" href="#">2011</a></span>), and designing ergonomic workspaces for workers with motion disabilities (Budziszewski, Grabowski, Milanowicz, Jankowski, &amp; Dwiarek, <span class="ref-lnk lazy-ref"><a data-rid="CIT0010" data-refLink="_i23" href="#">2011</a></span>). An important research topic is a userâs sense of presence in describing the illusion of being there in a mediated environment. This mediated environment can be virtual or real (e.g., an image from cameras mounted on tele-operated mobile robots; Lee &amp; Kim, <span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-refLink="_i23" href="#">2008</a></span>). A userâs sense of presence is influenced by different factors, like enjoyment (Sylaioua, Mania, Karoulis, &amp; White, <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-refLink="_i23" href="#">2010</a></span>), personality and individual abilities (Alsina-Jurnetn &amp; Gutierrez-Maldonado, <span class="ref-lnk lazy-ref"><a data-rid="CIT0001" data-refLink="_i23" href="#">2010</a></span>). Typically, sense of presence is measured with a proper questionnaire, such as the (MEC Spatial Presence Questionnaire MEC-SPQ; Bocking et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i23" href="#">2004</a></span>; Vorderer et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i23" href="#">2004</a></span>), which consists of 64 questions. However, in some cases it could be more useful to use only one question to subjectively measure the perceived realism of simulations.</p><p>VR immersion techniques are characterized by an almost total isolation of the human senses (visual and audio) from the real world. Artificial images and sounds of the simulated environment, instead of real images and sounds, are presented. There is a variety of different techniques used to achieve this, one of which is the use of stereoscopic Head Mounted Displays (HMDs) and stereo earphones. Data gloves are often used for human interaction with a virtual environment. These devices record finger movement, usually by bend sensors mounted along the fingers. This makes it possible to grasp and move virtual objects in a virtual world. Note that ray casting interaction techniques can be better than a virtual hand in the case of, for instance, object selection (Steed &amp; Parker, <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-refLink="_i23" href="#">2005</a></span>), and especially with projection technology (Argelaguet, Kulik, Kunert, Andujar, &amp; Froehlich, <span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-refLink="_i23" href="#">2011</a></span>; Bacim, Kopper, &amp; Bowman, <span class="ref-lnk lazy-ref"><a data-rid="CIT0004" data-refLink="_i23" href="#">2013</a></span>). To release an object, the fingers typically have to move outside the objectâs boundaries. However, due to the lack of physical motion constraints, the userâs (real) fingers often penetrate the virtual objects (for more details, see, e.g., Prachyabrued &amp; Borst, <span class="ref-lnk lazy-ref"><a data-rid="CIT0021" data-refLink="_i23" href="#">2012</a></span>). More advanced models of VR gloves also engage the sense of touch. Feedback between the virtual world and a user is usually achieved by means of tendons attached to the fingers. When a user touches a virtual object, further movement of the tendons is stopped, which prevents fingers from bending farther and gives the impression of touching a real object. In the literature on the topic, studies concerning the use of these gloves in the training of assembly tasks have been presented (see, e.g., Zaldivar-Colado &amp; Garbaya, <span class="ref-lnk lazy-ref"><a data-rid="CIT0031" data-refLink="_i23" href="#">2009</a></span>). It should be noted that the sense of touch can also be stimulated using vibrotactile technology (Rantala, Salminen, Raisamo, &amp; Surakka, <span class="ref-lnk lazy-ref"><a data-rid="CIT0023" data-refLink="_i23" href="#">2013</a></span>) or physical surrogate representations of virtual objects. It should be noted that the use of haptic devices is not limited to increasing immersion by engaging the sense of touch, as in the case of haptically enhanced 3D virtual environments created to investigate how visually impaired users perceive and explain a virtual space when haptics are the only input modality (J. Y. Lee, Bahn, &amp; Nam, <span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-refLink="_i23" href="#">2014</a></span>). According to Brown (<span class="ref-lnk lazy-ref"><a data-rid="CIT0009" data-refLink="_i23" href="#">2015</a></span>), current technology should already make it possible to record and replay a single pattern of movement or a series of interrelated movements; thus haptics may be also used to preserve tactile memories.</p><p>One of the most popular fields of VR application is training (in such training applications, haptic devices may be useful; see, e.g., Mateu, Lasala, &amp; Alamn, <span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-refLink="_i23" href="#">2014</a></span>; Millet, Lecuyer, Burkhardt, Haliyoa, &amp; Regniera, <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-refLink="_i23" href="#">2013</a></span>), as it can successfully support learning processes. The use of VR techniques seems to be particularly advantageous in situations where training under actual conditions is related to threats against human health and life. Therefore, training in virtual environments is often used in areas like medicine, e.g., virtual surgical procedures (Gallagher &amp; Cates, <span class="ref-lnk lazy-ref"><a data-rid="CIT0012" data-refLink="_i23" href="#">2004</a></span>), and in mining (Tichon &amp; Burgess-Limerick, <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i23" href="#">2011</a></span>). To prepare a useful training application, it is important to identify factors that influence the effectiveness of the learning experience, like the students cognitive-affective state (Bakera, DMello, Rodrigo, &amp; Graesser, <span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-refLink="_i23" href="#">2010</a></span>) and field of view (Ragan, Sowndararajan, Kopper, &amp; Bowman, <span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-refLink="_i23" href="#">2010</a></span>). Many training VR applications, like the simulation of assembly/disassembly operations for training and product design applications (Velaz, Lozano-Rodero, Suescun, &amp; Gutierrez, <span class="ref-lnk lazy-ref"><a data-rid="CIT0028" data-refLink="_i23" href="#">2013</a></span>), employ a variety of haptic devices. The form of most of them is similar to a robotâs arm. Note that such devices usually canât provide full tactile feedback for grasping, and the operation area is limited to the length of the haptic device arm, most of which are very small. Virtual worlds together with tangible user interfaces are used to offer a mixed reality experience. Mateu et al. (<span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-refLink="_i23" href="#">2014</a></span>) presented an example of this use focused on inclusive education, which shows that mixed reality applications have a high potential for use in this area.</p><p>Simulators, especially vehicle simulators (planes, cars, trucks, military vehicles like tanks), construction vehicles (wheel loaders and backhoe loaders), and industrial machines (like cranes; Juang, Hung, &amp; Kang, <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-refLink="_i23" href="#">2013</a></span>), constitute an important part of VR training applications. These simulators are often used for driver (operator) training. In this case, a virtual environment is usually presented on one or more screens. Simulators using HMDs are rare, as they tend to obstruct the view of steering elements such as the console and steering wheel. Tactile feedback in the case of vehicle simulators is obtained relatively easily, because such simulators are already equipped with steering elements, like the steering wheel, or controllers adopted from the video game industry (like joysticks or pads; see Juang et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-refLink="_i23" href="#">2013</a></span>). To some extent, a similar approach is used in the case of virtual firearm training simulators for soldiers and police officers, in the sense that they also see a virtual environment projected on a screen and they can use real objects (i.e., different kinds of weapons).</p></div><div id="S0002" class="NLM_sec NLM_sec_level_1"><h2 id="_i2" class="section-heading-2">2. OBJECTIVES</h2><p>The main research question is a quantitative assessment of the influence of engaging the sense of touch in VR using physical surrogates of virtual objects in relation to</p><ul class="NLM_list NLM_list-list_type-bullet"><li><p class="inline">time for task execution (4.1),</p></li><li><p class="inline">precision of movement (4.1),</p></li><li><p class="inline">spatial presence in virtual environment (4.2),</p></li><li><p class="inline">influence of spatial presence on task execution (4.3),</p></li><li><p class="inline">subjectively perceived realism of simulation (4.4), and</p></li><li><p class="inline">subjective assessment of the usefulness of physical surrogates in conducting research and training (4.5).</p></li></ul><p>In many training scenarios, users must move an object from point <i>A</i> to point <i>B</i>, the distance between which can be relatively large, even up to a few meters. Then they must place the object in the proper position and orientation (for reliable measurements), then move the object back to point <i>A</i>. (Please note that typical haptic devices cannot be used in such training scenarios because of their limited scope of operating area.) A good example of this would be a training procedure for measuring methane concentration levels during work in an underground coal mine (Grabowski &amp; Jankowski, <span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-refLink="_i23" href="#">2015</a></span>). This type of procedure would have to be repeated many times in a training scenario.</p><p>Precise manipulation of purely virtual objects may be difficult because of the lack of force-feedback. This process can be simplified using physical surrogates of virtual objects. Therefore the main objective of this study is to make a quantitative assessment of the usefulness of physical surrogates of virtual objects in terms of the time and precision of task execution.</p></div><div id="S0003" class="NLM_sec NLM_sec_level_1"><h2 id="_i3" class="section-heading-2">3. THE EXPERIMENT</h2><div id="S0003-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i4">3.1. Overview</h3><p>The basic method for assessing the virtual environment enriched by engaging the sense of touch is the User Testing method (Bach &amp; Scapin, <span class="ref-lnk lazy-ref"><a data-rid="CIT0003" data-refLink="_i23" href="#">2010</a></span>), which consists of a series of studies carried out with volunteers who are not experts in the field of VR (effectiveness of this method is much higher than the use of expert judgement). Therefore a study involving 30 volunteers (seven female, 23 male; all 2-year students at Warsaw University of Technology; 20 years old) was carried out. The study was preceded by short training (lasting about 10 min) designed to familiarize participants with the virtual environment in both research stages (A and B).</p><p>To investigate the influence of engaging the sense of touch, each person took part in two stages of research (15 min each) in random order:</p><ol class="NLM_list NLM_list-list_type-order"><li><p class="inline">without the use of physical surrogates (sense of touch was not involved), left of <a href="#F0001">Figure 1</a>. (A video showing this stage is available at <a class="ext-link" href="http://www.youtube.com/watch?v=Lt33fHug0l8" target="_blank">www.youtube.com/watch?v=Lt33fHug0l8</a>) <div class="figure figureViewer" id="F0001"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 1. </span> Simulated work performed without physical surrogates (left) and with physical surrogates for virtual objects (right).</p></div></td></div><a href="#" class="thumbnail"><img id="F0001image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0001_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0001"><p><span class="captionLabel">FIG. 1. </span> Simulated work performed without physical surrogates (left) and with physical surrogates for virtual objects (right).</p></div><div class="hidden rs_skip" id="figureFootNote-F0001"></div></p></li><li><p class="inline">with the use of physical surrogates (the sense of touch was involved during simulation), right of <a href="#F0001">Figure 1</a>. (A video showing this stage is available at <a class="ext-link" href="http://www.youtube.com/watch?v=anrWman9ru8" target="_blank">www.youtube.com/watch?v=anrWman9ru8</a>)</p></li></ol><p>During the study, each participant had to place elements in their proper place on the table (an example of the image seen in the HMD is shown in <a href="#F0002">Figure 2</a>). These elements have the form of a cylinder with a height of 25 cm and a diameter of 8.5 cm. Two red stripes were drawn on each cylinder (<a href="#F0003">Figure 3</a>). These stripes were to be aligned with stripes drawn on the table surface. One cycle of simulated work required the following steps:<div class="figure figureViewer" id="F0002"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 2. </span> Example of an image seen in the Head Mounted Display. <i>Note.</i> The avatar of the participants hand is visible on the right.</p></div></td></div><a href="#" class="thumbnail"><img id="F0002image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0002_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0002"><p><span class="captionLabel">FIG. 2. </span> Example of an image seen in the Head Mounted Display. <i>Note.</i> The avatar of the participants hand is visible on the right.</p></div><div class="hidden rs_skip" id="figureFootNote-F0002"></div> <div class="figure figureViewer" id="F0003"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 3. </span> Procedure for measuring the task precision: <i>a</i> cylinder displacement, Î± angle between current and desired orientation of the cylinder.</p></div></td></div><a href="#" class="thumbnail"><img id="F0003image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0003_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0003"><p><span class="captionLabel">FIG. 3. </span> Procedure for measuring the task precision: <i>a</i> cylinder displacement, Î± angle between current and desired orientation of the cylinder.</p></div><div class="hidden rs_skip" id="figureFootNote-F0003"></div></p><ol class="NLM_list NLM_list-list_type-order"><li><p class="inline">Move all elements from the left (right) side of the table to the right (left) side of the table and align the strips on the cylinders with the stripes on the table surface</p></li><li><p class="inline">Press the (virtual) button located on the table.</p></li></ol><p>Each time the virtual button was pressed, the position and spatial orientation of each cylinder was saved along with a time signature. These data enabled the calculation of the time needed to complete one cycle, as well as the precision of the task. Cylinder displacement and the Î± angle between the actual and desired rotation of the cylinder were analyzed. The procedure for measuring of the precision of the task is shown in <a href="#F0003">Figure 3</a>.</p><p>At the end of each stage (A and B), a spatial presence questionnaire (Witmer &amp; Singer, <span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-refLink="_i23" href="#">1998</a></span>), that is, MEC-SPQ (Bocking et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i23" href="#">2004</a></span>; Vorderer et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i23" href="#">2004</a></span>), was filled out. This type of questionnaire is often used to evaluate virtual environments. At the end of both stages, participants filled out a questionnaire on their subjective sense of the applications realism. </p><p>A <i>T</i>-test was then carried out in order to compare the results between the methods just described (Stages A and B).</p></div><div id="S0003-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i8">3.2. Hardware and Software</h3><p>The following hardware was used in the experiment: HMD Sony HMZ-T1 with two OLED displays (each with resolution of 1280 Ã 720) and DG5 VHand 2.0 virtual gloves (each glove contained five bend sensors).</p><p>A proper tracking system, that is, an electromagnetic or optical system, was required to determine the location of the head and hands. Because electromagnetic systems are very sensitive to the presence of metal objects and walls, it can be very difficult to use them in small rooms. Moreover, errors resulting from sensor positioning increase very rapidly with distance from the main antenna. For these reasons, a hybrid tracking system, based on cameras and Attitude Heading Reference System units, was used. This system is free of such drawbacks. Vision-based systems have great potential to track real objects (Grabowski, Jankowski, DÅºwiarek, &amp; KosiÅski, <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-refLink="_i23" href="#">2014</a></span>). Avatars of objects can be shown in the virtual environment. Hence, if the avatar of the hand interacts with the avatar of the object at the same time (and in the same way), the hand interacts with the real object. A similar approach may be used in the case of augmented reality (for more details, see Klemmera &amp; Landayb, <span class="ref-lnk lazy-ref"><a data-rid="CIT0016" data-refLink="_i23" href="#">2009</a></span>).</p><p>A comparison of results in the case of markers moving along straight line is shown in <a href="#F0004">Figure 4</a>. In the case of magnetic systems (Polhemus Liberty, left picture) measurement points lay on a curve (please note that the marker was far away from walls, floor, and objects made of metal). As we see at the right side of <a href="#F0004">Figure 4</a>, the tracking system used in the experiment gave accurate results. The motion capture system used in the experiments consisted of 12 synchronized cameras (Basler Ace acA645-100gc with 100 Hz frame rate) and a dedicated computer for calculations. The approximate maximum latency of the system was around 33 ms: 10 ms between frames from cameras, 5 ms for image data analysis, up to 1 ms for results transfer via TCP/IP protocol, 16.7 ms between two rendered images of virtual environment. However, the actual latency should be much smaller due to the fact that all of these actions were performed in parallel. <div class="figure figureViewer" id="F0004"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 4. </span> Results of measurements for the magnetic tracking system (left) and vision tracking system (right) for markers moved along a straight line.</p></div></td></div><a href="#" class="thumbnail"><img id="F0004image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0004_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0004"><p><span class="captionLabel">FIG. 4. </span> Results of measurements for the magnetic tracking system (left) and vision tracking system (right) for markers moved along a straight line.</p></div><div class="hidden rs_skip" id="figureFootNote-F0004"></div></p><p>To determine the orientation of the objects (i.e., HMD, hands and cylinders), a group of at least three markers was needed (see <a href="#F0001">Figure 1</a>).</p><p>Custom software was prepared for the simulation based on two open source projects: Ogre3D (<a class="ext-link" href="http://www.ogre3d.org" target="_blank">www.ogre3d.org</a>) for 3D graphics and the Bullet physics engine (<a class="ext-link" href="http://www.bulletphysics.org" target="_blank">www.bulletphysics.org</a>) for simulation of interactions between objects (as in between the hand avatar and the cylinder).</p></div></div><div id="S0004" class="NLM_sec NLM_sec_level_1"><h2 id="_i10" class="section-heading-2">4. RESULTS</h2><div id="S0004-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i11">4.1. Time and Precision of Task Execution</h3><p>Application of the location and movement mapping method of real objects in the virtual environment significantly influenced the average execution time of one cycle (in this case it was more than three times faster; see <a class="ref showTableEventRef" data-ID="T0001">Table 1</a> and <a href="#F0005">Figure 5</a>). It is interesting that the reduction of execution time of one cycle did not affect the accuracy of the task; on the contrary: engaging the sense of touch improved the values of <i>a</i> and Î±. The average value of the position of the cylinder displacement (<i>a</i>) was more than two times smaller, and the value of the Î± angle between actual and desired orientation of the cylinder was more than three times smaller. Differences between mean values are statistically significant for all parameters: Î<i>t</i>, <i>a</i>, and Î±. <div id="tableViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="tableView"><div class="tableCaption"><div class="short-legend"><h3><p><span class="captionLabel">TABLE 1 </span> Average Values of Parameters Describing Execution Time and Precision of the Task for Both Stages (A Without Physical Surrogates, B with Physical Surrogates</p></h3></div></div><div class="tableDownloadOption" data-hasCSVLnk="true" id="T0001-table-wrapper"><a id="CSVdownloadButton" class="downloadButton btn btn-sm" href="/action/downloadTable?id=T0001&amp;doi=10.1080%2F10447318.2015.1067497&amp;downloadType=CSV">CSV</a><a data-id="T0001" class="downloadButton btn btn-sm displaySizeTable" href="#">Display Table</a></div></div> <div class="figure figureViewer" id="F0005"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 5. </span> Box plot for parameters describing execution time and precision of the task for both stages (A without real objects, B with real objects). <i>Note.</i> Whiskers represent 2nd and 98th percentile.</p></div></td></div><a href="#" class="thumbnail"><img id="F0005image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0005_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0005"><p><span class="captionLabel">FIG. 5. </span> Box plot for parameters describing execution time and precision of the task for both stages (A without real objects, B with real objects). <i>Note.</i> Whiskers represent 2nd and 98th percentile.</p></div><div class="hidden rs_skip" id="figureFootNote-F0005"></div></p></div><div id="S0004-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i13">4.2. Spatial Presence</h3><p>In the case of the spatial presence questionnaire (i.e., MEC-SPQ; Bocking et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i23" href="#">2004</a></span>; Vorderer et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i23" href="#">2004</a></span>), the averages had similar values for all components of the questionnaire. The difference was statistically significant (<i>p</i> &lt; .0001) only regarding Spatial Presence: Possible actions (see <a class="ref showTableEventRef" data-ID="T0002">Table 2</a>). This component was defined as the perception of all actions possible to take, as anchored directly to the source of stimulation. Values of the questionnaire components are similar, because in both cases the environment and the presentation (displaying) method were the same. <div id="tableViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="tableView"><div class="tableCaption"><div class="short-legend"><h3><p><span class="captionLabel">TABLE 2 </span> Average Values of the Spatial Presence Questionnaire Components</p></h3></div></div><div class="tableDownloadOption" data-hasCSVLnk="true" id="T0002-table-wrapper"><a id="CSVdownloadButton" class="downloadButton btn btn-sm" href="/action/downloadTable?id=T0002&amp;doi=10.1080%2F10447318.2015.1067497&amp;downloadType=CSV">CSV</a><a data-id="T0002" class="downloadButton btn btn-sm displaySizeTable" href="#">Display Table</a></div></div> </p></div><div id="S0004-S2003" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i14">4.3. Correlations Between Spatial Presence and Task Execution</h3><p>In the case of stage A (no sense of touch), correlations between components of spatial presence and parameters describing task execution were statistically significant only for the angle (Î±) and average time of task execution (Î<i>t</i>). For the angle, the relationship between components of spatial presence and average value of Î± was linear, and the correlations were significant for the following components: involvement of attention (â0.78, <i>p</i> &lt; .0001), space situational model (â0.44, <i>p</i> = .02), spatial presence: possible actions (â0.40, <i>p</i> = .03), cognitive engagement (â0.44, <i>p</i> = .02), interest in the medium content (â0.45, <i>p</i> = .02). It is visible that the greater the value of spatial presence component, the greater the precision in task execution. The correlations are the strongest for involvement of attention.</p><p>For Stage B (with sense of touch), correlations are not statistically significant.</p></div><div id="S0004-S2004" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i15">4.4. Realism of Simulation</h3><p>The difference in the subjectively perceived realism of the simulation of the two phases (A and B) was clearly visible when survey results were compared. The respondents were asked to assess realism of the simulation on a scale of 1 (<i>lack of realism</i>) to 10 (<i>high realism</i>). Results are shown in <a href="#F0006">Figure 6</a>. In the case of Stage A, the average value of realism equaled 5.0 Â± 0.6, whereas in the case of Stage B, the average value was almost twice higher: 9.2 Â± 0.3. The results clearly indicate that the developed method for mapping the location and movement of real objects in the virtual environment significantly increases the realism of the simulation, which reached a near-maximum value. <div class="figure figureViewer" id="F0006"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 6. </span> Distribution of subjectively perceived realism of the simulation.</p></div></td></div><a href="#" class="thumbnail"><img id="F0006image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0006_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0006"><p><span class="captionLabel">FIG. 6. </span> Distribution of subjectively perceived realism of the simulation.</p></div><div class="hidden rs_skip" id="figureFootNote-F0006"></div></p><p>This large increase may be connected with the fact that participantsâ experiences were almost the same as they are in real life. When we are manipulating objects, two senses are crucial: sight and touch. Also note that in some cases, the sense of touch may be much more important. Both of these senses were engaged in the simulation. Moreover, stimuli in both senses were coherent; participants noticed that at the same time they were touching the objectsâ avatar, they were touching an actual object as well. Natural hand movements and stronger stimulation led to greater engagement in the simulation. The role of body movement in participant engagement is described in greater detail in Bianchi-Berthouze (<span class="ref-lnk lazy-ref"><a data-rid="CIT0007" data-refLink="_i23" href="#">2013</a></span>).</p><p>This result is particularly interesting in comparison with the results of the SPQ. It turns out that SPQ is not a precise-enough tool to measure the realism of the simulation for highly immersive virtual environments. However, the correlations between values of SPQ and subjectively perceived realism of the simulation are clearly visible. For Stage A, correlations are statistically significant for involvement of attention (0.45, <i>p</i> = .01), space situational model (0.61, <i>p</i> = .0003), spatial presence: possible actions (0.48, <i>p</i> = .006), cognitive engagement (0.42, <i>p</i> = .02), interest in the medium content (0.51, <i>p</i> = .004), and spatial imagination (0.42, <i>p</i> = .02). For Stage B, correlations are statistically significant only for cognitive engagement (0.45, <i>p</i> = .01) and sustaining disbelief (0.41, <i>p</i> = .02). Note that for Stage A, eight different values of subjectively perceived realism of the simulation were observed, but for Stage B only four values (see <a href="#F0006">Figure 6</a>).</p></div><div id="S0004-S2005" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i17">4.5. Subjective Assessment of Usefulness of Physical Surrogates</h3><p>After both stages, three questions were asked related to the usefulness of physical surrogates for virtual objects in research and training (see <a class="ref showTableEventRef" data-ID="T0003">Table 3</a>): <div id="tableViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="tableView"><div class="tableCaption"><div class="short-legend"><h3><p><span class="captionLabel">TABLE 3 </span> Usefulness of Physical Surrogates for Virtual Objects</p></h3></div></div><div class="tableDownloadOption" data-hasCSVLnk="true" id="T0003-table-wrapper"><a id="CSVdownloadButton" class="downloadButton btn btn-sm" href="/action/downloadTable?id=T0003&amp;doi=10.1080%2F10447318.2015.1067497&amp;downloadType=CSV">CSV</a><a data-id="T0003" class="downloadButton btn btn-sm displaySizeTable" href="#">Display Table</a></div></div> </p><ol class="NLM_list NLM_list-list_type-order"><li><p class="inline">How useful are physical surrogates for conducting research?</p></li><li><p class="inline">How useful are physical surrogates for training?</p></li><li><p class="inline">Do physical surrogates influence training quality and efficiency?</p></li></ol><p>According to the participants, physical surrogates are very useful for research and training, as well as increasing training quality and efficiency.</p></div></div><div id="S0005" class="NLM_sec NLM_sec_level_1"><h2 id="_i18" class="section-heading-2">5. Conclusions</h2><div id="S0005-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i19">5.1. Pros and Cons of Using Physical Surrogates for Virtual Objects</h3><p>The results of the research conducted clearly show that physical surrogates can greatly increase the realism of simulations and decrease the time needed for training. However, this approach also has some drawbacks and cannot be used for all training scenarios. First, it (slightly) increases cost and time needed to prepare the virtual environment. Physical surrogates of objects must be made and the tracking system has to be carefully calibrated. Spatial relations between real objects (including hands) and their corresponding virtual objects have to be the same, that is, when one touches a real object, one should see the handâs avatar touching the virtual representation of the object. There are no bilateral interactions between virtual objects and virtual objects with a physical surrogate; the latter can be moved only by its surrogate. For example, one could put a purely virtual cylinder on top of a real one, but not vice versa.</p><p>At first, tests with large (even two-handed) objects like drills (see <a href="#F0007">Figure 7</a>), were performed. However, it seems that it is better to use physical surrogates with relatively small objects that mainly interact with hands and other physical objects (a wall or table), especially if the participant must place the object precisely in a desired position and/or orientation. These types of objects are often used as surrogates for measuring devices (detectors) or control consoles, for example, a methane detector, a device checking the continuity of blasting lines prior to the connection of an electric detonator, or a control console for a crane that is remotely controlled from the ground. <div class="figure figureViewer" id="F0007"><div id="figureViewerArticleInfo" class="hidden"><h1>Sense of Touch in Training Tasks Demanding High Precision and Short Time of Execution</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Grabowski%2C+Andrzej"><span class="hlFld-ContribAuthor"><a href="/author/Grabowski%2C+Andrzej"><span class="NLM_given-names">Andrzej</span> Grabowski</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2015.1067497">https://doi.org/10.1080/10447318.2015.1067497</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>25 November 2015</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">FIG. 7. </span> Physical surrogate for a virtual drill.</p></div></td></div><a href="#" class="thumbnail"><img id="F0007image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209/images/medium/hihc_a_1067497_f0007_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0007"><p><span class="captionLabel">FIG. 7. </span> Physical surrogate for a virtual drill.</p></div><div class="hidden rs_skip" id="figureFootNote-F0007"></div></p><p>Physical surrogates for virtual objects should not be used if movement of the object must be blocked by purely virtual objects, because it can cause a break in presence. A very good example of this kind of physical surrogate would be a dummy of a two-handed virtual drill (see <a href="#F0007">Figure 7</a>). At the beginning, it highly increases the realism of the simulation, as the trainee feels tactile feedback and cannot squeeze or stretch the virtual drill (relative movement of hands is blocked). However, difficulties emerge if the trainee proceeds to drill a blasting hole. The lack of physical interaction between the virtual wall and the drill bit causes a break in presence.</p><p>It is also not recommended that one use physical surrogates if movement precision is not important in the training scenario.</p></div><div id="S0005-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i21">5.2. Summary</h3><p>During a study with 30 volunteers, the following parameters were measured: time of task execution and two parameters describing precision of task execution, and displacement of position and shift in the angle of the object carried by volunteers. It seems that engaging the sense of touch significantly reduces the average time of task execution (by approximately three times), the average displacement of the object (by approximately two times), and the average shift in the angle of the object (by approximately three times). Hence, the task was carried out much faster and with greater precision.</p><p>An interesting result of the research is that engaging the sense of touch can increase subjectively perceived realism of simulation by a factor of 2. However, results for the spatial presence questionnaire were the same for both stages (with and without the sense of touch). This indicates that typical tools measuring spatial presence, like the MEC-SPQ (Bocking et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i23" href="#">2004</a></span>; Vorderer et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i23" href="#">2004</a></span>), may be not sensitive enough in the case of highly immersive virtual environments.</p><p>One of the most important results is that the engagement of the sense of touch during simulation is an important factor that can significantly improve the efficiency of virtual training applications for workers. Eighty-three percent of the students believe that engaging the sense of touch in VR is very useful for training employees. Also, 83% of the students believe that the involvement of the sense of touch greatly increases the quality and effectiveness of training. It is also important that the developed method allows for a reduction of training time. These results have encouraged us to use this method in training applications prepared by the Central Institute for Labour Protection, for example, it is being used in the currently developed training applications for coal miners performing dangerous works, like blasting works.</p></div></div><div class="ack"><h2 class="section-heading-2">ACKNOWLEDGMENTS</h2><p>This article was prepared on the basis of the results of a research task carried out within the scope of the statutory activity of the Central Institute for Labour Protection â National Research Institute (project IIIâ38) supported by the Ministry of Science and Higher Education. Supplemental material for this article can be accessed at <a class="ext-link" href="https://www.youtube.com/watch?v=anrWman9ru8" target="_blank">https://www.youtube.com/watch?v=anrWman9ru8</a> and <a class="ext-link" href="https://www.youtube.com/watch?v=Lt33fHug0l8" target="_blank">https://www.youtube.com/watch?v=Lt33fHug0l8</a>.</p></div></div><script type="text/javascript">
                        window.figureViewer={doi:'10.1080/10447318.2015.1067497',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209',figures:[{i:'F0001',g:[{m:'hihc_a_1067497_f0001_oc.jpg',l:'hihc_a_1067497_f0001_oc.jpeg',size:'49 KB'}]}
                            ,{i:'F0002',g:[{m:'hihc_a_1067497_f0002_oc.jpg',l:'hihc_a_1067497_f0002_oc.jpeg',size:'48 KB'}]}
                            ,{i:'F0003',g:[{m:'hihc_a_1067497_f0003_oc.jpg',l:'hihc_a_1067497_f0003_oc.jpeg',size:'56 KB'}]}
                            ,{i:'F0004',g:[{m:'hihc_a_1067497_f0004_oc.jpg',l:'hihc_a_1067497_f0004_oc.jpeg',size:'37 KB'}]}
                            ,{i:'F0005',g:[{m:'hihc_a_1067497_f0005_b.gif',l:'hihc_a_1067497_f0005_b.jpeg',size:'46 KB'}]}
                            ,{i:'F0006',g:[{m:'hihc_a_1067497_f0006_oc.jpg',l:'hihc_a_1067497_f0006_oc.jpeg',size:'67 KB'}]}
                            ,{i:'F0007',g:[{m:'hihc_a_1067497_f0007_oc.jpg',l:'hihc_a_1067497_f0007_oc.jpeg',size:'67 KB'}]}
                            ]}</script><script type="text/javascript">window.tableViewer={doi:'10.1080/10447318.2015.1067497',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2015/hihc20.v031.i12/10447318.2015.1067497/20151209',tables:[{i:'T0001'},{i:'T0002'},{i:'T0003'}]}</script><script type="text/javascript">window.tableIDIndexMap = {"id":-1};window.tableIDIndexMap['T0001'] = 1; window.tableIDIndexMap['T0002'] = 2; window.tableIDIndexMap['T0003'] = 3; </script><div id="table-content-T0001" class="hidden"><table class="table frame_topbot"><div class="caption"><p><span class="captionLabel">TABLE 1 </span> Average Values of Parameters Describing Execution Time and Precision of the Task for Both Stages (A Without Physical Surrogates, B with Physical Surrogates</p></div><colgroup><col /><col /><col /><col /></colgroup><thead valign="bottom"><tr valign="top" class="rowsep1"><th align="left" valign="bottom" class="rowsep1 align_left">Â </th><th align="center" valign="bottom" class="rowsep1 align_center">Î<i>t</i> [s]</th><th align="center" valign="bottom" class="rowsep1 align_center">a [mm]</th><th align="center" valign="bottom" class="rowsep1 align_center last">Î± [radians]</th></tr></thead><tbody><tr valign="top"><td>Stage A</td><td>113 Â± 8</td><td>16.2 Â± 0.8</td><td class="last">0.32 Â± 0.03</td></tr><tr valign="top" class="last"><td>Stage B</td><td>36 Â± 1</td><td>7.8 Â± 0.4</td><td class="last">0.097 Â± 0.003</td></tr></tbody></table></div><div id="table-content-T0002" class="hidden"><table class="table frame_topbot"><div class="caption"><p><span class="captionLabel">TABLE 2 </span> Average Values of the Spatial Presence Questionnaire Components</p></div><colgroup><col /><col /><col /></colgroup><thead valign="bottom"><tr valign="top" class="rowsep1"><th align="left" valign="bottom" class="rowsep1 align_left">Â </th><th align="center" valign="bottom" class="rowsep1 align_center">Stage A</th><th align="center" valign="bottom" class="rowsep1 align_center last">Stage B</th></tr></thead><tbody><tr valign="top"><td>Involvement of attention</td><td>4.5 Â± 0.2</td><td class="last">4.3 Â± 0.3</td></tr><tr valign="top"><td>Space situational model</td><td>4.5 Â± 0.2</td><td class="last">4.6 Â± 0.2</td></tr><tr valign="top"><td>Spatial presence: Self-location</td><td>4.0 Â± 0.2</td><td class="last">4.2 Â± 0.3</td></tr><tr valign="top"><td>Spatial presence: Possible actions</td><td>4.1 Â± 0.2</td><td class="last">4.5 Â± 0.2</td></tr><tr valign="top"><td>Cognitive engagement</td><td>3.6 Â± 0.2</td><td class="last">3.7 Â± 0.4</td></tr><tr valign="top"><td>Sustaining disbelief</td><td>3.1 Â± 0.2</td><td class="last">3.2 Â± 0.2</td></tr><tr valign="top"><td>Interest in the medium content</td><td>3.1 Â± 0.3</td><td class="last">3.1 Â± 0.4</td></tr><tr valign="top" class="last"><td>Spatial imagination</td><td>4.4 Â± 0.2</td><td class="last">4.5 Â± 0.3</td></tr></tbody></table></div><div id="table-content-T0003" class="hidden"><table class="table frame_topbot"><div class="caption"><p><span class="captionLabel">TABLE 3 </span> Usefulness of Physical Surrogates for Virtual Objects</p></div><colgroup><col /><col /><col /><col /><col /><col /></colgroup><thead valign="bottom"><tr valign="top" class="rowsep1"><th align="left" valign="bottom" class="rowsep1 align_left">Â </th><th align="center" valign="bottom" class="rowsep1 align_center">Very Low</th><th align="center" valign="bottom" class="rowsep1 align_center">Low</th><th align="center" valign="bottom" class="rowsep1 align_center">No Opinion</th><th align="center" valign="bottom" class="rowsep1 align_center">High</th><th align="center" valign="bottom" class="rowsep1 align_center last">Very High</th></tr></thead><tbody><tr valign="top"><td>1. For research</td><td>0</td><td>0</td><td>1</td><td>6</td><td class="last">23</td></tr><tr valign="top"><td>2. For training</td><td>0</td><td>0</td><td>1</td><td>4</td><td class="last">25</td></tr><tr valign="bottom"><td align="center" valign="bottom" class=" align_center">Â </td><td align="center" valign="bottom" class=" align_center">High Decrease</td><td align="center" valign="bottom" class=" align_center">Decrease</td><td align="center" valign="bottom" class=" align_center">No Opinion</td><td align="center" valign="bottom" class=" align_center">Increase</td><td align="center" valign="bottom" class=" align_center last">High Increase</td></tr><tr valign="top" class="last"><td>3. Change in training efficiency</td><td>0</td><td>0</td><td>0</td><td>5</td><td class="last">25</td></tr></tbody></table></div><ul class="references numeric-ordered-list" id="references-Section"><div class="author-infos-ref"><h2 id="figures">REFERENCES</h2><li id="CIT0001"><span><span class="hlFld-ContribAuthor">Alsina-Jurnetn, <span class="NLM_given-names">I.</span></span>, &amp; <span class="hlFld-ContribAuthor">Gutierrez-Maldonado, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">2010</span>). <span class="NLM_article-title">Influence of personality and individual abilities on the sense of presence experienced in anxiety triggering virtual environments</span>. <i>International Journal of HumanâComputer Studies</i>, <i>68</i>, <span class="NLM_fpage">788</span>â<span class="NLM_lpage">801</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0001&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2010.07.001" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0001&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000281509200013" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=788-801&author=I.+Alsina-Jurnetn&author=J.+Gutierrez-Maldonado&title=Influence+of+personality+and+individual+abilities+on+the+sense+of+presence+experienced+in+anxiety+triggering+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0001&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2010.07.001&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2010.07.001%26sid%3Dliteratum%253Atandf%26aulast%3DAlsina-Jurnetn%26aufirst%3DI.%26date%3D2010%26atitle%3DInfluence%2520of%2520personality%2520and%2520individual%2520abilities%2520on%2520the%2520sense%2520of%2520presence%2520experienced%2520in%2520anxiety%2520triggering%2520virtual%2520environments%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D68%26spage%3D788%26epage%3D801" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0002"><span><span class="hlFld-ContribAuthor">Argelaguet, <span class="NLM_given-names">F.</span></span>, <span class="hlFld-ContribAuthor">Kulik, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Kunert, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Andujar, <span class="NLM_given-names">C.</span></span>, &amp; <span class="hlFld-ContribAuthor">Froehlich, <span class="NLM_given-names">B.</span></span> (<span class="NLM_year">2011</span>). <span class="NLM_article-title">See-through techniques for referential awareness in collaborative virtual reality</span>. <i>International Journal of HumanâComputer Studies</i>, <i>69</i>, <span class="NLM_fpage">387</span>â<span class="NLM_lpage">400</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2011.01.003" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0002&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000291455100004" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=387-400&author=F.+Argelaguet&author=A.+Kulik&author=A.+Kunert&author=C.+Andujar&author=B.+Froehlich&title=See-through+techniques+for+referential+awareness+in+collaborative+virtual+reality" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2011.01.003&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2011.01.003%26sid%3Dliteratum%253Atandf%26aulast%3DArgelaguet%26aufirst%3DF.%26date%3D2011%26atitle%3DSee-through%2520techniques%2520for%2520referential%2520awareness%2520in%2520collaborative%2520virtual%2520reality%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D69%26spage%3D387%26epage%3D400" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0003"><span><span class="hlFld-ContribAuthor">Bach, <span class="NLM_given-names">C.</span></span>, &amp; <span class="hlFld-ContribAuthor">Scapin, <span class="NLM_given-names">D.</span></span> (<span class="NLM_year">2010</span>). <span class="NLM_article-title">Comparing inspections and user testing for the evaluation of virtual environments</span>. <i>International Journal of HumanâComputer Interaction</i>, <i>26</i>, <span class="NLM_fpage">786</span>â<span class="NLM_lpage">824</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2010.487195" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0003&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000280384300002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=786-824&author=C.+Bach&author=D.+Scapin&title=Comparing+inspections+and+user+testing+for+the+evaluation+of+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0003&amp;dbid=16384&amp;doi=10.1080%2F10447318.2010.487195&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2010.487195%26sid%3Dliteratum%253Atandf%26aulast%3DBach%26aufirst%3DC.%26date%3D2010%26atitle%3DComparing%2520inspections%2520and%2520user%2520testing%2520for%2520the%2520evaluation%2520of%2520virtual%2520environments%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D26%26spage%3D786%26epage%3D824" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0004"><span><span class="hlFld-ContribAuthor">Bacim, <span class="NLM_given-names">F.</span></span>, <span class="hlFld-ContribAuthor">Kopper, <span class="NLM_given-names">R.</span></span>, &amp; <span class="hlFld-ContribAuthor">Bowman, <span class="NLM_given-names">D.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Design and evaluation of 3D selection techniques based on progressive refinement</span>. <i>International Journal of HumanâComputer Studies</i>, <i>71</i>, <span class="NLM_fpage">785</span>â<span class="NLM_lpage">802</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0004&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2013.03.003" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0004&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000322058100003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=785-802&author=F.+Bacim&author=R.+Kopper&author=D.+Bowman&title=Design+and+evaluation+of+3D+selection+techniques+based+on+progressive+refinement" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0004&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2013.03.003&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2013.03.003%26sid%3Dliteratum%253Atandf%26aulast%3DBacim%26aufirst%3DF.%26date%3D2013%26atitle%3DDesign%2520and%2520evaluation%2520of%25203D%2520selection%2520techniques%2520based%2520on%2520progressive%2520refinement%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D71%26spage%3D785%26epage%3D802" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0005"><span><span class="hlFld-ContribAuthor">Bakera, <span class="NLM_given-names">R.</span></span>, <span class="hlFld-ContribAuthor">DâMello, <span class="NLM_given-names">S.</span></span>, <span class="hlFld-ContribAuthor">Rodrigo, <span class="NLM_given-names">M.</span></span>, &amp; <span class="hlFld-ContribAuthor">Graesser, <span class="NLM_given-names">A.</span></span> (<span class="NLM_year">2010</span>). <span class="NLM_article-title">Better to be frustrated than bored: The incidence, persistence, and impact of learners cognitiveaffective states during interactions with three different computer-based learning environments</span>. <i>International Journal of HumanâComputer Studies</i>, <i>68</i>, <span class="NLM_fpage">223</span>â<span class="NLM_lpage">241</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0005&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2009.12.003" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0005&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000276129500004" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=223-241&author=R.+Bakera&author=S.+D%E2%80%99Mello&author=M.+Rodrigo&author=A.+Graesser&title=Better+to+be+frustrated+than+bored%3A+The+incidence%2C+persistence%2C+and+impact+of+learners+cognitiveaffective+states+during+interactions+with+three+different+computer-based+learning+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0005&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2009.12.003&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2009.12.003%26sid%3Dliteratum%253Atandf%26aulast%3DBakera%26aufirst%3DR.%26date%3D2010%26atitle%3DBetter%2520to%2520be%2520frustrated%2520than%2520bored%253A%2520The%2520incidence%252C%2520persistence%252C%2520and%2520impact%2520of%2520learners%2520cognitiveaffective%2520states%2520during%2520interactions%2520with%2520three%2520different%2520computer-based%2520learning%2520environments%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D68%26spage%3D223%26epage%3D241" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0006"><span><span class="hlFld-ContribAuthor">Banosa, <span class="NLM_given-names">R.</span></span>, <span class="hlFld-ContribAuthor">Guillena, <span class="NLM_given-names">V.</span></span>, <span class="hlFld-ContribAuthor">Querob, <span class="NLM_given-names">S.</span></span>, <span class="hlFld-ContribAuthor">Garcia-Palacios, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Alcanizc, <span class="NLM_given-names">M.</span></span>, &amp; <span class="hlFld-ContribAuthor">Botellab, <span class="NLM_given-names">C.</span></span> (<span class="NLM_year">2011</span>). <span class="NLM_article-title">A virtual reality system for the treatment of stress-related disorders: A preliminary analysis of efficacy compared to a standard cognitive behavioral program</span>. <i>International Journal of HumanâComputer Studies</i>, <i>69</i>, <span class="NLM_fpage">602</span>â<span class="NLM_lpage">613</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0006&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2011.06.002" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0006&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000293484600005" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=602-613&author=R.+Banosa&author=V.+Guillena&author=S.+Querob&author=A.+Garcia-Palacios&author=M.+Alcanizc&author=C.+Botellab&title=A+virtual+reality+system+for+the+treatment+of+stress-related+disorders%3A+A+preliminary+analysis+of+efficacy+compared+to+a+standard+cognitive+behavioral+program" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0006&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2011.06.002&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2011.06.002%26sid%3Dliteratum%253Atandf%26aulast%3DBanosa%26aufirst%3DR.%26date%3D2011%26atitle%3DA%2520virtual%2520reality%2520system%2520for%2520the%2520treatment%2520of%2520stress-related%2520disorders%253A%2520A%2520preliminary%2520analysis%2520of%2520efficacy%2520compared%2520to%2520a%2520standard%2520cognitive%2520behavioral%2520program%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D69%26spage%3D602%26epage%3D613" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0007"><span><span class="hlFld-ContribAuthor">Bianchi-Berthouze, <span class="NLM_given-names">N.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Understanding the role of body movement in player engagement</span>. <i>HumanâComputer Interaction</i>, <i>28</i>, <span class="NLM_fpage">40</span>â<span class="NLM_lpage">75</span>. doi:10.1080/07370024.2012.688468<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/07370024.2012.688468" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0007&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000312333900002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=40-75&author=N.+Bianchi-Berthouze&title=Understanding+the+role+of+body+movement+in+player+engagement" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0007&amp;dbid=16384&amp;doi=10.1080%2F07370024.2012.688468&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F07370024.2012.688468%26sid%3Dliteratum%253Atandf%26aulast%3DBianchi-Berthouze%26aufirst%3DN.%26date%3D2013%26atitle%3DUnderstanding%2520the%2520role%2520of%2520body%2520movement%2520in%2520player%2520engagement%26jtitle%3DHuman%25E2%2580%2593Computer%2520Interaction%26volume%3D28%26spage%3D40%26epage%3D75" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0008"><span><span class="hlFld-ContribAuthor">Bocking, <span class="NLM_given-names">S.</span></span>, <span class="hlFld-ContribAuthor">Gysbers, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Wirth, <span class="NLM_given-names">W.</span></span>, <span class="hlFld-ContribAuthor">Klimmt, <span class="NLM_given-names">C.</span></span>, <span class="hlFld-ContribAuthor">Hartmann, <span class="NLM_given-names">T.</span></span>, <span class="hlFld-ContribAuthor">Schramm, <span class="NLM_given-names">H.</span></span>, <span class="hlFld-ContribAuthor">â¦</span>, <span class="hlFld-ContribAuthor">Vorderer, <span class="NLM_given-names">P.</span></span> (<span class="NLM_year">2004</span>). <span class="NLM_article-title">Theoretical and empirical support for distinctions between components and conditions of spatial presence</span>. <i>Proceedings of the VII International Workshop on Presence</i>, <span class="NLM_fpage">224</span>â<span class="NLM_lpage">231</span>.<span class="refLink-block">Â <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=224-231&author=S.+Bocking&author=A.+Gysbers&author=W.+Wirth&author=C.+Klimmt&author=T.+Hartmann&author=H.+Schramm&author=+%E2%80%A6&author=P.+Vorderer&title=Theoretical+and+empirical+support+for+distinctions+between+components+and+conditions+of+spatial+presence" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0008&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBocking%26aufirst%3DS.%26date%3D2004%26atitle%3DTheoretical%2520and%2520empirical%2520support%2520for%2520distinctions%2520between%2520components%2520and%2520conditions%2520of%2520spatial%2520presence%26jtitle%3DProceedings%2520of%2520the%2520VII%2520International%2520Workshop%2520on%2520Presence%26spage%3D224%26epage%3D231" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0009"><span><span class="hlFld-ContribAuthor">Brown, <span class="NLM_given-names">J.N.A.</span></span> (<span class="NLM_year">2015</span>). <span class="NLM_article-title">Once more, with feeling: Using haptics to preserve tactile memories</span>. <i>International Journal of HumanâComputer Interaction</i>, <i>31</i>, <span class="NLM_fpage">65</span>â<span class="NLM_lpage">71</span>. doi:10.1080/10447318.2014.959100<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2014.959100" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0009&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000346194300006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&pages=65-71&author=J.N.A.+Brown&title=Once+more%2C+with+feeling%3A+Using+haptics+to+preserve+tactile+memories" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0009&amp;dbid=16384&amp;doi=10.1080%2F10447318.2014.959100&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2014.959100%26sid%3Dliteratum%253Atandf%26aulast%3DBrown%26aufirst%3DJ.N.A.%26date%3D2015%26atitle%3DOnce%2520more%252C%2520with%2520feeling%253A%2520Using%2520haptics%2520to%2520preserve%2520tactile%2520memories%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D31%26spage%3D65%26epage%3D71" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0010"><span><span class="hlFld-ContribAuthor">Budziszewski, <span class="NLM_given-names">P.</span></span>, <span class="hlFld-ContribAuthor">Grabowski, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Milanowicz, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Jankowski, <span class="NLM_given-names">J.</span></span>, &amp; <span class="hlFld-ContribAuthor">Dwiarek, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">2011</span>). <span class="NLM_article-title">Designing a workplace for workers with motion disability with computer simulation and virtual reality techniques</span>. <i>International Journal on Disability and Human Development</i>, <i>10</i>, <span class="NLM_fpage">355</span>â<span class="NLM_lpage">358</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0010&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1515%2FIJDHD.2011.054" target="_blank">[Crossref]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=355-358&author=P.+Budziszewski&author=A.+Grabowski&author=M.+Milanowicz&author=J.+Jankowski&author=M.+Dwiarek&title=Designing+a+workplace+for+workers+with+motion+disability+with+computer+simulation+and+virtual+reality+techniques" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0010&amp;dbid=16384&amp;doi=10.1515%2FIJDHD.2011.054&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1515%252FIJDHD.2011.054%26sid%3Dliteratum%253Atandf%26aulast%3DBudziszewski%26aufirst%3DP.%26date%3D2011%26atitle%3DDesigning%2520a%2520workplace%2520for%2520workers%2520with%2520motion%2520disability%2520with%2520computer%2520simulation%2520and%2520virtual%2520reality%2520techniques%26jtitle%3DInternational%2520Journal%2520on%2520Disability%2520and%2520Human%2520Development%26volume%3D10%26spage%3D355%26epage%3D358" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0011"><span><span class="hlFld-ContribAuthor">Cai, <span class="NLM_given-names">H.</span></span>, &amp; <span class="hlFld-ContribAuthor">Lin, <span class="NLM_given-names">Y.</span></span> (<span class="NLM_year">2011</span>). <span class="NLM_article-title">Modeling of operators emotion and task performance in a virtual driving environment</span>. <i>International Journal of HumanâComputer Studies</i>, <i>69</i>, <span class="NLM_fpage">571</span>â<span class="NLM_lpage">586</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0011&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2011.05.003" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0011&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000293484600003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=571-586&author=H.+Cai&author=Y.+Lin&title=Modeling+of+operators+emotion+and+task+performance+in+a+virtual+driving+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0011&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2011.05.003&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2011.05.003%26sid%3Dliteratum%253Atandf%26aulast%3DCai%26aufirst%3DH.%26date%3D2011%26atitle%3DModeling%2520of%2520operators%2520emotion%2520and%2520task%2520performance%2520in%2520a%2520virtual%2520driving%2520environment%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D69%26spage%3D571%26epage%3D586" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0012"><span><span class="hlFld-ContribAuthor">Gallagher, <span class="NLM_given-names">A.</span></span>, &amp; <span class="hlFld-ContribAuthor">Cates, <span class="NLM_given-names">C.</span></span> (<span class="NLM_year">2004</span>). <span class="NLM_article-title">Virtual reality training for the operating room and cardiac catheterisation laboratory</span>. <i>The Lancet</i>, <i>364</i>, <span class="NLM_fpage">1538</span>â<span class="NLM_lpage">1540</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0012&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2FS0140-6736%2804%2917278-4" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0012&amp;dbid=8&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=15500900" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0012&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000224645500033" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=1538-1540&author=A.+Gallagher&author=C.+Cates&title=Virtual+reality+training+for+the+operating+room+and+cardiac+catheterisation+laboratory" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0012&amp;dbid=16384&amp;doi=10.1016%2FS0140-6736%2804%2917278-4&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252FS0140-6736%252804%252917278-4%26sid%3Dliteratum%253Atandf%26aulast%3DGallagher%26aufirst%3DA.%26date%3D2004%26atitle%3DVirtual%2520reality%2520training%2520for%2520the%2520operating%2520room%2520and%2520cardiac%2520catheterisation%2520laboratory%26jtitle%3DThe%2520Lancet%26volume%3D364%26spage%3D1538%26epage%3D1540" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0013"><span><span class="hlFld-ContribAuthor">Grabowski, <span class="NLM_given-names">A.</span></span>, &amp; <span class="hlFld-ContribAuthor">Jankowski, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">2015</span>). <span class="NLM_article-title">Virtual reality-based pilot training for underground coal miners</span>. <i>Safety Science</i>, <i>72</i>, <span class="NLM_fpage">310</span>â<span class="NLM_lpage">314</span>. doi:10.1016/j.ssci.2014.09.017<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0013&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ssci.2014.09.017" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0013&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000348012100033" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&pages=310-314&author=A.+Grabowski&author=J.+Jankowski&title=Virtual+reality-based+pilot+training+for+underground+coal+miners" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0013&amp;dbid=16384&amp;doi=10.1016%2Fj.ssci.2014.09.017&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ssci.2014.09.017%26sid%3Dliteratum%253Atandf%26aulast%3DGrabowski%26aufirst%3DA.%26date%3D2015%26atitle%3DVirtual%2520reality-based%2520pilot%2520training%2520for%2520underground%2520coal%2520miners%26jtitle%3DSafety%2520Science%26volume%3D72%26spage%3D310%26epage%3D314" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0014"><span><span class="hlFld-ContribAuthor">Grabowski, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Jankowski, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">DÅºwiarek, <span class="NLM_given-names">M.</span></span>, &amp; <span class="hlFld-ContribAuthor">KosiÅski, <span class="NLM_given-names">R.</span></span> (<span class="NLM_year">2014</span>). <span class="NLM_article-title">Stereovision safety system for identifying workers presence: Results of tests</span>. <i>International Journal of Occupational Safety and Ergonomics</i>, <i>20</i>, <span class="NLM_fpage">3</span>â<span class="NLM_lpage">9</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10803548.2014.11077024" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0014&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000333479800001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=3-9&author=A.+Grabowski&author=J.+Jankowski&author=M.+D%C5%BAwiarek&author=R.+Kosi%C5%84ski&title=Stereovision+safety+system+for+identifying+workers+presence%3A+Results+of+tests" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0014&amp;dbid=16384&amp;doi=10.1080%2F10803548.2014.11077024&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10803548.2014.11077024%26sid%3Dliteratum%253Atandf%26aulast%3DGrabowski%26aufirst%3DA.%26date%3D2014%26atitle%3DStereovision%2520safety%2520system%2520for%2520identifying%2520workers%2520presence%253A%2520Results%2520of%2520tests%26jtitle%3DInternational%2520Journal%2520of%2520Occupational%2520Safety%2520and%2520Ergonomics%26volume%3D20%26spage%3D3%26epage%3D9" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0015"><span><span class="hlFld-ContribAuthor">Juang, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Hung, <span class="NLM_given-names">W.</span></span>, &amp; <span class="hlFld-ContribAuthor">Kang, <span class="NLM_given-names">S.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Simcrane 3d+: A crane simulator with kinesthetic and stereoscopic</span>. <i>Advanced Engineering Informatics</i>, <i>27</i>, <span class="NLM_fpage">506</span>â<span class="NLM_lpage">518</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0015&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.aei.2013.05.002" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0015&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000328525700010" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=506-518&author=J.+Juang&author=W.+Hung&author=S.+Kang&title=Simcrane+3d%2B%3A+A+crane+simulator+with+kinesthetic+and+stereoscopic" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0015&amp;dbid=16384&amp;doi=10.1016%2Fj.aei.2013.05.002&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.aei.2013.05.002%26sid%3Dliteratum%253Atandf%26aulast%3DJuang%26aufirst%3DJ.%26date%3D2013%26atitle%3DSimcrane%25203d%252B%253A%2520A%2520crane%2520simulator%2520with%2520kinesthetic%2520and%2520stereoscopic%26jtitle%3DAdvanced%2520Engineering%2520Informatics%26volume%3D27%26spage%3D506%26epage%3D518" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0016"><span><span class="hlFld-ContribAuthor">Klemmera, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Landayb, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">2009</span>). <span class="NLM_article-title">Toolkit support for integrating physical and digital interactions</span>. <i>HumanâComputer Interaction</i>, <i>24</i>, <span class="NLM_fpage">315</span>â<span class="NLM_lpage">366</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/07370020902990428" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0016&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000266871300002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=315-366&author=S.+Klemmera&author=J.+Landayb&title=Toolkit+support+for+integrating+physical+and+digital+interactions" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0016&amp;dbid=16384&amp;doi=10.1080%2F07370020902990428&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F07370020902990428%26sid%3Dliteratum%253Atandf%26aulast%3DKlemmera%26aufirst%3DS.%26date%3D2009%26atitle%3DToolkit%2520support%2520for%2520integrating%2520physical%2520and%2520digital%2520interactions%26jtitle%3DHuman%25E2%2580%2593Computer%2520Interaction%26volume%3D24%26spage%3D315%26epage%3D366" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0017"><span><span class="hlFld-ContribAuthor">Lee, <span class="NLM_given-names">J. Y.</span></span>, <span class="hlFld-ContribAuthor">Bahn, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Nam, <span class="NLM_given-names">C. S.</span></span> (<span class="NLM_year">2014</span>). <span class="NLM_article-title">Use of reference frame and movement pattern in haptically enhanced 3d virtual environment</span>. <i>International Journal of HumanâComputer Interaction</i>, <i>30</i>, <span class="NLM_fpage">11</span>. doi:10.1080/10447318.2014.941275<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2014.941275" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0017&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000342443300006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=11&author=J.+Y.+Lee&author=S.+Bahn&author=C.+S.+Nam&title=Use+of+reference+frame+and+movement+pattern+in+haptically+enhanced+3d+virtual+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0017&amp;dbid=16384&amp;doi=10.1080%2F10447318.2014.941275&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2014.941275%26sid%3Dliteratum%253Atandf%26aulast%3DLee%26aufirst%3DJ.%2520Y.%26date%3D2014%26atitle%3DUse%2520of%2520reference%2520frame%2520and%2520movement%2520pattern%2520in%2520haptically%2520enhanced%25203d%2520virtual%2520environment%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D30%26spage%3D11" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0018"><span><span class="hlFld-ContribAuthor">Lee, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Kim, <span class="NLM_given-names">G.</span></span> (<span class="NLM_year">2008</span>). <span class="NLM_article-title">Effects of haptic feedback, stereoscopy, and image resolution on performance and presence in remote navigation</span>. <i>International Journal of HumanâComputer Studies</i>, <i>66</i>, <span class="NLM_fpage">701</span>â<span class="NLM_lpage">717</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0018&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2008.05.001" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0018&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000259707100001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=701-717&author=S.+Lee&author=G.+Kim&title=Effects+of+haptic+feedback%2C+stereoscopy%2C+and+image+resolution+on+performance+and+presence+in+remote+navigation" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0018&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2008.05.001&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2008.05.001%26sid%3Dliteratum%253Atandf%26aulast%3DLee%26aufirst%3DS.%26date%3D2008%26atitle%3DEffects%2520of%2520haptic%2520feedback%252C%2520stereoscopy%252C%2520and%2520image%2520resolution%2520on%2520performance%2520and%2520presence%2520in%2520remote%2520navigation%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D66%26spage%3D701%26epage%3D717" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0019"><span><span class="hlFld-ContribAuthor">Mateu, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Lasala, <span class="NLM_given-names">M.J.</span></span>, &amp; <span class="hlFld-ContribAuthor">Alamn, <span class="NLM_given-names">X.</span></span> (<span class="NLM_year">2014</span>). <span class="NLM_article-title">Virtualtouch: A tool for developing mixed reality educational applications and an example of use for inclusive education</span>. <i>International Journal of HumanâComputer Interaction</i>, <i>30</i>, <span class="NLM_fpage">815</span>â<span class="NLM_lpage">828</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2014.927278" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0019&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000340174000006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=815-828&author=J.+Mateu&author=M.J.+Lasala&author=X.+Alamn&title=Virtualtouch%3A+A+tool+for+developing+mixed+reality+educational+applications+and+an+example+of+use+for+inclusive+education" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0019&amp;dbid=16384&amp;doi=10.1080%2F10447318.2014.927278&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2014.927278%26sid%3Dliteratum%253Atandf%26aulast%3DMateu%26aufirst%3DJ.%26date%3D2014%26atitle%3DVirtualtouch%253A%2520A%2520tool%2520for%2520developing%2520mixed%2520reality%2520educational%2520applications%2520and%2520an%2520example%2520of%2520use%2520for%2520inclusive%2520education%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D30%26spage%3D815%26epage%3D828" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0020"><span><span class="hlFld-ContribAuthor">Millet, <span class="NLM_given-names">G.</span></span>, <span class="hlFld-ContribAuthor">Lecuyer, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Burkhardt, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Haliyoa, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Regniera, <span class="NLM_given-names">S.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Haptics and graphic analogies for the understanding of atomic force microscopy</span>. <i>International Journal of HumanâComputer Studies</i>, <i>71</i>, <span class="NLM_fpage">608</span>â<span class="NLM_lpage">626</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2012.12.005" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0020&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000318382300006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=608-626&author=G.+Millet&author=A.+Lecuyer&author=J.+Burkhardt&author=S.+Haliyoa&author=S.+Regniera&title=Haptics+and+graphic+analogies+for+the+understanding+of+atomic+force+microscopy" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2012.12.005&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2012.12.005%26sid%3Dliteratum%253Atandf%26aulast%3DMillet%26aufirst%3DG.%26date%3D2013%26atitle%3DHaptics%2520and%2520graphic%2520analogies%2520for%2520the%2520understanding%2520of%2520atomic%2520force%2520microscopy%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D71%26spage%3D608%26epage%3D626" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0021"><span><span class="hlFld-ContribAuthor">Prachyabrued, <span class="NLM_given-names">M.</span></span>, &amp; <span class="hlFld-ContribAuthor">Borst, <span class="NLM_given-names">C.</span></span> (<span class="NLM_year">2012</span>). <span class="NLM_article-title">Virtual grasp release method and evaluation</span>. <i>International Journal of HumanâComputer Studies</i>, <i>70</i>, <span class="NLM_fpage">828</span>â<span class="NLM_lpage">848</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0021&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2012.06.002" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0021&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000310049200004" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=828-848&author=M.+Prachyabrued&author=C.+Borst&title=Virtual+grasp+release+method+and+evaluation" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0021&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2012.06.002&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2012.06.002%26sid%3Dliteratum%253Atandf%26aulast%3DPrachyabrued%26aufirst%3DM.%26date%3D2012%26atitle%3DVirtual%2520grasp%2520release%2520method%2520and%2520evaluation%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D70%26spage%3D828%26epage%3D848" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0022"><span><span class="hlFld-ContribAuthor">Ragan, <span class="NLM_given-names">E.</span></span>, <span class="hlFld-ContribAuthor">Sowndararajan, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Kopper, <span class="NLM_given-names">R.</span></span>, &amp; <span class="hlFld-ContribAuthor">Bowman, <span class="NLM_given-names">D.</span></span> (<span class="NLM_year">2010</span>). <span class="NLM_article-title">The effects of higher levels of immersion on procedure memorization performance and implications for educational virtual environments</span>. <i>Presence</i>, <i>19</i>, <span class="NLM_fpage">527</span>â<span class="NLM_lpage">543</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1162%2Fpres_a_00016" target="_blank">[Crossref]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=527-543&author=E.+Ragan&author=A.+Sowndararajan&author=R.+Kopper&author=D.+Bowman&title=The+effects+of+higher+levels+of+immersion+on+procedure+memorization+performance+and+implications+for+educational+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=16384&amp;doi=10.1162%2Fpres_a_00016&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1162%252Fpres_a_00016%26sid%3Dliteratum%253Atandf%26aulast%3DRagan%26aufirst%3DE.%26date%3D2010%26atitle%3DThe%2520effects%2520of%2520higher%2520levels%2520of%2520immersion%2520on%2520procedure%2520memorization%2520performance%2520and%2520implications%2520for%2520educational%2520virtual%2520environments%26jtitle%3DPresence%26volume%3D19%26spage%3D527%26epage%3D543" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0023"><span><span class="hlFld-ContribAuthor">Rantala, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Salminen, <span class="NLM_given-names">K.</span></span>, <span class="hlFld-ContribAuthor">Raisamo, <span class="NLM_given-names">R.</span></span>, &amp; <span class="hlFld-ContribAuthor">Surakka, <span class="NLM_given-names">V.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Touch gestures in communicating emotional intention via vibrotactile stimulation</span>. <i>International Journal of HumanâComputer Studies</i>, <i>71</i>, <span class="NLM_fpage">679</span>â<span class="NLM_lpage">690</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0023&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2013.02.004" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0023&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000319792000003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=679-690&author=J.+Rantala&author=K.+Salminen&author=R.+Raisamo&author=V.+Surakka&title=Touch+gestures+in+communicating+emotional+intention+via+vibrotactile+stimulation" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0023&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2013.02.004&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2013.02.004%26sid%3Dliteratum%253Atandf%26aulast%3DRantala%26aufirst%3DJ.%26date%3D2013%26atitle%3DTouch%2520gestures%2520in%2520communicating%2520emotional%2520intention%2520via%2520vibrotactile%2520stimulation%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D71%26spage%3D679%26epage%3D690" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0024"><span><span class="hlFld-ContribAuthor">Steed, <span class="NLM_given-names">A.</span></span>, &amp; <span class="hlFld-ContribAuthor">Parker, <span class="NLM_given-names">C.</span></span> (<span class="NLM_year">2005</span>). <span class="NLM_article-title">Evaluating effectiveness of interaction techniques across immersive virtual environmental systems</span>. <i>Presence Teleoperators and Virtual Environments</i>, <i>14</i>, <span class="NLM_fpage">511</span>â<span class="NLM_lpage">527</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0024&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1162%2F105474605774918750" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0024&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000202949400003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&pages=511-527&author=A.+Steed&author=C.+Parker&title=Evaluating+effectiveness+of+interaction+techniques+across+immersive+virtual+environmental+systems" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0024&amp;dbid=16384&amp;doi=10.1162%2F105474605774918750&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1162%252F105474605774918750%26sid%3Dliteratum%253Atandf%26aulast%3DSteed%26aufirst%3DA.%26date%3D2005%26atitle%3DEvaluating%2520effectiveness%2520of%2520interaction%2520techniques%2520across%2520immersive%2520virtual%2520environmental%2520systems%26jtitle%3DPresence%2520Teleoperators%2520and%2520Virtual%2520Environments%26volume%3D14%26spage%3D511%26epage%3D527" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0025"><span><span class="hlFld-ContribAuthor">Sylaioua, <span class="NLM_given-names">S.</span></span>, <span class="hlFld-ContribAuthor">Mania, <span class="NLM_given-names">K.</span></span>, <span class="hlFld-ContribAuthor">Karoulis, <span class="NLM_given-names">A.</span></span>, &amp; <span class="hlFld-ContribAuthor">White, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">2010</span>). <span class="NLM_article-title">Exploring the relationship between presence and enjoyment in a virtual museum</span>. <i>International Journal of HumanâComputer Studies</i>, <i>68</i>, <span class="NLM_fpage">243</span>â<span class="NLM_lpage">253</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0025&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2009.11.002" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0025&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000276710100001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=243-253&author=S.+Sylaioua&author=K.+Mania&author=A.+Karoulis&author=M.+White&title=Exploring+the+relationship+between+presence+and+enjoyment+in+a+virtual+museum" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0025&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2009.11.002&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2009.11.002%26sid%3Dliteratum%253Atandf%26aulast%3DSylaioua%26aufirst%3DS.%26date%3D2010%26atitle%3DExploring%2520the%2520relationship%2520between%2520presence%2520and%2520enjoyment%2520in%2520a%2520virtual%2520museum%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D68%26spage%3D243%26epage%3D253" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0026"><span><span class="hlFld-ContribAuthor">Tichon, <span class="NLM_given-names">J.</span></span>, &amp; <span class="hlFld-ContribAuthor">Burgess-Limerick, <span class="NLM_given-names">R.</span></span> (<span class="NLM_year">2011</span>). <span class="NLM_article-title">A review of virtual reality as a medium for safety related training in mining</span>. <i>Journal of Health Safety Research Practice</i>, <i>3</i>, <span class="NLM_fpage">33</span>â<span class="NLM_lpage">40</span>.<span class="refLink-block">Â <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=33-40&author=J.+Tichon&author=R.+Burgess-Limerick&title=A+review+of+virtual+reality+as+a+medium+for+safety+related+training+in+mining" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0026&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DTichon%26aufirst%3DJ.%26date%3D2011%26atitle%3DA%2520review%2520of%2520virtual%2520reality%2520as%2520a%2520medium%2520for%2520safety%2520related%2520training%2520in%2520mining%26jtitle%3DJournal%2520of%2520Health%2520Safety%2520Research%2520Practice%26volume%3D3%26spage%3D33%26epage%3D40" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0027"><span><span class="hlFld-ContribAuthor">Vatavun, <span class="NLM_given-names">R.-D.</span></span>, &amp; <span class="hlFld-ContribAuthor">Zait, <span class="NLM_given-names">I. A.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Automatic recognition of object size and shape via user-dependent measurements of the grasping hand</span>. <i>International Journal of HumanâComputer Studies</i>, <i>71</i>, <span class="NLM_fpage">590</span>â<span class="NLM_lpage">607</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0027&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1016%2Fj.ijhcs.2013.01.002" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0027&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000318382300005" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=590-607&author=R.-D.+Vatavun&author=I.+A.+Zait&title=Automatic+recognition+of+object+size+and+shape+via+user-dependent+measurements+of+the+grasping+hand" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0027&amp;dbid=16384&amp;doi=10.1016%2Fj.ijhcs.2013.01.002&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.ijhcs.2013.01.002%26sid%3Dliteratum%253Atandf%26aulast%3DVatavun%26aufirst%3DR.-D.%26date%3D2013%26atitle%3DAutomatic%2520recognition%2520of%2520object%2520size%2520and%2520shape%2520via%2520user-dependent%2520measurements%2520of%2520the%2520grasping%2520hand%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Studies%26volume%3D71%26spage%3D590%26epage%3D607" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0028"><span><span class="hlFld-ContribAuthor">Velaz, <span class="NLM_given-names">Y.</span></span>, <span class="hlFld-ContribAuthor">Lozano-Rodero, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Suescun, <span class="NLM_given-names">A.</span></span>, &amp; <span class="hlFld-ContribAuthor">Gutierrez, <span class="NLM_given-names">T.</span></span> (<span class="NLM_year">2013</span>). <span class="NLM_article-title">Natural and hybrid bimanual interaction for virtual assembly tasks</span>. <i>Virtual Reality</i>, <i>3</i>, <span class="NLM_fpage">161</span>â<span class="NLM_lpage">171</span>. doi:10.1007/s10055â013â0240ây<span class="refLink-block">Â <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=161-171&author=Y.+Velaz&author=A.+Lozano-Rodero&author=A.+Suescun&author=T.+Gutierrez&title=Natural+and+hybrid+bimanual+interaction+for+virtual+assembly+tasks" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0028&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DVelaz%26aufirst%3DY.%26date%3D2013%26atitle%3DNatural%2520and%2520hybrid%2520bimanual%2520interaction%2520for%2520virtual%2520assembly%2520tasks%26jtitle%3DVirtual%2520Reality%26volume%3D3%26spage%3D161%26epage%3D171" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0029"><span><span class="hlFld-ContribAuthor">Vorderer, <span class="NLM_given-names">P.</span></span>, <span class="hlFld-ContribAuthor">Wirth, <span class="NLM_given-names">W.</span></span>, <span class="hlFld-ContribAuthor">Gouveia, <span class="NLM_given-names">F.</span></span>, <span class="hlFld-ContribAuthor">Biocca, <span class="NLM_given-names">F.</span></span>, <span class="hlFld-ContribAuthor">Saari, <span class="NLM_given-names">T.</span></span>, <span class="hlFld-ContribAuthor">Jncke, <span class="NLM_given-names">L.</span></span>, <span class="hlFld-ContribAuthor">â¦</span>, <span class="hlFld-ContribAuthor">Jncke, <span class="NLM_given-names">P.</span></span> (<span class="NLM_year">2004</span>). <i>Development of the Mec Spatial Presence Questionnaire (MEC-SPQ)</i>. <span class="NLM_publisher-name">Report to the European Commission, Information, Society and Technology (IST) Program, Project Presence: MEC (IST-2001-37661)</span>.<span class="refLink-block">Â <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&author=P.+Vorderer&author=W.+Wirth&author=F.+Gouveia&author=F.+Biocca&author=T.+Saari&author=L.+Jncke&author=+%E2%80%A6&author=P.+Jncke&title=Development+of+the+Mec+Spatial+Presence+Questionnaire+%28MEC-SPQ%29" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0029&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DVorderer%26aufirst%3DP.%26date%3D2004%26btitle%3DDevelopment%2520of%2520the%2520Mec%2520Spatial%2520Presence%2520Questionnaire%2520%2528MEC-SPQ%2529%26pub%3DReport%2520to%2520the%2520European%2520Commission%252C%2520Information%252C%2520Society%2520and%2520Technology%2520%2528IST%2529%2520Program%252C%2520Project%2520Presence%253A%2520MEC%2520%2528IST-2001-37661%2529" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0030"><span><span class="hlFld-ContribAuthor">Witmer, <span class="NLM_given-names">B.</span></span>, &amp; <span class="hlFld-ContribAuthor">Singer, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">1998</span>). <span class="NLM_article-title">Measuring presence in virtual environments: A presence questionnaire</span>. <i>Presence: Teleoperators and Virtual Enviroments</i>, <i>7</i>, <span class="NLM_fpage">225</span>â<span class="NLM_lpage">240</span>.<span class="refLink-block">Â <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0030&amp;dbid=16&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=10.1162%2F105474698565686" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0030&amp;dbid=128&amp;doi=10.1080%2F10447318.2015.1067497&amp;key=000074183200001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">,Â <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1998&pages=225-240&author=B.+Witmer&author=M.+Singer&title=Measuring+presence+in+virtual+environments%3A+A+presence+questionnaire" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0030&amp;dbid=16384&amp;doi=10.1162%2F105474698565686&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1162%252F105474698565686%26sid%3Dliteratum%253Atandf%26aulast%3DWitmer%26aufirst%3DB.%26date%3D1998%26atitle%3DMeasuring%2520presence%2520in%2520virtual%2520environments%253A%2520A%2520presence%2520questionnaire%26jtitle%3DPresence%253A%2520Teleoperators%2520and%2520Virtual%2520Enviroments%26volume%3D7%26spage%3D225%26epage%3D240" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0031"><span><span class="hlFld-ContribAuthor">Zaldivar-Colado, <span class="NLM_given-names">U.</span></span>, &amp; <span class="hlFld-ContribAuthor">Garbaya, <span class="NLM_given-names">S.</span></span> (<span class="NLM_year">2009</span>). <span class="NLM_article-title">Virtual assembly environment modelling</span>. <i>Proceedings of the World Conference on Innovative VR 2009</i>, <span class="NLM_fpage">311</span>â<span class="NLM_lpage">321</span>.<span class="refLink-block">Â <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=311-321&author=U.+Zaldivar-Colado&author=S.+Garbaya&title=Virtual+assembly+environment+modelling" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0031&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DZaldivar-Colado%26aufirst%3DU.%26date%3D2009%26atitle%3DVirtual%2520assembly%2520environment%2520modelling%26jtitle%3DProceedings%2520of%2520the%2520World%2520Conference%2520on%2520Innovative%2520VR%25202009%26spage%3D311%26epage%3D321" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li></div></ul><div class="author-infos"><h2>Additional information</h2><h3>Author information</h3><div id="B0001" class="addAuthorInfo"><span class="AuthorInfoData"><h4><span class="NLM_given-names">Andrzej</span> Grabowski</h4><div></div><b>Andrzej Grabowski</b> is a physicist and computer scientist with an interest in human behavior in virtual environments; he is also the head of the Virtual Reality Laboratory and a professor at the Department of Safety Engineering of the Central Institute for Labour ProtectionâNational Research Institute.</span></div></div><div class="response"><div class="sub-article-title"></div></div>
</article>
</div>
<div class="tab tab-pane" id="relatedContent">
</div>
<div class="tab tab-pane " id="metrics-content">
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics" data-pb-dropzone="publicationContentDropZoneMetrics">
<div class="widget literatumArticleMetricsWidget none  widget-none" id="00886058-9b49-4cdf-9f1e-deb78b7818c3">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="articleMetricsContainer">
<div class="content fullView">
<h2>
Article Metrics
</h2>
<div class="section views">
<div class="title">
Views
</div>
<div class="circle">
<span class="value">225</span>
</div>
</div>
<div class="section citations">
<div class="title">
Citations
</div>
<a href="/doi/citedby/10.1080/10447318.2015.1067497" class="circle crossRef disableLink" target="_blank">
<span>
Crossref
</span>
<span class="value">0</span>
</a>
<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=CitingArticles&KeyUT=000366419600001" target="_blank" class="circle webOfScience disableLink">
<span>
Web of Science
</span>
<span class="value">0</span>
</a>
<a href="http://www.scopus.com/inward/citedby.url?partnerID=HzOxMe3b&scp=84949463357" target="_blank" class="circle scopus disableLink">
<span>
Scopus
</span>
<span class="value">0</span>
</a>
</div>
<div class="section altmetric-container">
<div class="title">
Altmetric
</div>
<script type="text/javascript">
    TandfUtils.appendScript(document.head, 'https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js', 'altmetric-embed-src', true, true);
</script>
<div class="metrics-badge linkToPopup">
<div class='altmetric-embed' data-badge-type='medium-donut' data-badge-details='right' data-condensed='true' data-template="tandf" data-hide-no-mentions="false" data-doi="10.1080/10447318.2015.1067497">
</div>
</div>
</div>
<div class="altmetricsPopup"></div>
</div>
</div>
<div class="metricsLinks">
<a href="/article-metrics">Article metrics information</a>
<br>
<a href="/article-citations-disclaimer">Disclaimer for citing articles</a>
</div></div>
</div>
</div>
</div>
</div>
<div class="access__limit" data-pb-dropzone="accessLimitPage">
</div>
</div>
</div>
</div>
<input id="viewLargeImageCaption" type="hidden" value="View Large Image" /></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-4 ">
<div class="contents" data-pb-dropzone="contents2">
<div class="widget general-bookmark-share none  widget-none  widget-compact-all" id="c8494935-e102-4ff5-9395-4ffa44a77f1c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<ul>
<li>
<div class="addthis_toolbox addthis_20x20_style">
<div class="custom_images">
<a class="addthis_button_twitter">
<span class="at-icon-twitter"></span>
</a>
<a class="addthis_button_facebook">
<span class="at-icon-facebook"></span>
</a>
<a class="addthis_button_email">
<span class="at-icon-email"></span>
</a>
<a class="addthis_button_none">
<span class="at-icon-none"></span>
</a>
<a class="addthis_button_compact"><span class="at-icon-wrapper"></span>
<span aria-describedby="shareOptions-description">
<span class="off-screen" id="shareOptions-description">More Share Options</span>
</span>
</a>
</div>
</div>
</li>
</ul>
<script type="text/javascript">
    
    var script = document.createElement('script');
    script.type='text/javascript';
    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';
    script.async = true;
    $('head').append(script)
</script>
</div>
</div>
</div>
<div class="widget general-html none  widget-none" id="16111d74-c554-42b2-a277-f2727ad2b285">
<div class="wrapped ">
<div class="widget-body body body-none ">&nbsp;</div>
</div>
</div>
<div class="widget general-html none  widget-none  widget-compact-all" id="649c3793-a589-4dfb-8de2-9b6cbc4dc15b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script defer src='//js.trendmd.com/trendmd.min.js' data-trendmdconfig='{"element":"#trendmd-suggestions"}'></script></div>
</div>
</div>
<div class="widget graphQueryWidget none  widget-none  widget-compact-all" id="6583d550-25db-458c-9c81-291f5c88b6ee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="trendmd-suggestions"></div></div>
</div>
</div>
<div class="widget pbOptimizerWidget none  widget-none  widget-compact-all" id="25efeb89-6948-4246-800a-5e246009698d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div data-optimizer data-widget-id="25efeb89-6948-4246-800a-5e246009698d" id="widget-25efeb89-6948-4246-800a-5e246009698d" data-observer>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget pageFooter none  widget-none  widget-compact-all" id="d97c173f-d838-4de1-bbd7-ed69f0d36a91">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><footer class="page-footer">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all" id="1f15adc0-4a59-4d27-93fe-8cbb14a5108a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-1-1 fit-padding">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget topicalIndex none  widget-none  widget-compact-all" id="9298c7a6-6903-4607-8380-4c83e2b7142f">
<div class="wrapped ">
<h1 class="widget-header header-none  header-compact-all">Browse journals by subject</h1>
<div class="widget-body body body-none  body-compact-all"><div class="topicalIndexBrowsingTips" data-pb-dropzone="topicalIndexBrowsingTips">
<div class="widget general-html none  widget-none  widget-compact-all" id="1ec3bad2-243b-45a9-a59a-5aceb80fc5a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a class="nav-top" href="#top">Back to top <span class="fa fa-angle-up"></span></a></div>
</div>
</div>
</div>
<div class="container">
<ul>
<li>
<a href="/topic/allsubjects/as?target=topic&amp;ConceptID=4251">Area Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ar?target=topic&amp;ConceptID=4250">Arts</a>
</li>
<li>
<a href="/topic/allsubjects/be?target=topic&amp;ConceptID=4252">Behavioral Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/bs?target=topic&amp;ConceptID=4253">Bioscience</a>
</li>
<li>
<a href="/topic/allsubjects/bu?target=topic&amp;ConceptID=4254">Built Environment</a>
</li>
<li>
<a href="/topic/allsubjects/cs?target=topic&amp;ConceptID=4256">Communication Studies</a>
</li>
<li>
<a href="/topic/allsubjects/cm?target=topic&amp;ConceptID=4255">Computer Science</a>
</li>
<li>
<a href="/topic/allsubjects/ds?target=topic&amp;ConceptID=4257">Development Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ea?target=topic&amp;ConceptID=4258">Earth Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/eb?target=topic&amp;ConceptID=4259">Economics, Finance, Business & Industry</a>
</li>
<li>
<a href="/topic/allsubjects/ed?target=topic&amp;ConceptID=4261">Education</a>
</li>
<li>
<a href="/topic/allsubjects/ec?target=topic&amp;ConceptID=4260">Engineering & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ag?target=topic&amp;ConceptID=4248">Environment & Agriculture</a>
</li>
<li>
<a href="/topic/allsubjects/es?target=topic&amp;ConceptID=4262">Environment and Sustainability</a>
</li>
<li>
<a href="/topic/allsubjects/fs?target=topic&amp;ConceptID=4263">Food Science & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ge?target=topic&amp;ConceptID=4264">Geography</a>
</li>
<li>
<a href="/topic/allsubjects/hs?target=topic&amp;ConceptID=4266">Health and Social Care</a>
</li>
<li>
<a href="/topic/allsubjects/hu?target=topic&amp;ConceptID=4267">Humanities</a>
 </li>
<li>
<a href="/topic/allsubjects/if?target=topic&amp;ConceptID=4268">Information Science</a>
</li>
<li>
<a href="/topic/allsubjects/la?target=topic&amp;ConceptID=4269">Language & Literature</a>
</li>
<li>
<a href="/topic/allsubjects/lw?target=topic&amp;ConceptID=4270">Law</a>
</li>
<li>
<a href="/topic/allsubjects/ma?target=topic&amp;ConceptID=4271">Mathematics & Statistics</a>
</li>
<li>
<a href="/topic/allsubjects/me?target=topic&amp;ConceptID=4272">Medicine, Dentistry, Nursing & Allied Health</a>
</li>
<li>
<a href="/topic/allsubjects/ah?target=topic&amp;ConceptID=4249">Museum and Heritage Studies</a>
</li>
<li>
<a href="/topic/allsubjects/pc?target=topic&amp;ConceptID=4273">Physical Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/pi?target=topic&amp;ConceptID=4274">Politics & International Relations</a>
</li>
<li>
<a href="/topic/allsubjects/sn?target=topic&amp;ConceptID=4278">Social Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/sl?target=topic&amp;ConceptID=4277">Sports and Leisure</a>
</li>
<li>
<a href="/topic/allsubjects/sp?target=topic&amp;ConceptID=4279">Tourism, Hospitality and Events</a>
</li>
<li>
<a href="/topic/allsubjects/us?target=topic&amp;ConceptID=4280">Urban Studies</a>
</li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none footer-links widget-none  widget-compact-horizontal" id="64a44adf-45ed-4da3-be26-ef25beb9dbee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="6918e9df-910a-4206-9bd0-1a02bc17f740">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="aa9510dd-52ed-4b74-8211-fb510cd9468e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Information for</h3>
<ul>
<li><a href="http://authorservices.taylorandfrancis.com/">Authors</a></li>
<li><a href="http://editorresources.taylorandfrancisgroup.com/">Editors</a></li>
<li><a href="/page/librarians">Librarians</a></li>
<li><a href="/societies">Societies</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Open access</h3>
<ul>
<li><a href="/openaccess">Overview</a></li>
<li><a href="/openaccess/openjournals">Open journals</a></li>
<li><a href="/openaccess/openselect">Open Select</a></li>
<li><a href="https://www.cogentoa.com/">Cogent OA</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents1">

<div class="widget responsive-layout none footer-responsive-container widget-none" id="fc564559-f496-499c-87c7-d851f371f061">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="f3fb3d36-db42-4373-9d0e-432958bf2fbc">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Help and info</h3>
<ul>
<li><a href="https://help.tandfonline.com">Help & contact</a></li>
<li><a href="https://newsroom.taylorandfrancisgroup.com/">Newsroom</a></li>
<li><a href="https://taylorandfrancis.com/partnership/commercial/">Commercial services</a></li>
<li><a href="/action/showPublications?pubType=journal">All journals</a></li>
<li><a href="https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot">Books</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="914433f6-0ea6-4a47-9781-07564061be86">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-social-label">
<h3>Keep up to date</h3>
</div>
<div class="font-size-correction-sml">Register to receive personalised research and resources by email</div>
<div class="bs">
<div class="pull-left links font-size-correction">
<a class="font-size-correction-link" href="https://taylorandfrancis.formstack.com/forms/tfoguest_signup"><i class="fa fa-envelope-square" title="Register to receive personalised research and resources by email"></i>Sign me up</a>
</div></div></div>
</div>
</div>
<div class="widget literatumSocialLinks none  widget-none  widget-compact-all" id="3b6a5e53-cd62-452f-adc1-92e187a0849d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="bs">
<div class="pull-left links">
<a href="http://facebook.com/TaylorandFrancisGroup">
<i class="icon-facebook" title="Taylor and Francis Group Facebook page" aria-hidden="true" role="button"></i>
<span aria-describedby="fb-description">
<span class="off-screen" id="fb-description">Taylor and Francis Group Facebook page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="https://twitter.com/tandfonline">
<i class="fa fa-twitter-square" title="Taylor and Francis Group Twitter page" aria-hidden="true" role="button"></i>
<span aria-describedby="twitter-description">
<span class="off-screen" id="twitter-description">Taylor and Francis Group Twitter page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://linkedin.com/company/taylor-&-francis-group">
<i class="fa fa-linkedin-square" title="Taylor and Francis Group LinkedIn page" aria-hidden="true" role="button"></i>
<span aria-describedby="linkedin-description">
<span class="off-screen" id="linkedin-description">Taylor and Francis Group Linkedin page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
<div class="pull-left links">
<a href="https://www.youtube.com/user/TaylorandFrancis">
<i class="fa fa-youtube-square" title="Taylor and Francis Group YouTube page" aria-hidden="true" role="button"></i>
<span aria-describedby="youtube-description">
<span class="off-screen" id="youtube-description">Taylor and Francis Group Youtube page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://www.weibo.com/tandfchina">
<i class="fa fa-weibo" title="Taylor and Francis Group Weibo page" aria-hidden="true" role="button"></i>
<span aria-describedby="weibo-description">
<span class="off-screen" id="weibo-description">Taylor and Francis Group Weibo page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
</div></div>
</div>
</div>
</div>
 </div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-horizontal" id="8d803f96-081d-4768-ab7d-280a77af723b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-sm  ">
<div class="col-sm-3-4 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none footer-info-container widget-none  widget-compact-vertical" id="b247ecb9-84c9-4762-b270-20f8be1f0ae4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="informa-group-info">
<span>Copyright Â© 2020 Informa UK Limited</span>
<span><a href="https://informa.com/privacy-policy/">Privacy policy</a></span>
<span><a href="/cookies">Cookies</a></span>
<span><a href="/terms-and-conditions">Terms & conditions</a></span>
<span><a href="/accessibility">Accessibility</a></span>
<p>Registered in England & Wales No. 3099067<br />
5 Howick Place | London | SW1P 1WG</p>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-4 footer_tandf_logo">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-image none  widget-none  widget-compact-vertical" id="b6bde365-079b-454f-94f6-1841291656a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><a href="http://taylorandfrancis.com/" title="Taylor and Francis Group">
<img src="/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png" alt="Taylor and Francis Group" />
</a></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget cookiePolicy none  widget-none  widget-compact-all" id="cea739ac-da2c-4d77-9cf1-cb3e0da7e31e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="banner">
<a href="#" class="btn">Accept</a>
<p class="message">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href="/cookies">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>
</div></div>
</div>
</div>
</div>
</footer></div>
</div>
</div>
</div>
</div>
<script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"rayId":"5b74d47ace0a7377","version":"2020.6.0","si":10}'></script>
</body>
</html>
