<!DOCTYPE html>
<html lang="en" class="pb-page" data-request-id="89b45ca4-d10b-4200-ad8c-416725c29234"><head data-pb-dropzone="head"><meta name="pbContext" content=";page:string:Article/Chapter View;ctype:string:Journal Content;journal:journal:hihc20;issue:issue:10.1080/hihc20.v033.i11;article:article:10.1080/10447318.2017.1296074;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text;requestedJournal:journal:hihc20" />
<link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><meta name="citation_journal_title" content="International Journal of Human–Computer Interaction" /><meta name="dc.Title" content="On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design" /><meta name="dc.Creator" content="Remi  Alkemade" /><meta name="dc.Creator" content="Fons J.  Verbeek" /><meta name="dc.Creator" content="Stephan G.  Lukosch" /><meta name="dc.Description" content="In the early stages of 3D design, sketches are used to quickly conceptualize ideas and gain insight into problems and possible solutions. Computer-aided design tools are widely used for 3D modeling..." /><meta name="Description" content="In the early stages of 3D design, sketches are used to quickly conceptualize ideas and gain insight into problems and possible solutions. Computer-aided design tools are widely used for 3D modeling..." /><meta name="dc.Publisher" content="Taylor &amp; Francis" /><meta name="dc.Date" scheme="WTN8601" content="31 Mar 2017" /><meta name="dc.Type" content="research-article" /><meta name="dc.Format" content="text/HTML" /><meta name="dc.Identifier" scheme="publisher-id" content="1296074" /><meta name="dc.Identifier" scheme="doi" content="10.1080/10447318.2017.1296074" /><meta name="dc.Source" content="http://dx.doi.org/10.1080/10447318.2017.1296074" /><meta name="dc.Language" content="en" /><meta name="dc.Coverage" content="world" /><meta name="dc.Rights" content="© 2017 Taylor &amp; Francis Group, LLC" />
<link rel="meta" type="application/atom+xml" href="https://doi.org/10.1080%2F10447318.2017.1296074" />
<link rel="meta" type="application/rdf+json" href="https://doi.org/10.1080%2F10447318.2017.1296074" />
<link rel="meta" type="application/unixref+xml" href="https://doi.org/10.1080%2F10447318.2017.1296074" />
<title>Full article: On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</title>
<meta charset="UTF-8">
<meta name="robots" content="noarchive" />
<meta name="pb-robots-disabled">

<meta property="og:title" content="On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.tandfonline.com/doi/abs/10.1080/10447318.2017.1296074" />
<meta property="og:image" content="https://www.tandfonline.com/doi/cover-img/10.1080/hihc20.v033.i11" />
<meta property="og:site_name" content="Taylor & Francis" />
<meta property="og:description" content="(2017). On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design. International Journal of Human&#x2013;Computer Interaction: Vol. 33, No. 11, pp. 882-901." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@tandfonline">
<meta name="viewport" content="width=device-width,initial-scale=1" />
<script>var tandfData = {"search":{"hasOpenAccess":true}};</script>
<link rel="stylesheet" type="text/css" href="/wro/j2y2~product.css"><link rel="stylesheet" type="text/css" href="/pb/css/t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" id="pb-css" data-pb-css-id="t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" />
<link href="//www.trendmd.com" rel="preconnect" />
<link href="//app.wizdom.ai" rel="preconnect" />
<link href="//connect.facebook.net" rel="preconnect" />
<link href="//go.taylorandfrancis.com" rel="preconnect" />
<link href="//pi.pardot.com" rel="preconnect" />
<link href="//static.hotjar.com" rel="preconnect" />
<link href="//cdn.pbgrd.com" rel="preconnect" />
<link href="//f1-eu.readspeaker.com" rel="preconnect" />
<link href="//www.googleadservices.com" rel="preconnect" />
<link href="https://ajax.googleapis.com" rel="preconnect" />
<link href="https://m.addthis.com" rel="preconnect" />
<link href="https://wl.figshare.com" rel="preconnect" />
<link href="https://pagead2.googlesyndication.com" rel="preconnect" />
<link href="https://www.googletagmanager.com" rel="preconnect" />
<link href="https://www.google-analytics.com" rel="preconnect" />
<script type="text/javascript" src="/wro/j2y2~loadinview.js"></script>
<script type="text/javascript" src="/wro/j2y2~product.js"></script>
<script type="text/javascript">
        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};
    </script>
<script type="application/javascript" src="//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr" id="read-speaker" async></script>
<script type="text/javascript" src="//cdn.pbgrd.com/core-tandf.js" async defer></script>
<script data-ad-client="ca-pub-5143040550582507" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script>

<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:864760,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>
<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script async src="https://www.colwiz.com/pubsol/widget/34000f34a146a2017e2b5acad48d6b07.js"></script>
<link href="//qa.colwiz.com" rel="preconnect" />
<script src="//scholar.google.com/scholar_js/casa.js" async></script>
</head>
<body class="pb-ui">

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<div class="skipContent off-screen">
<a href="#top-content-scroll" class="skipToContent" title="Skip to Main Content" tabIndex="0">Skip to Main Content</a>
</div>
<script type="text/javascript">
    if (true) {
        var skipToContent = document.getElementsByClassName("skipToContent");
        if (skipToContent != null) {
            skipToContent[0].onclick = function skipElement() {
                var element = document.getElementById('top-content-scroll');
                if (element == null || element === undefined) {
                    element = document.getElementsByClassName('top-content-scroll').item(0);
                }
                element.setAttribute('tabindex', '0');
                element.focus();
            }
        }
    }
    document.addEventListener("DOMContentLoaded",function(e){
        if(document.getElementsByClassName("mediaThumbnailContainer").length > 0){
            TandfUtils.appendScript(document.body,"/wro/j2y2~jwplayer.js","jwplayer_src",true,true);
        }
    });
</script>
<div id="pb-page-content" data-ng-non-bindable>
<div data-pb-dropzone="main" data-pb-dropzone-name="Main">
<div class="widget pageHeader none  widget-none  widget-compact-all" id="a4d4fdd3-c594-4d68-9f06-b69b8b37ed56">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><header class="page-header">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="036fa949-dc25-4ffe-9df0-d7daefee281b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-xs  ">
<div class="col-xs-1-6 header-index">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal" id="e817489e-2520-418b-a731-b62e247e74df">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/" title="Taylor and Francis Online">
<img src="/pb-assets/Global/tfo_logo-1444989687640.png" alt="Taylor and Francis Online" />
</a></div>
</div>
</div>
<div class="widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal" id="b3fe8380-8b88-4558-b004-6485d3aea155">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/">
<img src="/pb-assets/Global/tfo_logo_sm-1459688573210.png" />
</a></div>
</div>
</div>
</div>
</div>
<div class="col-xs-5-6 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget layout-inline-content alignRight  widget-none  widget-compact-all" id="a8a37801-55c7-4566-bdef-e4e738967e38">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget layout-inline-content none customLoginBar widget-none" id="fbe90803-b9c8-4bef-9365-cb53cc4bfa0e">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumInstitutionBanner none bannerWidth widget-none" id="3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="institution-image-text hidden-xs hidden-sm disable-click">Access provided by<strong> Copenhagen University Library</strong>
</div>
<div class="institution-image logout-institution-image">
</div></div>
</div>
</div>


<div class="widget literatumNavigationLoginBar none  widget-none  widget-compact-all" id="1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="loginBar not-logged-in">
<span class="icon-user"></span>
<a href="/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F10447318.2017.1296074" class="sign-in-link">
Log in
</a>
<span class="loginSeprator">&nbsp;|&nbsp;</span>
<a href="/action/registration?redirectUri=%2F" class="register-link">
Register
</a>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget eCommerceCartIndicatorWidget none literatumCartLink widget-none" id="9de10bb5-08af-48bc-b9f6-3f6433229f3e">
<div class="wrapped ">
<div class="widget-body body body-none "><a href="/action/showCart?FlowID=1" class="cartLabel">
<span class="hidden-xs hidden-sm visible-tl-inline-block">Cart</span>
<span class="cartItems" data-id="cart-size" role="status">
</span>
</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</header></div>
</div>
</div>

<div class="widget pageBody none  widget-none  widget-compact-all" id="35d9ca18-265e-4501-9038-4105e95a4b7d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<div class="page-body pagefulltext">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all" id="1728e801-36cd-4288-9f53-392bad29506a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-5-12 search_container ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget quickSearchWidget none search-customize-width widget-none  widget-compact-all" id="d46e3260-1f5c-4802-821a-28a03a699c82">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="quickSearchFormContainer ">
<form action="/action/doSearch" name="quickSearch" class="quickSearchForm " title="Quick Search" method="get" onsubmit="appendSearchFilters(this)" aria-label="Quick Search"><span class="simpleSearchBoxContainer">
 <input name="AllField" class="searchText main-search-field autocomplete" value="" type="search" id="searchText" title="Type search term here" aria-label="Search" placeholder="Enter keywords, authors, DOI, ORCID etc" autocomplete="off" data-history-items-conf="3" data-publication-titles-conf="3" data-publication-items-conf="3" data-topics-conf="3" data-contributors-conf="3" data-fuzzy-suggester="false" data-auto-complete-target="title-auto-complete" />
</span>
<span class="searchDropDownDivRight">
<label for="searchInSelector" class="visuallyhidden">Search in:</label>
<select id="searchInSelector" name="SeriesKey" class="js__searchInSelector">
<option value="hihc20" id="thisJournal" data-search-in="thisJournal">
This Journal
</option>
<option value="" data-search-in="default">
Anywhere
</option>
</select>
</span>
<span class="quick-search-btn">
<input class="mainSearchButton searchButtons pointer" title="Search" role="button" type="submit" value="" aria-label="Search" />
</span></form>
</div>
<div class="advancedSearchLinkDropZone" data-pb-dropzone="advancedSearchLinkDropZone">
<div class="widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all" id="323e2a31-1c81-4995-bd17-8e149458c214">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a href="/search/advanced" class="advSearchArticle">Advanced search</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 serNav_container">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumSeriesNavigation none  widget-none" id="7730bfe1-9fca-4cf4-a6d6-2a0148105437">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="issueSerialNavigation journal">
<div class="cover">
<img data-src='{"type":"image" , "src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/hihc20.v033.i11/20171002-01/hihc20.v033.i11.cover.jpg"}' src="//:0" alt="Publication Cover" width="120" height="156" />
</div>
<div class="info ">
<div class="title-container">
<span class="titleHeading">Journal</span>
<h1>
<a href="/toc/hihc20/current">
International Journal of Human&#x2013;Computer Interaction
</a>
</h1>
<h2>
Volume 33, 2017 - <a href="/toc/hihc20/33/11" class="nav-toc-list">Issue 11</a>
</h2>
</div>
<div class="seriesNavDropZone" data-pb-dropzone="seriesNavDropZone">
<div class="widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal" id="753455df-1eeb-47ca-bdc9-e19022075973">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="serial-action">
<a href="http://www.editorialmanager.com/ijhc" class="green submitAnArticle"><span>Submit an article</span></a>
<a href="/toc/hihc20/current" class="jHomepage"><span>Journal homepage</span></a>
</div></div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none" id="e42aea8f-434a-4d39-aaef-f56af3ff00dc">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all" id="6aacf107-e82d-494d-a14c-0c00bba52560">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="accessLogo">
<div>
<img class="accessIconLocation" data-src='{"type":"image" , "src":"/pb-assets/3rdPartyLogos/accessFull-1452596451717.png"}' src="//:0" alt="Full access" />
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none publicationContentHeader widget-none  widget-compact-all" id="63f402e4-3498-4709-8d7d-ee8e69f93467">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical" id="5afd8b6d-7e09-43ff-8ad6-afa3764e543c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="articleMetricsContainer">
<div class="content compactView">
<div class="section">
<div class="value">
474
</div>
<div class="title">
Views
</div>
</div>
<div class="section">
<div class="value">
12
</div>
<div class="title">
CrossRef citations to date
</div>
</div>
<div class="section score">
<div class="altmetric-score true">
<div class="value" data="10.1080/10447318.2017.1296074"></div>
<div class="title">
Altmetric
</div>
</div>
</div>
<div class="altmetric-Key hidden" data="be0ef6915d1b2200a248b7195d01ef22"></div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-2-3 ">
<div class="contents" data-pb-dropzone="contents1">

<div class="widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all" id="fa57727f-b942-4eb8-9ed2-ecfe11ac03f5">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="read-speaker-container" style="display: block">
<div id="readspeaker_button1" class="rs_skip rsbtn rs_preserve">
<a href="//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F10447318.2017.1296074" rel="nofollow" class="rsbtn_play" accesskey="L" title="Listen to this page using ReadSpeaker webReader" style="border-radius: 0 11.4px 11.4px 2px;">
<span class="rsbtn_left rsimg rspart"><span class="rsbtn_text"><span>Listen</span></span></span>
<span class="rsbtn_right rsimg rsplay rspart"></span>
</a>
</div>
</div>
<div class="toc-heading">
<h3>
Articles
</h3>
</div>
<h1><span class="NLM_article-title hlFld-title">On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</span></h1><span class="sub-title"><h2></h2></span><div class="literatumAuthors"><div class="publicationContentAuthors"><div class="hlFld-ContribAuthor"><span class="NLM_contrib-group"><span class="contribDegrees "><a class="entryAuthor" href="/author/Alkemade%2C+Remi">Remi Alkemade<span class="overlay"> Media Technology Programme, Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands<div class="author-extra-info" tabindex="0" data-authorsInfo="{&quot;id&quot; : &quot;B0001&quot;, &quot;hasFull&quot; : &quot;true&quot;}">View further author information</div></span></a>, </span><span class="contribDegrees corresponding "><a class="entryAuthor" href="/author/Verbeek%2C+Fons+J">Fons J. Verbeek<span class="overlay"> Imagery &amp; Media, Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands<span class="corr-sec"><span class="heading">Correspondence</span><span class="corr-email"><i class="fa fa-envelope" style="color: #10147E; padding-right: 7px" aria-hidden="true"></i><span data-mailto="mailto:f.j.verbeek@liacs.leidenuniv.nl">f.j.verbeek@liacs.leidenuniv.nl</span></span><br /></span><div class="author-extra-info" tabindex="0" data-authorsInfo="{&quot;id&quot; : &quot;B0002&quot;, &quot;hasFull&quot; : &quot;true&quot;}">View further author information</div></span></a> &amp; </span><span class="contribDegrees "><a class="entryAuthor" href="/author/Lukosch%2C+Stephan+G">Stephan G. Lukosch<span class="overlay"> Faculty of Technology, Policy and Management, Delft University of Technology, Delft, The Netherlands<div class="author-extra-info" tabindex="0" data-authorsInfo="{&quot;id&quot; : &quot;B0003&quot;, &quot;hasFull&quot; : &quot;true&quot;}">View further author information</div></span></a></span></span></div></div></div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="5f562208-b1d5-4e5a-81c7-356431240f04">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md gutterless ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget layout-inline-content none  widget-none  widget-compact-all" id="87ac5840-18fa-4a14-8eca-065b90ede3d7">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumContentItemPageRange none  widget-none  widget-compact-all" id="45057865-d60c-414c-bc81-646debb621b0">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><span class="contentItemPageRange">Pages 882-901
</span></div>
</div>
</div>
<div class="widget literatumContentItemHistory none  widget-none  widget-compact-all" id="32bf868e-52ce-411a-9dc3-717743aad997">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div>Accepted author version posted online: 23 Feb 2017</div><div>Published online: 31 Mar 2017</div></div>
</div>
</div>

<div class="widget literatumArticleToolsWidget none  widget-none  widget-compact-all" id="ed673666-7b5d-470e-bd33-c5c679d996cb">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="articleTools">
<ul class="linkList blockLinks separators centered">
<li class="downloadCitations">
<a href="/action/showCitFormats?doi=10.1080%2F10447318.2017.1296074"><i class="fa fa-quote-left" aria-hidden="true"></i>Download citation</a>
</li>
<li class="dx-doi">
<a href="https://doi.org/10.1080/10447318.2017.1296074"><i class="fa fa-external-link-square" style="margin: 0 0.25rem 0 0" aria-hidden="true"></i>https://doi.org/10.1080/10447318.2017.1296074</a>
</li>
<li class="cross-mark">
<a id="crossMark" data-doi="10.1080/10447318.2017.1296074" data-target="crossmark">
<img data-src='{"type":"image" , "src":"/templates/jsp/images/CROSSMARK_Color_horizontal.svg"}' src="//:0" alt="CrossMark Logo" width="100" />
<span aria-describedby="crossMark-description"><span class="off-screen" id="crossMark-description">CrossMark</span></span>
</a>
</li>
</ul>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents2">
</div>
</div>
</div>
</div></div>
</div>
</div>

<div class="widget responsive-layout none publicationContentBody widget-none" id="f4a74f7a-9ba2-4605-86b1-8094cb1f01de">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget sectionsNavigation none  widget-none" id="f15bd2de-bb18-4067-8ab9-03ea3be30bf7">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="sections-nav"><span class="title">In this article<a href="#" class="close" tabindex="-1"><span aria-describedby="close-description"><span class="off-screen" id="close-description">Close</span></span></a></span><ul class="sections-list"><li><a href="#abstract">ABSTRACT</a></li><li><span class="sub-art-heading"><a href="#_i2">1. Introduction</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i4">2. Related work</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i6">3. 3D interfaces</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i8">4. Hand gestures</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i14">5. Concept of gesture-based interaction</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i27">6. Experiment</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i34">7. Results</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i45">8. Conclusions and future work</a></span><ul class="sub-art-titles"></ul></li><li><a href="#references-Section">References</a></li></ul></div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none rs_readArea widget-none  widget-compact-all" id="9751b4f9-64b9-44c0-955b-f75246902839">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all" id="d29f04e9-776c-4996-a0d8-931023161e00">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
        "HTML-CSS": {scale: 70, linebreaks: {automatic: true, width: "container"}},
        SVG: {linebreaks: {automatic: true, width: "25%"}},
        menuSettings: {zoom: "Click"},

        /* This is necessary to lazy loading. */
        skipStartupTypeset: true
    });
</script>
<div class="articleMeta ja">
<div class="tocHeading">
<h2>Articles</h2>
</div>
<div class="hlFld-Title">
<div class="publicationContentTitle">
<h1 class="chaptertitle">
On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design
</h1>
</div>
</div>
<div class="copyrightStatement">
</div>
<div class="articleMetaDrop publicationContentDropZone" data-pb-dropzone="articleMetaDropZone">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone1" data-pb-dropzone="articleMetaDropZone1">
</div>
<div class="copyrightline">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone2" data-pb-dropzone="articleMetaDropZone2">
</div>
</div>
<div class="publication-tabs ja publication-tabs-dropdown">
<div class="tabs tabs-widget">
<ul class="tab-nav" role="tablist">
<li class="active" role="tab">
<a href="/doi/full/10.1080/10447318.2017.1296074?scroll=top&amp;needAccess=true" class="show-full">
<i class="fa fa-file-text" aria-hidden="true"></i>
<span class="nav-data">
Full Article
</span>
</a>
</li>
<li role="tab">
<a href="/doi/figure/10.1080/10447318.2017.1296074?scroll=top&amp;needAccess=true" class="show-figure">
<i class="fa fa-image" aria-hidden="true"></i>
<span class="nav-data">Figures & data</span>
</a>
</li>
<li role="tab">
<a href="/doi/ref/10.1080/10447318.2017.1296074?scroll=top" class="show-references">
<i class="fa fa-book" aria-hidden="true"></i>
<span class="nav-data">References</span>
</a>
</li>
<li class="citedbyTab " role="tab">
<a href="/doi/citedby/10.1080/10447318.2017.1296074?scroll=top&amp;needAccess=true">
<i class="fa fa-quote-left" aria-hidden="true"></i>
<span class="nav-data">
Citations
</span>
</a>
</li>
<li class="off-screen"></li>
<li role="tab" class="metrics-tab">
<a href="#metrics-content" class="show-metrics">
<i class="fa fa-bar-chart" aria-hidden="true"></i>
<span class="nav-data">Metrics</span>
</a>
</li>
<li role="tab" class="permissions-tab ">
<a href="/doi/abs/10.1080/10447318.2017.1296074?tab=permissions&amp;scroll=top" class="show-permissions">
<i class="fa fa-print" aria-hidden="true"></i>
<span class="nav-data">
Reprints & Permissions</span></a>
</li>
<li class="pdf-tab " role="button">
<a href="/doi/pdf/10.1080/10447318.2017.1296074?needAccess=true" class="show-pdf" target="_blank">
<span class="nav-data">
PDF
</span>
</a>
</li>
</ul>
<div class="tab-content ">
<a id="top-content-scroll"></a>
<div class="tab tab-pane active">
<article class="article">
<p class="fulltext"></p><div class="hlFld-Abstract"><p class="fulltext"></p><div class="sectionInfo abstractSectionHeading"><h2 id="abstract" class="section-heading-2">ABSTRACT</h2></div><div class="abstractSection abstractInFull"><p class="summary-title"><b>ABSTRACT</b></p><p>In the early stages of 3D design, sketches are used to quickly conceptualize ideas and gain insight into problems and possible solutions. Computer-aided design tools are widely used for 3D modeling and design, but their required precision and 2D mouse and screen-based interface inhibit the flow of ideas. A study was conducted to explore the efficiency of hand tracking and virtual reality (VR) for 3D object manipulations in conceptual design. Based on existing research on conceptual design and hand gestures, an intuitive hand-based interaction model is proposed. An experiment on basic 3D manipulation shows that participants using a simple VR and hand-tracking interface prototype have similar performance to those using a traditional mouse and screen interface. For the improvement of gestural conceptual design interfaces, the relevant issues are identified.</p></div></div><div class="hlFld-Fulltext"><div id="S0001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1"><h2 id="_i2" class="section-heading-2">1. Introduction</h2><p>In early, conceptual stages of 3D design, such as industrial design, 3D art (e.g., in games) and architectural design, designers try to gain insight of a given spatial problem. They start with vague ideas and explore the problem and possible solutions.</p><p>The conceptual design phase is crucial to the design process. Fundamental decisions are made in this phase and the impact of design decisions declines steeply as the design matures (Wang, Shen, &amp; Xie, <span class="ref-lnk lazy-ref"><a data-rid="CIT0050" data-refLink="_i46" href="#">2002</a></span>). Important issues that are overlooked in the early stages may be more difficult to overcome in later stages, if not taken into account from the start. During the conceptual design phase, decisions are made that will account for 75% of a product’s life cycle cost, including manufacturing, use, repair, and disposal (Nevins &amp; Whitney, <span class="ref-lnk lazy-ref"><a data-rid="CIT0033" data-refLink="_i46" href="#">1989</a></span>).</p><p>Design sketches are used not only to generate ideas (<i>ideation</i>), for external memory, for association by visual cues, but also as a physical setting in which design thoughts are constructed (Suwa, Gero, &amp; Purcell, <span class="ref-lnk lazy-ref"><a data-rid="CIT0047" data-refLink="_i46" href="#">2006</a></span>). These sketches are low on details and only show the essential elements of the design that are currently under consideration. Designers can use many tools for sketching, the most common being pencil and paper, but also clay, blocks, or other objects and materials can be used.</p><p>In 3D conceptual design, different sketching tools have their advantages and disadvantages. Pencil and paper perhaps support the quickest sketches for people who can draw, but the view can be drawn from only one side at a time. Objects and materials can be used for quick visualization of crude ideas, but the right materials need to be present at the right time.</p><p>Computer-aided design (CAD) software is a powerful tool to create complex designs and is often used for detailed modeling, presentation, and production schematics. Digital sketching has advantages over physical sketches, such as modifying existing designs, copying, and histories (undo/redo). However, CAD is mostly used in later stages of design and is less adequate for conceptual design (van Dijk, <span class="ref-lnk lazy-ref"><a data-rid="CIT0048" data-refLink="_i46" href="#">1995</a></span>; Wang et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0050" data-refLink="_i46" href="#">2002</a></span>). The (required) precision and features nested in menus and modes inhibit the flow of ideas.</p><p>For a CAD tool to be effective for conceptual shape design, van Dijk (<span class="ref-lnk lazy-ref"><a data-rid="CIT0048" data-refLink="_i46" href="#">1995</a></span>) collected a list of recommendations from literature: <ol class="NLM_list NLM_list-list_type-order"><li><p class="inline"><i>Easy data entry</i>: both entering new data and modifying existing data should be quick and simple.</p></li><li><p class="inline"><i>Hand movements</i>: hand movements in the interaction complement the thinking process.</p></li><li><p class="inline"><i>Imprecise data</i>: exact data requirements inhibit the flow of thoughts of the designer, so the software should accept imprecise data.</p></li><li><p class="inline"><i>Switch to details</i>: the designer should be able to focus on specific features in detail, while temporarily forgetting about others.</p></li><li><p class="inline"><i>Review alternatives</i>: it should be possible to compare different designs.</p></li><li><p class="inline"><i>Separate conceptual design system</i>: the tool for detailed design should be separated from the tool for conceptual design.</p></li><li><p class="inline"><i>Clay modeling</i>: clay modeling provides inherently 3D interactive, free-hand shape (de)formation.</p></li></ol></p><p>Moreover, conceptual design issues are often interdisciplinary and involve collaboration between customers, designers, and engineers (Wang et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0050" data-refLink="_i46" href="#">2002</a></span>). For all intended users to be able to learn and use the tool quickly, the tool should also be generally <i>intuitive</i>, allowing them to apply knowledge from different contexts to the tool (Blackler, Popovic, &amp; Mahar, <span class="ref-lnk lazy-ref"><a data-rid="CIT0003" data-refLink="_i46" href="#">2002</a></span>).</p><p>The most widely used interface for CAD is (and has been for many years) the keyboard, mouse, and screen. There is an inherent inefficiency in this interface that may be possible to improve upon in terms of conceptual design: the user currently works from the 3D real world, through a 2D input, to model a 3D virtual world, which is displayed on a 2D screen (<a href="#F0001">Figure 1</a>). This information flow requires compression from the 3D world (by the user) to the 2D interface and decompression from the interface (by the computer) to the 3D virtual world. This way, an entire spatial dimension of user input is unused by the interface and the user will have to learn how to compress their 3D concept into the 2D interface. In the visual output, the 2D display imposes restrictions on the user’s perception of the virtual environment. The monoscopic view impairs depth perception and the view cannot be adjusted by the user naturally (e.g., moving their head); instead, the view has to be adjusted by extra controls.<div class="figure figureViewer" id="F0001"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 1. </span> Traditional CAD interface.</p></div></td></div><a href="#" class="thumbnail"><img id="F0001image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0001_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0001"><p><span class="captionLabel">Figure 1. </span> Traditional CAD interface.</p></div><div class="hidden rs_skip" id="figureFootNote-F0001"></div></p><p>The development of advanced input technologies and virtual reality (VR) can take away the dimensionality transformations inherent to 2D interfaces for 3D design. VR can create a more natural perception of the virtual world, immersing the designer into the design; 3D input devices can replace the 2D interface for more natural and direct interaction with the virtual world. Moreover, gesture-based interfaces may provide an interface that is more intuitive than hand-held input devices, since humans naturally interact with the physical world using their hands.</p><p>In this research, we investigate whether a hand gesture-based interface (combined with VR or screen) can decrease task load of 3D object manipulations and improve time efficiency of such tasks, paving the way for future intuitive and more effective ideation interfaces. We hypothesize that a hand gesture-based VR interface allows 3D object manipulation tasks to be performed faster than conventional mouse and screen based interaction.</p><p>The remainder of the article is structured as follows. In the next section, we discuss related work on 3D conceptual design tools. In <a href="#S0003">Section 3</a>, we discuss hand gesture research to lay a basis for a gesture-based conceptual design interaction, which is then explained in <a href="#S0004">Section 4</a>. In <a href="#S0005">Section 5</a>, we describe the experiment we conducted to test the current<span class="ref-lnk fn-ref-lnk lazy-ref"><a data-rid="FN0001" href="#" data-refLink="fn"><sup>1</sup></a></span> feasibility of a hand-based VR interface for conceptual design and we discuss the results in <a href="#S0006">Section 6</a>. Our conclusions are presented in <a href="#S0007">Section 7</a>, along with a consideration of relevant future work.</p></div><div id="S0002" class="NLM_sec NLM_sec_level_1"><h2 id="_i4" class="section-heading-2">2. Related work</h2><p>Over the last 15 years, several novel interaction techniques have been proposed to improve the effectiveness of computer-aided conceptual design tools. They can be divided into two categories: improvements to the 2D–3D conversion interface and migration from 2D to 3D interfaces.</p><div id="S0002-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i5">2.1. 2D–3D conversion</h3><p>Many conceptual CAD techniques focus on improving the 2D interface to the 3D virtual world, often based on traditional concept sketching. Some detect pen gestures to create primitive shapes and perform actions, as implemented in SKETCH (Zeleznik, Herndon, &amp; Hughes, <span class="ref-lnk lazy-ref"><a data-rid="CIT0052" data-refLink="_i46" href="#">2007</a></span>). GIDeS (Pereira, Jorge, Branco, &amp; Ferreira, <span class="ref-lnk lazy-ref"><a data-rid="CIT0036" data-refLink="_i46" href="#">2000</a></span>) also allows free-hand sketches in top/front/side views that are combined into one 3D reconstruction. Naya, Jorge, Conesa, Contero, and Gomis (<span class="ref-lnk lazy-ref"><a data-rid="CIT0032" data-refLink="_i46" href="#">2002</a></span>) proposed a CAD system in which the user could directly draw the axonometric view of an object, which was then converted to a virtual 3D object. While this system assumed the input of straight-edged objects, Teddy (Igarashi, Matsuoka, &amp; Tanaka, <span class="ref-lnk lazy-ref"><a data-rid="CIT0023" data-refLink="_i46" href="#">2007</a></span>) assumes round shapes, which allows the user to quickly free-hand draw organic shapes (such as a teddy bear) that are converted into 3D models.</p><p>Benko, Ishak, and Feiner (<span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-refLink="_i46" href="#">2005</a></span>) explored a hybrid interaction model, combining 2D and 3D representations and manipulations of objects. Objects could be not only viewed and manipulated in 2D but also grabbed and pulled into a 3D (augmented reality [AR]) environment using hand gestures (and vice versa). The objects could be parametrically edited in the 2D user interface, with direct feedback in the 3D environment.</p><p>Although some of the authors of these systems report user preference for their system over traditional CAD, only GIDeS (Pereira et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0036" data-refLink="_i46" href="#">2000</a></span>) was subjected to experiments to measure effectiveness. GIDeS outperformed an unspecified commercial CAD system on seven unspecified modeling tasks in task completion time.</p></div></div><div id="S0003" class="NLM_sec NLM_sec_level_1"><h2 id="_i6" class="section-heading-2">3. 3D interfaces</h2><p>Construction of 3D objects based on 2D sketches faces an inherent problem: any algorithm performing this 2D–3D conversion requires assumptions for the third dimension. Replacing the 2D interface of a CAD environment with a 3D interface removes these assumptions and allows direct creation and manipulation in 3D space, which is shown to have usability advantages.</p><p>One type of 3D interface is based on free-hand tracking (e.g., Shen, Ong, and Nee, <span class="ref-lnk lazy-ref"><a data-rid="CIT0044" data-refLink="_i46" href="#">2011</a></span>), and although such technologies can still be much improved, Coelho and Verbeek (<span class="ref-lnk lazy-ref"><a data-rid="CIT0010" data-refLink="_i46" href="#">2014</a></span>) found that 3D pointing tasks were already performed more quickly using a 3D input device (Leap Motion) than using a mouse. When using the mouse, the third dimension was controlled with the scroll wheel. They concluded that 3D translation is “less cumbersome when the [third dimension] is provided as input based on real-life movement mappings” (page 84).</p><p>Moreover, Buchmann, Violich, Billinghurst, and Cockburn (<span class="ref-lnk lazy-ref"><a data-rid="CIT0007" data-refLink="_i46" href="#">2004</a></span>) found that users of their 3D fingertip-based interface were quickly able to learn the natural manipulative gestures of their system. They also noticed that many of their usability issues were caused by tracking constraints.</p><p>A design tool by Encarnação (<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-refLink="_i46" href="#">1999</a></span>) combines pen gestures and manipulation [similar to those of SKETCH (Zeleznik et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0052" data-refLink="_i46" href="#">2007</a></span>)] with a 3D AR setup. A virtual table and shutter glasses produce a stereoscopic vision of the scene, while a sketchpad and pen are tracked to interact with the 3D scene. The user can create and manipulate the 2D projection of the 3D scene with the sketchpad using pen gestures. The system was informally tested on several users, who generally found the design very appealing and were able to understand the tools with ease.</p><p>A similar approach was taken in Spacedesign (Fiorentino, Amicis, Monno, &amp; Stork, <span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-refLink="_i46" href="#">2002</a></span>), using infrared cameras to track several tools to interact with a 3D virtual environment. In addition to changing the viewpoint naturally, Spacedesign enables the user to create surfaces from curves that are drawn with a pen directly in 3D space, without the use of a sketchpad. Industrial designers in the automotive industry appreciate the simplicity of the system, because surfaces can be created without having to do the mathematics that are normally required. Because of its intuitiveness and real-time editing, stylist can be active part of the development process.</p><p>Besides sketching, some applications are based on clay modeling. VRClay (<span class="ref-lnk lazy-ref"><a data-rid="CIT0049" data-refLink="_i46" href="#">2014</a></span>) is a recent tool that uses Oculus Rift VR glasses and two Razer Hydra 6-degrees-of-freedom (6DoF) controllers, which are tracked accurately in any direction in 3D space. The user can hold, rotate, and move an object with one and sculpt it with the other. An important advantage of “virtual clay” over physical clay is that there are no restrictions on the addition of material or shape of parts (e.g., thin legs falling off by gravity). At the time of writing, this application is still being actively developed, but it seems to work very intuitively for organic shapes, as shown in their “sneak peek” video.</p><p>Other applications, more or less, follow established CAD principles but provide a 3D interface for more direct interaction. HoloSketch is a VR CAD tool based on simple 2D drawing systems (Deering, <span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-refLink="_i46" href="#">1995</a></span>). It uses a “Wand” to point and click in 3D space. Menus appear in mid-air and can be clicked with the Wand. From these menus, 3D objects can be selected and placed in the design. Evaluation was done by one artist with experience in traditional CAD software, during 1 month. Overall, she found the interface much more productive than traditional modelers.</p><p>A recent and very comprehensive VR CAD tool is MakeVR (Jerald, Mlyniec, Yoganandan, Rubin, Paullus, &amp; Solotko, <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-refLink="_i46" href="#">2013</a></span>); it is currently on its way to becoming one of the first VR CAD tools to enter the commercial market. Similar to VRClay, it uses two Razer Hydra controllers and Oculus Rift VR glasses. It is built on top of the ACIS CAD engine and enables operations such as moving objects in all directions in 3D space, boolean operations (e.g., cutting one shape out of the other), and sweeps (e.g., using a sphere to create an irregular-shaped pipe). Although it would be very interesting to see whether this tool would increase ideation effectiveness in conceptual design, experimental research was limited to a Likert scale usability questionnaire.</p><p>Many conceptual design interfaces have been proposed; however, only few are thoroughly tested for effectiveness. Rahimian and Ibrahim (<span class="ref-lnk lazy-ref"><a data-rid="CIT0039" data-refLink="_i46" href="#">2011</a></span>) investigated the impacts of a 3D sketching tool on designers’ cognitive and collaborative abilities in conceptual collaborative design. A 6DoF haptic device was connected to ClayTools CAD software and employed in a complete conceptual architectural design by three pairs of novice architects. An elaborate protocol study of the design process showed that the design sessions using the 3D sketching tool improved in terms of perceptual actions (more attention to visuospatial features), unexpected discoveries (discoveries from the sketch), coevolution of the problem space (problem finding), and collaborative actions (more proposals and arguments instead of questions and explanations).</p><div id="S0003-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i7">3.1. Research focus</h3><p>Generally, 3D conceptual design interfaces are received with enthusiasm by users and professionals when compared to traditional, mouse-based interfaces. Most of the drawbacks and complaints that are mentioned are focused on technical difficulties or implementation choices.</p><p>However, very few experiments have been conducted to formally compare 3D interfaces for conceptual design to their 2D counterparts. Moreover, research has been focused on 3D pointing and drawing tools to replace the mouse, while naturally, humans communicate and interact more directly with the world around them by using their hands. Therefore, in our research, we focus on hand-based interaction and experimentally test its effectiveness in basic design tasks.</p><p>In this article, we will create a theoretical basis for a gesture-based conceptual design interface, based on research on natural hand gestures. We will then use a limited vocabulary of these gestures in a first experimental step to test their efficiency, keeping the scope small so as to enhance the focus of the measurements. With a full-featured hand gesture-based conceptual design environment at the horizon, this experiment will be the first step to assess its effectiveness. The next section introduces and discusses the theoretical basis for the gesture set.</p></div></div><div id="S0004" class="NLM_sec NLM_sec_level_1"><h2 id="_i8" class="section-heading-2">4. Hand gestures</h2><p>Hand gestures are a natural way of communicating and interacting for humans. A design interface based on these natural gestures is therefore likely to provide intuitive interaction with a virtual environment. In this section, we explore the domain of natural hand gestures with relation to conceptual design so that we can establish a useful set of gestures for an experimental setup to investigate the efficiency of a gesture-based interface for object manipulations in conceptual design. Whether the described theories of inter-human communication also apply to human–computer interaction remains to be tested. However, they can nevertheless provide a good starting point for an intuitive gesture-based interface.</p><div id="S0004-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i9">4.1. Types of gestures</h3><p>Gestures can assume various roles in human communication. McNeill (<span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-refLink="_i46" href="#">1992</a></span>) listed and described a set of discrete gestures, such as <i>metaphoric</i> gestures (describing abstract ideas), <i>beats</i> (emphasizing words they co-occur with), and <i>deictics</i> (pointing at objects or abstract concepts). Particularly, interesting for our purpose are <i>iconic</i> gestures. These have a close similarity to their referents and can depict spatial features, such as size and shape. For example, the concept <i>up</i> could be accompanied by pointing a finger in upward direction.</p><p>The aforementioned gestures are discrete, communicative arm, or hand movements that can be mapped to a specific meaning through a dictionary. Quek et al. defined such gesturing systems as <i>semaphoric gestures</i> and distinguished these from <i>manipulative gestures</i>, which are intended “to control some entity by applying a tight relationship between the movement of the gesturing hand/arm with the entity being manipulated” (Quek, Mcneill, Bryll, &amp; Mccullough, <span class="ref-lnk lazy-ref"><a data-rid="CIT0038" data-refLink="_i46" href="#">2002</a></span>). An important difference between the two types of gestures is that semaphoric gestures are performed and recognized as a whole, while manipulative gestures require (and allow) continuous feedback from the action.</p><p>Sturman and Zeltzer (<span class="ref-lnk lazy-ref"><a data-rid="CIT0046" data-refLink="_i46" href="#">1993</a></span>) divide gestures into six classes: <i>direct, mapped</i> and <i>symbolic</i> gestures, each further divided into <i>continuous</i> and <i>discrete</i> gestures. The <i>continuous</i> gestures have continuous input to the system, whereas <i>discrete</i> gestures (generally postures) are encoded as a single command. <i>Direct</i> gestures control kinematically similar actions in the system, <i>mapped</i> gestures provide a vocabulary of signals mapped to arbitrary functions, and <i>symbolic</i> gestures are interpreted by the system in a wider context and have no one-to-one correspondence with a specific system function.</p></div><div id="S0004-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i10">4.2. Gestures for design</h3><p>Since gestures are part of the natural interaction of humans, they may provide a valuable interface language between a conceptual design tool and a designer.</p><p>Not only are gestures naturally communicative, they also help people think. Cash and Maier (<span class="ref-lnk lazy-ref"><a data-rid="CIT0009" data-refLink="_i46" href="#">2016</a></span>) show that designers use gestures to communicate ideas (directed) as well as for themselves (reflective). Other research (described in a review by Alibali, <span class="ref-lnk lazy-ref"><a data-rid="CIT0001" data-refLink="_i46" href="#">2005</a></span>) shows that gestures help speakers activate and remember spatial information (Morsella &amp; Krauss, <span class="ref-lnk lazy-ref"><a data-rid="CIT0031" data-refLink="_i46" href="#">2004</a></span>; Ruiter, <span class="ref-lnk lazy-ref"><a data-rid="CIT0040" data-refLink="_i46" href="#">2000</a></span>; Wesp, Hesse, Keutmann, &amp; Wheaton, <span class="ref-lnk lazy-ref"><a data-rid="CIT0051" data-refLink="_i46" href="#">2001</a></span>); speakers describing images produced more gestures when the images were no longer visible, than when the images where still visible.</p><p>An experiment conducted by Hostetter and Alibali (<span class="ref-lnk lazy-ref"><a data-rid="CIT0021" data-refLink="_i46" href="#">2004</a></span>) suggests that gestures help conceptualize information. In this experiment, participants were asked describe a dot pattern (audio-only) that was either easy or difficult to conceptualize in terms of geometrical shapes. Dot patterns that were difficult to conceptualize elicited more gestures, even though the participants knew their gestures did not contribute to the communication.</p><div id="S0004-S2002-S3001" class="NLM_sec NLM_sec_level_3"><h4 class="section-heading-4" id="_i11">Physical manipulation</h4><p>Perception is closely linked to action. We often need actions to perceive, like moving our head to see and moving our arms to feel. One of the central views in the theory of <i>embodied cognition</i> (Gibbs, <span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-refLink="_i46" href="#">2006</a></span>) is that memory, mental imagery, and problem solving are closely linked to physical interaction: “objects are perceived by imagining how they may be physically manipulated” (page 12).</p><p>Ellis and Tucker (<span class="ref-lnk lazy-ref"><a data-rid="CIT0016" data-refLink="_i46" href="#">2000</a></span>) found that a range of actions associated with an object is primed upon perception of the object. In their experiment, participants were faster to grab a cylinder in response to the presentation of objects that would be grasped with a similar grip (e.g., hammer, bottle, jar) than in response of objects that would be grasped with a precision grip (e.g., screw, nail, coin).</p><p>The way we can interact with objects would thus influence how we perceive those objects and, in a larger context, a problem space. Basing the interface on interaction with the physical world may link the designer’s perception of the virtual world more to that of the physical world, in which humans have a great deal of experience and insight.</p></div><div id="S0004-S2002-S3002" class="NLM_sec NLM_sec_level_3"><h4 class="section-heading-4" id="_i12">Iconic gestures</h4><p>To make proper use of digital capabilities, we cannot solely rely on physical interaction in our conceptual design tool. After all, some interactions (e.g., creation, boolean operations, etc.) are not part of basic interaction with the physical world. For these actions, we can use semaphoric gestures.</p><p>Iconic gestures may provide an intuitive interface for these non-physical interactions. They exist in our natural communicative repertoire and, by definition, bear close semantic resemblance to their referent. Doherty (<span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-refLink="_i46" href="#">1985</a></span>) reviewed several studies that showed that iconicity in signs facilitates their acquisition and retention (e.g., Luftig &amp; Lloyd, <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i46" href="#">1981</a></span>; Mandel, <span class="ref-lnk lazy-ref"><a data-rid="CIT0028" data-refLink="_i46" href="#">1977b</a></span>).</p><p>Marsh and Watt (<span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i46" href="#">1998</a></span>) researched what type of gestures was used by participants that were asked to describe different shapes using only their hands. Following Mandel (<span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i46" href="#">1977a</a></span>), a distinction in iconic gestures was made between <i>virtual depiction</i>, in which the hand(s) trace the outline of the object (e.g., tracing a star) and <i>substitutive depiction</i>, in which the hand(s) “become” the object (e.g., using the arms to form a bridge). The results showed a strong preference for virtual depiction over substitutive depiction for primitive shapes (e.g., cube, sphere, pyramid). For complex shapes, iconic gestures were still preferred, but occasionally accompanied or replaced by pantemimic (e.g., describing a car by pretending to drive) and other gestures.</p><p>Sowa and Wachsmuth (<span class="ref-lnk lazy-ref"><a data-rid="CIT0045" data-refLink="_i46" href="#">2002</a></span>) further examined the morphological features of iconic gestures accompanying speech of participants describing several complex shapes. The gestures used most (86%) were <i>dimensional</i> gestures, “representing an object’s outer dimensions via delimiting or enclosing”. Sowa and Wachsmuth also mentioned that these gestures are often subject to <i>dimensional underspecification</i>, depicting only one or two dimensions of the object.</p><p>More recently, a dataset was constructed by Sadeghipour, Morency, and Kopp (<span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-refLink="_i46" href="#">2012</a></span>), containing data of 29 participants describing 10 primitive and 10 complex shapes using gestures only. The gestures were recorded using a Microsoft Kinect and the dataset is available online as the 3D Iconic Gesture Dataset (3DIG) (Sadeghipour &amp; Morency, <span class="ref-lnk lazy-ref"><a data-rid="CIT0041" data-refLink="_i46" href="#">2014</a></span>). Sadeghipour and Morency built a classifier to recognize the 20 shapes, outperforming human judgment on gestures using a virtual depiction as a drawing technique.</p></div><div id="S0004-S2002-S3003" class="NLM_sec NLM_sec_level_3"><h4 class="section-heading-4" id="_i13">Combining gesture classes</h4><p>Mimicking physical manipulation in the virtual environment may provide an intuitive interface for manipulating virtual objects. However, since these are limited to manipulations we can perform in the physical world, we should use other gestures to extend the “unrealistic” functionality, such as object creation. For this purpose, iconic gestures seem very suitable, since they are easy to remember and contain high amounts of spatial information, both due to their close similarity to their referents.</p><p>In the next section, we therefore propose a gesture-based interface for conceptual design, based on these principles, including a selection of physical manipulative and iconic gestures. Part of this set we will later use to investigate the efficiency of a hand gesture-based VR interface in basic object manipulations for conceptual design.</p></div></div></div><div id="S0005" class="NLM_sec NLM_sec_level_1"><h2 id="_i14" class="section-heading-2">5. Concept of gesture-based interaction</h2><p>From the theoretical basis discussed in <a href="#S0003">Section 3</a>, we propose a gesture-based, VR interaction model that supports a wide range of 3D virtual object manipulations for conceptual design. By including a third dimension of input to the interface, there can be a direct coupling between physical and virtual location and this tool should therefore remove some of the restrictions posed by 2D interfaces on 3D conceptual design. We based the interactions on natural human communication and interaction for an intuitive interface with the virtual world, allowing easy, imprecise, and efficient data input using the hands.</p><p>We selected this set of gestures to form a theoretically useful interaction model for conceptual design, based on the previously discussed literature. We can then use a subset of these gestures to test the efficiency of a prototype VR hand gesture interface for basic object manipulations in the domain of conceptual design. <a href="#F0003">Figure 3</a> shows a participany using our prototype setup.</p><div id="S0005-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i15">5.1. Interaction</h3><p>Although interaction mimicking natural physical interaction might be intuitive, some actions that add to conceptual design are not natural (e.g., creating objects from thin air). The virtual world is not equal to our physical world. This, together with some technological limitations (e.g., accurate tracking), requires us to make a trade-off between intuitiveness and functionality. The interactions described in this section were chosen to form a set of interactions that will support the designs of a large variety of 3D composite shapes, that demonstrates the expressive power of hand gestures in this context, and that we can use to investigate the efficiency of gesture-based interaction in conceptual design. The corresponding gestures are illustrated in <a href="#F0002">Figure 2</a>.<div class="figure figureViewer" id="F0002"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 2. </span> Gestures for conceptual design.</p></div></td></div><a href="#" class="thumbnail"><img id="F0002image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0002_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0002"><p><span class="captionLabel">Figure 2. </span> Gestures for conceptual design.</p></div><div class="hidden rs_skip" id="figureFootNote-F0002"></div></p></div><div id="S0005-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i17">5.2. Object creation</h3><p>Since object creation is not a natural manipulation, we use iconic gestures to create primitive shapes, which can then be combined and manipulated. Since people usually use virtual depiction (Marsh &amp;Watt, <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i46" href="#">1998</a></span>) to describe physical shapes using gestures, the program traces the gesture to set the outlines of a new object. To deal with dimensional underspecification (Sowa &amp; Wachsmuth, <span class="ref-lnk lazy-ref"><a data-rid="CIT0045" data-refLink="_i46" href="#">2002</a></span>), we assume the depicted outlines are the two most dominant (i.e., largest) dimensions. The third dimension will be set to an arbitrary minimum size. We also assume the depicted outlines are those of a flat face, perpendicular to the third dimension.</p><p>To allow the user to also create some important primitives that do not satisfy these assumptions, we created a vocabulary of iconic gestures to create a sphere, cylinder or cube. For this vocabulary we used the most popular iconic gestures used to describe these primitives in the 3DIG Dataset (Sadeghipour &amp; Morency, <span class="ref-lnk lazy-ref"><a data-rid="CIT0041" data-refLink="_i46" href="#">2014</a></span>).</p></div><div id="S0005-S2003" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i18">5.3. Grabbing</h3><p>For grabbing objects, we mimic physical manipulation: an object is grabbed by closing a hand around it. The interaction is designed without force feedback, so hand and fingers can freely pass through virtual objects. A more realistic, physics-based grasping technique was proposed by Borst and Indugula (<span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-refLink="_i46" href="#">2006</a></span>), using a virtual spring model of a hand that would apply force on the boundaries of the object and be restricted by it. The visual feedback would be the virtual hand grasping the virtual object, while the real fingers would pass through the virtual object. However, in their user tests, this created some practical difficulties, such as delayed release because of a deep grasp and difficult precision manipulations (for small objects) due to imperfections in tracking and hand model geometry.</p><p>We propose a pinching gesture for object grabbing consistent for all objects. When thumb and index finger touch, the pinching gesture is activated at the location of the touch. If the pinching gesture is activated within an object, the object is grabbed. This pinching gesture provides a single grabbing location, which simplifies the interaction model in contrast to full-hand grabbing. Pinching is also more easily recognized by hand-tracking devices such as the Leap Motion.</p><p>Grabbing objects enable different actions to be performed on that object, such as movement, scaling, and (un)grouping.</p></div><div id="S0005-S2004" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i19">5.4. Moving and rotating</h3><p>When an object is grabbed, moving the grabbing hand will move the object along with it. The 3D hand tracking allows a 1:1 translation of the hand position to the virtual object’s position. Furthermore, the relative rotation of the hand, while the object is grabbed, can be applied directly to the object. By grabbing an object, the user can therefore rotate and move the object directly in 6DoF.</p><p>This direct translation, being based on physical manipulation, should be intuitive and draw on the 3D eye–hand coordination that is learned from young age.</p></div><div id="S0005-S2005" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i20">5.5. Scaling</h3><p>Scaling is performed by an interaction mimicking stretching materials. The user can grab an object on two sides and increase or decrease the distance between the hands. The object boundaries will follow this hand movement, causing it to scale with that distance. Since the boundaries of the object “stick” to the user’s hands, it can still be moved and rotated during scaling.</p><p>As with the moving and rotating interaction, scaling draws on the learned coordination of the user with physical materials.</p></div><div id="S0005-S2006" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i21">5.6. Removing</h3><p>Removing an object from the virtual scene mimics the human behavior of throwing something away. Grabbing the object, moving it quickly, and releasing it during the movement will remove the object, after it has flown out of the screen.</p><p>With this action, it does not matter where the object will land. Therefore, the movement can be fast and uncontrolled. The user is likely to be familiar with throwing away objects.</p></div><div id="S0005-S2007" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i22">5.7. Grouping</h3><p>In CAD software, it is possible to group and select multiple objects for simultaneous movement. This way, the designer can create complex from several smaller ones, regarding the group as one larger object. In 2D interfaces, multiple objects can be selected by drawing a rectangle around the shapes.</p><p>In 3D, however, Oh, Stuerzlinger, and Dadgari (<span class="ref-lnk lazy-ref"><a data-rid="CIT0035" data-refLink="_i46" href="#">2006</a></span>) proposed a hierarchical grouping technique that, in a small user test, showed potential to make group selection more efficient. This grouping technique was based either on gravity or connectivity. In the first, objects that were positioned on top of each other would be grouped hierarchically such that moving the lower object would simultaneously move all objects on top of it. In the latter, objects that touched were automatically grouped.</p><p>A gravity-based grouping mechanism would assume the scene contains mostly loose objects stacked on top of each other. Due to the restrictions this assumption poses, we consider the connectivity grouping more practical for conceptual design. This would mimic physical manipulation based on sticky objects, making objects stick when pushed together. Grouping would therefore be as simple as moving one object partly into another, making them “stick” together. When, afterward, one of the objects is grabbed and moved or scaled, any grouped object will transform with it.</p><p>Ungrouping would similarly be based on the physical manipulation of ripping objects apart. In this interaction, there is a possible conflict with scaling. Grabbing two objects in the same group and moving them in different directions could release the grouping and enable independent movement, but the interaction already refers to scaling the object (group). We therefore propose an open hand pose for one of the hands, mimicking the action of holding back the other objects, while “ripping” the grabbed object apart from them.</p><p>When ungrouping two objects that are connected indirectly through other objects within the group, a historical hierarchy is maintained, separating the group at the link that was most recently made. This interaction assumes that components are first modeled independently before being attached to each other. The “weakest” link in the two connected components would then be the connection that was made last, joining the components together. Should this not be the case, the user can still separate the group at the desired link by grabbing the two objects directly involved in that link. That link will then be renewed and become the weakest link in the group. This allows smaller “modules” to be made and then combined, after which they can easily be detached from the others.</p></div><div id="S0005-S2008" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i23">5.8. Cutting</h3><p>The shape of objects can be controlled in more detail by a cutting operation. Shapes can be cut out of objects in a similar way as shapes can be created, by sketching its outlines with the index finger. Instead of sketching in the air, however, a cutting shape is sketched on the surface of an object. A tunnel is then created in the shape of the hole, all the way to the other side of the object. The hole can then be manipulated like any other object (e.g., moved around within the object, rotated, and scaled). Once again, this gesture draws on the natural use of iconic gestures.</p></div><div id="S0005-S2009" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i24">5.9. Changing view</h3><p>Using VR with head tracking brings the advantage of natural view changing. The user can simply move their head to look from a different angle, the same way they would do in the real world. However, if this is the only way of changing the view, it may put the designer in uncomfortable positions when viewing the model from certain angles for prolonged periods of time.</p><p>For this reason, we enable the user to move and rotate the workspace in two dimensions (left–right and forward–backward). The view can also be “zoomed” by enlarging or reducing the workspace and all objects in it. When both hands touch the workspace surface (which appears as a solid, flat cube, as if it were a table top), the workspace is moved, rotated, and scaled so as to keep the touching points at the hands. This interaction (except for the scaling) is similar to moving around a physical model on a tabletop. To zoom in or out, the user would place the hands on the surface and move them apart or together, respectively.</p></div><div id="S0005-S2010" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i25">5.10. Undo</h3><p>Undoing a previous action is the only action in this set that does not require accurate spatial coordination with an object. Therefore, we can use a relatively uncoordinated gesture so that it is easily distinguishable from the other gestures (both for the user to learn and for the system to recognize). The undo action has neither dimensions nor a “physical” virtual representation, so we cannot apply physical manipulation or iconic gestures. We therefore chose a simple “waving” gesture for the undo action. Unlike the other gestures, this does not need to be coordinated and targeted at a specific object or location. It is also easy to perform and yet is not easily triggered by mistake.</p></div><div id="S0005-S2011" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i26">5.11. Selection</h3><p>This set of interactive gestures was assembled based on the theoretical basis of natural gestures and conceptual design. Alternative selections of gestures are possible; however, this set of gestures is a theoretically suitable interaction model for conceptual design and will allow us to investigate the efficiency of gesture-based VR object manipulations for conceptual design.</p></div></div><div id="S0006" class="NLM_sec NLM_sec_level_1"><h2 id="_i27" class="section-heading-2">6. Experiment</h2><p>In an experimental setup, we compared the usability, task load, and performance of two gesture-based interfaces (with screen or VR) and a traditional mouse and screen interface, with regards to basic object manipulation. For this, we investigated a basic subset of design interactions, including movement, rotation, scaling, and undo. Creation and deletion of objects were left out in order to limit the complexity of the experiment.</p><p>We implemented a prototype consisting of off-the-shelf consumer components, to investigate whether this novel interface could be achieved with current, mainstream technology. The hardware behind the prototype consisted of a laptop (running the graphics smoothly), the Oculus Rift (Development Kit 1) VR headset, and the Leap Motion (version 2) hand-tracking device. The Oculus Rift has a wide angle view and head-orientation tracking and the Leap Motion is a small optical device that performs full-hand tracking in an area of about 60 cm × 60 cm × 60 cm. Since the focus of this experiment was on interaction and not on feature richness, we implemented the prototype in the Unity 4.6 game engine on Windows 7, rather than a CAD tool, since the first provides more tools for customization of interaction than the latter. Both Oculus and Leap Motion provide assets (plugins) for Unity to program with the hardware.</p><p>In our prototype, we implemented the interactions as described in <a href="#F0002">Figure 2</a>: movement (e), rotation (e/f), scaling (f), and undo (j). Virtual hands were visible, following the movement and pose of the user’s hands. When thumb and index finger touched, the touch point was used as the location for a grabbing gesture (for movement, rotation and scaling). That point on the object would then follow the touch point. The undo gesture was implemented as three direction changes (above a threshold magnitude) of a hand in 0.5 s. This would undo the transformations made on the current object.</p><p>We compared the new interactive system with a traditional setup, using a mouse and a screen. The Unity Editor itself provided typical manipulation tools for object movement, rotation, scaling, and undo (<a href="#F0003">Figure 4</a>), using the mouse, keyboard, and screen. At the top of the screen, there were buttons for three main object manipulation modes: moving, rotating, and scaling. Clicking an object in either of these modes would show the handles that could be dragged to manipulate the object in that mode. The camera could be zoomed using the scroll wheel, rotated by pressing Alt while dragging the mouse, and moved using the “hand” tool or pressing Alt Gr while dragging the mouse. All buttons and features of the Unity Editor that were irrelevant for the basic object manipulations were hidden. These editor tools allowed us to test the different interfaces on similar object manipulation tasks in the same environment.<div class="figure figureViewer" id="F0003"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 4. </span> Object manipulation in Unity Editor. Buttons top-left switch modes (e.g., move/scale/rotate), handles on the object can be dragged to control the mode’s dimensions. Spacebar could be used to undo the current transformations. All other interfaces of the editor were hidden from the participant.</p></div></td></div><a href="#" class="thumbnail"><img id="F0003image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0003_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0003"><p><span class="captionLabel">Figure 4. </span> Object manipulation in Unity Editor. Buttons top-left switch modes (e.g., move/scale/rotate), handles on the object can be dragged to control the mode’s dimensions. Spacebar could be used to undo the current transformations. All other interfaces of the editor were hidden from the participant.</p></div><div class="hidden rs_skip" id="figureFootNote-F0003"></div></p><p>Each participant was randomly assigned to one of three conditions: (1) Mouse &amp; Screen, (2) Hands &amp; Screen, and (3) Hands &amp; VR. The second condition was added to gain extra insight on the influence of the VR component on the modeling tasks.</p><div id="S0006-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i29">6.1. Tasks</h3><p>Each participant was asked to go through one <i>trial</i>. Each trial consisted of six to nine <i>tasks</i>, in which a yellow <i>shape</i> appeared and had to be transformed (moved, scaled and/or rotated) to match a red, transparent <i>target</i> (<a href="#F0004">Figure 5</a>). Considering the sketching nature of conceptual design, the target did not require 100% overlap with the shape. Instead, the target was completed when at least 40% of the target and shape overlapped, which, in our view, is a reasonable fit for a 3D sketch. After each task, the previous shape would turn transparent gray and the next shape-target set appeared. The order of the tasks within each trial was randomized to control for order effects. Finally, the correctly transformed shapes would together form a model (e.g., a house or table and chair), which was transparently visible throughout the trial.<div class="figure figureViewer" id="F0004"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 5. </span> Experiment task. The yellow shape must be fitted into the red target shape by moving, scaling, and/or rotating. When the target is completed (40% overlap of shape and target), the shape turns gray, the next target is highlighted red and the corresponding shape appears at its start position. In this example, the yellow shape must be moved forward and scaled horizontally to fit in the red target. The gray shapes are (inactive) previous and next tasks.</p></div></td></div><a href="#" class="thumbnail"><img id="F0004image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0004_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0004"><p><span class="captionLabel">Figure 5. </span> Experiment task. The yellow shape must be fitted into the red target shape by moving, scaling, and/or rotating. When the target is completed (40% overlap of shape and target), the shape turns gray, the next target is highlighted red and the corresponding shape appears at its start position. In this example, the yellow shape must be moved forward and scaled horizontally to fit in the red target. The gray shapes are (inactive) previous and next tasks.</p></div><div class="hidden rs_skip" id="figureFootNote-F0004"></div></p><p>To find differences between transformation types, tasks were chosen to be in four different categories: Move, MoveRotate, MoveScale, and MoveRotateScale. These categories we call <i>task types</i> and they encode what kind of transformations has to be performed on the shape to fit the target (for instance, shapes in the task category MoveRotate should be moved in and rotated). Example tasks for each task type are illustrated in <a href="#F0005">Figure 6</a>.<div class="figure figureViewer" id="F0005"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 6. </span> Task types.</p></div></td></div><a href="#" class="thumbnail"><img id="F0005image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0005_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0005"><p><span class="captionLabel">Figure 6. </span> Task types.</p></div><div class="hidden rs_skip" id="figureFootNote-F0005"></div></p><p>The objective was to complete the trial as quickly as possible. The trial was presented as a game and high scores with the other participants were displayed at the end of the trial, to motivate participants to complete it as quickly as possible, instead of taking the time to try out the technology. When, during the trial, it was clear to the experimenter that a participant had misunderstood or forgotten about the rules or controls, they were explained again. This was done to focus the experiment on the possible proficiency with the used medium instead of the retainment of knowledge about the specific interface.</p></div><div id="S0006-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i32">6.2. Procedure</h3><p>Before each trial, the participant was introduced to the interactions with the CAD software, during a 5-min “Sandbox” mode, in which they could freely manipulate a single cube in the virtual space. Since the Leap Motion hand tracking showed inaccuracies in usage during preliminary testing, some extra time was taken in the Hands &amp; Screen and Hands &amp; VR conditions, to instruct participants about the optimal hand poses and interactions with the Leap Motion tracker. Since participants generally already know how to use a mouse, but not the Leap Motion, we consider this a fair instruction.</p><p>After the Sandbox mode, a practice task was presented to the participant, containing trial instructions on the screen. The measured trial started as soon as the participant finished the practice task. Since each participant needed some training time, six to nine tasks were presented per participant in order to gather a reasonable amount of data. This is taken into account in the analysis of the results.</p></div><div id="S0006-S2003" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i33">6.3. Methods</h3><p>During the trial, duration and accuracy of each task was recorded automatically, as well as the type of transformation (moving/rotating/scaling). Although the tasks were randomized to minimize any learning effects, the task number (in order of appearance) was recorded to find any learning effect, if present.</p><p>At the end of each trial, the participant was asked to fill out a questionnaire with demographic and experience data, standardized System Usability Scale (SUS) questions (Brooke, <span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-refLink="_i46" href="#">1996</a></span>), a modified version of NASA Task Load Index (TLX) questions (Hart &amp; Staveland, <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-refLink="_i46" href="#">1988</a></span>) and specific questions on the interactions. For all questionnaires, a 5-point Likert scale was used. The used questionnaires are shown in Appendix A.</p><p>Finally, observations and user comments were noted by the experimenter, for qualitative analysis.</p></div></div><div id="S0007" class="NLM_sec NLM_sec_level_1"><h2 id="_i34" class="section-heading-2">7. Results</h2><p>In the experiments, 33 participants performed a total of 243 tasks, in 3 (randomly assigned) groups of 11 participants per condition (Mouse &amp; Screen, Hands &amp; Screen, Hands &amp; VR). All participants were volunteers, 24 of whom were undergraduates in the Media Technology program at Leiden University in the Netherlands. Most of the participants (23) did not have any prior experience with Leap Motion and 11 of them had no prior experience with CAD. There were 23 males and 10 females between 22 and 31 years old, the majority (21) of Dutch nationality. The complete experiment took about 20 min per participant including the instructions. The recorded trials took an average of 11 min, with a standard deviation of 7 min.</p><div id="S0007-S2001" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i35">7.1. Task performance</h3><p>The three interfaces (conditions) were compared on the users’ ability to quickly transform shapes to match a target’s position, rotation, and/or size. Due to the experimental design with multiple tasks per participant, a mixed model ANOVA with random effects was run in R (R Core Team, <span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-refLink="_i46" href="#">2014</a></span>), package nlme (Pinheiro, Bates, DebRoy, Sarkar, &amp; R Core Team, <span class="ref-lnk lazy-ref"><a data-rid="CIT0037" data-refLink="_i46" href="#">2014</a></span>) to compare the differences of the means between the three interfaces. In the mixed model, task duration was the dependent variable with condition and task type as fixed effects. Since a different interface might perform differently for different task types, an interaction effect was modeled between condition and task type. To control for having multiple samples per participant, the participant factor was added as a random effect.</p><p>There was a statistically significant difference of the mean duration over both the conditions (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0001.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0001.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0002.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0002.gif&quot;}" /><span class="no-mml-formula"></span></span>) and task types (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0003.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0003.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0004.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0004.gif&quot;}" /><span class="no-mml-formula"></span></span>). Also the interaction between condition and task type was significant (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0005.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0005.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0006.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0006.gif&quot;}" /><span class="no-mml-formula"></span></span>). A Tukey <i>post-hoc</i> test using the multcomp package (Hothorn, Bretz, &amp; Westfall, <span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-refLink="_i46" href="#">2008</a></span>) revealed a significant difference between conditions Hands &amp; Screen and Mouse &amp; Screen (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0007.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0007.gif&quot;}" /><span class="no-mml-formula"></span></span>), as well as between Hands &amp; Screen and Hands &amp; VR (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0008.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0008.gif&quot;}" /><span class="no-mml-formula"></span></span>). However, no significant difference was found between Mouse &amp; Screen and Hands &amp; VR. The mean duration and standard deviation of each task are displayed in <a href="#F0007">Figure 7</a>. Participants in the Mouse &amp; Screen condition were fastest, however, not significantly faster than those in the Hands &amp; VR condition. On the other hand, participants the Hands &amp; Screen condition required nearly twice as much time as those in the Mouse &amp; Screen condition.<div class="figure figureViewer" id="F0006"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 3. </span> Participant resizing a shape during experiment. Left: The participant wearing an Oculus Rift VR headset, his hands being tracked by the Leap Motion. Right: The stereoscopic image the participant could see through the display.</p></div></td></div><a href="#" class="thumbnail"><img id="F0006image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0006_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0006"><p><span class="captionLabel">Figure 3. </span> Participant resizing a shape during experiment. Left: The participant wearing an Oculus Rift VR headset, his hands being tracked by the Leap Motion. Right: The stereoscopic image the participant could see through the display.</p></div><div class="hidden rs_skip" id="figureFootNote-F0006"></div><div class="figure figureViewer" id="F0007"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 7. </span> Means, standard-deviations, and coefficient of variation of task duration in the different interface conditions.</p></div></td></div><a href="#" class="thumbnail"><img id="F0007image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0007_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0007"><p><span class="captionLabel">Figure 7. </span> Means, standard-deviations, and coefficient of variation of task duration in the different interface conditions.</p></div><div class="hidden rs_skip" id="figureFootNote-F0007"></div></p><p>The mean duration of each task type per condition is displayed in <a href="#F0008">Figure 8</a>. In nearly all task types, participants in the Hands &amp; Screen condition were the slowest, while the other two conditions did not seem to differ much. The exception seems to be the task type MoveRotateScale, in which the condition Mouse &amp; Screen appears to take more time than the other conditions, and with a larger variance.<div class="figure figureViewer" id="F0008"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 8. </span> Tukey boxplots of distributions of task duration per condition and task type.</p></div></td></div><a href="#" class="thumbnail"><img id="F0008image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0008_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0008"><p><span class="captionLabel">Figure 8. </span> Tukey boxplots of distributions of task duration per condition and task type.</p></div><div class="hidden rs_skip" id="figureFootNote-F0008"></div></p></div><div id="S0007-S2002" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i39">7.2. Questionnaires</h3><p>The questionnaires at the end of each trial contained SUS questions, NASA TLX questions, and extra questions specifically about these tasks. The distribution of answers to these questions per condition is visualized in Figures B1, B2, and B3 (Appendix B), respectively.</p><p>The SUS questions and TLX questions were converted to scale numbers (0–4) and summed up per participant. For negatively posed questions, contributions were calculated as <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0009.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0009.gif&quot;}" /><span class="no-mml-formula"></span></span>, as described by Brooke (<span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-refLink="_i46" href="#">1996</a></span>). For the TLX questionnaires, the weights of the questions in the sums were set equal. Finally, the sums were normalized to a range between 0 and 100. The distribution of scores is displayed in <a href="#F0009">Figure 9</a>.<div class="figure figureViewer" id="F0009"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 9. </span> SUS and TLX scores per condition.</p></div></td></div><a href="#" class="thumbnail"><img id="F0009image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0009_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0009"><p><span class="captionLabel">Figure 9. </span> SUS and TLX scores per condition.</p></div><div class="hidden rs_skip" id="figureFootNote-F0009"></div></p><p>Both SUS and TLX scores were statistically analyzed using a pairwise <i>t</i>-test in R (R Core Team, <span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-refLink="_i46" href="#">2014</a></span>), with a Bonferroni correction for multiple comparisons. In the SUS score analysis, none of the groups were significantly different from the others, as shown in <a href="#F0010">Figure 10</a>. In the TLX score analysis, a significant difference was found only between the conditions Hands &amp; Screen and Mouse &amp; Screen, as shown in <a href="#F0011">Figure 11</a>. Participants in the Mouse &amp; Screen condition reported a lower perceived task load than those in the Hands &amp; Screen condition.<div class="figure figureViewer" id="F0010"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 10. </span> <i>p</i> and <i>t</i>-values of differences between pairs of conditions on SUS scores, as a result of a pairwise <i>t</i>-test with Bonferroni correction for multiple comparisons.</p></div></td></div><a href="#" class="thumbnail"><img id="F0010image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0010_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0010"><p><span class="captionLabel">Figure 10. </span> <i>p</i> and <i>t</i>-values of differences between pairs of conditions on SUS scores, as a result of a pairwise <i>t</i>-test with Bonferroni correction for multiple comparisons.</p></div><div class="hidden rs_skip" id="figureFootNote-F0010"></div><div class="figure figureViewer" id="F0011"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure 11. </span> <i>p</i> and <i>t</i>-values of differences between pairs of conditions on TLX scores, as a result of a pairwise <i>t</i>-test with Bonferroni correction for multiple comparisons. Marked with an asterisk (*) is a significant difference (<i>p &lt; </i>0.05).</p></div></td></div><a href="#" class="thumbnail"><img id="F0011image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0011_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0011"><p><span class="captionLabel">Figure 11. </span> <i>p</i> and <i>t</i>-values of differences between pairs of conditions on TLX scores, as a result of a pairwise <i>t</i>-test with Bonferroni correction for multiple comparisons. Marked with an asterisk (*) is a significant difference (<i>p &lt; </i>0.05).</p></div><div class="hidden rs_skip" id="figureFootNote-F0011"></div></p><p>All questions were tested using a Kruskal–Wallis test, which is suitable for a Likert-style questionnaire because it deals with ordinal data, does not require normal distributions, and allows more than two groups to be tested. When a significant difference was found between the conditions on Likert-style questions, the contrasts between conditions were tested using a <i>post-hoc</i> Dunn test, which is appropriate to follow after the Kruskal–Wallis test, again with a Bonferroni adjustment to control for multiple comparisons. The, per question computed, <i>p</i>-values are reported in Appendix C, Figure C1, C2, and C3 for SUS, TLX and extra questions, respectively.</p><p>In the SUS questions, a significant difference was found in Kruskal–Wallis test on the statement that the participant required support of a technical person (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0010.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0010.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0011.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0011.gif&quot;}" /><span class="no-mml-formula"></span></span>). The Dunn-test showed that participants in the Hands &amp; Screen condition agreed significantly more with this statement than those in the Hands &amp; VR condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0012.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0012.gif&quot;}" /><span class="no-mml-formula"></span></span>) and than those in the Hands &amp; VR condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0013.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0013.gif&quot;}" /><span class="no-mml-formula"></span></span>). Another significant difference was found on the statement that the interface was easy to use (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0014.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0014.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0015.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0015.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Mouse &amp; Screen interface agreed more than in the Hands &amp; Screen interface (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0016.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0016.gif&quot;}" /><span class="no-mml-formula"></span></span>). A third significant difference was found on the statement that the system was unnecessarily complex (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0017.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0017.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0018.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0018.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Hands &amp; Screen condition agreed significantly more than those in the Hands &amp; VR condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0019.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0019.gif&quot;}" /><span class="no-mml-formula"></span></span>). The contrasts between conditions that were not mentioned in these three questions were statistically insignificant.</p><p>All conditions were statistically indistinguishable in the (dis)agreement of participants on the SUS statements that they would use the system frequently (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0020.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0020.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0021.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0021.gif&quot;}" /><span class="no-mml-formula"></span></span>), the various functions were well integrated (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0022.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0022.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0023.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0023.gif&quot;}" /><span class="no-mml-formula"></span></span>), there was too much inconsistency (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0024.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0024.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0025.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0025.gif&quot;}" /><span class="no-mml-formula"></span></span>), the system was very cumbersome to use (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0026.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0026.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0027.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0027.gif&quot;}" /><span class="no-mml-formula"></span></span>), participants felt confident using the system (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0028.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0028.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0029.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0029.gif&quot;}" /><span class="no-mml-formula"></span></span>), they needed to learn a lot before they could get going (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0030.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0030.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0031.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0031.gif&quot;}" /><span class="no-mml-formula"></span></span>), and that they would image most people would learn to use the system quickly (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0032.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0032.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0033.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0033.gif&quot;}" /><span class="no-mml-formula"></span></span>).</p><p>In the TLX questions, a significant difference was found in the statement that the tasks were physically demanding (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0034.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0034.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0035.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0035.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Hands &amp; VR condition agreed more than those in the Mouse &amp; Screen condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0036.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0036.gif&quot;}" /><span class="no-mml-formula"></span></span>). Also, the statement that the participant found they were successful in accomplishing their tasks showed a significant difference (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0037.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0037.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0038.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0038.gif&quot;}" /><span class="no-mml-formula"></span></span>): participants in the Mouse &amp; Screen condition agreed more than those in the Hands &amp; Screen condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0039.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0039.gif&quot;}" /><span class="no-mml-formula"></span></span>). The last significant difference in TLX questionnaire was found in the statement that the participant had to work hard to accomplish their level of performance (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0040.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0040.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0041.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0041.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Hands &amp; Screen condition agreed significantly more than those in the Mouse &amp; Screen condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0042.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0042.gif&quot;}" /><span class="no-mml-formula"></span></span>).</p><p>No significant differences were found in the reported (dis)agreement of participants on the statements that the tasks were mentally demanding (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0043.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0043.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0044.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0044.gif&quot;}" /><span class="no-mml-formula"></span></span>), the tasks were rushed or hurried (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0045.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0045.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0046.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0046.gif&quot;}" /><span class="no-mml-formula"></span></span>), the participants were insecure, discouraged, irritated, stressed, or annoyed (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0047.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0047.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0048.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0048.gif&quot;}" /><span class="no-mml-formula"></span></span>).</p><p>In our extra questions, there was a significant difference between conditions in the agreements to the statement that the participant could easily rotate objects to their desired state (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0049.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0049.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0050.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0050.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Mouse &amp; Screen state agreed more than those in the Hands &amp; VR state (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0051.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0051.gif&quot;}" /><span class="no-mml-formula"></span></span>). A significant difference was also found in the statement that the input device worked according to their intentions (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0052.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0052.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0053.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0053.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Mouse &amp; Screen condition agreed more than in those in the Hands &amp; VR condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0054.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0054.gif&quot;}" /><span class="no-mml-formula"></span></span>). The final significant difference was found in the statement that the participant had trouble perceiving 3D locations of objects (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0055.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0055.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0056.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0056.gif&quot;}" /><span class="no-mml-formula"></span></span>), where participants in the Hands &amp; VR condition agreed significantly less than those in the Mouse &amp; Screen condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0057.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0057.gif&quot;}" /><span class="no-mml-formula"></span></span>) and the Hands &amp; Screen condition (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0058.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0058.gif&quot;}" /><span class="no-mml-formula"></span></span>). There were no significant differences found between conditions on the statements regarding ease of moving shapes (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0059.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0059.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0060.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0060.gif&quot;}" /><span class="no-mml-formula"></span></span>), ease of scaling shapes (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0061.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0061.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0062.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0062.gif&quot;}" /><span class="no-mml-formula"></span></span>), amount of spatial overview (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0063.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0063.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0064.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0064.gif&quot;}" /><span class="no-mml-formula"></span></span>), enjoyment of working with the tool (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0065.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0065.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0066.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0066.gif&quot;}" /><span class="no-mml-formula"></span></span>), whether the tool could help them gain insight into spatial problems or designs (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0067.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0067.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0068.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0068.gif&quot;}" /><span class="no-mml-formula"></span></span>), and naturalness of the interface (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0069.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0069.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0070.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0070.gif&quot;}" /><span class="no-mml-formula"></span></span>).</p><p>Over all conditions, 66% of participants reported a preference for using the hands over a mouse and 10% preferred using a mouse over the hands (24% had no preference). Furthermore, 59% reported a preference for using VR over a screen and 14% preferred using a screen over VR (27% had no preference). There was no statistically significant difference in preference between conditions (<span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0071.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0071.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0072.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0072.gif&quot;}" /><span class="no-mml-formula"></span></span> and <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0073.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0073.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0074.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0074.gif&quot;}" /><span class="no-mml-formula"></span></span>, respectively), meaning these preferences were regardless of the interface the participant used.</p></div><div id="S0007-S2003" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i43">7.3. Observations and user comments</h3><p>During the experiments, observations were made about the interactions of the participants with the interface. Together with the open questions in the questionnaire, they might shed some light on some issues that are difficult to quantify.</p><p>First of all, participants generally had to get familiar with the tracking behavior of the Leap Motion. Not all hand poses are equally easy to recognize for the tracking device. For example, at all times during interaction, the camera inside the device should have a clear line of sight to the user’s index finger and thumb (which were the main fingers used in this experiment). Besides the hand pose, participants needed to get used to the limited working area. The Leap Motion has a tracking area of roughly 60 cm × 60 cm × 60 cm. To interact anywhere outside that area, the user must move the view toward that point.</p><p>A more technology independent issue is letting go of an object. As discussed in <a href="#S0004-S2001">Section 4.1</a>, it was observed that the point at which the virtual object is released is critical for accurate placement of the object. Many users experienced virtual objects “sticking” to their thumb while they were already retracting their hand.</p><p>Another note to be made is that the system suggests a higher level of naturalness and realism than it can support. When users first see their “own” hand as a virtual hand, moving the same way as they feel it moving, many of them seem to expect it to behave much the same as their real hand would, including interaction with objects. Without instructions, the first response of many users to a virtual cube is to grab it with the whole hand, while that is actually one of the hardest hand poses to track.</p><p>Some participants also commented about a lack of feedback during the interactions. Some mentioned haptic feedback as a way to make the interaction feel more natural, others suggested more visual feedback as a way to communicate the internal state of the software. For example, a (clearer) visual signal when the virtual hand entered its “pinching” state.</p></div><div id="S0007-S2004" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i44">7.4. Discussion</h3><p>Our hypothesis that a hand-based VR interface would allow faster 3D object manipulation was not yet completely supported by the results. Our prototype was generally received with enthusiasm and participants’ performance could not be distinguished between the new media (VR and hand tracking) and the traditional mouse and screen; however, some issues should be addressed if this novel interaction technique is to outperform the latter.</p><p>The results and observations of our experiment suggest that there are some issues yet to overcome before the proposed type of interaction could exceed the accuracy, user training, and feature-richness of the established mouse-and-screen interface.</p><p>Tracking errors were one issue that seemed to inhibit task efficiency. The limited working area and accuracy of hand pose tracking did not allow the users to interact as freely and naturally as is necessary for a productive ideation process.</p><p>Another issue was found in the interaction design: since the virtual hands look very natural and follow the user’s real hand quite accurately up to some point, the interface suggested to the users that the interaction would be very natural as well. Instead, the interactions and effective hand poses were limited. This caused a mismatch between the user’s gestures and the gestures that could be recognized by the system and the tracker. In terms of the <i>Gulfs of execution and evaluation</i> of Norman (<span class="ref-lnk lazy-ref"><a data-rid="CIT0034" data-refLink="_i46" href="#">1986</a></span>), we attempted to bridge the Gulf of Execution by bringing the interaction closer to the user’s natural intentions. However, by suggesting interaction the system cannot support, the user’s expectations of the system state as a result of their actions are not met. Therefore, the Gulf of Evaluation should be bridged to a greater extent, by providing the user with feedback that accurately reflects the system’s complexity and capabilities. It should be investigated which type of feedback is most effective in enabling the user to make this mental mapping.</p><p>The results in our experiment give an indication of the comparison between hand-tracking interfaces in VR and conventional interfaces, with regards to 3D object manipulations.</p><p>An interesting result was the significant difference between the hands and screen interface, compared to the hands and VR interface. The VR component of our interface seems to have a positive effect on the visual perception of 3D space, allowing us to better perceive distance toward the objects we are interacting with. This is backed up by the questionnaire results, in which users reported having less trouble perceiving depth in this Hands &amp; VR condition. A similar conclusion was drawn by Loup-Escande, Jamet, Ragot, Erhel, and Michinov (<span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-refLink="_i46" href="#">2017</a></span>), after an experiment in which participants were more successful in a spatial learning task using stereoscopic display than those using monoscopic display.</p><p>The results from the questionnaires confirm that our proposed Hands &amp; VR interface performs similarly to the traditional Mouse &amp; Screen interface on most measured aspects, with a few differences.</p><p>Apparently, object rotation was found easier using a mouse and screen, which may be due to restricted wrist movement in the hand-tracking interface. While we estimate a wrist to be able to rotate an object for 100° at most before requiring a new grasp, the mouse-based interface allows infinite rotation by using 2D handles.</p><p>Furthermore, the mouse was considered more reliable as an input device than the hand-tracking Leap Motion. As consumer-grade hand-tracking technology is still in its early phases, we hope this difference will soon disappear. We expect the ease of manipulation (moving, rotating, scaling, etc.) to improve with it.</p><p>According to the task load results, the third dimension provided by hand tracking comes at the cost of more physical demand. The arms cannot rest on a desk while gesturing, as they can while using a mouse. This would make the interface less suitable for extended use. It would be better suited for short, early conceptual design sessions.</p><p>Two final notes on the experiment should be considered. First, as a representative of the mouse-based interaction, the Unity3D Editor was chosen. Although this allows an equal comparison between hand-based interaction and mouse-based interaction using the same software in the same environment, the Unity3D Editor, as a game engine, may or may not be an accurate representative of the mouse-based CAD. We have not tested how the Unity Editor controls compare to other CAD interfaces. However, the interface had similar features to other CAD software, such as draggable handles on the object and axis-locked transformations and 2D transformations. In any case, it allowed us to measure, to some extent, the coordination of the participants during basic object manipulations.</p><p>Second, the tasks, in which participants were asked to transform one shape into another, were created by the experimenter. For each task type, several tasks were created manually to fit within that task type. The 243 recorded tasks therefore consist of a multiple of approximately 15 unique tasks (6–9 per participant). This allowed for more equal comparison between conditions; however, the possibility of the tasks not being representative for the task types was not specifically probed.</p><p>Our hand tracking, VR interface did not yet outperform the mouse and screen interface; however, it also did not perform worse. The mouse has been developed over many years to become the leading pointing device in human–computer interaction. It has an integrated role in our society: most people learn to use one already at a young age. In comparison, most participants in this study used the hand-tracking input device for the first time. That being said, (consumer) hand-tracking technology is still in its infancy and will probably receive a large development boost in the next few years. Performance may improve with user practice, along with many configurable parameters in hand tracking technology, which is similar to the conclusion by Canare, Chaparro, and He (<span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i46" href="#">2015</a></span>) after a comparative study between hand/gaze tracking and a mouse for 2D pointing tasks.</p><p>With further improvements in tracking accuracy and research in hand-based interaction models, along with the development of VR, a 3D hand-based VR conceptual design tool will have a strong reason for existence in the near future.</p></div></div><div id="S0008" class="NLM_sec NLM_sec_level_1"><h2 id="_i45" class="section-heading-2">8. Conclusions and future work</h2><p>CAD software is suitable for later stages of design but often less practical for the conceptual design phase. The required detail of the input inhibits the flow of ideas and the traditional 2D mouse and screen based interface requires 2D–3D conversion of input and output, restricting the natural mapping from the physical to the virtual world. While many researchers replace the 2D mouse with a 3D pointing device, we investigated the use of hand gestures and VR as a natural, intuitive 3D interface.</p><p>We proposed a gestural interaction model for conceptual design in VR. The gestures are based on natural manipulative and communicative gestures that stimulate thought, insight and conceptualization.</p><p>Participants using the VR and hand-tracking interface were, in general, not detectably slower nor faster than those using the mouse and screen interface on basic 3D object manipulations. For tasks that included moving and rotating as well as scaling of objects, participants with hand tracking and VR seemed to outperform those with the mouse and screen in efficiency, and with a lower variance. However, this difference was not yet found to be statistically significant.</p><p>Participants using VR and hands outperformed those using only hands and a screen. They also reported having less trouble with depth perception, compared to both screen conditions, suggesting the VR headset has a positive effect on eye–hand coordination, improving the efficiency at which users can manually interact with objects in virtual space.</p><p>Improvements could be made in both the interaction design and tracking technology. The interface should clearly reflect which gestures are (not) supported by the system. A model of a physical hand can therefore be confusing if it does not support the perceived affordance. The used tracking technology did not support all user intended interactions during the basic object manipulation tasks. Improved tracking technology could therefore also contribute toward a more effective conceptual design interface.</p><p>In the experiment, we tested three conditions: Mouse &amp; Screen, Hands &amp; Screen, and Hands &amp; VR. One (possibly very interesting) condition could also be explored: Mouse &amp; VR. To measure the impact of the VR display independently from the hand-tracking interface, it would be very interesting to include this condition in an experiment. However, this would bring an important issue to take into account: the combination of a 2D input device with a 3D display would require a new interaction paradigm. The mouse pointer (which is able to move in 2D) needs to be displayed in 3D space. Therefore, a <i>z</i>-value must be chosen for the <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0075.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0075.gif&quot;}" /><span class="no-mml-formula"></span></span>-location of the cursor. Furthermore, on a 2D screen, object selection can be detected simply by comparing <i>x</i> and <i>y</i> values. In 3D space on the 2D screen, a “ray” is cast from the screen into 3D space and the first hit object is selected. In a 3D (stereoscopic) interface, the origin of the ray cannot be chosen in such a way that, for both eyes, the cursor occludes the point clicked in 3D space, as long as the cursor has a fixed <i>z</i> value from the camera.</p><p>This research focused on an interface using free hand-gesture interaction in VR; however, other setups could still be explored. For instance, an AR setup in which virtual objects are overlayed on the image of the user’s hands would allow the use of real objects as “tools” to manipulate virtual objects (Datcu, Lukosch, &amp; Brazier, <span class="ref-lnk lazy-ref"><a data-rid="CIT0012" data-refLink="_i46" href="#">2015</a></span>). Furthermore, allowing users to specify their own set of gestures (Bordegoni, <span class="ref-lnk lazy-ref"><a data-rid="CIT0004" data-refLink="_i46" href="#">1994</a></span>) might take more time to setup the system, but for those users could, in the long run, improve efficiency.</p><p>For further exploration of the potential of hand-based interaction in 3D conceptual design, learning rate could be measured in a longitudinal study over an extended period of time. Moreover, measuring distance in manipulation tasks would give insight into the performance at different scales.</p><p>The focus of this experiment was on using off-the-shelf components, to investigate the feasibility of the proposed interaction model using current mainstream technology. For insight in the future potential of the interaction model, the experiment could be repeated using state-of-the-art components, such as high-accuracy data gloves and a high resolution VR headset. Such systems may have a reduced error rate compared to the interfaces used in this experiment and even outperform mouse and screen interface. With the presented data as a baseline, the development of new technologies can be monitored over time in longitudinal follow-up studies. The availability of benchmark data for new technologies would help to shape improvements.</p><p>Our experiment provides insight on the usability of a gesture-based VR interface in virtual object manipulations; however, further research is required to evaluate its effectiveness in conceptual design. Therefore, a complete prototype of the proposed interaction model should be compared to traditional tools in real world conceptual design tasks, using metrics based on models of creativity and ideation effectiveness (e.g., Shah, Smith, &amp; Vargas-Hernandez, <span class="ref-lnk lazy-ref"><a data-rid="CIT0043" data-refLink="_i46" href="#">2003</a></span>; Suwa et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0047" data-refLink="_i46" href="#">2006</a></span>).</p></div><div class="NLM_app-group"><div id="app0001" class="NLM_sec NLM_sec-type_appendix"><div id="S0009" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i47">Appendix A. QUESTIONNAIRES</h3><p>The following are the questions from the questionnaires given to participants after the trials. Participants could rate each question on a 5-point Likert scale (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree).</p><p><b>SUS questions</b></p><p>1. I think that I would like to use this system frequently.</p><p>2. I found the system unnecessarily complex.</p><p>3. I thought that the system was easy to use.</p><p>4. I think that I would need the support of a technical person to be able to use this system.</p><p>5. I found that the various functions in this system were well integrated.</p><p>6. I thought there was too much inconsistency in this system.</p><p>7. I would imagine that most people would learn to use this system very quickly.</p><p>8. I found the system very cumbersome to use.</p><p>9. I felt very confident using the system.</p><p>10. I needed to learn a lot of things before I could get going with this system.</p><p><b>TLX questions</b></p><p>1. The tasks were mentally demanding.</p><p>2. The tasks were physically demanding.</p><p>3. The tasks were very rushed or hurried.</p><p>4. I was successful in accomplishing what I was asked to do.</p><p>5. I had to work hard to accomplish my level of performance.</p><p>6. I was insecure, discouraged, irritated, stressed, or annoyed.</p><p><b>Extra questions</b></p><p>1. I could easily move the shapes to their desired position.</p><p>2. I could easily rotate the shapes to their desired state.</p><p>3. I could easily scale the shapes to their desired size.</p><p>4. I had a good spatial overview while performing the tasks.</p><p>5. This tool could help me gain insight into a spatial problem/design.</p><p>6. The input device worked according to my intentions.</p><p>7. I had trouble perceiving the 3D locations of objects.</p><p>8. The interface felt natural.</p><p>9. I enjoyed working with this tool.</p><p>10. I would prefer designing with my hands over using a mouse (assuming they both flawlessly).</p><p>11. I would prefer designing in virtual reality over using a screen.</p></div></div><div id="app0002" class="NLM_sec NLM_sec-type_appendix"><div id="S0010" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i48">Appendix B. QUESTIONNAIRE RESPONSE DISTRIBUTIONS</h3><p>The figures below illustrate the results of the questionnaires for each answer, grouped by condition. <div class="figure figureViewer" id="F0012"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure B1. </span> Distribution of answers to SUS questionnaire, grouped per condition.</p></div></td></div><a href="#" class="thumbnail"><img id="F0012image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0012_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0012"><p><span class="captionLabel">Figure B1. </span> Distribution of answers to SUS questionnaire, grouped per condition.</p></div><div class="hidden rs_skip" id="figureFootNote-F0012"></div> <div class="figure figureViewer" id="F0013"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure B2. </span> Distribution of answers to TLX questionnaire, grouped per condition.</p></div></td></div><a href="#" class="thumbnail"><img id="F0013image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0013_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0013"><p><span class="captionLabel">Figure B2. </span> Distribution of answers to TLX questionnaire, grouped per condition.</p></div><div class="hidden rs_skip" id="figureFootNote-F0013"></div> <div class="figure figureViewer" id="F0014"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure B3. </span> Distribution of answers to extra questions, grouped per condition.</p></div></td></div><a href="#" class="thumbnail"><img id="F0014image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0014_oc.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0014"><p><span class="captionLabel">Figure B3. </span> Distribution of answers to extra questions, grouped per condition.</p></div><div class="hidden rs_skip" id="figureFootNote-F0014"></div></p></div></div><div id="app0003" class="NLM_sec NLM_sec-type_appendix"><div id="S0011" class="NLM_sec NLM_sec_level_2"><h3 class="section-heading-3" id="_i52">Appendix C. QUESTIONNAIRE SIGNIFICANCE VALUES</h3><p>The tables show the results of the Kruskal–Wallis test by ranks and the Dunn <i>post-hoc</i> test for group comparisons. Asterisks (*) indicate significant <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0076.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0076.gif&quot;}" /><span class="no-mml-formula"></span></span>-values (at significance level <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0077.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0077.gif&quot;}" /><span class="no-mml-formula"></span></span>). Group comparisons are only considered significant if the grouped <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0078.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/hihc_a_1296074_ilm0078.gif&quot;}" /><span class="no-mml-formula"></span></span>-value is significant. <div class="figure figureViewer" id="F0015"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure C1. </span> SUS Questionnaire Dunn and Kruskal–Wallis test results. M is for Mouse, S is for Screen, H is for Hands, and V is for VR.</p></div></td></div><a href="#" class="thumbnail"><img id="F0015image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0015_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0015"><p><span class="captionLabel">Figure C1. </span> SUS Questionnaire Dunn and Kruskal–Wallis test results. M is for Mouse, S is for Screen, H is for Hands, and V is for VR.</p></div><div class="hidden rs_skip" id="figureFootNote-F0015"></div> <div class="figure figureViewer" id="F0016"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure C2. </span> TLX Questionnaire Dunn and Kruskal–Wallis test results. M is for Mouse, S is for Screen, H is for Hands, and V is for VR.</p></div></td></div><a href="#" class="thumbnail"><img id="F0016image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0016_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0016"><p><span class="captionLabel">Figure C2. </span> TLX Questionnaire Dunn and Kruskal–Wallis test results. M is for Mouse, S is for Screen, H is for Hands, and V is for VR.</p></div><div class="hidden rs_skip" id="figureFootNote-F0016"></div> <div class="figure figureViewer" id="F0017"><div id="figureViewerArticleInfo" class="hidden"><h1>On the Efficiency of a VR Hand Gesture-Based Interface for 3D Object Manipulations in Conceptual Design</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Alkemade%2C+Remi"><span class="hlFld-ContribAuthor"><a href="/author/Alkemade%2C+Remi"><span class="NLM_given-names">Remi</span> Alkemade</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Verbeek%2C+Fons+J"><span class="hlFld-ContribAuthor"><a href="/author/Verbeek%2C+Fons+J"><span class="NLM_given-names">Fons J.</span> Verbeek</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Lukosch%2C+Stephan+G"><span class="hlFld-ContribAuthor"><a href="/author/Lukosch%2C+Stephan+G"><span class="NLM_given-names">Stephan G.</span> Lukosch</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2017.1296074">https://doi.org/10.1080/10447318.2017.1296074</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>31 March 2017</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"><p><span class="captionLabel">Figure C3. </span> Extra Questionnaire Dunn and Kruskal–Wallis test results. M is for Mouse, S is for Screen, H is for Hands, and V is for VR.</p></div></td></div><a href="#" class="thumbnail"><img id="F0017image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002/images/medium/hihc_a_1296074_f0017_b.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0017"><p><span class="captionLabel">Figure C3. </span> Extra Questionnaire Dunn and Kruskal–Wallis test results. M is for Mouse, S is for Screen, H is for Hands, and V is for VR.</p></div><div class="hidden rs_skip" id="figureFootNote-F0017"></div></p></div></div></div></div><script type="text/javascript">
                        window.figureViewer={doi:'10.1080/10447318.2017.1296074',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2017/hihc20.v033.i11/10447318.2017.1296074/20171002',figures:[{i:'F0001',g:[{m:'hihc_a_1296074_f0001_b.gif',l:'hihc_a_1296074_f0001_b.jpeg',size:'41 KB'}]}
                            ,{i:'F0002',g:[{m:'hihc_a_1296074_f0002_oc.jpg',l:'hihc_a_1296074_f0002_oc.jpeg',size:'132 KB'}]}
                            ,{i:'F0003',g:[{m:'hihc_a_1296074_f0003_oc.jpg',l:'hihc_a_1296074_f0003_oc.jpeg',size:'80 KB'}]}
                            ,{i:'F0004',g:[{m:'hihc_a_1296074_f0004_oc.jpg',l:'hihc_a_1296074_f0004_oc.jpeg',size:'65 KB'}]}
                            ,{i:'F0005',g:[{m:'hihc_a_1296074_f0005_oc.jpg',l:'hihc_a_1296074_f0005_oc.jpeg',size:'87 KB'}]}
                            ,{i:'F0006',g:[{m:'hihc_a_1296074_f0006_oc.jpg',l:'hihc_a_1296074_f0006_oc.jpeg',size:'56 KB'}]}
                            ,{i:'F0007',g:[{m:'hihc_a_1296074_f0007_b.gif',l:'hihc_a_1296074_f0007_b.jpeg',size:'67 KB'}]}
                            ,{i:'F0008',g:[{m:'hihc_a_1296074_f0008_oc.jpg',l:'hihc_a_1296074_f0008_oc.jpeg',size:'62 KB'}]}
                            ,{i:'F0009',g:[{m:'hihc_a_1296074_f0009_oc.jpg',l:'hihc_a_1296074_f0009_oc.jpeg',size:'62 KB'}]}
                            ,{i:'F0010',g:[{m:'hihc_a_1296074_f0010_b.gif',l:'hihc_a_1296074_f0010_b.jpeg',size:'60 KB'}]}
                            ,{i:'F0011',g:[{m:'hihc_a_1296074_f0011_b.gif',l:'hihc_a_1296074_f0011_b.jpeg',size:'61 KB'}]}
                            ,{i:'F0012',g:[{m:'hihc_a_1296074_f0012_oc.jpg',l:'hihc_a_1296074_f0012_oc.jpeg',size:'274 KB'}]}
                            ,{i:'F0013',g:[{m:'hihc_a_1296074_f0013_oc.jpg',l:'hihc_a_1296074_f0013_oc.jpeg',size:'165 KB'}]}
                            ,{i:'F0014',g:[{m:'hihc_a_1296074_f0014_oc.jpg',l:'hihc_a_1296074_f0014_oc.jpeg',size:'296 KB'}]}
                            ,{i:'F0015',g:[{m:'hihc_a_1296074_f0015_b.gif',l:'hihc_a_1296074_f0015_b.jpeg',size:'103 KB'}]}
                            ,{i:'F0016',g:[{m:'hihc_a_1296074_f0016_b.gif',l:'hihc_a_1296074_f0016_b.jpeg',size:'71 KB'}]}
                            ,{i:'F0017',g:[{m:'hihc_a_1296074_f0017_b.gif',l:'hihc_a_1296074_f0017_b.jpeg',size:'111 KB'}]}
                            ]}</script><a id="inline_frontnotes"></a><h2>Notes</h2><div class="summation-section"><a id="FN0001"></a><p>1 Using consumer-grade products.</p></div><ul class="references numeric-ordered-list" id="references-Section"><div class="author-infos-ref"><h2 id="figures">References</h2><li id="CIT0001"><span><span class="hlFld-ContribAuthor">Alibali, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">2005</span>). <span class="NLM_article-title">Gesture in spatial cognition: Expressing, communicating, and thinking about spatial information</span>. <i>Spatial Cognition and Computation</i>, 5(4), <span class="NLM_fpage">307</span>–<span class="NLM_lpage">331</span>. doi:<span class="NLM_pub-id">10.1207/s15427633scc0504</span><span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1207/s15427633scc0504_2" target="_blank">[Taylor &amp; Francis Online]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&pages=307-331&issue=4&author=M.+Alibali&title=Gesture+in+spatial+cognition%3A+Expressing%2C+communicating%2C+and+thinking+about+spatial+information" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0001&amp;dbid=16384&amp;doi=10.1207%2Fs15427633scc0504_2&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1207%252Fs15427633scc0504_2%26sid%3Dliteratum%253Atandf%26aulast%3DAlibali%26aufirst%3DM.%26date%3D2005%26atitle%3DGesture%2520in%2520spatial%2520cognition%253A%2520Expressing%252C%2520communicating%252C%2520and%2520thinking%2520about%2520spatial%2520information%26jtitle%3DSpatial%2520Cognition%2520and%2520Computation%26volume%3D5%26issue%3D4%26spage%3D307%26epage%3D331" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0002"><span><span class="hlFld-ContribAuthor">Benko, <span class="NLM_given-names">H.</span></span>, <span class="hlFld-ContribAuthor">Ishak, <span class="NLM_given-names">E.</span></span>, &amp; <span class="hlFld-ContribAuthor">Feiner, <span class="NLM_given-names">S.</span></span> (<span class="NLM_year">2005</span>). <span class="NLM_article-title">Cross-dimensional gestural interaction techniques for hybrid immersive environments</span>. <i>IEEE Proceedings. VR 2005. Virtual Reality, 2005</i>, <span class="NLM_fpage">209</span>–<span class="NLM_lpage">217</span>. doi:<span class="NLM_pub-id">10.1109/VR.2005.1492776</span><span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1109%2FVR.2005.1492776" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&pages=209-217&author=H.+Benko&author=E.+Ishak&author=S.+Feiner&title=Cross-dimensional+gestural+interaction+techniques+for+hybrid+immersive+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=16384&amp;doi=10.1109%2FVR.2005.1492776&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1109%252FVR.2005.1492776%26sid%3Dliteratum%253Atandf%26aulast%3DBenko%26aufirst%3DH.%26date%3D2005%26atitle%3DCross-dimensional%2520gestural%2520interaction%2520techniques%2520for%2520hybrid%2520immersive%2520environments%26jtitle%3DIEEE%2520Proceedings.%2520VR%25202005.%2520Virtual%2520Reality%252C%25202005%26spage%3D209%26epage%3D217" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0003"><span><span class="hlFld-ContribAuthor">Blackler, <span class="NLM_given-names">A. L.</span></span>, <span class="hlFld-ContribAuthor">Popovic, <span class="NLM_given-names">V.</span></span>, &amp; <span class="hlFld-ContribAuthor">Mahar, <span class="NLM_given-names">D. P.</span></span> (<span class="NLM_year">2002</span>). <span class="NLM_article-title">Intuitive Use of Products</span>. In <i>Common Ground. Design Research Society International Conference 2002</i> (pp. <span class="NLM_fpage">120</span>–<span class="NLM_lpage">134</span>). London, UK: Design Research Society. Retrieved from <a class="ext-link" href="http://eprints.qut.edu.au/1879/" target="_blank">http://eprints.qut.edu.au/1879/</a><span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=120-134&author=A.+L.+Blackler&author=V.+Popovic&author=D.+P.+Mahar&title=Intuitive+Use+of+Products" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0003&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBlackler%26aufirst%3DA.%2520L.%26date%3D2002%26atitle%3DIntuitive%2520Use%2520of%2520Products%26jtitle%3DIn%2520Common%2520Ground.%2520Design%2520Research%2520Society%2520International%2520Conference%25202002%26spage%3D120%26epage%3D134http%3A%2F%2Feprints.qut.edu.au%2F1879%2F" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0004"><span><span class="hlFld-ContribAuthor">Bordegoni, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">1994</span>). <span class="NLM_article-title">Parallel use of hand gestures and force-input device for interacting with 3d and virtual reality environments</span>. <i>International Journal of Human–Computer Interaction</i>, 6(4), <span class="NLM_fpage">391</span>–<span class="NLM_lpage">413</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447319409526103" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0004&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=A1994QD07300004" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1994&pages=391-413&issue=4&author=M.+Bordegoni&title=Parallel+use+of+hand+gestures+and+force-input+device+for+interacting+with+3d+and+virtual+reality+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0004&amp;dbid=16384&amp;doi=10.1080%2F10447319409526103&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447319409526103%26sid%3Dliteratum%253Atandf%26aulast%3DBordegoni%26aufirst%3DM.%26date%3D1994%26atitle%3DParallel%2520use%2520of%2520hand%2520gestures%2520and%2520force-input%2520device%2520for%2520interacting%2520with%25203d%2520and%2520virtual%2520reality%2520environments%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D6%26issue%3D4%26spage%3D391%26epage%3D413" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0005"><span><span class="hlFld-ContribAuthor">Borst, <span class="NLM_given-names">C. W.</span></span>, &amp; <span class="hlFld-ContribAuthor">Indugula, <span class="NLM_given-names">A. P.</span></span> (<span class="NLM_year">2006</span>). <span class="NLM_article-title">A spring model for whole-hand virtual grasping</span>. <i>Presence</i>, 15(1), <span class="NLM_fpage">47</span>–<span class="NLM_lpage">61</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0005&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1162%2Fpres.2006.15.1.47" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=47-61&issue=1&author=C.+W.+Borst&author=A.+P.+Indugula&title=A+spring+model+for+whole-hand+virtual+grasping" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0005&amp;dbid=16384&amp;doi=10.1162%2Fpres.2006.15.1.47&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1162%252Fpres.2006.15.1.47%26sid%3Dliteratum%253Atandf%26aulast%3DBorst%26aufirst%3DC.%2520W.%26date%3D2006%26atitle%3DA%2520spring%2520model%2520for%2520whole-hand%2520virtual%2520grasping%26jtitle%3DPresence%26volume%3D15%26issue%3D1%26spage%3D47%26epage%3D61" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0006"><span><span class="hlFld-ContribAuthor">Brooke, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">1996</span>). <span class="NLM_article-title">SUS - A quick and dirty usability scale</span>. <i>Usability Evaluation in Industry</i>, 189, <span class="NLM_fpage">4</span>–<span class="NLM_lpage">7</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1996&pages=4-7&author=J.+Brooke&title=SUS+-+A+quick+and+dirty+usability+scale" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0006&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBrooke%26aufirst%3DJ.%26date%3D1996%26atitle%3DSUS%2520-%2520A%2520quick%2520and%2520dirty%2520usability%2520scale%26jtitle%3DUsability%2520Evaluation%2520in%2520Industry%26volume%3D189%26spage%3D4%26epage%3D7" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0007"><span><span class="hlFld-ContribAuthor">Buchmann, <span class="NLM_given-names">V.</span></span>, <span class="hlFld-ContribAuthor">Violich, <span class="NLM_given-names">S.</span></span>, <span class="hlFld-ContribAuthor">Billinghurst, <span class="NLM_given-names">M.</span></span>, &amp; <span class="hlFld-ContribAuthor">Cockburn, <span class="NLM_given-names">A.</span></span> (<span class="NLM_year">2004</span>). <span class="NLM_article-title">FingARtips âA<sup>˘</sup> S¸ Gesture based direct manipulation in augmented reality</span>. <i>Proceedings of the 2nd International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia</i>, 1(212), <span class="NLM_fpage">212</span>–<span class="NLM_lpage">221</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=212-221&issue=212&author=V.+Buchmann&author=S.+Violich&author=M.+Billinghurst&author=A.+Cockburn&title=FingARtips+%C3%A2A%CB%98+S%C2%B8+Gesture+based+direct+manipulation+in+augmented+reality" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0007&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBuchmann%26aufirst%3DV.%26date%3D2004%26atitle%3DFingARtips%2520%25C3%25A2A%25CB%2598%2520S%25C2%25B8%2520Gesture%2520based%2520direct%2520manipulation%2520in%2520augmented%2520reality%26jtitle%3DProceedings%2520of%2520the%25202nd%2520International%2520Conference%2520on%2520Computer%2520Graphics%2520and%2520Interactive%2520Techniques%2520in%2520Australasia%2520and%2520South%2520East%2520Asia%26volume%3D1%26issue%3D212%26spage%3D212%26epage%3D221" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0008"><span><span class="hlFld-ContribAuthor">Canare, <span class="NLM_given-names">D.</span></span>, <span class="hlFld-ContribAuthor">Chaparro, <span class="NLM_given-names">B.</span></span>, &amp; <span class="hlFld-ContribAuthor">He, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">2015</span>). <span class="NLM_article-title">A comparison of gaze-based and gesture-based input for a point-and- click task</span>. <i>International Conference on Universal Access in Human–Computer Interaction</i>, 9176, <span class="NLM_fpage">15</span>–<span class="NLM_lpage">24</span>. doi:<span class="NLM_pub-id">10.1007/978-3-319-20681-3</span><span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0008&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1007%2F978-3-319-20681-3" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&pages=15-24&author=D.+Canare&author=B.+Chaparro&author=J.+He&title=A+comparison+of+gaze-based+and+gesture-based+input+for+a+point-and-+click+task" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0008&amp;dbid=16384&amp;doi=10.1007%2F978-3-319-20681-3&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1007%252F978-3-319-20681-3%26sid%3Dliteratum%253Atandf%26aulast%3DCanare%26aufirst%3DD.%26date%3D2015%26atitle%3DA%2520comparison%2520of%2520gaze-based%2520and%2520gesture-based%2520input%2520for%2520a%2520point-and-%2520click%2520task%26jtitle%3DInternational%2520Conference%2520on%2520Universal%2520Access%2520in%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D9176%26spage%3D15%26epage%3D24" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0009"><span><span class="hlFld-ContribAuthor">Cash, <span class="NLM_given-names">P.</span></span>, &amp; <span class="hlFld-ContribAuthor">Maier, <span class="NLM_given-names">A.</span></span> (<span class="NLM_year">2016</span>). <span class="NLM_article-title">Prototyping with your hands: The many roles of gesture in the communication of de- sign concepts</span>. <i>Journal of Engineering Design</i>, 27(1–3), <span class="NLM_fpage">118</span>–<span class="NLM_lpage">145</span>. doi:<span class="NLM_pub-id">10.1080/09544828.2015.1126702</span><span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/09544828.2015.1126702" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0009&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000370628300006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2016&pages=118-145&issue=1%E2%80%933&author=P.+Cash&author=A.+Maier&title=Prototyping+with+your+hands%3A+The+many+roles+of+gesture+in+the+communication+of+de-+sign+concepts" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0009&amp;dbid=16384&amp;doi=10.1080%2F09544828.2015.1126702&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F09544828.2015.1126702%26sid%3Dliteratum%253Atandf%26aulast%3DCash%26aufirst%3DP.%26date%3D2016%26atitle%3DPrototyping%2520with%2520your%2520hands%253A%2520The%2520many%2520roles%2520of%2520gesture%2520in%2520the%2520communication%2520of%2520de-%2520sign%2520concepts%26jtitle%3DJournal%2520of%2520Engineering%2520Design%26volume%3D27%26issue%3D1%25E2%2580%25933%26spage%3D118%26epage%3D145" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0010"><span><span class="hlFld-ContribAuthor">Coelho, <span class="NLM_given-names">J.</span></span>, &amp; <span class="hlFld-ContribAuthor">Verbeek, <span class="NLM_given-names">F.</span></span> (<span class="NLM_year">2014</span>). <span class="NLM_article-title">Pointing Task Evaluation of Leap Motion Controller in 3D Virtual Environment</span>. In <i>Creating the Difference: Proceedings of the Chi Sparks 2014 Conference</i>. (p. 78). <span class="NLM_publisher-loc">The Hague, The Netherlands</span>: <span class="NLM_publisher-name">The Hague University of Applied Sciences and Chi Nederland</span>. Retrieved from <a class="ext-link" href="http://chi-sparks.nl/2014/proceedings/" target="_blank">http://chi-sparks.nl/2014/proceedings/</a><span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=J.+Coelho&author=F.+Verbeek&title=Pointing+Task+Evaluation+of+Leap+Motion+Controller+in+3D+Virtual+Environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0010&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DCoelho%26aufirst%3DJ.%26date%3D2014%26atitle%3DPointing%2520Task%2520Evaluation%2520of%2520Leap%2520Motion%2520Controller%2520in%25203D%2520Virtual%2520Environment%26jtitle%3DCreating%2520the%2520Difference%253A%2520Proceedings%2520of%2520the%2520Chi%2520Sparks%25202014%2520Conference.%2520%2528p.%252078%2529%26pub%3DThe%2520Hague%2520University%2520of%2520Applied%2520Sciences%2520and%2520Chi%2520Nederlandhttp%3A%2F%2Fchi-sparks.nl%2F2014%2Fproceedings%2F" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0011"><span>R Core Team. (<span class="NLM_year">2014</span>). <i>R: A language and environment for statistical computing</i> (Computer Software Manual). <span class="NLM_publisher-loc">Vienna, Austria</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=R+Core+Team&title=R%3A+A+language+and+environment+for+statistical+computing+%28Computer+Software+Manual%29" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0011&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aucorp%3DR%2520Core%2520Team%26date%3D2014%26btitle%3DR%253A%2520A%2520language%2520and%2520environment%2520for%2520statistical%2520computing%2520%2528Computer%2520Software%2520Manual%2529" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0012"><span><span class="hlFld-ContribAuthor">Datcu, <span class="NLM_given-names">D.</span></span>, <span class="hlFld-ContribAuthor">Lukosch, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Brazier, <span class="NLM_given-names">F.</span></span> (<span class="NLM_year">2015</span>). <span class="NLM_article-title">On the usability and e<i>ff</i>ectiveness of di<i>ff</i>erent interaction types in augmented reality</span>. <i>International Journal of Human–Computer Interaction</i>, 31 (3), <span class="NLM_fpage">193</span>–<span class="NLM_lpage">209</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2014.994193" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0012&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000352319800003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&pages=193-209&issue=3&author=D.+Datcu&author=S.+Lukosch&author=F.+Brazier&title=On+the+usability+and+effectiveness+of+different+interaction+types+in+augmented+reality" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0012&amp;dbid=16384&amp;doi=10.1080%2F10447318.2014.994193&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2014.994193%26sid%3Dliteratum%253Atandf%26aulast%3DDatcu%26aufirst%3DD.%26date%3D2015%26atitle%3DOn%2520the%2520usability%2520and%2520effectiveness%2520of%2520different%2520interaction%2520types%2520in%2520augmented%2520reality%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D31%26issue%3D3%26spage%3D193%26epage%3D209" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0013"><span><span class="hlFld-ContribAuthor">Deering, <span class="NLM_given-names">M. F.</span></span> (<span class="NLM_year">1995</span>, <span class="NLM_month">September</span>). <span class="NLM_article-title">HoloSketch: A virtual reality sketching/animation tool</span>. <i>ACM Transactions on Computer–Human Interaction</i>, 2(3), <span class="NLM_fpage">220</span>–<span class="NLM_lpage">238</span>. doi:<span class="NLM_pub-id">10.1145/210079.210087</span><span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0013&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1145%2F210079.210087" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1995&pages=220-238&issue=3&author=M.+F.+Deering&title=HoloSketch%3A+A+virtual+reality+sketching%2Fanimation+tool" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0013&amp;dbid=16384&amp;doi=10.1145%2F210079.210087&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1145%252F210079.210087%26sid%3Dliteratum%253Atandf%26aulast%3DDeering%26aufirst%3DM.%2520F.%26date%3D1995%26atitle%3DHoloSketch%253A%2520A%2520virtual%2520reality%2520sketching%252Fanimation%2520tool%26jtitle%3DACM%2520Transactions%2520on%2520Computer%25E2%2580%2593Human%2520Interaction%26volume%3D2%26issue%3D3%26spage%3D220%26epage%3D238" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0014"><span>de Ruiter, J. P. (<span class="NLM_year">2000</span>). <span class="NLM_article-title">The production of gesture and speech</span>. <i>Language and Gesture</i>, 2, <span class="NLM_fpage">284</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0014&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1017%2FCBO9780511620850.018" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=284&author=de+Ruiter%2C+J.+P.&title=The+production+of+gesture+and+speech" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0014&amp;dbid=16384&amp;doi=10.1017%2FCBO9780511620850.018&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1017%252FCBO9780511620850.018%26sid%3Dliteratum%253Atandf%26aucorp%3Dde%2520Ruiter%252C%2520J.%2520P.%26date%3D2000%26atitle%3DThe%2520production%2520of%2520gesture%2520and%2520speech%26jtitle%3DLanguage%2520and%2520Gesture%26volume%3D2%26spage%3D284" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0015"><span><span class="hlFld-ContribAuthor">Doherty, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">1985</span>, <span class="NLM_month">January</span>). <span class="NLM_article-title">The effects of sign characteristics on sign acquisition and retention: An integrative review of the literature</span>. <i>Augmentative and Alternative Communication</i>, 1(3), <span class="NLM_fpage">108</span>–<span class="NLM_lpage">121</span>. doi:<span class="NLM_pub-id">10.1080/07434618512331273601</span><span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/07434618512331273601" target="_blank">[Taylor &amp; Francis Online]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1985&pages=108-121&issue=3&author=J.+Doherty&title=The+effects+of+sign+characteristics+on+sign+acquisition+and+retention%3A+An+integrative+review+of+the+literature" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0015&amp;dbid=16384&amp;doi=10.1080%2F07434618512331273601&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F07434618512331273601%26sid%3Dliteratum%253Atandf%26aulast%3DDoherty%26aufirst%3DJ.%26date%3D1985%26atitle%3DThe%2520effects%2520of%2520sign%2520characteristics%2520on%2520sign%2520acquisition%2520and%2520retention%253A%2520An%2520integrative%2520review%2520of%2520the%2520literature%26jtitle%3DAugmentative%2520and%2520Alternative%2520Communication%26volume%3D1%26issue%3D3%26spage%3D108%26epage%3D121" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0016"><span><span class="hlFld-ContribAuthor">Ellis, <span class="NLM_given-names">R.</span></span>, &amp; <span class="hlFld-ContribAuthor">Tucker, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">2000</span>). <span class="NLM_article-title">Micro-a<i>ff</i>ordance: The potentiation of components of action by seen objects</span>. <i>British Journal of Psychology</i>, 91, <span class="NLM_fpage">451</span>–<span class="NLM_lpage">471</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0016&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1348%2F000712600161934" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0016&amp;dbid=8&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=11104173" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0016&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000165312100001" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=451-471&author=R.+Ellis&author=M.+Tucker&title=Micro-affordance%3A+The+potentiation+of+components+of+action+by+seen+objects" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0016&amp;dbid=16384&amp;doi=10.1348%2F000712600161934&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1348%252F000712600161934%26sid%3Dliteratum%253Atandf%26aulast%3DEllis%26aufirst%3DR.%26date%3D2000%26atitle%3DMicro-affordance%253A%2520The%2520potentiation%2520of%2520components%2520of%2520action%2520by%2520seen%2520objects%26jtitle%3DBritish%2520Journal%2520of%2520Psychology%26volume%3D91%26spage%3D451%26epage%3D471" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0017"><span><span class="hlFld-ContribAuthor">Encarnação, <span class="NLM_given-names">L.</span></span> (<span class="NLM_year">1999</span>). <span class="NLM_article-title">A translucent sketchpad for the virtual table exploring motion-based gesture recognition</span>. <i>Computer Graphics Forum</i>, 18 (3), <span class="NLM_fpage">277</span>–<span class="NLM_lpage">286</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0017&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1111%2F1467-8659.00348" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0017&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000082816100030" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1999&pages=277-286&issue=3&author=L.+Encarna%C3%A7%C3%A3o&title=A+translucent+sketchpad+for+the+virtual+table+exploring+motion-based+gesture+recognition" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0017&amp;dbid=16384&amp;doi=10.1111%2F1467-8659.00348&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1111%252F1467-8659.00348%26sid%3Dliteratum%253Atandf%26aulast%3DEncarna%25C3%25A7%25C3%25A3o%26aufirst%3DL.%26date%3D1999%26atitle%3DA%2520translucent%2520sketchpad%2520for%2520the%2520virtual%2520table%2520exploring%2520motion-based%2520gesture%2520recognition%26jtitle%3DComputer%2520Graphics%2520Forum%26volume%3D18%26issue%3D3%26spage%3D277%26epage%3D286" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0018"><span>Fiorentino, M., de Amicis, R., Monno, G., &amp; Stork, A. (<span class="NLM_year">2002</span>). <span class="NLM_article-title">Spacedesign: A Mixed Reality Workspace for Aesthetic Industrial Design</span>. In <i>Proceedings of the 1st International Symposium on Mixed and Augmented Reality</i> (p. <span class="NLM_fpage">86</span>). Washington, DC, USA: IEEE Computer Society. Retrieved from <a class="ext-link" href="http://dl.acm.org/citation.cfm?id=850976.854976" target="_blank">http://dl.acm.org/citation.cfm?id=850976.854976</a><span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&author=Fiorentino%2C+M.%2C+de+Amicis%2C+R.%2C+Monno%2C+G.%2C+%26+Stork%2C+A.&title=Spacedesign%3A+A+Mixed+Reality+Workspace+for+Aesthetic+Industrial+Design" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0018&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aucorp%3DFiorentino%252C%2520M.%252C%2520de%2520Amicis%252C%2520R.%252C%2520Monno%252C%2520G.%252C%2520%2526%2520Stork%252C%2520A.%26date%3D2002%26atitle%3DSpacedesign%253A%2520A%2520Mixed%2520Reality%2520Workspace%2520for%2520Aesthetic%2520Industrial%2520Design%26spage%3D86http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D850976.854976" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0019"><span><span class="hlFld-ContribAuthor">Gibbs, <span class="NLM_given-names">R. W. J.</span></span> (<span class="NLM_year">2006</span>). <i>Embodiment and cognitive science</i>. Cambridge, UK: Cambridge University Press.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0019&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1017%2FCBO9780511805844" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&author=R.+W.+J.+Gibbs&title=Embodiment+and+cognitive+science" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0019&amp;dbid=16384&amp;doi=10.1017%2FCBO9780511805844&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1017%252FCBO9780511805844%26sid%3Dliteratum%253Atandf%26aulast%3DGibbs%26aufirst%3DR.%2520W.%2520J.%26date%3D2006%26btitle%3DEmbodiment%2520and%2520cognitive%2520science" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0020"><span><span class="hlFld-ContribAuthor">Hart, <span class="NLM_given-names">S. G.</span></span>, &amp; <span class="hlFld-ContribAuthor">Staveland, <span class="NLM_given-names">L. E.</span></span> (<span class="NLM_year">1988</span>). <span class="NLM_article-title">Development of NASA-TLX: Results of empirical and theoretical research</span>. <i>Advances in Psychology</i>, 52, <span class="NLM_fpage">139</span>–<span class="NLM_lpage">183</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1016%2FS0166-4115%2808%2962386-9" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1988&pages=139-183&author=S.+G.+Hart&author=L.+E.+Staveland&title=Development+of+NASA-TLX%3A+Results+of+empirical+and+theoretical+research" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16384&amp;doi=10.1016%2FS0166-4115%2808%2962386-9&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252FS0166-4115%252808%252962386-9%26sid%3Dliteratum%253Atandf%26aulast%3DHart%26aufirst%3DS.%2520G.%26date%3D1988%26atitle%3DDevelopment%2520of%2520NASA-TLX%253A%2520Results%2520of%2520empirical%2520and%2520theoretical%2520research%26jtitle%3DAdvances%2520in%2520Psychology%26volume%3D52%26spage%3D139%26epage%3D183" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0021"><span><span class="hlFld-ContribAuthor">Hostetter, <span class="NLM_given-names">A. B.</span></span>, &amp; <span class="hlFld-ContribAuthor">Alibali, <span class="NLM_given-names">M. W.</span></span> (<span class="NLM_year">2004</span>). <span class="NLM_article-title">On the tip of the mind: Gesture as a key to conceptualization</span>. In <span class="NLM_conf-name"><i>Proceedings of the twenty-sixth annual conference of the cognitive science society</i></span> (Vol. 26). Chicago, IL: Cognitive Science Society.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&author=A.+B.+Hostetter&author=M.+W.+Alibali&title=On+the+tip+of+the+mind%3A+Gesture+as+a+key+to+conceptualization" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0021&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DHostetter%26aufirst%3DA.%2520B.%26date%3D2004%26atitle%3DOn%2520the%2520tip%2520of%2520the%2520mind%253A%2520Gesture%2520as%2520a%2520key%2520to%2520conceptualization%26volume%3DVol.%252026" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0022"><span><span class="hlFld-ContribAuthor">Hothorn, <span class="NLM_given-names">T.</span></span>, <span class="hlFld-ContribAuthor">Bretz, <span class="NLM_given-names">F.</span></span>, &amp; <span class="hlFld-ContribAuthor">Westfall, <span class="NLM_given-names">P.</span></span> (<span class="NLM_year">2008</span>). <span class="NLM_article-title">Simultaneous inference in general parametric models</span>. <i>Biometrical Journal</i>, 50(3), <span class="NLM_fpage">346</span>–<span class="NLM_lpage">363</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1002%2Fbimj.200810425" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0022&amp;dbid=8&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=18481363" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0022&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000257256900002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=346-363&issue=3&author=T.+Hothorn&author=F.+Bretz&author=P.+Westfall&title=Simultaneous+inference+in+general+parametric+models" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=16384&amp;doi=10.1002%2Fbimj.200810425&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1002%252Fbimj.200810425%26sid%3Dliteratum%253Atandf%26aulast%3DHothorn%26aufirst%3DT.%26date%3D2008%26atitle%3DSimultaneous%2520inference%2520in%2520general%2520parametric%2520models%26jtitle%3DBiometrical%2520Journal%26volume%3D50%26issue%3D3%26spage%3D346%26epage%3D363" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0023"><span><span class="hlFld-ContribAuthor">Igarashi, <span class="NLM_given-names">T.</span></span>, <span class="hlFld-ContribAuthor">Matsuoka, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Tanaka, <span class="NLM_given-names">H.</span></span> (<span class="NLM_year">2007</span>). <span class="NLM_article-title">Teddy: A sketching interface for 3D freeform design</span>. In <span class="NLM_conf-name"><i>ACM SIGGRAPH 2007 Courses</i></span>. New York, NY: ACM.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&author=T.+Igarashi&author=S.+Matsuoka&author=H.+Tanaka&title=Teddy%3A+A+sketching+interface+for+3D+freeform+design" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0023&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DIgarashi%26aufirst%3DT.%26date%3D2007%26atitle%3DTeddy%253A%2520A%2520sketching%2520interface%2520for%25203D%2520freeform%2520design" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0024"><span>Jerald, J., Mlyniec, P., Yoganandan, A., Rubin, A., Paullus, D., &amp; Solotko, S. (<span class="NLM_year">2013</span>). <span class="NLM_chapter-title">Makevr: A 3d world-building interface</span>. In <i>3D User Interfaces (3DUI), 2013 IEEE Symposium on</i> (pp. <span class="NLM_fpage">197</span>–<span class="NLM_lpage">198</span>). Orlando, FL.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=197-198&author=Jerald%2C+J.%2C+Mlyniec%2C+P.%2C+Yoganandan%2C+A.%2C+Rubin%2C+A.%2C+Paullus%2C+D.%2C+%26+Solotko%2C+S.&title=Makevr%3A+A+3d+world-building+interface" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0024&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aucorp%3DJerald%252C%2520J.%252C%2520Mlyniec%252C%2520P.%252C%2520Yoganandan%252C%2520A.%252C%2520Rubin%252C%2520A.%252C%2520Paullus%252C%2520D.%252C%2520%2526%2520Solotko%252C%2520S.%26date%3D2013%26atitle%3DMakevr%253A%2520A%25203d%2520world-building%2520interface%26btitle%3D3D%2520User%2520Interfaces%2520%25283DUI%2529%252C%25202013%2520IEEE%2520Symposium%2520on%26spage%3D197%26epage%3D198" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0025"><span><span class="hlFld-ContribAuthor">Loup-Escande, <span class="NLM_given-names">E.</span></span>, <span class="hlFld-ContribAuthor">Jamet, <span class="NLM_given-names">E.</span></span>, <span class="hlFld-ContribAuthor">Ragot, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Erhel, <span class="NLM_given-names">S.</span></span>, &amp; <span class="hlFld-ContribAuthor">Michinov, <span class="NLM_given-names">N.</span></span> (<span class="NLM_year">2017</span>). <span class="NLM_article-title">Effects of stereoscopic display on learning and user experience in an educational virtual environment</span>. <i>International Journal of Human–Computer Interaction</i>, 33 (2), <span class="NLM_fpage">115</span>–<span class="NLM_lpage">122</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2016.1220105" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0025&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000394235600003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=115-122&issue=2&author=E.+Loup-Escande&author=E.+Jamet&author=M.+Ragot&author=S.+Erhel&author=N.+Michinov&title=Effects+of+stereoscopic+display+on+learning+and+user+experience+in+an+educational+virtual+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0025&amp;dbid=16384&amp;doi=10.1080%2F10447318.2016.1220105&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2016.1220105%26sid%3Dliteratum%253Atandf%26aulast%3DLoup-Escande%26aufirst%3DE.%26date%3D2017%26atitle%3DEffects%2520of%2520stereoscopic%2520display%2520on%2520learning%2520and%2520user%2520experience%2520in%2520an%2520educational%2520virtual%2520environment%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D33%26issue%3D2%26spage%3D115%26epage%3D122" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0026"><span><span class="hlFld-ContribAuthor">Luftig, <span class="NLM_given-names">R. L.</span></span>, &amp; <span class="hlFld-ContribAuthor">Lloyd, <span class="NLM_given-names">L. L.</span></span> (<span class="NLM_year">1981</span>). <span class="NLM_article-title">Manual sign translucency and referential concreteness in the learning of signs</span>. <i>Sign Language Studies</i>, 30(1), <span class="NLM_fpage">49</span>–<span class="NLM_lpage">60</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0026&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1353%2Fsls.1981.0005" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1981&pages=49-60&issue=1&author=R.+L.+Luftig&author=L.+L.+Lloyd&title=Manual+sign+translucency+and+referential+concreteness+in+the+learning+of+signs" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0026&amp;dbid=16384&amp;doi=10.1353%2Fsls.1981.0005&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1353%252Fsls.1981.0005%26sid%3Dliteratum%253Atandf%26aulast%3DLuftig%26aufirst%3DR.%2520L.%26date%3D1981%26atitle%3DManual%2520sign%2520translucency%2520and%2520referential%2520concreteness%2520in%2520the%2520learning%2520of%2520signs%26jtitle%3DSign%2520Language%2520Studies%26volume%3D30%26issue%3D1%26spage%3D49%26epage%3D60" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0027"><span><span class="hlFld-ContribAuthor">Mandel, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">1977a</span>). <span class="NLM_article-title">Iconic devices in American sign language</span>. <i>On the Other Hand: New Perspectives on American Sign Language</i>, 1, <span class="NLM_fpage">57</span>–<span class="NLM_lpage">107</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1977a&pages=57-107&author=M.+Mandel&title=Iconic+devices+in+American+sign+language" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0027&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMandel%26aufirst%3DM.%26date%3D1977a%26atitle%3DIconic%2520devices%2520in%2520American%2520sign%2520language%26jtitle%3DOn%2520the%2520Other%2520Hand%253A%2520New%2520Perspectives%2520on%2520American%2520Sign%2520Language%26volume%3D1%26spage%3D57%26epage%3D107" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0028"><span><span class="hlFld-ContribAuthor">Mandel, <span class="NLM_given-names">M.</span></span> (<span class="NLM_year">1977b</span>). <span class="NLM_article-title">Iconicity of signs and their learnability by non-signers</span>. In <span class="NLM_conf-name"><i>Proceedings of the first national symposium on sign language research and teaching</i></span> (pp. <span class="NLM_fpage">259</span>–<span class="NLM_lpage">266</span>). Chicago, IL.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1977b&author=M.+Mandel&title=Iconicity+of+signs+and+their+learnability+by+non-signers" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0028&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMandel%26aufirst%3DM.%26date%3D1977b%26atitle%3DIconicity%2520of%2520signs%2520and%2520their%2520learnability%2520by%2520non-signers%26spage%3D259%26epage%3D266" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0029"><span><span class="hlFld-ContribAuthor">Marsh, <span class="NLM_given-names">T.</span></span>, &amp; <span class="hlFld-ContribAuthor">Watt, <span class="NLM_given-names">A.</span></span> (<span class="NLM_year">1998</span>). <span class="NLM_article-title">Shape your imagination: iconic gestural-based interaction</span>. In <span class="NLM_conf-name">Virtual Reality Annual International Symposium, 1998. Proceedings, IEEE 1998</span> (pp. <span class="NLM_page-range">122–125</span>). <a class="ext-link" href="http://doi.org/10.1109/VRAIS.1998.658465" target="_blank">http://doi.org/10.1109/VRAIS.1998.658465</a><span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1998&author=T.+Marsh&author=A.+Watt&title=Shape+your+imagination%3A+iconic+gestural-based+interaction" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0029&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMarsh%26aufirst%3DT.%26date%3D1998%26atitle%3DShape%2520your%2520imagination%253A%2520iconic%2520gestural-based%2520interactionhttp%3A%2F%2Fdoi.org%2F10.1109%2FVRAIS.1998.658465" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0030"><span><span class="hlFld-ContribAuthor">McNeill, <span class="NLM_given-names">D.</span></span> (<span class="NLM_year">1992</span>). <i>Hand and mind: What gestures reveal about thought</i>. Chicago, IL: University of Chicago Press.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1992&author=D.+McNeill&title=Hand+and+mind%3A+What+gestures+reveal+about+thought" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0030&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMcNeill%26aufirst%3DD.%26date%3D1992%26btitle%3DHand%2520and%2520mind%253A%2520What%2520gestures%2520reveal%2520about%2520thought" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0031"><span><span class="hlFld-ContribAuthor">Morsella, <span class="NLM_given-names">E.</span></span>, &amp; <span class="hlFld-ContribAuthor">Krauss, <span class="NLM_given-names">R. M.</span></span> (<span class="NLM_year">2004</span>). <span class="NLM_article-title">The role of gestures in spatial working memory and speech</span>. <i>The American Journal of Psychology</i>, 117, <span class="NLM_fpage">411</span>–<span class="NLM_lpage">424</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0031&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.2307%2F4149008" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0031&amp;dbid=8&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=15457809" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0031&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000223646200005" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=411-424&author=E.+Morsella&author=R.+M.+Krauss&title=The+role+of+gestures+in+spatial+working+memory+and+speech" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0031&amp;dbid=16384&amp;doi=10.2307%2F4149008&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.2307%252F4149008%26sid%3Dliteratum%253Atandf%26aulast%3DMorsella%26aufirst%3DE.%26date%3D2004%26atitle%3DThe%2520role%2520of%2520gestures%2520in%2520spatial%2520working%2520memory%2520and%2520speech%26jtitle%3DThe%2520American%2520Journal%2520of%2520Psychology%26volume%3D117%26spage%3D411%26epage%3D424" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0032"><span><span class="hlFld-ContribAuthor">Naya, <span class="NLM_given-names">F.</span></span>, <span class="hlFld-ContribAuthor">Jorge, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Conesa, <span class="NLM_given-names">J.</span></span>, Contero, M., &amp; Gomis, J. M. (<span class="NLM_year">2002</span>). <span class="NLM_chapter-title">Direct modeling: from sketches to 3D models</span>. In <i>Proceedings of the 1st Ibero-American Symposium in Computer Graphics SIACG</i> (pp. <span class="NLM_fpage">109</span>–<span class="NLM_lpage">117</span>). Guimarães, Portugal<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=109-117&author=F.+Naya&author=J.+Jorge&author=J.+Conesa&title=Direct+modeling%3A+from+sketches+to+3D+models" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0032&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DNaya%26aufirst%3DF.%26date%3D2002%26atitle%3DDirect%2520modeling%253A%2520from%2520sketches%2520to%25203D%2520models%26btitle%3DProceedings%2520of%2520the%25201st%2520Ibero-American%2520Symposium%2520in%2520Computer%2520Graphics%2520SIACG%26spage%3D109%26epage%3D117" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0033"><span><span class="hlFld-ContribAuthor">Nevins, <span class="NLM_given-names">J. L.</span></span>, &amp; <span class="hlFld-ContribAuthor">Whitney, <span class="NLM_given-names">D. E.</span></span> (<span class="NLM_year">1989</span>). <i>Concurrent design of products and processes: A strategy for the next generation in manufacturing</i>. <span class="NLM_publisher-name">New York, NY: McGraw-Hill Companies</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1989&author=J.+L.+Nevins&author=D.+E.+Whitney&title=Concurrent+design+of+products+and+processes%3A+A+strategy+for+the+next+generation+in+manufacturing" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0033&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DNevins%26aufirst%3DJ.%2520L.%26date%3D1989%26btitle%3DConcurrent%2520design%2520of%2520products%2520and%2520processes%253A%2520A%2520strategy%2520for%2520the%2520next%2520generation%2520in%2520manufacturing%26pub%3DNew%2520York%252C%2520NY%253A%2520McGraw-Hill%2520Companies" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0034"><span><span class="hlFld-ContribAuthor">Norman, <span class="NLM_given-names">D. A.</span></span> (<span class="NLM_year">1986</span>). <span class="NLM_article-title">Cognitive engineering</span>. <i>User Centered System Design: New Perspectives on Human-Computer Interaction, 3161</i>, <span class="NLM_fpage">31</span>–<span class="NLM_lpage">61</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1986&pages=31-61&author=D.+A.+Norman&title=Cognitive+engineering" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0034&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DNorman%26aufirst%3DD.%2520A.%26date%3D1986%26atitle%3DCognitive%2520engineering%26jtitle%3DUser%2520Centered%2520System%2520Design%253A%2520New%2520Perspectives%2520on%2520Human-Computer%2520Interaction%252C%25203161%26spage%3D31%26epage%3D61" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0035"><span><span class="hlFld-ContribAuthor">Oh, <span class="NLM_given-names">J.-Y.</span></span>, <span class="hlFld-ContribAuthor">Stuerzlinger, <span class="NLM_given-names">W.</span></span>, &amp; <span class="hlFld-ContribAuthor">Dadgari, <span class="NLM_given-names">D.</span></span> (<span class="NLM_year">2006</span>). <span class="NLM_chapter-title">Group selection techniques for efficient 3D modeling</span>. In <i>3D User Interfaces, 2006. 3DUI 2006. IEEE Symposium on</i> (pp. <span class="NLM_fpage">95</span>–<span class="NLM_lpage">102</span>). Alexandria, VA.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=95-102&author=J.-Y.+Oh&author=W.+Stuerzlinger&author=D.+Dadgari&title=Group+selection+techniques+for+efficient+3D+modeling" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0035&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DOh%26aufirst%3DJ.-Y.%26date%3D2006%26atitle%3DGroup%2520selection%2520techniques%2520for%2520efficient%25203D%2520modeling%26btitle%3D3D%2520User%2520Interfaces%252C%25202006.%25203DUI%25202006.%2520IEEE%2520Symposium%2520on%26spage%3D95%26epage%3D102" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0036"><span><span class="hlFld-ContribAuthor">Pereira, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Jorge, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Branco, <span class="NLM_given-names">V.</span></span>, &amp; <span class="hlFld-ContribAuthor">Ferreira, <span class="NLM_given-names">F.</span></span> (<span class="NLM_year">2000</span>). <span class="NLM_article-title">Towards calligraphic interfaces: sketching 3D scenes with gestures and context icons</span>. In <span class="NLM_conf-name"><i>WSCG ‘2000: Conference proceeding: The 8th International Conference in Central Europe on Computers Graphics, Visualization and Interaktive Digital Media ‘2000 in cooperation with EUROGRAPHICS and IFIP WG 5.10</i></span> (pp. 314–321). Plzen, Czech Republic: University of West Bohemia.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&author=J.+Pereira&author=J.+Jorge&author=V.+Branco&author=F.+Ferreira&title=Towards+calligraphic+interfaces%3A+sketching+3D+scenes+with+gestures+and+context+icons" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0036&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DPereira%26aufirst%3DJ.%26date%3D2000%26atitle%3DTowards%2520calligraphic%2520interfaces%253A%2520sketching%25203D%2520scenes%2520with%2520gestures%2520and%2520context%2520icons" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0037"><span><span class="hlFld-ContribAuthor">Pinheiro, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Bates, <span class="NLM_given-names">D.</span></span>, <span class="hlFld-ContribAuthor">DebRoy, <span class="NLM_given-names">S.</span></span>, <span class="hlFld-ContribAuthor">Sarkar, <span class="NLM_given-names">D.</span></span>, &amp; R Core Team. (<span class="NLM_year">2014</span>). <i>{nlme}: Linear and Nonlinear Mixed Effects Models. R Foundation for Statistical Computing</i>. Retrieved from <a class="ext-link" href="https://cran.r-project.org/package=nlme" target="_blank">https://cran.r-project.org/package=nlme</a>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=J.+Pinheiro&author=D.+Bates&author=S.+DebRoy&author=D.+Sarkar&author=R+Core+Team&title=%7Bnlme%7D%3A+Linear+and+Nonlinear+Mixed+Effects+Models.+R+Foundation+for+Statistical+Computing" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0037&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DPinheiro%26aufirst%3DJ.%26aucorp%3DR%2520Core%2520Team%26date%3D2014https%3A%2F%2Fcran.r-project.org%2Fpackage%3Dnlme" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0038"><span><span class="hlFld-ContribAuthor">Quek, <span class="NLM_given-names">F.</span></span>, <span class="hlFld-ContribAuthor">Mcneill, <span class="NLM_given-names">D.</span></span>, <span class="hlFld-ContribAuthor">Bryll, <span class="NLM_given-names">R.</span></span>, &amp; <span class="hlFld-ContribAuthor">Mccullough, <span class="NLM_given-names">K. E.</span></span> (<span class="NLM_year">2002</span>). <span class="NLM_article-title">Multimodal human discourse: Gesture and speech university of illinois at Chicago</span>. <i>ACM Transactions on Computer-Human Interaction (TOCHI)</i>, 9(3), <span class="NLM_fpage">171</span>–<span class="NLM_lpage">193</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0038&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1145%2F568513.568514" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=171-193&issue=3&author=F.+Quek&author=D.+Mcneill&author=R.+Bryll&author=K.+E.+Mccullough&title=Multimodal+human+discourse%3A+Gesture+and+speech+university+of+illinois+at+Chicago" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0038&amp;dbid=16384&amp;doi=10.1145%2F568513.568514&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1145%252F568513.568514%26sid%3Dliteratum%253Atandf%26aulast%3DQuek%26aufirst%3DF.%26date%3D2002%26atitle%3DMultimodal%2520human%2520discourse%253A%2520Gesture%2520and%2520speech%2520university%2520of%2520illinois%2520at%2520Chicago%26jtitle%3DACM%2520Transactions%2520on%2520Computer-Human%2520Interaction%2520%2528TOCHI%2529%26volume%3D9%26issue%3D3%26spage%3D171%26epage%3D193" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0039"><span><span class="hlFld-ContribAuthor">Rahimian, <span class="NLM_given-names">F. P.</span></span>, &amp; <span class="hlFld-ContribAuthor">Ibrahim, <span class="NLM_given-names">R.</span></span> (<span class="NLM_year">2011</span>, <span class="NLM_month">May</span>). <span class="NLM_article-title">Impacts of VR 3D sketching on novice designersâA<sup>˘</sup> Z<sup>´</sup> spatial cognition in collaborative conceptual architectural de- sign</span>. <i>Design Studies</i>, 32(3), <span class="NLM_fpage">255</span>–<span class="NLM_lpage">291</span>. doi:<span class="NLM_pub-id">10.1016/j.destud.2010.10.003</span><span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0039&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1016%2Fj.destud.2010.10.003" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0039&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000290083300003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=255-291&issue=3&author=F.+P.+Rahimian&author=R.+Ibrahim&title=Impacts+of+VR+3D+sketching+on+novice+designers%C3%A2A%CB%98+Z%C2%B4+spatial+cognition+in+collaborative+conceptual+architectural+de-+sign" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0039&amp;dbid=16384&amp;doi=10.1016%2Fj.destud.2010.10.003&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.destud.2010.10.003%26sid%3Dliteratum%253Atandf%26aulast%3DRahimian%26aufirst%3DF.%2520P.%26date%3D2011%26atitle%3DImpacts%2520of%2520VR%25203D%2520sketching%2520on%2520novice%2520designers%25C3%25A2A%25CB%2598%2520Z%25C2%25B4%2520spatial%2520cognition%2520in%2520collaborative%2520conceptual%2520architectural%2520de-%2520sign%26jtitle%3DDesign%2520Studies%26volume%3D32%26issue%3D3%26spage%3D255%26epage%3D291" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0040"><span><span class="hlFld-ContribAuthor">Ruiter, <span class="NLM_given-names">J. D.</span></span> (<span class="NLM_year">2000</span>). <span class="NLM_article-title">The production of gesture and speech</span>. <i>Language and Gesture</i>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0040&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1017%2FCBO9780511620850.018" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&author=J.+D.+Ruiter&title=The+production+of+gesture+and+speech" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0040&amp;dbid=16384&amp;doi=10.1017%2FCBO9780511620850.018&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1017%252FCBO9780511620850.018%26sid%3Dliteratum%253Atandf%26aulast%3DRuiter%26aufirst%3DJ.%2520D.%26date%3D2000%26atitle%3DThe%2520production%2520of%2520gesture%2520and%2520speech%26jtitle%3DLanguage%2520and%2520Gesture" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0041"><span><span class="hlFld-ContribAuthor">Sadeghipour, <span class="NLM_given-names">A.</span></span>, &amp; <span class="hlFld-ContribAuthor">Morency, <span class="NLM_given-names">L.-P.</span></span> (<span class="NLM_year">2014</span>). <i>3DIG - 3D Iconic gesture dataset</i>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=A.+Sadeghipour&author=L.-P.+Morency&title=3DIG+-+3D+Iconic+gesture+dataset" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0041&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSadeghipour%26aufirst%3DA.%26date%3D2014%26btitle%3D3DIG%2520-%25203D%2520Iconic%2520gesture%2520dataset" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0042"><span><span class="hlFld-ContribAuthor">Sadeghipour, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Morency, <span class="NLM_given-names">L.-P.</span></span>, &amp; <span class="hlFld-ContribAuthor">Kopp, <span class="NLM_given-names">S.</span></span> (<span class="NLM_year">2012</span>). <span class="NLM_article-title"><i>Gesture-based object recognition using histograms of guiding strokes</i></span>. In <i>Proceedings of the British Machine Vision Conference</i> (p. <span class="NLM_page-range">44.1–44.11</span>). Durham, UK: BMVA Press.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&author=A.+Sadeghipour&author=L.-P.+Morency&author=S.+Kopp&title=Gesture-based+object+recognition+using+histograms+of+guiding+strokes" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0042&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSadeghipour%26aufirst%3DA.%26date%3D2012%26atitle%3DGesture-based%2520object%2520recognition%2520using%2520histograms%2520of%2520guiding%2520strokes%26jtitle%3DIn%2520Proceedings%2520of%2520the%2520British%2520Machine%2520Vision%2520Conference" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0043"><span><span class="hlFld-ContribAuthor">Shah, <span class="NLM_given-names">J. J.</span></span>, <span class="hlFld-ContribAuthor">Smith, <span class="NLM_given-names">S. M.</span></span>, &amp; <span class="hlFld-ContribAuthor">Vargas-Hernandez, <span class="NLM_given-names">N.</span></span> (<span class="NLM_year">2003</span>, <span class="NLM_month">March</span>). <span class="NLM_article-title">Metrics for measuring ideation e<i>ff</i>ectiveness</span>. <i>Design Studies</i>, 24(2), <span class="NLM_fpage">111</span>–<span class="NLM_lpage">134</span>. doi:<span class="NLM_pub-id">10.1016/S0142-694X(02)00034-0</span><span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0043&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1016%2FS0142-694X%2802%2900034-0" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2003&pages=111-134&issue=2&author=J.+J.+Shah&author=S.+M.+Smith&author=N.+Vargas-Hernandez&title=Metrics+for+measuring+ideation+effectiveness" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0043&amp;dbid=16384&amp;doi=10.1016%2FS0142-694X%2802%2900034-0&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252FS0142-694X%252802%252900034-0%26sid%3Dliteratum%253Atandf%26aulast%3DShah%26aufirst%3DJ.%2520J.%26date%3D2003%26atitle%3DMetrics%2520for%2520measuring%2520ideation%2520effectiveness%26jtitle%3DDesign%2520Studies%26volume%3D24%26issue%3D2%26spage%3D111%26epage%3D134" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0044"><span><span class="hlFld-ContribAuthor">Shen, <span class="NLM_given-names">Y.</span></span>, <span class="hlFld-ContribAuthor">Ong, <span class="NLM_given-names">S.-K.</span></span>, &amp; <span class="hlFld-ContribAuthor">Nee, <span class="NLM_given-names">A. Y.</span></span> (<span class="NLM_year">2011</span>). <span class="NLM_article-title">Vision-based hand interaction in augmented reality environment</span>. <i>International Journal of Human–Computer Interaction</i>, 27(6), <span class="NLM_fpage">523</span>–<span class="NLM_lpage">544</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="https://www.tandfonline.com/doi/10.1080/10447318.2011.555297" target="_blank">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0044&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000290407700003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=523-544&issue=6&author=Y.+Shen&author=S.-K.+Ong&author=A.+Y.+Nee&title=Vision-based+hand+interaction+in+augmented+reality+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0044&amp;dbid=16384&amp;doi=10.1080%2F10447318.2011.555297&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447318.2011.555297%26sid%3Dliteratum%253Atandf%26aulast%3DShen%26aufirst%3DY.%26date%3D2011%26atitle%3DVision-based%2520hand%2520interaction%2520in%2520augmented%2520reality%2520environment%26jtitle%3DInternational%2520Journal%2520of%2520Human%25E2%2580%2593Computer%2520Interaction%26volume%3D27%26issue%3D6%26spage%3D523%26epage%3D544" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0045"><span><span class="hlFld-ContribAuthor">Sowa, <span class="NLM_given-names">T.</span></span>, &amp; <span class="hlFld-ContribAuthor">Wachsmuth, <span class="NLM_given-names">I.</span></span> (<span class="NLM_year">2002</span>). <span class="NLM_article-title">Interpretation of shape- related iconic gestures in virtual environments</span>. In I. Wachsmuth &amp; T. Sowa (Eds.), <i>Gesture and Sign Language in Human-Computer Interaction: International Gesture Workshop, GW 2001 London, UK, April 18&amp;20, 2001 Revised Papers</i> (pp. <span class="NLM_fpage">21</span>–<span class="NLM_lpage">33</span>). Berlin, Heidelberg, Germany: Springer Berlin Heidelberg.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=21-33&author=T.+Sowa&author=I.+Wachsmuth&title=Interpretation+of+shape-+related+iconic+gestures+in+virtual+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0045&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSowa%26aufirst%3DT.%26date%3D2002%26atitle%3DInterpretation%2520of%2520shape-%2520related%2520iconic%2520gestures%2520in%2520virtual%2520environments%26jtitle%3DGesture%2520and%2520Sign%2520Language%2520in%2520Human-Computer%2520Interaction%253A%2520International%2520Gesture%2520Workshop%252C%2520GW%25202001%2520London%252C%2520UK%252C%2520April%252018%252620%252C%25202001%2520Revised%2520Papers%26spage%3D21%26epage%3D33" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0046"><span><span class="hlFld-ContribAuthor">Sturman, <span class="NLM_given-names">D. J.</span></span>, &amp; <span class="hlFld-ContribAuthor">Zeltzer, <span class="NLM_given-names">D.</span></span> (<span class="NLM_year">1993</span>). <span class="NLM_article-title">A Design Method for “Whole-hand” Human-computer Interaction</span>. <i>ACM Trans. Inf. Syst</i>., 11(3), <span class="NLM_fpage">219</span>–<span class="NLM_lpage">238</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0046&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1145%2F159161.159159" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0046&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=A1993MF93900003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1993&pages=219-238&issue=3&author=D.+J.+Sturman&author=D.+Zeltzer&title=A+Design+Method+for+%E2%80%9CWhole-hand%E2%80%9D+Human-computer+Interaction" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0046&amp;dbid=16384&amp;doi=10.1145%2F159161.159159&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1145%252F159161.159159%26sid%3Dliteratum%253Atandf%26aulast%3DSturman%26aufirst%3DD.%2520J.%26date%3D1993%26atitle%3DA%2520Design%2520Method%2520for%2520%25E2%2580%259CWhole-hand%25E2%2580%259D%2520Human-computer%2520Interaction%26jtitle%3DACM%2520Trans.%2520Inf.%2520Syst%26volume%3D11%26issue%3D3%26spage%3D219%26epage%3D238" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0047"><span><span class="hlFld-ContribAuthor">Suwa, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Gero, <span class="NLM_given-names">J.</span></span>, &amp; <span class="hlFld-ContribAuthor">Purcell, <span class="NLM_given-names">T.</span></span> (<span class="NLM_year">2006</span>). <span class="NLM_chapter-title"><i>Unexpected discoveries and s-inventions of design requirements: A key to creative designs</i></span> (Computational Models of Creative Design IV, Key Centre of Design Computing and Cognition). <span class="NLM_publisher-loc">Sydney, Australia</span>: <span class="NLM_publisher-name">University of Sydney</span>.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&author=M.+Suwa&author=J.+Gero&author=T.+Purcell&title=Unexpected+discoveries+and+s-inventions+of+design+requirements%3A+A+key+to+creative+designs" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0047&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSuwa%26aufirst%3DM.%26date%3D2006%26atitle%3DUnexpected%2520discoveries%2520and%2520s-inventions%2520of%2520design%2520requirements%253A%2520A%2520key%2520to%2520creative%2520designs%26btitle%3DComputational%2520Models%2520of%2520Creative%2520Design%2520IV%252C%2520Key%2520Centre%2520of%2520Design%2520Computing%2520and%2520Cognition%2529%26pub%3DUniversity%2520of%2520Sydney" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0048"><span><span class="hlFld-ContribAuthor">van Dijk, <span class="NLM_given-names">C. G.</span></span> (<span class="NLM_year">1995</span>, <span class="NLM_month">January</span>). <span class="NLM_article-title">New insights in computer-aided conceptual design</span>. <i>Design Studies</i>, 16(1), <span class="NLM_fpage">62</span>–<span class="NLM_lpage">80</span>. doi:<span class="NLM_pub-id">10.1016/0142-694X(95)90647-X</span><span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0048&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1016%2F0142-694X%2895%2990647-X" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1995&pages=62-80&issue=1&author=C.+G.+van+Dijk&title=New+insights+in+computer-aided+conceptual+design" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0048&amp;dbid=16384&amp;doi=10.1016%2F0142-694X%2895%2990647-X&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252F0142-694X%252895%252990647-X%26sid%3Dliteratum%253Atandf%26aulast%3Dvan%2520Dijk%26aufirst%3DC.%2520G.%26date%3D1995%26atitle%3DNew%2520insights%2520in%2520computer-aided%2520conceptual%2520design%26jtitle%3DDesign%2520Studies%26volume%3D16%26issue%3D1%26spage%3D62%26epage%3D80" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0049"><span>VRClay. (<span class="NLM_year">2014</span>). <span class="NLM_article-title"><i>VRClay - Sculpting in virtual reality</i></span>. Retrieved from <a class="ext-link" href="http://vrclay.com/" target="_blank">http://vrclay.com/</a><span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=VRClay&title=VRClay+-+Sculpting+in+virtual+reality" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0049&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aucorp%3DVRClay%26date%3D2014%26atitle%3DVRClay%2520-%2520Sculpting%2520in%2520virtual%2520realityhttp%3A%2F%2Fvrclay.com%2F" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0050"><span><span class="hlFld-ContribAuthor">Wang, <span class="NLM_given-names">L.</span></span>, <span class="hlFld-ContribAuthor">Shen, <span class="NLM_given-names">W.</span></span>, &amp; <span class="hlFld-ContribAuthor">Xie, <span class="NLM_given-names">H.</span></span> (<span class="NLM_year">2002</span>). <span class="NLM_article-title">Collaborative conceptual designâA<sup>˘</sup>T<sup>ˇ</sup> state of the art and future trends</span>. <i>Computer-Aided Design</i>, 34(13), <span class="NLM_fpage">981</span>–<span class="NLM_lpage">996</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0050&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.1016%2FS0010-4485%2801%2900157-9" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0050&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000177032400002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=981-996&issue=13&author=L.+Wang&author=W.+Shen&author=H.+Xie&title=Collaborative+conceptual+design%C3%A2A%CB%98T%CB%87+state+of+the+art+and+future+trends" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0050&amp;dbid=16384&amp;doi=10.1016%2FS0010-4485%2801%2900157-9&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252FS0010-4485%252801%252900157-9%26sid%3Dliteratum%253Atandf%26aulast%3DWang%26aufirst%3DL.%26date%3D2002%26atitle%3DCollaborative%2520conceptual%2520design%25C3%25A2A%25CB%2598T%25CB%2587%2520state%2520of%2520the%2520art%2520and%2520future%2520trends%26jtitle%3DComputer-Aided%2520Design%26volume%3D34%26issue%3D13%26spage%3D981%26epage%3D996" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0051"><span><span class="hlFld-ContribAuthor">Wesp, <span class="NLM_given-names">R.</span></span>, <span class="hlFld-ContribAuthor">Hesse, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Keutmann, <span class="NLM_given-names">D.</span></span>, &amp; <span class="hlFld-ContribAuthor">Wheaton, <span class="NLM_given-names">K.</span></span> (<span class="NLM_year">2001</span>). <span class="NLM_article-title">Gestures maintain spatial imagery</span>. <i>The American Journal of Psychology</i>, 114, <span class="NLM_fpage">591</span>.<span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0051&amp;dbid=16&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=10.2307%2F1423612" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0051&amp;dbid=8&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=11789342" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0051&amp;dbid=128&amp;doi=10.1080%2F10447318.2017.1296074&amp;key=000172777700006" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2001&pages=591&author=R.+Wesp&author=J.+Hesse&author=D.+Keutmann&author=K.+Wheaton&title=Gestures+maintain+spatial+imagery" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0051&amp;dbid=16384&amp;doi=10.2307%2F1423612&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.2307%252F1423612%26sid%3Dliteratum%253Atandf%26aulast%3DWesp%26aufirst%3DR.%26date%3D2001%26atitle%3DGestures%2520maintain%2520spatial%2520imagery%26jtitle%3DThe%2520American%2520Journal%2520of%2520Psychology%26volume%3D114%26spage%3D591" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0052"><span><span class="hlFld-ContribAuthor">Zeleznik, <span class="NLM_given-names">R.</span></span>, <span class="hlFld-ContribAuthor">Herndon, <span class="NLM_given-names">K.</span></span>, &amp; <span class="hlFld-ContribAuthor">Hughes, <span class="NLM_given-names">J.</span></span> (<span class="NLM_year">2007</span>). <span class="NLM_article-title">SKETCH: An Interface for Sketching 3D Scenes</span>. In <span class="NLM_conf-name"><i>ACM SIGGRAPH 2007</i> Courses</span>. New York, NY, USA: ACM.<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&author=R.+Zeleznik&author=K.+Herndon&author=J.+Hughes&title=SKETCH%3A+An+Interface+for+Sketching+3D+Scenes" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0052&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DZeleznik%26aufirst%3DR.%26date%3D2007%26atitle%3DSKETCH%253A%2520An%2520Interface%2520for%2520Sketching%25203D%2520Scenes" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li></div></ul><div class="author-infos"><h2>Additional information</h2><h3>Author information</h3><div id="B0001" class="addAuthorInfo"><span class="AuthorInfoData"><h4><span class="NLM_given-names">Remi</span> Alkemade</h4><div></div><b>Remi Alkemade</b> (<a class="email" href="mailto:remi@remialkemade.nl">remi@remialkemade.nl</a>) is a computer scientist with an interest in new interaction modalities and artificial intelligence; he is an independent researcher who graduated in early 2015 from the Media Technology master’s program at the Leiden University in the Netherlands.</span></div><div id="B0002" class="addAuthorInfo"><span class="AuthorInfoData"><h4><span class="NLM_given-names">Fons J.</span> Verbeek</h4><div></div><b>Fons J. Verbeek</b> (<a class="email" href="mailto:f.j.verbeek@liacs.leidenuniv.nl">f.j.verbeek@liacs.leidenuniv.nl</a>, <a class="ext-link" href="http://liacs.leidenuniv.nl/~verbeekfj" target="_blank">http://liacs.leidenuniv.nl/~verbeekfj</a>) is head of the Imagery &amp; Media cluster of Leiden Institute of Advanced Computer Science; he heads the section Imaging &amp; BioInformatics. Apart from data analysis in images, he is specifically interested in image interaction and interactivity in data spaces. Hence, new interaction devices are studied in his research group in correspondence projects in data analysis.</span></div><div id="B0003" class="addAuthorInfo"><span class="AuthorInfoData"><h4><span class="NLM_given-names">Stephan G.</span> Lukosch</h4><div></div><b>Stephan G. Lukosch</b> (<a class="email" href="mailto:s.g.lukosch@tudelft.nl">s.g.lukosch@tudelft.nl</a>, <a class="ext-link" href="http://www.tudelft.nl/sglukosch" target="_blank">http://www.tudelft.nl/sglukosch</a>) is a computer scientist with an interest in virtual co-location using augmented reality; he is an Associate Professor in the Faculty of Technology, Policy, and Management at Delft University of Technology.</span></div></div><div class="response"><div class="sub-article-title"></div></div>
</article>
</div>
<div class="tab tab-pane" id="relatedContent">
</div>
<div class="tab tab-pane " id="metrics-content">
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics" data-pb-dropzone="publicationContentDropZoneMetrics">
<div class="widget literatumArticleMetricsWidget none  widget-none" id="00886058-9b49-4cdf-9f1e-deb78b7818c3">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="articleMetricsContainer">
<div class="content fullView">
<h2>
Article Metrics
</h2>
<div class="section views">
<div class="title">
Views
</div>
<div class="circle">
<span class="value">474</span>
</div>
</div>
<div class="section citations">
<div class="title">
Citations
</div>
<a href="/doi/citedby/10.1080/10447318.2017.1296074" class="circle crossRef " target="_blank">
<span>
Crossref
</span>
<span class="value">12</span>
</a>
<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=CitingArticles&KeyUT=000413911500003" target="_blank" class="circle webOfScience ">
<span>
 Web of Science
</span>
<span class="value">4</span>
</a>
<a href="http://www.scopus.com/inward/citedby.url?partnerID=HzOxMe3b&scp=85016468084" target="_blank" class="circle scopus ">
<span>
Scopus
</span>
<span class="value">6</span>
</a>
</div>
<div class="section altmetric-container">
<div class="title">
Altmetric
</div>
<script type="text/javascript">
    TandfUtils.appendScript(document.head, 'https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js', 'altmetric-embed-src', true, true);
</script>
<div class="metrics-badge linkToPopup">
<div class='altmetric-embed' data-badge-type='medium-donut' data-badge-details='right' data-condensed='true' data-template="tandf" data-hide-no-mentions="false" data-doi="10.1080/10447318.2017.1296074">
</div>
</div>
</div>
<div class="altmetricsPopup"></div>
</div>
</div>
<div class="metricsLinks">
<a href="/article-metrics">Article metrics information</a>
<br>
<a href="/article-citations-disclaimer">Disclaimer for citing articles</a>
</div></div>
</div>
</div>
</div>
</div>
<div class="access__limit" data-pb-dropzone="accessLimitPage">
</div>
</div>
</div>
</div>
<input id="viewLargeImageCaption" type="hidden" value="View Large Image" /></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-4 ">
<div class="contents" data-pb-dropzone="contents2">
<div class="widget general-bookmark-share none  widget-none  widget-compact-all" id="c8494935-e102-4ff5-9395-4ffa44a77f1c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<ul>
<li>
<div class="addthis_toolbox addthis_20x20_style">
<div class="custom_images">
<a class="addthis_button_twitter">
<span class="at-icon-twitter"></span>
</a>
<a class="addthis_button_facebook">
<span class="at-icon-facebook"></span>
</a>
<a class="addthis_button_email">
<span class="at-icon-email"></span>
</a>
<a class="addthis_button_none">
<span class="at-icon-none"></span>
</a>
<a class="addthis_button_compact"><span class="at-icon-wrapper"></span>
<span aria-describedby="shareOptions-description">
<span class="off-screen" id="shareOptions-description">More Share Options</span>
</span>
</a>
</div>
</div>
</li>
</ul>
<script type="text/javascript">
    
    var script = document.createElement('script');
    script.type='text/javascript';
    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';
    script.async = true;
    $('head').append(script)
</script>
</div>
</div>
</div>
<div class="widget general-html none  widget-none" id="16111d74-c554-42b2-a277-f2727ad2b285">
<div class="wrapped ">
<div class="widget-body body body-none ">&nbsp;</div>
</div>
</div>
<div class="widget general-html none  widget-none  widget-compact-all" id="649c3793-a589-4dfb-8de2-9b6cbc4dc15b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script defer src='//js.trendmd.com/trendmd.min.js' data-trendmdconfig='{"element":"#trendmd-suggestions"}'></script></div>
</div>
</div>
<div class="widget graphQueryWidget none  widget-none  widget-compact-all" id="6583d550-25db-458c-9c81-291f5c88b6ee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="trendmd-suggestions"></div></div>
</div>
</div>
<div class="widget pbOptimizerWidget none  widget-none  widget-compact-all" id="25efeb89-6948-4246-800a-5e246009698d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div data-optimizer data-widget-id="25efeb89-6948-4246-800a-5e246009698d" id="widget-25efeb89-6948-4246-800a-5e246009698d" data-observer>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget pageFooter none  widget-none  widget-compact-all" id="d97c173f-d838-4de1-bbd7-ed69f0d36a91">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><footer class="page-footer">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all" id="1f15adc0-4a59-4d27-93fe-8cbb14a5108a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-1-1 fit-padding">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget topicalIndex none  widget-none  widget-compact-all" id="9298c7a6-6903-4607-8380-4c83e2b7142f">
<div class="wrapped ">
<h1 class="widget-header header-none  header-compact-all">Browse journals by subject</h1>
<div class="widget-body body body-none  body-compact-all"><div class="topicalIndexBrowsingTips" data-pb-dropzone="topicalIndexBrowsingTips">
<div class="widget general-html none  widget-none  widget-compact-all" id="1ec3bad2-243b-45a9-a59a-5aceb80fc5a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a class="nav-top" href="#top">Back to top <span class="fa fa-angle-up"></span></a></div>
</div>
</div>
</div>
<div class="container">
<ul>
<li>
<a href="/topic/allsubjects/as?target=topic&amp;ConceptID=4251">Area Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ar?target=topic&amp;ConceptID=4250">Arts</a>
</li>
<li>
<a href="/topic/allsubjects/be?target=topic&amp;ConceptID=4252">Behavioral Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/bs?target=topic&amp;ConceptID=4253">Bioscience</a>
</li>
<li>
<a href="/topic/allsubjects/bu?target=topic&amp;ConceptID=4254">Built Environment</a>
</li>
<li>
<a href="/topic/allsubjects/cs?target=topic&amp;ConceptID=4256">Communication Studies</a>
</li>
<li>
<a href="/topic/allsubjects/cm?target=topic&amp;ConceptID=4255">Computer Science</a>
</li>
<li>
<a href="/topic/allsubjects/ds?target=topic&amp;ConceptID=4257">Development Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ea?target=topic&amp;ConceptID=4258">Earth Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/eb?target=topic&amp;ConceptID=4259">Economics, Finance, Business & Industry</a>
</li>
<li>
<a href="/topic/allsubjects/ed?target=topic&amp;ConceptID=4261">Education</a>
</li>
<li>
<a href="/topic/allsubjects/ec?target=topic&amp;ConceptID=4260">Engineering & Technology</a>
</li>
 <li>
<a href="/topic/allsubjects/ag?target=topic&amp;ConceptID=4248">Environment & Agriculture</a>
</li>
<li>
<a href="/topic/allsubjects/es?target=topic&amp;ConceptID=4262">Environment and Sustainability</a>
</li>
<li>
<a href="/topic/allsubjects/fs?target=topic&amp;ConceptID=4263">Food Science & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ge?target=topic&amp;ConceptID=4264">Geography</a>
</li>
<li>
<a href="/topic/allsubjects/hs?target=topic&amp;ConceptID=4266">Health and Social Care</a>
</li>
<li>
<a href="/topic/allsubjects/hu?target=topic&amp;ConceptID=4267">Humanities</a>
</li>
<li>
<a href="/topic/allsubjects/if?target=topic&amp;ConceptID=4268">Information Science</a>
</li>
<li>
<a href="/topic/allsubjects/la?target=topic&amp;ConceptID=4269">Language & Literature</a>
</li>
<li>
<a href="/topic/allsubjects/lw?target=topic&amp;ConceptID=4270">Law</a>
</li>
<li>
<a href="/topic/allsubjects/ma?target=topic&amp;ConceptID=4271">Mathematics & Statistics</a>
</li>
<li>
<a href="/topic/allsubjects/me?target=topic&amp;ConceptID=4272">Medicine, Dentistry, Nursing & Allied Health</a>
</li>
<li>
<a href="/topic/allsubjects/ah?target=topic&amp;ConceptID=4249">Museum and Heritage Studies</a>
</li>
<li>
<a href="/topic/allsubjects/pc?target=topic&amp;ConceptID=4273">Physical Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/pi?target=topic&amp;ConceptID=4274">Politics & International Relations</a>
</li>
<li>
<a href="/topic/allsubjects/sn?target=topic&amp;ConceptID=4278">Social Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/sl?target=topic&amp;ConceptID=4277">Sports and Leisure</a>
</li>
<li>
<a href="/topic/allsubjects/sp?target=topic&amp;ConceptID=4279">Tourism, Hospitality and Events</a>
</li>
<li>
<a href="/topic/allsubjects/us?target=topic&amp;ConceptID=4280">Urban Studies</a>
</li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none footer-links widget-none  widget-compact-horizontal" id="64a44adf-45ed-4da3-be26-ef25beb9dbee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="6918e9df-910a-4206-9bd0-1a02bc17f740">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="aa9510dd-52ed-4b74-8211-fb510cd9468e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Information for</h3>
<ul>
<li><a href="http://authorservices.taylorandfrancis.com/">Authors</a></li>
<li><a href="http://editorresources.taylorandfrancisgroup.com/">Editors</a></li>
<li><a href="/page/librarians">Librarians</a></li>
<li><a href="/societies">Societies</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Open access</h3>
<ul>
<li><a href="/openaccess">Overview</a></li>
<li><a href="/openaccess/openjournals">Open journals</a></li>
<li><a href="/openaccess/openselect">Open Select</a></li>
<li><a href="https://www.cogentoa.com/">Cogent OA</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="fc564559-f496-499c-87c7-d851f371f061">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="f3fb3d36-db42-4373-9d0e-432958bf2fbc">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Help and info</h3>
<ul>
<li><a href="https://help.tandfonline.com">Help & contact</a></li>
<li><a href="https://newsroom.taylorandfrancisgroup.com/">Newsroom</a></li>
<li><a href="https://taylorandfrancis.com/partnership/commercial/">Commercial services</a></li>
<li><a href="/action/showPublications?pubType=journal">All journals</a></li>
<li><a href="https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot">Books</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">

<div class="widget general-html none  widget-none  widget-compact-all" id="914433f6-0ea6-4a47-9781-07564061be86">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-social-label">
<h3>Keep up to date</h3>
</div>
<div class="font-size-correction-sml">Register to receive personalised research and resources by email</div>
<div class="bs">
<div class="pull-left links font-size-correction">
<a class="font-size-correction-link" href="https://taylorandfrancis.formstack.com/forms/tfoguest_signup"><i class="fa fa-envelope-square" title="Register to receive personalised research and resources by email"></i>Sign me up</a>
</div></div></div>
</div>
</div>
<div class="widget literatumSocialLinks none  widget-none  widget-compact-all" id="3b6a5e53-cd62-452f-adc1-92e187a0849d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="bs">
<div class="pull-left links">
<a href="http://facebook.com/TaylorandFrancisGroup">
<i class="icon-facebook" title="Taylor and Francis Group Facebook page" aria-hidden="true" role="button"></i>
<span aria-describedby="fb-description">
<span class="off-screen" id="fb-description">Taylor and Francis Group Facebook page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="https://twitter.com/tandfonline">
<i class="fa fa-twitter-square" title="Taylor and Francis Group Twitter page" aria-hidden="true" role="button"></i>
<span aria-describedby="twitter-description">
<span class="off-screen" id="twitter-description">Taylor and Francis Group Twitter page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://linkedin.com/company/taylor-&-francis-group">
<i class="fa fa-linkedin-square" title="Taylor and Francis Group LinkedIn page" aria-hidden="true" role="button"></i>
<span aria-describedby="linkedin-description">
<span class="off-screen" id="linkedin-description">Taylor and Francis Group Linkedin page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
<div class="pull-left links">
<a href="https://www.youtube.com/user/TaylorandFrancis">
<i class="fa fa-youtube-square" title="Taylor and Francis Group YouTube page" aria-hidden="true" role="button"></i>
<span aria-describedby="youtube-description">
<span class="off-screen" id="youtube-description">Taylor and Francis Group Youtube page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://www.weibo.com/tandfchina">
<i class="fa fa-weibo" title="Taylor and Francis Group Weibo page" aria-hidden="true" role="button"></i>
<span aria-describedby="weibo-description">
<span class="off-screen" id="weibo-description">Taylor and Francis Group Weibo page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-horizontal" id="8d803f96-081d-4768-ab7d-280a77af723b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-sm  ">
<div class="col-sm-3-4 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none footer-info-container widget-none  widget-compact-vertical" id="b247ecb9-84c9-4762-b270-20f8be1f0ae4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="informa-group-info">
<span>Copyright © 2020 Informa UK Limited</span>
<span><a href="https://informa.com/privacy-policy/">Privacy policy</a></span>
<span><a href="/cookies">Cookies</a></span>
<span><a href="/terms-and-conditions">Terms & conditions</a></span>
<span><a href="/accessibility">Accessibility</a></span>
<p>Registered in England & Wales No. 3099067<br />
5 Howick Place | London | SW1P 1WG</p>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-4 footer_tandf_logo">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-image none  widget-none  widget-compact-vertical" id="b6bde365-079b-454f-94f6-1841291656a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><a href="http://taylorandfrancis.com/" title="Taylor and Francis Group">
<img src="/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png" alt="Taylor and Francis Group" />
</a></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget cookiePolicy none  widget-none  widget-compact-all" id="cea739ac-da2c-4d77-9cf1-cb3e0da7e31e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="banner">
<a href="#" class="btn">Accept</a>
<p class="message">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href="/cookies">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>
</div></div>
</div>
</div>
</div>
</footer></div>
</div>
</div>
</div>
</div>
</body>
</html>
