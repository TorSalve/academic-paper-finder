<!DOCTYPE html>
<html lang="en" class="pb-page" data-request-id="a16b543f-3fc0-4119-8666-1b72b8eadafb"><head data-pb-dropzone="head"><meta name="pbContext" content=";issue:issue:10.1080/hihc20.v027.i06;page:string:Article/Chapter View;ctype:string:Journal Content;journal:journal:hihc20;article:article:10.1080/10447318.2011.555297;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text;requestedJournal:journal:hihc20" />
<link rel="schema.DC" href="http://purl.org/DC/elements/1.0/" /><meta name="citation_journal_title" content="Intl. Journal of Human–Computer Interaction" /><meta name="dc.Title" content="Vision-Based Hand Interaction in Augmented Reality Environment" /><meta name="dc.Creator" content=" Y.    Shen " /><meta name="dc.Creator" content=" S. K.    Ong " /><meta name="dc.Creator" content=" A. Y. C.    Nee " /><meta name="dc.Description" content="A new vision-based framework and system for human–computer interaction in an Augmented Reality environment is presented in this article. The system allows the users to interact with computer-genera..." /><meta name="Description" content="A new vision-based framework and system for human–computer interaction in an Augmented Reality environment is presented in this article. The system allows the users to interact with computer-genera..." /><meta name="dc.Publisher" content=" Taylor &amp; Francis Group " /><meta name="dc.Date" scheme="WTN8601" content="9 May 2011" /><meta name="dc.Type" content="research-article" /><meta name="dc.Format" content="text/HTML" /><meta name="dc.Identifier" scheme="publisher-id" content="555297" /><meta name="dc.Identifier" scheme="doi" content="10.1080/10447318.2011.555297" /><meta name="dc.Identifier" scheme="coden" content="Intl. Journal of Human–Computer Interaction, Vol. 27, No. 6, June 2011: pp. 523–544" /><meta name="dc.Source" content="http://dx.doi.org/10.1080/10447318.2011.555297" /><meta name="dc.Language" content="en" /><meta name="dc.Coverage" content="world" /><meta name="dc.Rights" content="Copyright Taylor and Francis Group, LLC" />
<link rel="meta" type="application/atom+xml" href="https://doi.org/10.1080%2F10447318.2011.555297" />
<link rel="meta" type="application/rdf+json" href="https://doi.org/10.1080%2F10447318.2011.555297" />
<link rel="meta" type="application/unixref+xml" href="https://doi.org/10.1080%2F10447318.2011.555297" />
<title>Full article: Vision-Based Hand Interaction in Augmented Reality Environment</title>
<meta charset="UTF-8">
<meta name="robots" content="noarchive" />
<meta name="pb-robots-disabled">

<meta property="og:title" content="Vision-Based Hand Interaction in Augmented Reality Environment" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.tandfonline.com/doi/abs/10.1080/10447318.2011.555297" />
<meta property="og:image" content="https://www.tandfonline.com/doi/cover-img/10.1080/hihc20.v027.i06" />
<meta property="og:site_name" content="Taylor & Francis" />
<meta property="og:description" content="(2011). Vision-Based Hand Interaction in Augmented Reality Environment. International Journal of Human&#x2013;Computer Interaction: Vol. 27, No. 6, pp. 523-544." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@tandfonline">
<meta name="viewport" content="width=device-width,initial-scale=1" />
<script>var tandfData = {"search":{"hasOpenAccess":true}};</script>
<link rel="stylesheet" type="text/css" href="/wro/j2y2~product.css"><link rel="stylesheet" type="text/css" href="/pb/css/t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" id="pb-css" data-pb-css-id="t1594192466000-v1594192466000/head_4_698_1485_2139_2347_7872.css" />
<link href="//www.trendmd.com" rel="preconnect" />
<link href="//app.wizdom.ai" rel="preconnect" />
<link href="//connect.facebook.net" rel="preconnect" />
<link href="//go.taylorandfrancis.com" rel="preconnect" />
<link href="//pi.pardot.com" rel="preconnect" />
<link href="//static.hotjar.com" rel="preconnect" />
<link href="//cdn.pbgrd.com" rel="preconnect" />
<link href="//f1-eu.readspeaker.com" rel="preconnect" />
<link href="//www.googleadservices.com" rel="preconnect" />
<link href="https://ajax.googleapis.com" rel="preconnect" />
<link href="https://m.addthis.com" rel="preconnect" />
<link href="https://wl.figshare.com" rel="preconnect" />
<link href="https://pagead2.googlesyndication.com" rel="preconnect" />
<link href="https://www.googletagmanager.com" rel="preconnect" />
<link href="https://www.google-analytics.com" rel="preconnect" />
<script type="text/javascript" src="/wro/j2y2~loadinview.js"></script>
<script type="text/javascript" src="/wro/j2y2~product.js"></script>
<script type="text/javascript">
        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};
    </script>
<script type="application/javascript" src="//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr" id="read-speaker" async></script>
<script type="text/javascript" src="//cdn.pbgrd.com/core-tandf.js" async defer></script>
<script data-ad-client="ca-pub-5143040550582507" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script>

<script>
    (function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:864760,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>
<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script async src="https://www.colwiz.com/pubsol/widget/34000f34a146a2017e2b5acad48d6b07.js"></script>
<link href="//qa.colwiz.com" rel="preconnect" />
<script src="//scholar.google.com/scholar_js/casa.js" async></script>
</head>
<body class="pb-ui">

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<div class="skipContent off-screen">
<a href="#top-content-scroll" class="skipToContent" title="Skip to Main Content" tabIndex="0">Skip to Main Content</a>
</div>
<script type="text/javascript">
    if (true) {
        var skipToContent = document.getElementsByClassName("skipToContent");
        if (skipToContent != null) {
            skipToContent[0].onclick = function skipElement() {
                var element = document.getElementById('top-content-scroll');
                if (element == null || element === undefined) {
                    element = document.getElementsByClassName('top-content-scroll').item(0);
                }
                element.setAttribute('tabindex', '0');
                element.focus();
            }
        }
    }
    document.addEventListener("DOMContentLoaded",function(e){
        if(document.getElementsByClassName("mediaThumbnailContainer").length > 0){
            TandfUtils.appendScript(document.body,"/wro/j2y2~jwplayer.js","jwplayer_src",true,true);
        }
    });
</script>
<div id="pb-page-content" data-ng-non-bindable>
<div data-pb-dropzone="main" data-pb-dropzone-name="Main">
<div class="widget pageHeader none  widget-none  widget-compact-all" id="a4d4fdd3-c594-4d68-9f06-b69b8b37ed56">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><header class="page-header">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="036fa949-dc25-4ffe-9df0-d7daefee281b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-xs  ">
<div class="col-xs-1-6 header-index">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal" id="e817489e-2520-418b-a731-b62e247e74df">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/" title="Taylor and Francis Online">
<img src="/pb-assets/Global/tfo_logo-1444989687640.png" alt="Taylor and Francis Online" />
</a></div>
</div>
</div>
<div class="widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal" id="b3fe8380-8b88-4558-b004-6485d3aea155">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><a href="/">
<img src="/pb-assets/Global/tfo_logo_sm-1459688573210.png" />
</a></div>
</div>
</div>
</div>
</div>
<div class="col-xs-5-6 ">
<div class="contents" data-pb-dropzone="contents1">

<div class="widget layout-inline-content alignRight  widget-none  widget-compact-all" id="a8a37801-55c7-4566-bdef-e4e738967e38">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget layout-inline-content none customLoginBar widget-none" id="fbe90803-b9c8-4bef-9365-cb53cc4bfa0e">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumInstitutionBanner none bannerWidth widget-none" id="3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="institution-image-text hidden-xs hidden-sm disable-click">Access provided by<strong> Copenhagen University Library</strong>
</div>
<div class="institution-image logout-institution-image">
</div></div>
</div>
</div>
<div class="widget literatumNavigationLoginBar none  widget-none  widget-compact-all" id="1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="loginBar not-logged-in">
<span class="icon-user"></span>
<a href="/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F10447318.2011.555297" class="sign-in-link">
Log in
</a>
<span class="loginSeprator">&nbsp;|&nbsp;</span>
<a href="/action/registration?redirectUri=%2F" class="register-link">
Register
</a>
</div></div>
</div>
 </div>

</div></div>
</div>
</div>
<div class="widget eCommerceCartIndicatorWidget none literatumCartLink widget-none" id="9de10bb5-08af-48bc-b9f6-3f6433229f3e">
<div class="wrapped ">
<div class="widget-body body body-none "><a href="/action/showCart?FlowID=1" class="cartLabel">
<span class="hidden-xs hidden-sm visible-tl-inline-block">Cart</span>
<span class="cartItems" data-id="cart-size" role="status">
</span>
</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</header></div>
</div>
</div>
<div class="widget pageBody none  widget-none  widget-compact-all" id="35d9ca18-265e-4501-9038-4105e95a4b7d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<div class="page-body pagefulltext">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all" id="1728e801-36cd-4288-9f53-392bad29506a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-5-12 search_container ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget quickSearchWidget none search-customize-width widget-none  widget-compact-all" id="d46e3260-1f5c-4802-821a-28a03a699c82">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="quickSearchFormContainer ">
<form action="/action/doSearch" name="quickSearch" class="quickSearchForm " title="Quick Search" method="get" onsubmit="appendSearchFilters(this)" aria-label="Quick Search"><span class="simpleSearchBoxContainer">
<input name="AllField" class="searchText main-search-field autocomplete" value="" type="search" id="searchText" title="Type search term here" aria-label="Search" placeholder="Enter keywords, authors, DOI, ORCID etc" autocomplete="off" data-history-items-conf="3" data-publication-titles-conf="3" data-publication-items-conf="3" data-topics-conf="3" data-contributors-conf="3" data-fuzzy-suggester="false" data-auto-complete-target="title-auto-complete" />
</span>
<span class="searchDropDownDivRight">
<label for="searchInSelector" class="visuallyhidden">Search in:</label>
<select id="searchInSelector" name="SeriesKey" class="js__searchInSelector">
<option value="hihc20" id="thisJournal" data-search-in="thisJournal">
This Journal
</option>
<option value="" data-search-in="default">
Anywhere
</option>
</select>
</span>
<span class="quick-search-btn">
<input class="mainSearchButton searchButtons pointer" title="Search" role="button" type="submit" value="" aria-label="Search" />
</span></form>
</div>
<div class="advancedSearchLinkDropZone" data-pb-dropzone="advancedSearchLinkDropZone">
<div class="widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all" id="323e2a31-1c81-4995-bd17-8e149458c214">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a href="/search/advanced" class="advSearchArticle">Advanced search</a></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 serNav_container">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumSeriesNavigation none  widget-none" id="7730bfe1-9fca-4cf4-a6d6-2a0148105437">

<div class="wrapped ">
<div class="widget-body body body-none "><div class="issueSerialNavigation journal">
<div class="cover">
<img data-src='{"type":"image" , "src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/hihc20.v027.i06/production/hihc20.v027.i06.cover.jpg"}' src="//:0" alt="Publication Cover" width="120" height="156" />
</div>
<div class="info ">
<div class="title-container">
<span class="titleHeading">Journal</span>
<h1>
<a href="/toc/hihc20/current">
International Journal of Human&#x2013;Computer Interaction
</a>
</h1>
<h2>
Volume 27, 2011 - <a href="/toc/hihc20/27/6" class="nav-toc-list">Issue 6</a>
</h2>
</div>
<div class="seriesNavDropZone" data-pb-dropzone="seriesNavDropZone">
<div class="widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal" id="753455df-1eeb-47ca-bdc9-e19022075973">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="serial-action">
<a href="http://www.editorialmanager.com/ijhc" class="green submitAnArticle"><span>Submit an article</span></a>
<a href="/toc/hihc20/current" class="jHomepage"><span>Journal homepage</span></a>
</div></div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>

</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none" id="e42aea8f-434a-4d39-aaef-f56af3ff00dc">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all" id="6aacf107-e82d-494d-a14c-0c00bba52560">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="accessLogo">
<div>
<img class="accessIconLocation" data-src='{"type":"image" , "src":"/pb-assets/3rdPartyLogos/accessFull-1452596451717.png"}' src="//:0" alt="Full access" />
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none publicationContentHeader widget-none  widget-compact-all" id="63f402e4-3498-4709-8d7d-ee8e69f93467">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical" id="5afd8b6d-7e09-43ff-8ad6-afa3764e543c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="articleMetricsContainer">
<div class="content compactView">
<div class="section">
<div class="value">
888
</div>
<div class="title">
Views
</div>
</div>
<div class="section">
<div class="value">
48
</div>
<div class="title">
CrossRef citations to date
</div>
</div>
<div class="section score">
<div class="altmetric-score true">
<div class="value" data="10.1080/10447318.2011.555297"></div>
<div class="title">
Altmetric
</div>
</div>
</div>
<div class="altmetric-Key hidden" data="be0ef6915d1b2200a248b7195d01ef22"></div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-2-3 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all" id="fa57727f-b942-4eb8-9ed2-ecfe11ac03f5">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="read-speaker-container" style="display: block">
<div id="readspeaker_button1" class="rs_skip rsbtn rs_preserve">
<a href="//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F10447318.2011.555297" rel="nofollow" class="rsbtn_play" accesskey="L" title="Listen to this page using ReadSpeaker webReader" style="border-radius: 0 11.4px 11.4px 2px;">
<span class="rsbtn_left rsimg rspart"><span class="rsbtn_text"><span>Listen</span></span></span>
<span class="rsbtn_right rsimg rsplay rspart"></span>
</a>
</div>
</div>
<div class="toc-heading">
<h3>
Articles
</h3>
</div>
<h1><span class="NLM_article-title hlFld-title">Vision-Based Hand Interaction in Augmented Reality Environment</span></h1><span class="sub-title"><h2></h2></span><div class="literatumAuthors"><div class="publicationContentAuthors"><div class="hlFld-ContribAuthor"><span class="NLM_contrib-group"><span class="contribDegrees corresponding "><a class="entryAuthor" href="/author/Shen%2C+Y"> Y. Shen <span class="overlay"> National University of Singapore , Singapore <span class="corr-sec"><span class="heading">Correspondence</span><span class="corr-email"><i class="fa fa-envelope" style="color: #10147E; padding-right: 7px" aria-hidden="true"></i><span data-mailto="mailto:mpesy@nus.edu.sg">mpesy@nus.edu.sg</span></span><br /></span></span></a>, </span><span class="contribDegrees "><a class="entryAuthor" href="/author/Ong%2C+S+K"> S. K. Ong <span class="overlay"> National University of Singapore , Singapore </span></a> &amp; </span><span class="contribDegrees "><a class="entryAuthor" href="/author/Nee%2C+A+Y+C"> A. Y. C. Nee <span class="overlay"> National University of Singapore , Singapore </span></a></span></span></div></div></div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-all" id="5f562208-b1d5-4e5a-81c7-356431240f04">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md gutterless ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">

<div class="widget layout-inline-content none  widget-none  widget-compact-all" id="87ac5840-18fa-4a14-8eca-065b90ede3d7">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="inline-dropzone" data-pb-dropzone="content">
<div class="widget literatumContentItemPageRange none  widget-none  widget-compact-all" id="45057865-d60c-414c-bc81-646debb621b0">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><span class="contentItemPageRange">Pages 523-544
</span></div>
</div>
</div>
<div class="widget literatumContentItemHistory none  widget-none  widget-compact-all" id="32bf868e-52ce-411a-9dc3-717743aad997">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div>Published online: 09 May 2011</div></div>
</div>
</div>
<div class="widget literatumArticleToolsWidget none  widget-none  widget-compact-all" id="ed673666-7b5d-470e-bd33-c5c679d996cb">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="articleTools">
<ul class="linkList blockLinks separators centered">
<li class="downloadCitations">
<a href="/action/showCitFormats?doi=10.1080%2F10447318.2011.555297"><i class="fa fa-quote-left" aria-hidden="true"></i>Download citation</a>
</li>
<li class="dx-doi">
<a href="https://doi.org/10.1080/10447318.2011.555297"><i class="fa fa-external-link-square" style="margin: 0 0.25rem 0 0" aria-hidden="true"></i>https://doi.org/10.1080/10447318.2011.555297</a>
</li>
</ul>
</div></div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents2">
</div>
</div>
</div>
</div></div>
</div>
</div>

<div class="widget responsive-layout none publicationContentBody widget-none" id="f4a74f7a-9ba2-4605-86b1-8094cb1f01de">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container">
<div class="row row-md  ">
<div class="col-md-1-6 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget sectionsNavigation none  widget-none" id="f15bd2de-bb18-4067-8ab9-03ea3be30bf7">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="sections-nav"><span class="title">In this article<a href="#" class="close" tabindex="-1"><span aria-describedby="close-description"><span class="off-screen" id="close-description">Close</span></span></a></span><ul class="sections-list"><li><span class="sub-art-heading"><a href="#_i1">1. INTRODUCTION</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i2">2. RESEARCH BACKGROUND</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i4">3. METHODOLOGIES</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i24">4. IMPLEMENTATION</a></span><ul class="sub-art-titles"></ul></li><li><span class="sub-art-heading"><a href="#_i42">5. CONCLUSIONS</a></span><ul class="sub-art-titles"></ul></li><li><a href="#references-Section">References</a></li></ul></div></div>
</div>
</div>
</div>
</div>
<div class="col-md-7-12 ">
 <div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none rs_readArea widget-none  widget-compact-all" id="9751b4f9-64b9-44c0-955b-f75246902839">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container-fluid">
<div class="row row-md  ">
<div class="col-md-1-1 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all" id="d29f04e9-776c-4996-a0d8-931023161e00">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
        "HTML-CSS": {scale: 70, linebreaks: {automatic: true, width: "container"}},
        SVG: {linebreaks: {automatic: true, width: "25%"}},
        menuSettings: {zoom: "Click"},

        /* This is necessary to lazy loading. */
        skipStartupTypeset: true
    });
</script>
<div class="articleMeta ja">
<div class="tocHeading">
<h2>Articles</h2>
</div>
<div class="hlFld-Title">
<div class="publicationContentTitle">
<h1 class="chaptertitle">
Vision-Based Hand Interaction in Augmented Reality Environment
</h1>
</div>
</div>
<div class="copyrightStatement">
</div>
<div class="articleMetaDrop publicationContentDropZone" data-pb-dropzone="articleMetaDropZone">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone1" data-pb-dropzone="articleMetaDropZone1">
</div>
<div class="copyrightline">
</div>
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZone2" data-pb-dropzone="articleMetaDropZone2">
</div>
</div>
<div class="publication-tabs ja publication-tabs-dropdown">
<div class="tabs tabs-widget">
<ul class="tab-nav" role="tablist">
<li class="active" role="tab">
<a href="/doi/full/10.1080/10447318.2011.555297?scroll=top&amp;needAccess=true" class="show-full">
<i class="fa fa-file-text" aria-hidden="true"></i>
<span class="nav-data">
Full Article
</span>
</a>
</li>
<li role="tab">
<a href="/doi/figure/10.1080/10447318.2011.555297?scroll=top&amp;needAccess=true" class="show-figure">
<i class="fa fa-image" aria-hidden="true"></i>
<span class="nav-data">Figures & data</span>
</a>
</li>
<li role="tab">
<a href="/doi/ref/10.1080/10447318.2011.555297?scroll=top" class="show-references">
<i class="fa fa-book" aria-hidden="true"></i>
<span class="nav-data">References</span>
</a>
</li>
<li class="citedbyTab " role="tab">
<a href="/doi/citedby/10.1080/10447318.2011.555297?scroll=top&amp;needAccess=true">
<i class="fa fa-quote-left" aria-hidden="true"></i>
<span class="nav-data">
Citations
</span>
</a>
</li>
<li class="off-screen"></li>
<li role="tab" class="metrics-tab">
<a href="#metrics-content" class="show-metrics">
<i class="fa fa-bar-chart" aria-hidden="true"></i>
<span class="nav-data">Metrics</span>
</a>
</li>
<li role="tab" class="permissions-tab ">
<a href="/doi/abs/10.1080/10447318.2011.555297?tab=permissions&amp;scroll=top" class="show-permissions">
<i class="fa fa-print" aria-hidden="true"></i>
<span class="nav-data">
Reprints & Permissions</span></a>
</li>
<li class="pdf-tab " role="button">
<a href="/doi/pdf/10.1080/10447318.2011.555297?needAccess=true" class="show-pdf" target="_blank">
<span class="nav-data">
PDF
</span>
</a>
</li>
</ul>
<div class="tab-content ">
<a id="top-content-scroll"></a>
<div class="tab tab-pane active">
<article class="article">
<p class="fulltext"></p><div class="hlFld-Abstract"><p class="fulltext"></p><div class="sectionInfo abstractSectionHeading"><h2 id="abstract" class="section-heading-2">Abstract</h2></div><div class="abstractSection abstractInFull"> <p>A new vision-based framework and system for human–computer interaction in an Augmented Reality environment is presented in this article. The system allows the users to interact with computer-generated virtual objects using their hands directly. With an efficient color segmentation algorithm, the system is adaptable to different light conditions and backgrounds. It is also suitable for real-time applications. The dominant features on the palm are detected and tracked to estimate the camera pose. After the camera pose relative to the user's hand has been reconstructed, 3D virtual objects can be augmented naturally onto the palm for the user to inspect and manipulate. With off-the-shelf web camera and computer, natural bare-hand based interactions with 2D and 3D virtual objects can be achieved with low cost.</p> </div></div><div class="hlFld-Fulltext"><div id="S0001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1"> <h2 id="_i1" class="section-heading-2">1. INTRODUCTION</h2> <p>Augmented Reality (AR) provides a new paradigm for human–computer interaction (HCI) in which virtual and real objects can be fused in the user interface. With this interface, a user wearing a Head-Mounted Display could experience a different 3D space and interacting experience compared with the traditional computer monitor-based interaction. With the characteristics of real-time interactive experience, higher sense of presence and ease of use, 3D systems would be superior to 2D systems (S.-Y. <span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-refLink="_i43" href="#">Yoon, Laffey, &amp; Oh, 2008</a></span>). In an AR environment, the interaction techniques would largely determine the effectiveness and intuitiveness of the visualizing and manipulating activities.</p> <p>Many useful interaction options have been developed for AR systems. Handheld or hand-controlled devices have been implemented in many systems for object visualization and manipulation. Multidimensional input devices would be more intuitive interaction tools for the 3D interface by directly mapping the user's intentions to the application and providing more efficient interacting methods (<span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-refLink="_i43" href="#">Sundin &amp; Fjeld, 2009</a></span>). Computer mice with six degrees of freedom (DOFs), physical panels and pens tracked with sensors of six DOFs, joysticks and PHANToM™ are the usual devices to interact with 3D computer-generated objects. Compared to these devices in which specific hardware is required, the devices tracked using computer vision techniques are more flexible and cheaper. Vision-tracked fiducial markers have been widely used to register the virtual objects onto the real world and interact with the virtual objects (<span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-refLink="_i43" href="#">Chong, Ong, &amp; Nee, 2007</a></span>). For interaction via 2D interfaces, methods to track a uniform color have been proposed in some AR systems (<span class="ref-lnk lazy-ref"><a data-rid="CIT0031" data-refLink="_i43" href="#">Yuan, Ong, &amp; Nee, 2004</a></span>), with which a handheld stylus can be designed.</p> <p>In recent years, the research issue of interacting with virtual interfaces and objects using hands directly has been explored in some studies due to its natural and flexible properties (<span class="ref-lnk lazy-ref"><a data-rid="CIT0009" data-refLink="_i43" href="#">Ha &amp; Woo, 2006</a></span>). The interactions with hands can be roughly classified into two groups—device-assisted hand interaction and bare-hand interaction. For the systems in the first group, specific devices, such as pinch gloves with fiducial markers (<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-refLink="_i43" href="#">Piekarski &amp; Smith, 2006</a></span>) and the PHANToM™ device with a deformable foam ball (<span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-refLink="_i43" href="#">Pihuit, Kryt, &amp; Cani, 2008</a></span>), have been designed and used. In bare-hand interaction, the natural features on the hands, such as the skin color, shape, and so on, are detected and tracked by processing the images captured by the cameras. <span class="ref-lnk lazy-ref"><a data-rid="CIT0009" data-refLink="_i43" href="#">Ha and Woo (2006</a></span>) observed that the users prefer bare-hand interaction over interaction with the help of other devices.</p> <p>To be an effective interaction method, bare-hand interaction, which is based on computer vision, should be able to work successfully under uncontrolled light conditions and background. In addition, hand segmentation and tracking should be able to operate in real time for it to be applied in interactive applications. Last, interaction with computer-generated 3D objects should be supported and should not be based on complex representation of hand shapes as it makes the applications non–real time.</p> </div><div id="S0002" class="NLM_sec NLM_sec_level_1"> <h2 id="_i2" class="section-heading-2">2. RESEARCH BACKGROUND</h2> <p>The use of hand gesture provides an attractive alternative to cumbersome HCI devices. To use gestures in HCI, they have to be interpreted using certain algorithms and techniques. An earlier attempt to solve this problem used mechanical devices to measure the movements of the hand for controlling the virtual objects (<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-refLink="_i43" href="#">Piekarski &amp; Smith, 2006</a></span>). The use of glove-based devices for measuring the shape of a user's hand has been widely studied in the past, especially in the field of virtual reality. Although glove-based devices can measure the hand movements with high accuracy and speed, glove-based gesture interfaces require the user to wear cumbersome devices that are connected to the computer via cables. This is more costly, limits the user's motion, and hinders the natural interaction feel.</p> <p>Due to these reasons, interaction methods based on computer vision have been studied in recent years. Computer-vision-based methods have been considered to be more promising for natural HCI (<span class="ref-lnk lazy-ref"><a data-rid="CIT0007" data-refLink="_i43" href="#">Erol, Bebis, Nicolescu, Boyle, &amp; Twombly, 2007</a></span>). One approach is to attach markers to a user's hand or fingertips so those points can be easily identified. <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-refLink="_i43" href="#">Maggioni (1993</a></span>) proposed a vision-based system to estimate hand position and orientation using a specially designed glove. In his research, there are two slightly off-centered circular regions with different colors; by analyzing the extracted circles and extracting the geometrical properties, the hand gesture can be obtained. In the system developed by H. <span class="ref-lnk lazy-ref"><a data-rid="CIT0010" data-refLink="_i43" href="#">Kim and Fellner (2004)</a></span>, white thimble-shaped fingertip markers are used under a “black light” source to detect the locations of the fingertips. With black light emitted from the light source, the white markers can be highlighted and detected from the images, which are usually of low quality and low contrast due to the dimmed lighting conditions of the back-projection wall environment and the dynamic lighting conditions caused by the light emitted from the projection screen. This design is low cost and works under the condition of low contrast. <span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-refLink="_i43" href="#">Piekarski and Smith (2006</a></span>) have tried to attach fiducial markers on the thumbs of gloves to detect the 3D position of the thumbs so as to support 3D modeling in an outdoor AR environment.</p> <p>Another computer-vision-based approach involves extracting the image regions of the hand and estimating the locations of the fingertips or the orientation of the hand. To extract the hand region from a complex background, specific photographic imaging techniques using specialized cameras have been explored and implemented in some studies. <span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-refLink="_i43" href="#">Sato, Kobayashi, and Koike (2000</a></span>) developed a fast and robust method to track the hands with an infrared camera. Template matching is used to track the fingertips. In the gesture recognition system developed by <span class="ref-lnk lazy-ref"><a data-rid="CIT0003" data-refLink="_i43" href="#">Breuer et al. (2007</a></span>), an infrared time-of-flight range camera is used to measure the 3D information of the surface points on the hands. The 3D data acquired are matched with the 3D hand model through minimizing the distance between the model and the 3D data cloud using an iterative algorithm. With the specialized cameras, the detection process is more robust against illumination changes than an optical hand localization based on skin color. However, these specialized cameras are usually very expensive.</p> <p>The studies of vision-based hand detection using normal cameras, such as the web cameras, have also been explored. Hand interaction based on normal cameras would be more appealing to everyday users. To extract the image regions corresponding to the human skin, either background image subtraction (<span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i43" href="#">von Hardenberg &amp; Bérard, 2001</a></span>) or color segmentation (<span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-refLink="_i43" href="#">Seo, Choi, Han, Park, &amp; Park, 2008</a></span>) is typically used. The main challenge in these studies is to identify the image regions in the input images. In an AR application in which the user is moving around, the background would be complex and continuously changing. Methods based on background image subtraction would have difficulties in producing reliable segmentation of the hand region. For methods based on color segmentation, as the color of the human skin is not completely uniform and changes from person to person, the conventional methods based on a predefined skin color model will not always work well.</p> <p>After the hand regions of a hand have been extracted, the regions are analyzed to establish the position and orientation of the hand or used to estimate the locations of the fingertips. The methods to analyze the hand regions include shape extraction and fitting (<span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i43" href="#">von Hardenberg &amp; Bérard, 2001</a></span>), template matching (<span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-refLink="_i43" href="#">Sato et al., 2000</a></span>), and 3D model fitting (<span class="ref-lnk lazy-ref"><a data-rid="CIT0023" data-refLink="_i43" href="#">Stenger, Thayananthan, Torr, &amp; Cipolla, 2006</a></span>). Due to the high dimensionality and DOF of a user's hand, the 3D modeling and fitting method would provide a general but computationally expensive solution for hand detection, which makes it not well suited for interactive applications. By applying a simplified 3D hand model and defining hand constraints, the dimensions of the hand configuration can be reduced so that the system can work in real time (<span class="ref-lnk lazy-ref"><a data-rid="CIT0028" data-refLink="_i43" href="#">Wu, Lin, &amp; Huang, 2001</a></span>).</p> <p>Computer-vision-based hand interaction has been applied to facilitate either the manipulative or the communicative skills of humans (<span class="ref-lnk lazy-ref"><a data-rid="CIT0007" data-refLink="_i43" href="#">Erol et al., 2007</a></span>). Typical applications of hand interaction are object manipulation (e.g., finger-controlled mice; <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-refLink="_i43" href="#">von Hardenberg &amp; Bérard, 2001</a></span>), system/application control (e.g., bare-hand game control; J.-H. <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-refLink="_i43" href="#">Yoon Park, &amp; Sung, 2006</a></span>), and communication (e.g., recognition of simple sign language; <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-refLink="_i43" href="#">Nielsen, Störring, Moeslund, &amp; Granum, 2003</a></span>). Most of these applications are 2D-based interaction, such as controlling the mouse/virtual cursor using the fingertips, controlling the navigation direction through swinging the hand, and controlling the system/communication using predefined gestures.</p> <p>Interaction with 3D virtual objects is still limited to the selection and manipulation by pointing or gesturing. Holding and inspecting 3D virtual objects are mainly supported in a virtual environment with the help of data gloves. In recent years, using bare hands to hold, inspect, and interact with 3D virtual objects directly has been explored. Two systems have been reported recently (<span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-refLink="_i43" href="#">Lee &amp; Höllerer, 2007</a></span>; <span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-refLink="_i43" href="#">Seo et al., 2008</a></span>) where their methods reported are similar to the methods proposed and developed in this article. In the HandyAR system (<span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-refLink="_i43" href="#">Lee &amp; Höllerer, 2007</a></span>), the human hand is used as a distinctive marker to render virtual objects on the palm. The user can inspect the virtual objects from different perspectives naturally. Fingertips are detected and tracked to reconstruct the camera pose frame by frame. As the 3D coordinate system is developed based on the fingertips, when the fingertips are flexing, the coordinate system is changing accordingly, which would bring about changes in the translation, rotation, and scale between the 3D coordinate system and the camera coordinate system. As the fingertips would be flexing subconsciously during when the hand is moving, much effort is required to make the fingertips stiff to avoid changes in the 3D coordinate system. In the HandyAR system, a calibration process is required in which a user has to place his hand with the fingers pointing along the negative direction of the <i>y</i>-axis in the screen coordinate system. In the system proposed in this article, the convexity defect points between the fingers are used to reconstruct the camera pose, and these points are relatively static during the hand movements. In addition, the methods in the proposed system avoid the calibration stage by tracking and indexing the five fingers.</p> <p> <span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-refLink="_i43" href="#">Seo et al. (2008</a></span>) presented an AR system on mobile devices in which the virtual objects are augmented onto the palm. The virtual objects would react according to the opening and closing movements of the hand, which can be detected through tracking the fingertips. The four points used to estimate the camera pose are calculated based on the palm direction, the starting point of the forearm, and the convexity defect point between the thumb and the index finger. The convexity defect point is the defect point formed by the contour points of the hand contour, and it is always positioned between the fingers. The palm width (L<sub>1</sub>) is determined by a line starting from the convexity defect point, ending at the contour of the hand and orthogonal to the palm direction. The palm height (L<sub>2</sub>) is the shortest distance between the starting point of the forearm and the line of the palm width. The four points to set up the coordinate system are defined as the four corners of a virtual square with the size of the palm width and palm height, which are illustrated in <a href="#F0001">Figure 1</a>. The four points are changing during the hand movements as the palm direction is varying with the movements such that the coordinate system created on the palm would not be as stable as the system proposed in this article. In addition, the interactions based on the natural features of the hand, such as the fingertips and the center point of the hand region, are also supported in the proposed system. <div class="figure figureViewer" id="F0001"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 1</b> The virtual square defining the coordinate system (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0001image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0001g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0001"> <p> <b>FIGURE 1</b> The virtual square defining the coordinate system (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0001"></div> </p> <p>In the research presented in this article, efficient algorithms have been formulated and implemented to segment the hand region and track the features of the palm. Using the extracted features, the human hand can interact with the 2D virtual objects directly. The four convexity defect points on the palm are detected and tracked to estimate the camera pose. For the same user, the 3D coordinates of the four convexity defect points are fixed when a coordinate system is established on the palm. As compared with the fingertips, the four convexity defect points are relatively static during the hand movements. Therefore, the virtual objects rendered would be relatively stable. With the camera pose computed, 3D objects can be rendered naturally on the palm without any fiducial markers. In addition, the user can inspect and manipulate the virtual objects directly.</p> </div><div id="S0003" class="NLM_sec NLM_sec_level_1"> <h2 id="_i4" class="section-heading-2">3. METHODOLOGIES</h2> <div id="S2001" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i5">3.1. Framework Overview</h3> <p>The key idea of interacting with virtual objects using a bare hand is to detect and track the natural features on the palm of the bare hand through analyzing the images captured by the camera. The first step is to segment the hand region from the background with an efficient tracking and segmenting method. The second step is to detect and track the dominant natural features on the palm, such as the fingertips and the convexity defect points between the fingers. The third step is to estimate the position and orientation of the palm relative to the camera based on the tracked features to render the virtual objects.</p> <p>With the detected information, a bare hand can be implemented to interact with 2D and 3D virtual objects naturally. Interaction with 2D virtual objects is mainly based on the fingertips and hand direction, whereas interaction with 3D virtual objects is based on the computed camera pose and the pinhole camera model. The flowchart for hand tracking and object interaction is as shown in <a href="#F0002">Figure 2</a>, in which the process to render the 3D virtual objects on the palm is also presented. In <a class="ref showTableEventRef" data-ID="T0001">Table 1</a>, detailed descriptions of the methods in the flowchart are provided, where the new methods proposed in this research are highlighted and the adopted algorithms are listed. <div class="figure figureViewer" id="F0002"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 2</b> The flowchart of the system (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0002image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0002g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0002"> <p> <b>FIGURE 2</b> The flowchart of the system (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0002"></div> <div id="tableViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="tableView"><div class="tableCaption"><div class="short-legend"><h3> <b>Table 1: A Detailed Description of the Methods in the Proposed System</b> </h3></div></div><div class="tableDownloadOption" data-hasCSVLnk="true" id="T0001-table-wrapper"><a id="CSVdownloadButton" class="downloadButton btn btn-sm" href="/action/downloadTable?id=T0001&amp;doi=10.1080%2F10447318.2011.555297&amp;downloadType=CSV">CSV</a><a data-id="T0001" class="downloadButton btn btn-sm displaySizeTable" href="#">Display Table</a></div></div> </p> </div> <div id="S2002" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i8">3.2. Hand Segmentation</h3> <p>To segment the hand region from the background, a skin color segmentation approach is implemented. An efficient color segmentation approach should be able to segment the skin color of the hand from the uncontrolled background with a small amount of computation. The existing skin color modelling methods can be roughly classified into three categories, namely, explicitly defined skin models (<span class="ref-lnk lazy-ref"><a data-rid="CIT0012" data-refLink="_i43" href="#">Kovac, Peer, &amp; Solina, 2003</a></span>), parametric methods (<span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-refLink="_i43" href="#">Greenspan, Goldberger, &amp; Eshet, 2001</a></span>), and the nonparametric methods (<span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-refLink="_i43" href="#">Chai &amp; Bouzerdoum, 2000</a></span>). The skin color distribution can vary significantly among different people and under various lighting conditions. Therefore, it is difficult to achieve a high recognition rate with explicitly defined skin models and the parametric methods in an uncontrolled environment. The key idea of the nonparametric methods is to estimate the skin color distribution from the training data without deriving an explicit model of the skin color. Therefore, it is fast in usage and independent of the shape of the skin color distribution. The Continuously Adaptive Mean Shift (CAMSHIFT) algorithm (<span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-refLink="_i43" href="#">Bradski, 1998</a></span>; K. I. <span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-refLink="_i43" href="#">Kim, Jung, &amp; Kim, 2003</a></span>) is adopted in the proposed system to efficiently segment a human hand. The CAMSHIFT algorithm is computationally efficient and can deal with the challenges of irregular object motions due to variations in perspective and image noise (<span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-refLink="_i43" href="#">Bradski, 1998</a></span>). Therefore, this algorithm can overcome the problems of hand tracking, such as the changes in the hand shape during the movements and the varied skin colors across the hand.</p> <p>The HSV color space, which stands for <i>h</i>ue, <i>s</i>aturation and <i>v</i>alue, is used to detect the skin color. The advantage of using this color space is that all humans (except albinos) have basically the same hue. In the CAMSHIFT algorithm, the hue is ambiguously defined when the saturation or value is very high or low. Therefore, this model will ignore the pixels with saturation or value at either extremes by assigning them zero probability. The flowchart of the hand image segmentation algorithm is shown in <a href="#F0003">Figure 3</a>. <div class="figure figureViewer" id="F0003"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 3</b> Flowchart of hand image segmentation (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0003image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0003g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0003"> <p> <b>FIGURE 3</b> Flowchart of hand image segmentation (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0003"></div> </p> <p>In this system, the training results, which include information of the color distribution, the histogram, and the search window, are recorded in a text file. This text file would be reloaded automatically the next time the user wants to track the same object. With this configuration, the system is efficient and easy to be used, as the training process for the same object can be skipped.</p> <p>For each incoming frame in a live video, every pixel of the frame can be directly categorized to be either a skin-color pixel or a non-skin-color pixel. A binary image can be obtained in which the value of the skin-color pixel is 1 and the value of the non-skin-color pixel is 0. The binary image of the captured frame in <a href="#F0004">Figure 4a</a> is shown in <a href="#F0004">Figure 4b</a>. The contours in the extracted regions are detected using the <span class="ref-lnk lazy-ref"><a data-rid="CIT0016" data-refLink="_i43" href="#">OpenCV library (OpenCV, 2000)</a></span>. The connected component with the largest perimeter is selected as the hand contour. The segmentation result is shown in <a href="#F0004">Figure 4c</a>. <div class="figure figureViewer" id="F0004"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 4</b> (a) Captured image, (b) binary image, and (c) hand contour (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0004image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0004g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0004"> <p> <b>FIGURE 4</b> (a) Captured image, (b) binary image, and (c) hand contour (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0004"></div> </p> </div> <div id="S2003" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i11">3.3. Feature Detection</h3> <p>The natural features on the palm of a bare hand that would be useful for HCI are the fingertips, the center point and the direction of the hand region, and the convexity defect points between the fingers.</p> <p>In this system, fingertips are detected from the hand contour using a curvature-based algorithm (<span class="ref-lnk lazy-ref"><a data-rid="CIT0021" data-refLink="_i43" href="#">Segen &amp; Kumar, 1998</a></span>). The k-curvature of a contour point, which is the cosine of the angle between the two vectors, is determined based on <a href="#M0001">Equation 1</a>. In this system, k is a constant and set as 15. The vector through the contour point particular to the surface of the contour is computed according to <a href="#M0002">Equation 2</a>, <disp-formula-group id="M0001"> <span class="NLM_disp-formula-image disp-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0001.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0001.gif&quot;}" /><span class="no-mml-formula"><span class="disp_formula_label_div"><span id="" class="disp-formula-label">(1)</span></span></span></span> </disp-formula-group> <disp-formula-group id="M0002"> <span class="NLM_disp-formula-image disp-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0002.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0002.gif&quot;}" /><span class="no-mml-formula"><span class="disp_formula_label_div"><span id="" class="disp-formula-label">(2)</span></span></span></span> </disp-formula-group> where <i>K</i>(<i>P<sub>i</sub> </i>) is the k-curvature of the <i>i</i> th point on the hand contour. <i>D</i>(<i>P<sub>i</sub> </i>) is the vector that is particular to the vectors <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0001.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0001.gif&quot;}" /><span class="no-mml-formula"></span></span> and <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0002.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0002.gif&quot;}" /><span class="no-mml-formula"></span></span>. <i>P<sub>i</sub> </i> is the <i>i</i>th point on the hand contour, and <i>P<sub>i</sub> </i> <sub>–1</sub> and <i>P<sub>i</sub> </i> <sub>+1</sub> are the preceding and following points, respectively.</p> <p>An angle between the two vectors through the contour point that is close to 0 represents a potential peak or valley along the hand contour. A threshold degree θ<sub> <i>k</i> </sub> = 60 is implemented in this system, such that only points below this angle will be considered as the potential candidates. Therefore, the threshold for the k-curvature is 0.5, and only points with curvature values higher than this threshold are selected.</p> <p>The vectors <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0003.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0003.gif&quot;}" /><span class="no-mml-formula"></span></span> and <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0004.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0004.gif&quot;}" /><span class="no-mml-formula"></span></span> can be represented as 3D vectors lying in the xy-plane with zero z components. Therefore, <i>D</i>(<i>P<sub>i</sub> </i>) would be a vector along the <i>z</i>-axis. The direction of <i>D</i>(<i>P<sub>i</sub> </i>) will be reversed when the vectors pass through the fingertips and the convexity defect point between the fingertips. Thus, points with curvature values higher than a threshold and <i>z</i> components of <i>D</i>(<i>P<sub>i</sub> </i>) larger than zero are selected as fingertip candidates.</p> <p>To fit an ellipse to the hand contour using the least-square fitting method, the center point of the hand region is defined as the center of the ellipse and the direction of the hand is defined as the direction of the semimajor axis, as shown in <a href="#F0005">Figure 5</a>. The fingertip candidates are clustered based on the distances between the pixels. If the distance between the candidates is larger than a threshold, the candidates are in different clusters. For each cluster, the distances between the candidates and the center point of the hand region are computed and compared. Points with the largest distances will be specified as the fingertips for the initial detection. The fingertips detected after the initial detection are indicated by the points on the fingertips in <a href="#F0005">Figure 5</a>. Due to the noises in the hand region segmentation and contour detection, false candidates can be detected after the initial detection. To remove theses false candidates, the most stable fingertips during the detection will be chosen as the final detected fingertips. As shown in <a href="#F0006">Figure 6</a>, there are six fingertips detected in the initial detection. After the filtering process, five fingertips are detected and indexed from 0 to 5. <div class="figure figureViewer" id="F0005"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 5</b> The detected hand features (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0005image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0005g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0005"> <p> <b>FIGURE 5</b> The detected hand features (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0005"></div> <div class="figure figureViewer" id="F0006"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 6</b> The fingertips and false candidates (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0006image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0006g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0006"> <p> <b>FIGURE 6</b> The fingertips and false candidates (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0006"></div> </p> <p>The thumb is defined as the fingertip that is farthest away from the center position of the fingertips, which is obtained by averaging the positions of the fingertips as shown in <a href="#M0003">Equation 3</a>. After the thumb position has been obtained, the other fingertips are indexed based on the distances between the fingertips and the thumb. The index and little fingers are defined as the fingertips closest and farthest from the thumb. In <a href="#F0007">Figure 7</a>, the fingertips from the thumb to the little finger are indexed from 0 to 5. <div class="figure figureViewer" id="F0007"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 7</b> The fingertips and the convexity defect points (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0007image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0007g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0007"> <p> <b>FIGURE 7</b> The fingertips and the convexity defect points (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0007"></div> <disp-formula-group id="M0003"> <span class="NLM_disp-formula-image disp-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0003.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0003.gif&quot;}" /><span class="no-mml-formula"><span class="disp_formula_label_div"><span id="" class="disp-formula-label">(3)</span></span></span></span> </disp-formula-group> </p> <p>The convexity defect points between the fingers are extracted from the contour of the convex hull bounding the hand contour. In the initial detection, the convexity defect points are detected in a clockwise sequence. To ensure that the system can be used for both left and right hands, the sequence will be automatically adjusted according to the position of the thumb. Due to the slightly different complexion on the fingertips, there are always noises in image segmentation in these positions, which would cause false detection during the initial detection of convexity defect points. To remove these false positives, a characteristic of the hand (viz., the distances between the fingertips and the convexity defect points should be about the length of the semi-major axis of the ellipse that would fit the hand region) is used as the constraints for the detection. Based on the sequences of the fingertips, the convexity defect points are indexed from 0 to 3, as shown in <a href="#F0007">Figure 7</a>.</p> </div> <div id="S2004" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i18">3.4. Feature Tracking</h3> <p>The detected features on the hand will not remain in the same position over time. Thus, the system has to track these features while they are moving based on certain tracking algorithms. The first tracking approach usually predicts the position of the object based on last detected position and some predefined constraints of the possible moving patterns of this object between the frames. When the hand is moving quickly, the tracking process would search through the entire image capture for each frame. In addition, the tracking process needs to be restarted when the tracking is lost due to blurred movements or occlusions. The second tracking approach tracks moving objects by reexecuting the detection process for each frame. This approach can be used for unconstrained or unpredictable hand motion, and it is useful for tracking fast hand movements. However, systems based on this approach require a very fast detection algorithm to ensure that the systems can operate in real time.</p> <p>The hand segmentation and detection processes are sufficiently fast for the second tracking method to be applied, which would restart the detection process for each frame.</p> </div> <div id="S2005" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i19">3.5. Camera Pose Detection</h3> <p>Using the four convexity defect points that have been detected and tracked and the intrinsic camera parameters, the camera pose, position, and orientation can be estimated. After the four convexity defect points have been detected and tracked successfully, the pose estimation method would have sufficient point correspondences. The 3D coordinates of the convexity defect points can be obtained after a 3D coordinate is constructed on the palm and the origin has been defined. The 3D coordinates of the four points can be obtained in a one-time initialization process by manually placing the palm on a piece of paper with the drawn 3D coordinates system, as shown in <a href="#F0008">Figure 8</a>. The <i>z</i> coordinates are assumed to be zero for all the convexity defect points, which means that the 3D coordinate system is set up on the surface of the palm. Therefore, the 3D coordinates for the four convexity defect points are (0, 40, 0), (33, 22, 0), (40, 0, 0), and (31, −20, 0), as shown in <a href="#F0009">Figure 9</a>. <div class="figure figureViewer" id="F0008"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 8</b> Obtain the 3D coordinates of the convexity defect points.</p> </div></td></div><a href="#" class="thumbnail"><img id="F0008image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0008g.gif"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0008"> <p> <b>FIGURE 8</b> Obtain the 3D coordinates of the convexity defect points.</p> </div><div class="hidden rs_skip" id="figureFootNote-F0008"></div> <div class="figure figureViewer" id="F0009"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 9</b> The 3D coordinate system on the palm (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0009image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0009g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0009"> <p> <b>FIGURE 9</b> The 3D coordinate system on the palm (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0009"></div> </p> <p>The robust and accurate algorithm developed by <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-refLink="_i43" href="#">Schweighofer and Pinz (2006</a></span>) has solved the pose ambiguity problem in pose estimation. This algorithm is implemented in this system to estimate the camera pose. The camera pose can be computed after inputting the four 3D-2D correspondence points into the algorithm. Using the camera model perspective projection, which is shown in <a href="#M0004">Equation 4</a>, the virtual objects can be augmented onto the palm. The result to render an axis on the palm is shown in <a href="#F0009">Figure 9</a>. It can be observed that the rendering is correct according to the 3D coordinates of the four convexity defect points. In <a href="#F0010">Figure 10</a>, a teapot with dimension of 20 mm is augmented onto the palm. <div class="figure figureViewer" id="F0010"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 10</b> Augmenting a teapot on the palm (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0010image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0010g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0010"> <p> <b>FIGURE 10</b> Augmenting a teapot on the palm (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0010"></div> <disp-formula-group id="M0004"> <span class="NLM_disp-formula-image disp-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0004.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0004.gif&quot;}" /><span class="no-mml-formula"><span class="disp_formula_label_div"><span id="" class="disp-formula-label">(4)</span></span></span></span> </disp-formula-group> where <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0005.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0005.gif&quot;}" /><span class="no-mml-formula"></span></span>, <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0006.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0006.gif&quot;}" /><span class="no-mml-formula"></span></span> is an arbitrary factor.</p> </div> </div><div id="S0004" class="NLM_sec NLM_sec_level_1"> <h2 id="_i24" class="section-heading-2">4. IMPLEMENTATION</h2> <p>The proposed methods and system have been implemented based on using VC++6.0 on a P4 3 GHz PC equipped with 1 GB RAM. A web camera is used to capture the hand movements. The intrinsic camera parameters were measured using the calibration method developed by <span class="ref-lnk lazy-ref"><a data-rid="CIT0032" data-refLink="_i43" href="#">Zhang (2000</a></span>) and the OpenCV library with a 6 × 8 checkerboard pattern. The system can successfully find the fingertips, convexity defect points, and the center point of the hand region. In this system, the users can interact with 2D/3D virtual objects using the bare hand naturally.</p> <div id="S2006" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i25">4.1. Interaction With 2D Virtual Objects</h3> <div id="S3001" class="NLM_sec NLM_sec_level_3"> <h4 class="section-heading-4" id="_i26"></h4> <div id="S4001" class="NLM_sec NLM_sec_level_4"> <h5 class="section-heading-5" id="_i27">Using the fingertips to interact with 2D virtual keyboard</h5> <p> <span class="ref-lnk lazy-ref"><a data-rid="CIT0004" data-refLink="_i43" href="#">Card, MacKinlay, and Robertson (1991</a></span>) predicted that the interaction method making greater use of the fingers would be a promising alternative to the conventional mouse, and <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-refLink="_i43" href="#">Ullman (2004)</a></span> developed a pen-shaped device to replace the mouse. The algorithms developed in this research allow the user to control a virtual cursor with a bare hand. This has been implemented in a Virtual Keyboard application, where the user can use the Virtual Keyboard to control appliances, such as a TV and a fan, and operate computer applications using a bare hand. The user can move an outstretched finger in front of the camera to control the movement of the virtual cursor to interact with virtual buttons on the Virtual Keyboard that has been developed in an AR environment. The clicking of a virtual button is triggered by keeping the fingertip in the same button for several seconds. This application can provide fast and intuitive interaction. From the results, it can be observed that the tracking of the fingertip will not be lost even when the user moves the finger quickly. The results are shown in <a href="#F0011">Figure 11a</a> and <a href="#F0011">Figure 11b</a>. <div class="figure figureViewer" id="F0011"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 11</b> (a) The tracked fingertip, and (b) interaction with the virtual panel (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0011image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0011g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0011"> <p> <b>FIGURE 11</b> (a) The tracked fingertip, and (b) interaction with the virtual panel (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0011"></div> </p> <p>The steps to detect the position of the outstretched fingertip and use it to trigger the virtual buttons are as follows: <table class="listgroup" border="0" width="95%" list-type="bullet"><tr class="li1"><td valign="top" class="list-td">•</td><td colspan="5" valign="top"><p>Determine the center point of the hand region</p></td></tr><tr class="li1"><td valign="top" class="list-td">•</td><td colspan="5" valign="top"><p>Determine the distances between all the fingertip candidates and the center point</p></td></tr><tr class="li1"><td valign="top" class="list-td">•</td><td colspan="5" valign="top"><p>Define the fingertip candidate with the maximal distance as the outstretched fingertip</p></td></tr><tr class="li1"><td valign="top" class="list-td">•</td><td colspan="5" valign="top"><p>Reset the position of the cursor to be the position of the detected fingertip</p></td></tr></table> </p> <p>Using the same mechanism and the Win32 API, the cursor on a computer display can also be controlled using a bare hand instead of a physical mouse.</p> </div> <div id="S4002" class="NLM_sec NLM_sec_level_4"> <h5 class="section-heading-5" id="_i29">Using the fingertips to interact with assembly parts</h5> <p>In this application, the user can control and assemble virtual assembly parts with a bare hand. A marker-based tracking method is used to detect the camera pose and render the virtual objects onto the real scene. The ARToolkitPlus library (<span class="ref-lnk lazy-ref"><a data-rid="CIT0001" data-refLink="_i43" href="#">ARToolkitPlus, n.d</a></span>.) is used for marker-based tracking.</p> <p>Based on the positions of two fingertips, the middle point of the two fingertips can be computed. When the middle point is sufficiently near an assembled part, the part will be selected and connected to the middle point using a green line. Moving the two fingertips, the selected part will be moved. According to the spatial relationship between the selected part and the base part following the assembly constraints predefined in the application, the selected part will be assembled onto the base part and the color of the selected part would be changed from red to green.</p> <p>In <a href="#F0012">Figure 12</a>, two fingertips (i.e., the thumb and the index finger) are used to control and assemble two virtual motorcycle shield panels in a 2D space. The base panel was located stably on the marker. The middle panel was loaded into the AR environment and assembled onto the base panel using bare-hand interaction. The assembly processes are shown in <a href="#F0012">Figure 12a</a> to <a href="#F0012">12d</a>. More details of this application are described in the reference (<span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i43" href="#">Wang, Shen, Ong, &amp; Nee, 2009</a></span>). <div class="figure figureViewer" id="F0012"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 12</b> Assembly simulation using bare-hand interaction in 2D space (<span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i43" href="#">Wang et al., 2009</a></span>) (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0012image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0012g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0012"> <p> <b>FIGURE 12</b> Assembly simulation using bare-hand interaction in 2D space (<span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-refLink="_i43" href="#">Wang et al., 2009</a></span>) (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0012"></div> </p> </div> </div> </div> <div id="S2007" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i31">4.2. Interaction With 3D Virtual Object</h3> <p>Using the methodologies proposed in this article, 3D virtual objects can be augmented onto a real palm seamlessly. The users can inspect the virtual objects naturally by moving, tilting, and rotating the palm, as shown in <a href="#F0013">Figure 13</a>. <div class="figure figureViewer" id="F0013"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 13</b> Inspecting the teapot (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0013image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0013g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0013"> <p> <b>FIGURE 13</b> Inspecting the teapot (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0013"></div> </p> <p>An application based on marker-based tracking, bare-hand tracking, and interaction with 3D virtual objects has been designed to recover the reaching and extension functions of the upper limb. The ARToolkitPlus library (ARToolkitPlus) is used for marker-based tracking to establish a world coordinate system in which the virtual objects will be rendered. In this exercising application, there are falling balls from the air at random positions. The balls are falling down with an initial velocity of zero due to the effect of gravity. Before the balls touch the floor, the user has to extend his hand and move to the correct position to catch the balls.</p> <p>To detect whether the user has caught a ball, the position of the ball and the position of the hand have to be transformed into the same coordinate system. There are two coordinate systems in the environment, that is, the world coordinate system based on the “Hiro” marker and the coordinate system established on the palm, as shown in <a href="#F0014">Figure 14</a>. Analyzing the images captured by the camera, the transformation matrices between the camera coordinate system and these two coordinate systems can be obtained. Based on these transformation matrices, the spatial relationship between the palm and the ball can be estimated, as shown in <a href="#F0014">Figure 14</a>. Thus, the 3D position of the palm in the world coordinate system based on the “Hiro” marker can be estimated, and the 3D distances between the palm and the ball in the world coordinate system can be computed. When the 3D distance between the palm and the ball is sufficiently close, the system will define the status of the ball as “caught,” reset the position of the ball to be the position of the hand in the world coordinate system, and move the ball with the hand for several seconds. Ten seconds is defined in this application to provide the feeling of holding the virtual ball in his hand. <div class="figure figureViewer" id="F0014"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 14</b> Coordinate systems transformations (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0014image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0014g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0014"> <p> <b>FIGURE 14</b> Coordinate systems transformations (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0014"></div> </p> <p>In <a href="#F0015">Figure 15a</a>, when both the marker and the hand are detected, the game will start with a ball falling from the air at a random position. While the ball is falling down, the user would move the hand to the estimated position, as shown in <a href="#F0015">Figure 15b</a>. In <a href="#F0015">Figure 15c</a>, the ball is caught in the palm, and the color of the ball has been changed. <div class="figure figureViewer" id="F0015"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 15</b> (a) Start the game, (b) move the hand to catch the ball, and (c) the ball is caught and laid on the palm (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0015image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0015g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0015"> <p> <b>FIGURE 15</b> (a) Start the game, (b) move the hand to catch the ball, and (c) the ball is caught and laid on the palm (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0015"></div> </p> <p>The perceptual accuracy, depth perception, and the reaction speed pertaining to hand–eye coordination of the users can be trained using this application. In this application, the user can interact with the virtual objects rendered in the environment naturally using a bare hand.</p> </div> <div id="S2008" class="NLM_sec NLM_sec_level_2"> <h3 class="section-heading-3" id="_i35">4.3. Performance and Accuracy</h3> <p>Although the codes have not been optimized, the current implementation of this system is near real time, at approximately 21 frames per second for a frame resolution of 640 × 480. The root mean square (RMS) errors are used to measure the accuracy of the interaction methods. For a set of n values <span class="NLM_disp-formula-image inline-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0007.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_ilm0007.gif&quot;}" /><span class="no-mml-formula"></span></span>, the RMS value is calculated using <a href="#M0005">Equation 5</a>, <disp-formula-group id="M0005"> <span class="NLM_disp-formula-image disp-formula"><noscript><img src="/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0005.gif" alt="" /></noscript><img src="//:0" alt="" class="no-mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/hihc_a_555297_o_m0005.gif&quot;}" /><span class="no-mml-formula"><span class="disp_formula_label_div"><span id="" class="disp-formula-label">(5)</span></span></span></span> </disp-formula-group> </p> <div id="S3002" class="NLM_sec NLM_sec_level_3"> <h4 class="section-heading-4" id="_i37"></h4> <div id="S4003" class="NLM_sec NLM_sec_level_4"> <h5 class="section-heading-5" id="_i38">Accuracy of the fingertip detection</h5> <p>The accuracy of the fingertip detection method is measured by comparing the differences between the position of a fingertip and the predefined screen point, the position of which is kept unchanged during the measurement process. As the fingertips are detected in 2D images, a 2D point with a predefined screen coordinate (500, 500) is rendered as a reference point. During the data collection process, the user is required to point at this reference 2D point using the tip of the index finger, as shown in <a href="#F0016">Figure 16</a>. The screen coordinates of the fingertip detected using the methods presented in this article are recorded. A total of 1,000 data points were collected. The scale factor between the camera image and the screen image is determined. Next, the coordinates of the fingertip and the coordinates of reference point are multiplied with this scale factor so as to map these points into the screen image in terms of pixels. The RMS errors are calculated based on the deviations between the coordinates of the fingertip from the coordinates of the reference point. The RMS errors of the fingertip detection method are 2.47 pixels and 2.63 pixels in <i>x</i>- and <i>y</i>-axes, respectively. <div class="figure figureViewer" id="F0016"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 16</b> Measuring the accuracy of the fingertip detection method (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0016image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0016g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0016"> <p> <b>FIGURE 16</b> Measuring the accuracy of the fingertip detection method (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0016"></div> </p> </div> <div id="S4004" class="NLM_sec NLM_sec_level_4"> <h5 class="section-heading-5" id="_i40">Accuracy of the registration using the palm as a natural marker</h5> <p>The central idea to measure the accuracy of the registration is to compare the 3D coordinates estimated using the camera pose with those taken from a predefined origin. The 3D coordinates, measured physically from the predefined origin, are used as the reference to calculate the RMS errors.</p> <p>A marker is placed on a piece of graph paper (<a href="#F0008">Figure 8</a>) with a 2D coordinates system drawn on it. The predefined origin is the origin of the ideal coordinate system established on the palm and would have the <i>x</i>- and <i>y</i>-axes along the same directions as those shown in <a href="#F0008">Figure 8</a>. The <i>z</i>-axis would be perpendicular to this graph paper. As the predefined origin is on the palm, all the Z coordinates of the points on the paper are −20 mm. Because the width of the marker is 78 mm, the 3D coordinates of the four corners of the marker on the graph paper are therefore (193, −99, −20), (115, −99, −20), (115, −21, −20), and (193, −21, −20).</p> <p>Based on the camera pose estimated using natural features, the image coordinate detected using corner detection and the known Z coordinate of the corners (–20), the 3D coordinates of the corners can be calculated using <a href="#M0004">Equation 4</a>. In <a href="#F0017">Figure 17</a>, four solid spheres are rendered on the four corners of the marker based on the estimated 3D coordinates. From <a href="#F0017">Figure 17</a>, it can be observed that the estimated coordinates of the four corners are well aligned with the predefined coordinates. <div class="figure figureViewer" id="F0017"><div id="figureViewerArticleInfo" class="hidden"><h1>Vision-Based Hand Interaction in Augmented Reality Environment</h1><div class="articleAuthors articleInfoSection"><div class="authorsHeading">All authors</div><div class="authors"><a class="entryAuthor" href="/action/doSearch?Contrib=Shen%2C+Y"><span class="hlFld-ContribAuthor"><a href="/author/Shen%2C+Y"><span class="NLM_given-names">Y.</span> Shen</a></span>, </a><a class="entryAuthor" href="/action/doSearch?Contrib=Ong%2C+S+K"><span class="hlFld-ContribAuthor"><a href="/author/Ong%2C+S+K"><span class="NLM_given-names">S. K.</span> Ong</a></span> &amp; </a><a class="entryAuthor" href="/action/doSearch?Contrib=Nee%2C+A+Y+C"><span class="hlFld-ContribAuthor"><a href="/author/Nee%2C+A+Y+C"><span class="NLM_given-names">A. Y. C.</span> Nee</a></span></a></div></div><div class="articleLowerInfo articleInfoSection"><div class="articleLowerInfoSection articleInfoDOI"><a href="https://doi.org/10.1080/10447318.2011.555297">https://doi.org/10.1080/10447318.2011.555297</a></div><div class="articleInfoPublicationDate articleLowerInfoSection border"><h6>Published online:</h6>09 May 2011</div></div></div><div class="figureThumbnailContainer"><div class="figureInfo"><td align="left" valign="top" width="100%"><div class="short-legend"> <p> <b>FIGURE 17</b> Measuring the accuracy of the registration method (color figure available online).</p> </div></td></div><a href="#" class="thumbnail"><img id="F0017image" src="//:0" data-src='{"type":"image","src":"/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production/images/medium/hihc_a_555297_o_f0017g.jpg"}' /></a><div class="figureDownloadOptions"><a href="#" class="downloadBtn btn btn-sm" id="displaySizeFig" role="button">Display full size</a></div></div></div><div class="hidden rs_skip" id="fig-description-F0017"> <p> <b>FIGURE 17</b> Measuring the accuracy of the registration method (color figure available online).</p> </div><div class="hidden rs_skip" id="figureFootNote-F0017"></div> </p> <p>Comparing the estimated coordinates with the predefined coordinates, the RMS errors in the <i>x</i>- and <i>y</i>-axes can be obtained. A total of 400 data points were collected. The RMS errors of the <i>X</i> and <i>Y</i> coordinates are 5.12 mm and 13.81 mm, respectively.</p> </div> </div> </div> </div><div id="S0005" class="NLM_sec NLM_sec_level_1"> <h2 id="_i42" class="section-heading-2">5. CONCLUSIONS</h2> <p>In this article, an HCI method in AR environment based on hand detection and tracking is introduced, where the users can interact naturally with 2D/3D virtual objects in a real-life scene using bare hands. The hand region is segmented and tracked based on skin color distribution. Fingertips and convexity defect points between the fingers are detected. With the features detected from the hand region, the user can interact with 2D objects using a bare hand. The camera pose can be estimated based on the tracking feature positions in order to augment virtual objects on the palm of the user's bare hand. From the results, it can be observed that the virtual objects can be rendered on the palm stably. The methods are robust to ensure that the virtual objects can be augmented naturally without any markers. In the future, different hand gestures would be investigated to provide more controlling methods. The method to solve the self-occlusion problem in hand feature detection will be explored by including more features on the palm during the tracking process.</p> </div></div><script type="text/javascript">
                        window.figureViewer={doi:'10.1080/10447318.2011.555297',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production',figures:[{i:'F0001',g:[{m:'hihc_a_555297_o_f0001g.jpg',l:'hihc_a_555297_o_f0001g.jpeg',size:'55 KB'}]}
                            ,{i:'F0002',g:[{m:'hihc_a_555297_o_f0002g.jpg',l:'hihc_a_555297_o_f0002g.jpeg',size:'69 KB'}]}
                            ,{i:'F0003',g:[{m:'hihc_a_555297_o_f0003g.jpg',l:'hihc_a_555297_o_f0003g.jpeg',size:'80 KB'}]}
                            ,{i:'F0004',g:[{m:'hihc_a_555297_o_f0004g.jpg',l:'hihc_a_555297_o_f0004g.jpeg',size:'80 KB'}]}
                            ,{i:'F0005',g:[{m:'hihc_a_555297_o_f0005g.jpg',l:'hihc_a_555297_o_f0005g.jpeg',size:'40 KB'}]}
                            ,{i:'F0006',g:[{m:'hihc_a_555297_o_f0006g.jpg',l:'hihc_a_555297_o_f0006g.jpeg',size:'35 KB'}]}
                            ,{i:'F0007',g:[{m:'hihc_a_555297_o_f0007g.jpg',l:'hihc_a_555297_o_f0007g.jpeg',size:'34 KB'}]}
                            ,{i:'F0008',g:[{m:'hihc_a_555297_o_f0008g.gif',l:'hihc_a_555297_o_f0008g.jpeg',size:'156 KB'}]}
                            ,{i:'F0009',g:[{m:'hihc_a_555297_o_f0009g.jpg',l:'hihc_a_555297_o_f0009g.jpeg',size:'39 KB'}]}
                            ,{i:'F0010',g:[{m:'hihc_a_555297_o_f0010g.jpg',l:'hihc_a_555297_o_f0010g.jpeg',size:'42 KB'}]}
                            ,{i:'F0011',g:[{m:'hihc_a_555297_o_f0011g.jpg',l:'hihc_a_555297_o_f0011g.jpeg',size:'53 KB'}]}
                            ,{i:'F0012',g:[{m:'hihc_a_555297_o_f0012g.jpg',l:'hihc_a_555297_o_f0012g.jpeg',size:'110 KB'}]}
                            ,{i:'F0013',g:[{m:'hihc_a_555297_o_f0013g.jpg',l:'hihc_a_555297_o_f0013g.jpeg',size:'55 KB'}]}
                            ,{i:'F0014',g:[{m:'hihc_a_555297_o_f0014g.jpg',l:'hihc_a_555297_o_f0014g.jpeg',size:'65 KB'}]}
                            ,{i:'F0015',g:[{m:'hihc_a_555297_o_f0015g.jpg',l:'hihc_a_555297_o_f0015g.jpeg',size:'100 KB'}]}
                            ,{i:'F0016',g:[{m:'hihc_a_555297_o_f0016g.jpg',l:'hihc_a_555297_o_f0016g.jpeg',size:'30 KB'}]}
                            ,{i:'F0017',g:[{m:'hihc_a_555297_o_f0017g.jpg',l:'hihc_a_555297_o_f0017g.jpeg',size:'32 KB'}]}
                            ]}</script><script type="text/javascript">window.tableViewer={doi:'10.1080/10447318.2011.555297',path:'/na101/home/literatum/publisher/tandf/journals/content/hihc20/2011/hihc20.v027.i06/10447318.2011.555297/production',tables:[{i:'T0001'}]}</script><script type="text/javascript">window.tableIDIndexMap = {"id":-1};window.tableIDIndexMap['T0001'] = 1; </script><div id="table-content-T0001" class="hidden"><table class="table frame_topbot"><div class="caption"> <b>Table 1: A Detailed Description of the Methods in the Proposed System</b> </div><colgroup><col /><col /></colgroup><thead valign="bottom"><tr valign="top" class="rowsep1"><th align="left" valign="bottom" class="rowsep1 align_left">Methods in theProposed System</th><th align="center" valign="bottom" class="rowsep1 align_center last">Description</th></tr></thead><tbody><tr valign="top"><td align="left" class=" align_left">Hand segmentation method</td><td align="left" class=" align_left last">The hand segmentation algorithm is adopted from previous studies. In this research, this algorithm has been enhanced where automatic saving and loading of the training results can be achieved.</td></tr><tr valign="top"><td align="left" class=" align_left">Detection of fingertips and convexity defect points</td><td align="left" class=" align_left last">In the new fingertip detection method developed in this research, the tracking criteria of the features in the detection process are based on data and information from hand anthropometry studies.</td></tr><tr valign="top" class="last"><td align="left" class=" align_left">3D tracking and interactions</td><td align="left" class=" align_left last">In this research, a novel method was developed to construct the coordinate system based on the four convexity defect points of a hand. The application of the vision-based hand interaction in the rehabilitation is innovative and has not been reported previously. The pose estimation is solved using the method proposed by <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-refLink="_i43" href="#">Schweighofer and Pinz (2006</a></span>).</td></tr></tbody></table></div><ul class="references numeric-ordered-list" id="references-Section"><h2 id="figures">REFERENCES</h2><li id="CIT0001"><span> ARTookitPlus. (n.d.). <a class="ext-link" href="http://studierstube.icg.tu-graz.ac.at/handheld_ar/ artoolkitplus.php" target="_blank">http://studierstube.icg.tu-graz.ac.at/handheld_ar/ artoolkitplus.php (http://studierstube.icg.tu-graz.ac.at/handheld_ar/ artoolkitplus.php)</a> <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+ARTookitPlus.+%28n.d.%29.+http%3A%2F%2Fstudierstube.icg.tu-graz.ac.at%2Fhandheld_ar%2F+artoolkitplus.php+%28http%3A%2F%2Fstudierstube.icg.tu-graz.ac.at%2Fhandheld_ar%2F+artoolkitplus.php%29+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0001&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Fstudierstube.icg.tu-graz.ac.at%2Fhandheld_ar%2F+artoolkitplus.php+%28http%3A%2F%2Fstudierstube.icg.tu-graz.ac.at%2Fhandheld_ar%2F+artoolkitplus.php%29" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0002"><span><span class="hlFld-ContribAuthor">Bradski, <span class="NLM_given-names">G. R.</span></span> <span class="NLM_article-title">Real time face and object tracking as a component of a perceptual user interface</span>. <span class="NLM_conf-name">Proceedings of the 4th IEEE Workshop on Applications of Computer Vision</span>. pp.<span class="NLM_fpage">214</span>–<span class="NLM_lpage">219</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0ABradski+%2C+G.+R.+Real+time+face+and+object+tracking+as+a+component+of+a+perceptual+user+interface.+Proceedings+of+the+4th+IEEE+Workshop+on+Applications+of+Computer+Vision.+pp.214%E2%80%93219.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0002&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBradski%26aufirst%3DG.%2520R.%26atitle%3DReal%2520time%2520face%2520and%2520object%2520tracking%2520as%2520a%2520component%2520of%2520a%2520perceptual%2520user%2520interface%26spage%3D214%26epage%3D219" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0003"><span><span class="hlFld-ContribAuthor">Breuer, <span class="NLM_given-names">P.</span></span>, <span class="hlFld-ContribAuthor">Eckes, <span class="NLM_given-names">C.</span></span> and <span class="hlFld-ContribAuthor">Müller, <span class="NLM_given-names">S.</span></span> <span class="NLM_year">2007</span>. “<span class="NLM_article-title">Hand gesture recognition with a novel IR time-of-flight range camera: a pilot study</span>”. In <i>Proceedings of the 3rd international conference on Computer vision/computer graphics collaboration techniques</i> <span class="NLM_fpage">274</span>–<span class="NLM_lpage">260</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=274-260&author=P.+Breuer&author=C.+Eckes&author=S.+M%C3%BCller&title=+Proceedings+of+the+3rd+international+conference+on+Computer+vision%2Fcomputer+graphics+collaboration+techniques+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0003&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DBreuer%26aufirst%3DP.%26date%3D2007%26atitle%3DHand%2520gesture%2520recognition%2520with%2520a%2520novel%2520IR%2520time-of-flight%2520range%2520camera%253A%2520a%2520pilot%2520study%26btitle%3DProceedings%2520of%2520the%25203rd%2520international%2520conference%2520on%2520Computer%2520vision%252Fcomputer%2520graphics%2520collaboration%2520techniques%26spage%3D274%26epage%3D260" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0004"><span><span class="hlFld-ContribAuthor">Card, <span class="NLM_given-names">S. K.</span></span>, <span class="hlFld-ContribAuthor">MacKinlay, <span class="NLM_given-names">J. D.</span></span> and <span class="hlFld-ContribAuthor">Robertson, <span class="NLM_given-names">G. G.</span></span> <span class="NLM_year">1991</span>. <span class="NLM_article-title">A morphological analysis of the design space of input devices</span>. <i>ACM Transactions on Information Systems</i>, 9: <span class="NLM_fpage">99</span>–<span class="NLM_lpage">122</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=1991&pages=99-122&author=S.+K.+Card&author=J.+D.+MacKinlay&author=G.+G.+Robertson&title=A+morphological+analysis+of+the+design+space+of+input+devices" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0004&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DCard%26aufirst%3DS.%2520K.%26date%3D1991%26atitle%3DA%2520morphological%2520analysis%2520of%2520the%2520design%2520space%2520of%2520input%2520devices%26jtitle%3DACM%2520Transactions%2520on%2520Information%2520Systems%26volume%3D9%26spage%3D99%26epage%3D122" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0005"><span><span class="hlFld-ContribAuthor">Chai, <span class="NLM_given-names">D.</span></span> and <span class="hlFld-ContribAuthor">Bouzerdoum, <span class="NLM_given-names">A.</span></span> <span class="NLM_article-title">A Bayesian approach to skin colour classification in YCbCr colour space</span>. <span class="NLM_conf-name">IEEE Region Ten Conference (TENCON’2000)</span>. pp.<span class="NLM_fpage">421</span>–<span class="NLM_lpage">424</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AChai+%2C+D.+and+%0ABouzerdoum+%2C+A.+A+Bayesian+approach+to+skin+colour+classification+in+YCbCr+colour+space.+IEEE+Region+Ten+Conference+%28TENCON%E2%80%992000%29.+pp.421%E2%80%93424.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0005&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DChai%26aufirst%3DD.%26atitle%3DA%2520Bayesian%2520approach%2520to%2520skin%2520colour%2520classification%2520in%2520YCbCr%2520colour%2520space%26spage%3D421%26epage%3D424" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0006"><span><span class="hlFld-ContribAuthor">Chong, <span class="NLM_given-names">J. W. S.</span></span>, <span class="hlFld-ContribAuthor">Ong, <span class="NLM_given-names">S. K.</span></span> and <span class="hlFld-ContribAuthor">Nee, <span class="NLM_given-names">A. Y. C.</span></span> <span class="NLM_year">2007</span>. <span class="NLM_article-title">Methodologies for immersive robot programming in an augmented reality environment</span>. <i>International Journal of Virtual Reality</i>, 6(1): <span class="NLM_fpage">69</span>–<span class="NLM_lpage">79</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=69-79&issue=1&author=J.+W.+S.+Chong&author=S.+K.+Ong&author=A.+Y.+C.+Nee&title=Methodologies+for+immersive+robot+programming+in+an+augmented+reality+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0006&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DChong%26aufirst%3DJ.%2520W.%2520S.%26date%3D2007%26atitle%3DMethodologies%2520for%2520immersive%2520robot%2520programming%2520in%2520an%2520augmented%2520reality%2520environment%26jtitle%3DInternational%2520Journal%2520of%2520Virtual%2520Reality%26volume%3D6%26issue%3D1%26spage%3D69%26epage%3D79" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0007"><span><span class="hlFld-ContribAuthor">Erol, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Bebis, <span class="NLM_given-names">G.</span></span>, <span class="hlFld-ContribAuthor">Nicolescu, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Boyle, <span class="NLM_given-names">R. D.</span></span> and <span class="hlFld-ContribAuthor">Twombly, <span class="NLM_given-names">X.</span></span> <span class="NLM_year">2007</span>. <span class="NLM_article-title">Vision-based hand pose estimation: A review</span>. <i>Computer Vision and Image Understanding</i>, 108(1–2): <span class="NLM_fpage">52</span>–<span class="NLM_lpage">73</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0007&amp;dbid=16&amp;doi=10.1080%2F10447318.2011.555297&amp;key=10.1016%2Fj.cviu.2006.10.012" target="_blank">[Crossref]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=52-73&issue=1%E2%80%932&author=A.+Erol&author=G.+Bebis&author=M.+Nicolescu&author=R.+D.+Boyle&author=X.+Twombly&title=Vision-based+hand+pose+estimation%3A+A+review" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0007&amp;dbid=16384&amp;doi=10.1016%2Fj.cviu.2006.10.012&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1016%252Fj.cviu.2006.10.012%26sid%3Dliteratum%253Atandf%26aulast%3DErol%26aufirst%3DA.%26date%3D2007%26atitle%3DVision-based%2520hand%2520pose%2520estimation%253A%2520A%2520review%26jtitle%3DComputer%2520Vision%2520and%2520Image%2520Understanding%26volume%3D108%26issue%3D1%25E2%2580%25932%26spage%3D52%26epage%3D73" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0008"><span><span class="hlFld-ContribAuthor">Greenspan, <span class="NLM_given-names">H.</span></span>, <span class="hlFld-ContribAuthor">Goldberger, <span class="NLM_given-names">J.</span></span> and <span class="hlFld-ContribAuthor">Eshet, <span class="NLM_given-names">I.</span></span> <span class="NLM_year">2001</span>. <span class="NLM_article-title">Mixture model for face colour modelling and segmentation</span>. <i>Pattern Recognition Letters</i>, 22: <span class="NLM_fpage">1525</span>–<span class="NLM_lpage">1536</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2001&pages=1525-1536&author=H.+Greenspan&author=J.+Goldberger&author=I.+Eshet&title=Mixture+model+for+face+colour+modelling+and+segmentation" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0008&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DGreenspan%26aufirst%3DH.%26date%3D2001%26atitle%3DMixture%2520model%2520for%2520face%2520colour%2520modelling%2520and%2520segmentation%26jtitle%3DPattern%2520Recognition%2520Letters%26volume%3D22%26spage%3D1525%26epage%3D1536" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0009"><span><span class="hlFld-ContribAuthor">Ha, <span class="NLM_given-names">T.</span></span> and <span class="hlFld-ContribAuthor">Woo, <span class="NLM_given-names">W.</span></span> <span class="NLM_year">2006</span>. <span class="NLM_article-title">Bare hand interface for interaction in the video see-through HMD based wearable AR environment</span>. <i>Lecture Notes in Computer Science</i>, 4161: <span class="NLM_fpage">354</span>–<span class="NLM_lpage">357</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=354-357&author=T.+Ha&author=W.+Woo&title=Bare+hand+interface+for+interaction+in+the+video+see-through+HMD+based+wearable+AR+environment" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0009&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DHa%26aufirst%3DT.%26date%3D2006%26atitle%3DBare%2520hand%2520interface%2520for%2520interaction%2520in%2520the%2520video%2520see-through%2520HMD%2520based%2520wearable%2520AR%2520environment%26jtitle%3DLecture%2520Notes%2520in%2520Computer%2520Science%26volume%3D4161%26spage%3D354%26epage%3D357" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0010"><span><span class="hlFld-ContribAuthor">Kim, <span class="NLM_given-names">H.</span></span> and <span class="hlFld-ContribAuthor">Fellner, <span class="NLM_given-names">D. W.</span></span> <span class="NLM_article-title">Interaction with hand gesture for a backprojection wall</span>. <span class="NLM_conf-name">Proceedings of Computer Graphics International 2004 (CGI’04)</span>. pp.<span class="NLM_fpage">395</span>–<span class="NLM_lpage">402</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AKim+%2C+H.+and+%0AFellner+%2C+D.+W.+Interaction+with+hand+gesture+for+a+backprojection+wall.+Proceedings+of+Computer+Graphics+International+2004+%28CGI%E2%80%9904%29.+pp.395%E2%80%93402.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0010&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DKim%26aufirst%3DH.%26atitle%3DInteraction%2520with%2520hand%2520gesture%2520for%2520a%2520backprojection%2520wall%26spage%3D395%26epage%3D402" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0011"><span><span class="hlFld-ContribAuthor">Kim, <span class="NLM_given-names">K. I.</span></span>, <span class="hlFld-ContribAuthor">Jung, <span class="NLM_given-names">K.</span></span> and <span class="hlFld-ContribAuthor">Kim, <span class="NLM_given-names">J. H.</span></span> <span class="NLM_year">2003</span>. <span class="NLM_article-title">Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 25: <span class="NLM_fpage">1631</span>–<span class="NLM_lpage">1639</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2003&pages=1631-1639&author=K.+I.+Kim&author=K.+Jung&author=J.+H.+Kim&title=Texture-based+approach+for+text+detection+in+images+using+support+vector+machines+and+continuously+adaptive+mean+shift+algorithm" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0011&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DKim%26aufirst%3DK.%2520I.%26date%3D2003%26atitle%3DTexture-based%2520approach%2520for%2520text%2520detection%2520in%2520images%2520using%2520support%2520vector%2520machines%2520and%2520continuously%2520adaptive%2520mean%2520shift%2520algorithm%26jtitle%3DIEEE%2520Transactions%2520on%2520Pattern%2520Analysis%2520and%2520Machine%2520Intelligence%26volume%3D25%26spage%3D1631%26epage%3D1639" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0012"><span><span class="hlFld-ContribAuthor">Kovac, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Peer, <span class="NLM_given-names">P.</span></span> and <span class="hlFld-ContribAuthor">Solina, <span class="NLM_given-names">F.</span></span> <span class="NLM_article-title">Human skin colour clustering for face detection</span>. <span class="NLM_conf-name">Proceedings of EUROCON 2003</span>. pp.<span class="NLM_fpage">144</span>–<span class="NLM_lpage">148</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AKovac+%2C+J.+%2C+%0APeer+%2C+P.+and+%0ASolina+%2C+F.+Human+skin+colour+clustering+for+face+detection.+Proceedings+of+EUROCON+2003.+pp.144%E2%80%93148.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0012&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DKovac%26aufirst%3DJ.%26atitle%3DHuman%2520skin%2520colour%2520clustering%2520for%2520face%2520detection%26spage%3D144%26epage%3D148" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0013"><span><span class="hlFld-ContribAuthor">Lee, <span class="NLM_given-names">T.</span></span> and <span class="hlFld-ContribAuthor">Höllerer, <span class="NLM_given-names">T.</span></span> <span class="NLM_article-title">Handy AR: Markerless inspection of augmented reality objects using fingertip tracking</span>. <span class="NLM_conf-name">Proceedings of IEEE International Symposium on Wearable Computers (ISWC)</span>. pp.<span class="NLM_fpage">83</span>–<span class="NLM_lpage">90</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0ALee+%2C+T.+and+%0AH%C3%B6llerer+%2C+T.+Handy+AR%3A+Markerless+inspection+of+augmented+reality+objects+using+fingertip+tracking.+Proceedings+of+IEEE+International+Symposium+on+Wearable+Computers+%28ISWC%29.+pp.83%E2%80%9390.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0013&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DLee%26aufirst%3DT.%26atitle%3DHandy%2520AR%253A%2520Markerless%2520inspection%2520of%2520augmented%2520reality%2520objects%2520using%2520fingertip%2520tracking%26spage%3D83%26epage%3D90" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0014"><span><span class="hlFld-ContribAuthor">Maggioni, <span class="NLM_given-names">C.</span></span> <span class="NLM_article-title">A novel gestural input device for virtual reality</span>. <span class="NLM_conf-name">Proceedings of 1993 IEEE Annual Virtual Reality International Symposium</span>. pp.<span class="NLM_fpage">118</span>–<span class="NLM_lpage">124</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AMaggioni+%2C+C.+A+novel+gestural+input+device+for+virtual+reality.+Proceedings+of+1993+IEEE+Annual+Virtual+Reality+International+Symposium.+pp.118%E2%80%93124.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0014&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DMaggioni%26aufirst%3DC.%26atitle%3DA%2520novel%2520gestural%2520input%2520device%2520for%2520virtual%2520reality%26spage%3D118%26epage%3D124" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0015"><span><span class="hlFld-ContribAuthor">Nielsen, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Störring, <span class="NLM_given-names">M.</span></span>, <span class="hlFld-ContribAuthor">Moeslund, <span class="NLM_given-names">T. B.</span></span> and <span class="hlFld-ContribAuthor">Granum, <span class="NLM_given-names">E.</span></span> <span class="NLM_article-title">A procedure for developing intuitive and ergonomic gesture interfaces for HCI</span>. <span class="NLM_conf-name">Proceedings of the 5th International Gesture Workshop</span>. pp.<span class="NLM_fpage">409</span>–<span class="NLM_lpage">420</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0ANielsen+%2C+M.+%2C+%0ASt%C3%B6rring+%2C+M.+%2C+%0AMoeslund+%2C+T.+B.+and+%0AGranum+%2C+E.+A+procedure+for+developing+intuitive+and+ergonomic+gesture+interfaces+for+HCI.+Proceedings+of+the+5th+International+Gesture+Workshop.+pp.409%E2%80%93420.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0015&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DNielsen%26aufirst%3DM.%26atitle%3DA%2520procedure%2520for%2520developing%2520intuitive%2520and%2520ergonomic%2520gesture%2520interfaces%2520for%2520HCI%26spage%3D409%26epage%3D420" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0016"><span> OpenCV. (2000). <i>Open Source Computer Vision Library</i>. Santa Clara, CA: Intel Corporation. <a class="ext-link" href="http://sourceforge.net/projects/opencvlibrary/" target="_blank">http://sourceforge.net/projects/opencvlibrary/ (http://sourceforge.net/projects/opencvlibrary/)</a> <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+OpenCV.+%282000%29.+Open+Source+Computer+Vision+Library.+Santa+Clara%2C+CA%3A+Intel+Corporation.+http%3A%2F%2Fsourceforge.net%2Fprojects%2Fopencvlibrary%2F+%28http%3A%2F%2Fsourceforge.net%2Fprojects%2Fopencvlibrary%2F%29+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0016&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Fsourceforge.net%2Fprojects%2Fopencvlibrary%2F+%28http%3A%2F%2Fsourceforge.net%2Fprojects%2Fopencvlibrary%2F%29" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0017"><span><span class="hlFld-ContribAuthor">Piekarski, <span class="NLM_given-names">W.</span></span> and <span class="hlFld-ContribAuthor">Smith, <span class="NLM_given-names">R.</span></span> <span class="NLM_year">2006</span>. <span class="NLM_article-title">Robust gloves for 3D interaction in mobile outdoor AR environments</span>. <i>ISMAR</i>, 2006: <span class="NLM_fpage">251</span>–<span class="NLM_lpage">252</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=251-252&author=W.+Piekarski&author=R.+Smith&title=Robust+gloves+for+3D+interaction+in+mobile+outdoor+AR+environments" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0017&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DPiekarski%26aufirst%3DW.%26date%3D2006%26atitle%3DRobust%2520gloves%2520for%25203D%2520interaction%2520in%2520mobile%2520outdoor%2520AR%2520environments%26jtitle%3DISMAR%26volume%3D2006%26spage%3D251%26epage%3D252" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0018"><span><span class="hlFld-ContribAuthor">Pihuit, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Kryt, <span class="NLM_given-names">P. G.</span></span> and <span class="hlFld-ContribAuthor">Cani, <span class="NLM_given-names">M.-P.</span></span> <span class="NLM_article-title">Hands on virtual clay</span>. <span class="NLM_conf-name">Proceedings of IEEE International Conference on Shape Modelling and Applications (SMI)</span>. pp.<span class="NLM_fpage">267</span>–<span class="NLM_lpage">268</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0APihuit+%2C+A.+%2C+%0AKryt+%2C+P.+G.+and+%0ACani+%2C+M.-P.+Hands+on+virtual+clay.+Proceedings+of+IEEE+International+Conference+on+Shape+Modelling+and+Applications+%28SMI%29.+pp.267%E2%80%93268.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0018&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DPihuit%26aufirst%3DA.%26atitle%3DHands%2520on%2520virtual%2520clay%26spage%3D267%26epage%3D268" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0019"><span><span class="hlFld-ContribAuthor">Sato, <span class="NLM_given-names">Y.</span></span>, <span class="hlFld-ContribAuthor">Kobayashi, <span class="NLM_given-names">Y.</span></span> and <span class="hlFld-ContribAuthor">Koike, <span class="NLM_given-names">H.</span></span> <span class="NLM_article-title">Fast tracking of hands and fingertips in infrared images for augmented desk interface</span>. <span class="NLM_conf-name">Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition</span>. pp.<span class="NLM_fpage">462</span>–<span class="NLM_lpage">467</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0ASato+%2C+Y.+%2C+%0AKobayashi+%2C+Y.+and+%0AKoike+%2C+H.+Fast+tracking+of+hands+and+fingertips+in+infrared+images+for+augmented+desk+interface.+Proceedings+of+the+Fourth+IEEE+International+Conference+on+Automatic+Face+and+Gesture+Recognition.+pp.462%E2%80%93467.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0019&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSato%26aufirst%3DY.%26atitle%3DFast%2520tracking%2520of%2520hands%2520and%2520fingertips%2520in%2520infrared%2520images%2520for%2520augmented%2520desk%2520interface%26spage%3D462%26epage%3D467" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0020"><span><span class="hlFld-ContribAuthor">Schweighofer, <span class="NLM_given-names">G.</span></span> and <span class="hlFld-ContribAuthor">Pinz, <span class="NLM_given-names">A.</span></span> <span class="NLM_year">2006</span>. <span class="NLM_article-title">Robust pose estimation from a planar target</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 28: <span class="NLM_fpage">2024</span>–<span class="NLM_lpage">2030</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16&amp;doi=10.1080%2F10447318.2011.555297&amp;key=10.1109%2FTPAMI.2006.252" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0020&amp;dbid=8&amp;doi=10.1080%2F10447318.2011.555297&amp;key=17108375" target="_blank">[PubMed]</a>, <a href="/servlet/linkout?suffix=CIT0020&amp;dbid=128&amp;doi=10.1080%2F10447318.2011.555297&amp;key=000241195700011" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=2024-2030&author=G.+Schweighofer&author=A.+Pinz&title=Robust+pose+estimation+from+a+planar+target" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0020&amp;dbid=16384&amp;doi=10.1109%2FTPAMI.2006.252&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1109%252FTPAMI.2006.252%26sid%3Dliteratum%253Atandf%26aulast%3DSchweighofer%26aufirst%3DG.%26date%3D2006%26atitle%3DRobust%2520pose%2520estimation%2520from%2520a%2520planar%2520target%26jtitle%3DIEEE%2520Transactions%2520on%2520Pattern%2520Analysis%2520and%2520Machine%2520Intelligence%26volume%3D28%26spage%3D2024%26epage%3D2030" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0021"><span><span class="hlFld-ContribAuthor">Segen, <span class="NLM_given-names">J.</span></span> and <span class="hlFld-ContribAuthor">Kumar, <span class="NLM_given-names">S.</span></span> <span class="NLM_article-title">Gesture VR: Vision-based 3D hand interface for spatial interaction</span>. <span class="NLM_conf-name">Proceedings of the sixth ACM international conference on Multimedia, Bristol, UK</span>. pp.<span class="NLM_fpage">455</span>–<span class="NLM_lpage">464</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0ASegen+%2C+J.+and+%0AKumar+%2C+S.+Gesture+VR%3A+Vision-based+3D+hand+interface+for+spatial+interaction.+Proceedings+of+the+sixth+ACM+international+conference+on+Multimedia%2C+Bristol%2C+UK.+pp.455%E2%80%93464.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0021&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSegen%26aufirst%3DJ.%26atitle%3DGesture%2520VR%253A%2520Vision-based%25203D%2520hand%2520interface%2520for%2520spatial%2520interaction%26spage%3D455%26epage%3D464" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0022"><span><span class="hlFld-ContribAuthor">Seo, <span class="NLM_given-names">B.-K.</span></span>, <span class="hlFld-ContribAuthor">Choi, <span class="NLM_given-names">J.</span></span>, <span class="hlFld-ContribAuthor">Han, <span class="NLM_given-names">J.-H.</span></span>, <span class="hlFld-ContribAuthor">Park, <span class="NLM_given-names">H.</span></span> and <span class="hlFld-ContribAuthor">Park, <span class="NLM_given-names">J.</span></span> <span class="NLM_article-title">One-handed interaction with augmented virtual objects on mobile devices</span>. <span class="NLM_conf-name">Proceedings of VRCAI 2008</span>. [CD-ROM]<span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0ASeo+%2C+B.-K.+%2C+%0AChoi+%2C+J.+%2C+%0AHan+%2C+J.-H.+%2C+%0APark+%2C+H.+and+%0APark+%2C+J.+One-handed+interaction+with+augmented+virtual+objects+on+mobile+devices.+Proceedings+of+VRCAI+2008.+%5BCD-ROM%5D" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0022&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DSeo%26aufirst%3DB.-K.%26atitle%3DOne-handed%2520interaction%2520with%2520augmented%2520virtual%2520objects%2520on%2520mobile%2520devices" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0023"><span><span class="hlFld-ContribAuthor">Stenger, <span class="NLM_given-names">B.</span></span>, <span class="hlFld-ContribAuthor">Thayananthan, <span class="NLM_given-names">A.</span></span>, <span class="hlFld-ContribAuthor">Torr, <span class="NLM_given-names">P. H. S.</span></span> and <span class="hlFld-ContribAuthor">Cipolla, <span class="NLM_given-names">R.</span></span> <span class="NLM_year">2006</span>. <span class="NLM_article-title">Model-based hand tracking using a hierarchical Bayesian filter</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 28: <span class="NLM_fpage">1372</span>–<span class="NLM_lpage">1384</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=1372-1384&author=B.+Stenger&author=A.+Thayananthan&author=P.+H.+S.+Torr&author=R.+Cipolla&title=Model-based+hand+tracking+using+a+hierarchical+Bayesian+filter" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0023&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DStenger%26aufirst%3DB.%26date%3D2006%26atitle%3DModel-based%2520hand%2520tracking%2520using%2520a%2520hierarchical%2520Bayesian%2520filter%26jtitle%3DIEEE%2520Transactions%2520on%2520Pattern%2520Analysis%2520and%2520Machine%2520Intelligence%26volume%3D28%26spage%3D1372%26epage%3D1384" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0024"><span><span class="hlFld-ContribAuthor">Sundin, <span class="NLM_given-names">M.</span></span> and <span class="hlFld-ContribAuthor">Fjeld, <span class="NLM_given-names">M.</span></span> <span class="NLM_year">2009</span>. <span class="NLM_article-title">Softly Elastic 6 DOF Input</span>. <i>International Journal of Human-Computer Interaction</i>, 25: <span class="NLM_fpage">647</span>–<span class="NLM_lpage">691</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0024&amp;dbid=20&amp;doi=10.1080%2F10447318.2011.555297&amp;key=10.1080%2F10447310902964124&amp;tollfreelink=144713_3972746_5a5655e201230d88da7aefed71f0b8868dc15fde821b75a28c81ab0af6cb3bca">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0024&amp;dbid=128&amp;doi=10.1080%2F10447318.2011.555297&amp;key=000272798400002" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=647-691&author=M.+Sundin&author=M.+Fjeld&title=Softly+Elastic+6+DOF+Input" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0024&amp;dbid=16384&amp;doi=10.1080%2F10447310902964124&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447310902964124%26sid%3Dliteratum%253Atandf%26aulast%3DSundin%26aufirst%3DM.%26date%3D2009%26atitle%3DSoftly%2520Elastic%25206%2520DOF%2520Input%26jtitle%3DInternational%2520Journal%2520of%2520Human-Computer%2520Interaction%26volume%3D25%26spage%3D647%26epage%3D691" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0025"><span> Ullman, J. (2004). PenClicmouse™. <a class="ext-link" href="http://www.ullmantech.se/features.html" target="_blank">http://www.ullmantech.se/features.html (http://www.ullmantech.se/features.html)</a> <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=+Ullman%2C+J.+%282004%29.+PenClicmouse%E2%84%A2.+http%3A%2F%2Fwww.ullmantech.se%2Ffeatures.html+%28http%3A%2F%2Fwww.ullmantech.se%2Ffeatures.html%29+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0025&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandfhttp%3A%2F%2Fwww.ullmantech.se%2Ffeatures.html+%28http%3A%2F%2Fwww.ullmantech.se%2Ffeatures.html%29" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0026"><span><span class="hlFld-ContribAuthor">von Hardenberg, <span class="NLM_given-names">C.</span></span> and <span class="hlFld-ContribAuthor">Bérard, <span class="NLM_given-names">F.</span></span> <span class="NLM_article-title">Bare-hand human–computer interaction</span>. <span class="NLM_conf-name">Proceedings of the ACM Workshop on Perceptive User Interface</span>. pp.<span class="NLM_fpage">113</span>–<span class="NLM_lpage">120</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0Avon+Hardenberg+%2C+C.+and+%0AB%C3%A9rard+%2C+F.+Bare-hand+human%E2%80%93computer+interaction.+Proceedings+of+the+ACM+Workshop+on+Perceptive+User+Interface.+pp.113%E2%80%93120.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0026&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3Dvon%2520Hardenberg%26aufirst%3DC.%26atitle%3DBare-hand%2520human%25E2%2580%2593computer%2520interaction%26spage%3D113%26epage%3D120" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0027"><span><span class="hlFld-ContribAuthor">Wang, <span class="NLM_given-names">Z. B.</span></span>, <span class="hlFld-ContribAuthor">Shen, <span class="NLM_given-names">Y.</span></span>, <span class="hlFld-ContribAuthor">Ong, <span class="NLM_given-names">S. K.</span></span> and <span class="hlFld-ContribAuthor">Nee, <span class="NLM_given-names">A. Y. C.</span></span> <span class="NLM_article-title">Assembly design and evaluation based on bare-hand interaction in an augmented reality environment</span>. <span class="NLM_conf-name">International Conference on CYBERWORLDS</span>. pp.<span class="NLM_fpage">21</span>–<span class="NLM_lpage">28</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AWang+%2C+Z.+B.+%2C+%0AShen+%2C+Y.+%2C+%0AOng+%2C+S.+K.+and+%0ANee+%2C+A.+Y.+C.+Assembly+design+and+evaluation+based+on+bare-hand+interaction+in+an+augmented+reality+environment.+International+Conference+on+CYBERWORLDS.+pp.21%E2%80%9328.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0027&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DWang%26aufirst%3DZ.%2520B.%26atitle%3DAssembly%2520design%2520and%2520evaluation%2520based%2520on%2520bare-hand%2520interaction%2520in%2520an%2520augmented%2520reality%2520environment%26spage%3D21%26epage%3D28" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0028"><span><span class="hlFld-ContribAuthor">Wu, <span class="NLM_given-names">Y.</span></span>, <span class="hlFld-ContribAuthor">Lin, <span class="NLM_given-names">J. Y.</span></span> and <span class="hlFld-ContribAuthor">Huang, <span class="NLM_given-names">T. S.</span></span> <span class="NLM_article-title">Capturing natural hand articulation</span>. <span class="NLM_conf-name">Proceedings of the Eighth IEEE International Conference on Computer Vision</span>. pp.<span class="NLM_fpage">426</span>–<span class="NLM_lpage">432</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AWu+%2C+Y.+%2C+%0ALin+%2C+J.+Y.+and+%0AHuang+%2C+T.+S.+Capturing+natural+hand+articulation.+Proceedings+of+the+Eighth+IEEE+International+Conference+on+Computer+Vision.+pp.426%E2%80%93432.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0028&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DWu%26aufirst%3DY.%26atitle%3DCapturing%2520natural%2520hand%2520articulation%26spage%3D426%26epage%3D432" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0029"><span><span class="hlFld-ContribAuthor">Yoon, <span class="NLM_given-names">J.-H.</span></span>, <span class="hlFld-ContribAuthor">Park, <span class="NLM_given-names">J.-S.</span></span> and <span class="hlFld-ContribAuthor">Sung, <span class="NLM_given-names">M. Y.</span></span> <span class="NLM_article-title">Vision-based bare-hand gesture interface for interactive augmented reality applications</span>. <span class="NLM_conf-name">Proceedings of ICEC 2006</span>. pp.<span class="NLM_fpage">386</span>–<span class="NLM_lpage">389</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=%0AYoon+%2C+J.-H.+%2C+%0APark+%2C+J.-S.+and+%0ASung+%2C+M.+Y.+Vision-based+bare-hand+gesture+interface+for+interactive+augmented+reality+applications.+Proceedings+of+ICEC+2006.+pp.386%E2%80%93389.+" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0029&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DYoon%26aufirst%3DJ.-H.%26atitle%3DVision-based%2520bare-hand%2520gesture%2520interface%2520for%2520interactive%2520augmented%2520reality%2520applications%26spage%3D386%26epage%3D389" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0030"><span><span class="hlFld-ContribAuthor">Yoon, <span class="NLM_given-names">S.-Y.</span></span>, <span class="hlFld-ContribAuthor">Laffey, <span class="NLM_given-names">J.</span></span> and <span class="hlFld-ContribAuthor">Oh, <span class="NLM_given-names">H.</span></span> <span class="NLM_year">2008</span>. <span class="NLM_article-title">Understanding usability and user experience of web-based 3D graphics technology</span>. <i>International Journal of Human-Computer Interaction</i>, 24: <span class="NLM_fpage">288</span>–<span class="NLM_lpage">306</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0030&amp;dbid=20&amp;doi=10.1080%2F10447318.2011.555297&amp;key=10.1080%2F10447310801920516&amp;tollfreelink=144713_3972746_c195dd140ad520ae685376a8459146f8705832cc377c465ce2c48e544ed2fe5d">[Taylor &amp; Francis Online]</a>, <a href="/servlet/linkout?suffix=CIT0030&amp;dbid=128&amp;doi=10.1080%2F10447318.2011.555297&amp;key=000254796600003" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=288-306&author=S.-Y.+Yoon&author=J.+Laffey&author=H.+Oh&title=Understanding+usability+and+user+experience+of+web-based+3D+graphics+technology" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0030&amp;dbid=16384&amp;doi=10.1080%2F10447310801920516&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1080%252F10447310801920516%26sid%3Dliteratum%253Atandf%26aulast%3DYoon%26aufirst%3DS.-Y.%26date%3D2008%26atitle%3DUnderstanding%2520usability%2520and%2520user%2520experience%2520of%2520web-based%25203D%2520graphics%2520technology%26jtitle%3DInternational%2520Journal%2520of%2520Human-Computer%2520Interaction%26volume%3D24%26spage%3D288%26epage%3D306" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0031"><span><span class="hlFld-ContribAuthor">Yuan, <span class="NLM_given-names">M. L.</span></span>, <span class="hlFld-ContribAuthor">Ong, <span class="NLM_given-names">S. K.</span></span> and <span class="hlFld-ContribAuthor">Nee, <span class="NLM_given-names">A. Y. C.</span></span> <span class="NLM_year">2004</span>. <span class="NLM_article-title">The virtual interaction panel: An easy control tool in augmented reality systems</span>. <i>Computer Animation and Virtual Worlds Journal</i>, 15: <span class="NLM_fpage">425</span>–<span class="NLM_lpage">432</span>. <span class="refLink-block"> <span class="xlinks-container"></span><span class="googleScholar-container"><a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=425-432&author=M.+L.+Yuan&author=S.+K.+Ong&author=A.+Y.+C.+Nee&title=The+virtual+interaction+panel%3A+An+easy+control+tool+in+augmented+reality+systems" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0031&amp;dbid=16384&amp;doi=&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26sid%3Dliteratum%253Atandf%26aulast%3DYuan%26aufirst%3DM.%2520L.%26date%3D2004%26atitle%3DThe%2520virtual%2520interaction%2520panel%253A%2520An%2520easy%2520control%2520tool%2520in%2520augmented%2520reality%2520systems%26jtitle%3DComputer%2520Animation%2520and%2520Virtual%2520Worlds%2520Journal%26volume%3D15%26spage%3D425%26epage%3D432" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li><li id="CIT0032"><span><span class="hlFld-ContribAuthor">Zhang, <span class="NLM_given-names">Z. Y.</span></span> <span class="NLM_year">2000</span>. <span class="NLM_article-title">A flexible new technique for camera calibration</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 22: <span class="NLM_fpage">1330</span>–<span class="NLM_lpage">1334</span>. <span class="refLink-block"> <span class="xlinks-container"><a href="/servlet/linkout?suffix=CIT0032&amp;dbid=16&amp;doi=10.1080%2F10447318.2011.555297&amp;key=10.1109%2F34.888718" target="_blank">[Crossref]</a>, <a href="/servlet/linkout?suffix=CIT0032&amp;dbid=128&amp;doi=10.1080%2F10447318.2011.555297&amp;key=000165355200011" target="_blank">[Web of Science &#0174;]</a></span><span class="googleScholar-container">, <a class="google-scholar" href="http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=1330-1334&author=Z.+Y.+Zhang&title=A+flexible+new+technique+for+camera+calibration" target="_blank">[Google Scholar]</a></span></span><a href="/servlet/linkout?suffix=CIT0032&amp;dbid=16384&amp;doi=10.1109%2F34.888718&amp;type=refOpenUrl&amp;url=https%3A%2F%2Fsoeg.kb.dk%2Fdiscovery%2Fopenurl%3Finstitution%3D45KBDK_KGL%26vid%3D45KBDK_KGL%3AKGL%26%26id%3Ddoi%3A10.1109%252F34.888718%26sid%3Dliteratum%253Atandf%26aulast%3DZhang%26aufirst%3DZ.%2520Y.%26date%3D2000%26atitle%3DA%2520flexible%2520new%2520technique%2520for%2520camera%2520calibration%26jtitle%3DIEEE%2520Transactions%2520on%2520Pattern%2520Analysis%2520and%2520Machine%2520Intelligence%26volume%3D22%26spage%3D1330%26epage%3D1334" title="OpenURL Copenhagen University Library" class="sfxLink" aria-label="OpenURL Copenhagen University Library"><img src="/userimages/78554/sfxbutton" alt="OpenURL Copenhagen University Library" /></a></span></li></ul><div class="response"><div class="sub-article-title"></div></div>
</article>
</div>
<div class="tab tab-pane" id="relatedContent">
</div>
<div class="tab tab-pane " id="metrics-content">
<div class="articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics" data-pb-dropzone="publicationContentDropZoneMetrics">
<div class="widget literatumArticleMetricsWidget none  widget-none" id="00886058-9b49-4cdf-9f1e-deb78b7818c3">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="articleMetricsContainer">
<div class="content fullView">
<h2>
Article Metrics
</h2>
<div class="section views">
<div class="title">
Views
</div>
<div class="circle">
<span class="value">888</span>
</div>
</div>
<div class="section citations">
<div class="title">
Citations
</div>
<a href="/doi/citedby/10.1080/10447318.2011.555297" class="circle crossRef " target="_blank">
<span>
Crossref
</span>
<span class="value">48</span>
</a>
<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=CitingArticles&KeyUT=000290407700003" target="_blank" class="circle webOfScience ">
<span>
Web of Science
</span>
<span class="value">30</span>
</a>
<a href="http://www.scopus.com/inward/citedby.url?partnerID=HzOxMe3b&scp=79956325794" target="_blank" class="circle scopus ">
<span>
Scopus
</span>
<span class="value">49</span>
</a>
</div>
<div class="section altmetric-container">
<div class="title">
Altmetric
</div>
<script type="text/javascript">
    TandfUtils.appendScript(document.head, 'https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js', 'altmetric-embed-src', true, true);
</script>
<div class="metrics-badge linkToPopup">
<div class='altmetric-embed' data-badge-type='medium-donut' data-badge-details='right' data-condensed='true' data-template="tandf" data-hide-no-mentions="false" data-doi="10.1080/10447318.2011.555297">
</div>
</div>
</div>
<div class="altmetricsPopup"></div>
</div>
</div>
<div class="metricsLinks">
<a href="/article-metrics">Article metrics information</a>
<br>
<a href="/article-citations-disclaimer">Disclaimer for citing articles</a>
</div></div>
</div>
</div>
</div>
</div>
<div class="access__limit" data-pb-dropzone="accessLimitPage">
</div>
</div>
</div>
</div>
<input id="viewLargeImageCaption" type="hidden" value="View Large Image" /></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-4 ">
<div class="contents" data-pb-dropzone="contents2">
<div class="widget general-bookmark-share none  widget-none  widget-compact-all" id="c8494935-e102-4ff5-9395-4ffa44a77f1c">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all">
<ul>
<li>
<div class="addthis_toolbox addthis_20x20_style">
<div class="custom_images">
<a class="addthis_button_twitter">
<span class="at-icon-twitter"></span>
</a>
<a class="addthis_button_facebook">
<span class="at-icon-facebook"></span>
</a>
<a class="addthis_button_email">
<span class="at-icon-email"></span>
</a>
<a class="addthis_button_none">
<span class="at-icon-none"></span>
</a>
<a class="addthis_button_compact"><span class="at-icon-wrapper"></span>
<span aria-describedby="shareOptions-description">
<span class="off-screen" id="shareOptions-description">More Share Options</span>
</span>
</a>
</div>
</div>
</li>
</ul>
<script type="text/javascript">
    
    var script = document.createElement('script');
    script.type='text/javascript';
    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';
    script.async = true;
    $('head').append(script)
</script>
</div>
</div>
</div>
<div class="widget general-html none  widget-none" id="16111d74-c554-42b2-a277-f2727ad2b285">
<div class="wrapped ">
<div class="widget-body body body-none ">&nbsp;</div>
</div>
</div>
<div class="widget general-html none  widget-none  widget-compact-all" id="649c3793-a589-4dfb-8de2-9b6cbc4dc15b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><script defer src='//js.trendmd.com/trendmd.min.js' data-trendmdconfig='{"element":"#trendmd-suggestions"}'></script></div>
</div>
</div>

<div class="widget graphQueryWidget none  widget-none  widget-compact-all" id="6583d550-25db-458c-9c81-291f5c88b6ee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div id="trendmd-suggestions"></div></div>
</div>
</div>
<div class="widget pbOptimizerWidget none  widget-none  widget-compact-all" id="25efeb89-6948-4246-800a-5e246009698d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div data-optimizer data-widget-id="25efeb89-6948-4246-800a-5e246009698d" id="widget-25efeb89-6948-4246-800a-5e246009698d" data-observer>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget pageFooter none  widget-none  widget-compact-all" id="d97c173f-d838-4de1-bbd7-ed69f0d36a91">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><footer class="page-footer">
<div data-pb-dropzone="main">
<div class="widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all" id="1f15adc0-4a59-4d27-93fe-8cbb14a5108a">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="container">
<div class="row row-md gutterless ">
<div class="col-md-1-1 fit-padding">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget topicalIndex none  widget-none  widget-compact-all" id="9298c7a6-6903-4607-8380-4c83e2b7142f">
<div class="wrapped ">
<h1 class="widget-header header-none  header-compact-all">Browse journals by subject</h1>
<div class="widget-body body body-none  body-compact-all"><div class="topicalIndexBrowsingTips" data-pb-dropzone="topicalIndexBrowsingTips">
<div class="widget general-html none  widget-none  widget-compact-all" id="1ec3bad2-243b-45a9-a59a-5aceb80fc5a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><a class="nav-top" href="#top">Back to top <span class="fa fa-angle-up"></span></a></div>
</div>
</div>
</div>
<div class="container">
<ul>
<li>
<a href="/topic/allsubjects/as?target=topic&amp;ConceptID=4251">Area Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ar?target=topic&amp;ConceptID=4250">Arts</a>
</li>
<li>
<a href="/topic/allsubjects/be?target=topic&amp;ConceptID=4252">Behavioral Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/bs?target=topic&amp;ConceptID=4253">Bioscience</a>
</li>
<li>
<a href="/topic/allsubjects/bu?target=topic&amp;ConceptID=4254">Built Environment</a>
</li>
<li>
<a href="/topic/allsubjects/cs?target=topic&amp;ConceptID=4256">Communication Studies</a>
</li>
<li>
<a href="/topic/allsubjects/cm?target=topic&amp;ConceptID=4255">Computer Science</a>
</li>
<li>
<a href="/topic/allsubjects/ds?target=topic&amp;ConceptID=4257">Development Studies</a>
</li>
<li>
<a href="/topic/allsubjects/ea?target=topic&amp;ConceptID=4258">Earth Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/eb?target=topic&amp;ConceptID=4259">Economics, Finance, Business & Industry</a>
</li>
<li>
<a href="/topic/allsubjects/ed?target=topic&amp;ConceptID=4261">Education</a>
</li>
<li>
<a href="/topic/allsubjects/ec?target=topic&amp;ConceptID=4260">Engineering & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ag?target=topic&amp;ConceptID=4248">Environment & Agriculture</a>
</li>
<li>
<a href="/topic/allsubjects/es?target=topic&amp;ConceptID=4262">Environment and Sustainability</a>
</li>
<li>
<a href="/topic/allsubjects/fs?target=topic&amp;ConceptID=4263">Food Science & Technology</a>
</li>
<li>
<a href="/topic/allsubjects/ge?target=topic&amp;ConceptID=4264">Geography</a>
</li>
<li>
<a href="/topic/allsubjects/hs?target=topic&amp;ConceptID=4266">Health and Social Care</a>
</li>
<li>
<a href="/topic/allsubjects/hu?target=topic&amp;ConceptID=4267">Humanities</a>
</li>
<li>
<a href="/topic/allsubjects/if?target=topic&amp;ConceptID=4268">Information Science</a>
</li>
<li>
<a href="/topic/allsubjects/la?target=topic&amp;ConceptID=4269">Language & Literature</a>
</li>
<li>
<a href="/topic/allsubjects/lw?target=topic&amp;ConceptID=4270">Law</a>
</li>
<li>
<a href="/topic/allsubjects/ma?target=topic&amp;ConceptID=4271">Mathematics & Statistics</a>
</li>
<li>
<a href="/topic/allsubjects/me?target=topic&amp;ConceptID=4272">Medicine, Dentistry, Nursing & Allied Health</a>
</li>
<li>
<a href="/topic/allsubjects/ah?target=topic&amp;ConceptID=4249">Museum and Heritage Studies</a>
</li>
<li>
<a href="/topic/allsubjects/pc?target=topic&amp;ConceptID=4273">Physical Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/pi?target=topic&amp;ConceptID=4274">Politics & International Relations</a>
</li>
<li>
<a href="/topic/allsubjects/sn?target=topic&amp;ConceptID=4278">Social Sciences</a>
</li>
<li>
<a href="/topic/allsubjects/sl?target=topic&amp;ConceptID=4277">Sports and Leisure</a>
</li>
<li>
<a href="/topic/allsubjects/sp?target=topic&amp;ConceptID=4279">Tourism, Hospitality and Events</a>
</li>
<li>
<a href="/topic/allsubjects/us?target=topic&amp;ConceptID=4280">Urban Studies</a>
</li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none footer-links widget-none  widget-compact-horizontal" id="64a44adf-45ed-4da3-be26-ef25beb9dbee">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-md  ">
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="6918e9df-910a-4206-9bd0-1a02bc17f740">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="aa9510dd-52ed-4b74-8211-fb510cd9468e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Information for</h3>
<ul>
<li><a href="http://authorservices.taylorandfrancis.com/">Authors</a></li>
<li><a href="http://editorresources.taylorandfrancisgroup.com/">Editors</a></li>
<li><a href="/page/librarians">Librarians</a></li>
<li><a href="/societies">Societies</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce">

<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Open access</h3>
<ul>
<li><a href="/openaccess">Overview</a></li>
<li><a href="/openaccess/openjournals">Open journals</a></li>
<li><a href="/openaccess/openselect">Open Select</a></li>
<li><a href="https://www.cogentoa.com/">Cogent OA</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-md-1-2 ">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget responsive-layout none footer-responsive-container widget-none" id="fc564559-f496-499c-87c7-d851f371f061">
<div class="wrapped ">
<div class="widget-body body body-none "><div class="container-fluid">
<div class="row row-sm  ">
<div class="col-sm-1-2 footer_left_col">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none  widget-none  widget-compact-all" id="f3fb3d36-db42-4373-9d0e-432958bf2fbc">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-info-list">
<h3>Help and info</h3>
<ul>
<li><a href="https://help.tandfonline.com">Help & contact</a></li>
<li><a href="https://newsroom.taylorandfrancisgroup.com/">Newsroom</a></li>
<li><a href="https://taylorandfrancis.com/partnership/commercial/">Commercial services</a></li>
<li><a href="/action/showPublications?pubType=journal">All journals</a></li>
<li><a href="https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot">Books</a></li>
</ul>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-2 footer_right_col">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-html none  widget-none  widget-compact-all" id="914433f6-0ea6-4a47-9781-07564061be86">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="footer-social-label">
<h3>Keep up to date</h3>
</div>
<div class="font-size-correction-sml">Register to receive personalised research and resources by email</div>
<div class="bs">
<div class="pull-left links font-size-correction">
<a class="font-size-correction-link" href="https://taylorandfrancis.formstack.com/forms/tfoguest_signup"><i class="fa fa-envelope-square" title="Register to receive personalised research and resources by email"></i>Sign me up</a>
</div></div></div>
</div>
</div>
<div class="widget literatumSocialLinks none  widget-none  widget-compact-all" id="3b6a5e53-cd62-452f-adc1-92e187a0849d">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="bs">
<div class="pull-left links">
<a href="http://facebook.com/TaylorandFrancisGroup">
<i class="icon-facebook" title="Taylor and Francis Group Facebook page" aria-hidden="true" role="button"></i>
<span aria-describedby="fb-description">
<span class="off-screen" id="fb-description">Taylor and Francis Group Facebook page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="https://twitter.com/tandfonline">
<i class="fa fa-twitter-square" title="Taylor and Francis Group Twitter page" aria-hidden="true" role="button"></i>
<span aria-describedby="twitter-description">
<span class="off-screen" id="twitter-description">Taylor and Francis Group Twitter page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://linkedin.com/company/taylor-&-francis-group">
<i class="fa fa-linkedin-square" title="Taylor and Francis Group LinkedIn page" aria-hidden="true" role="button"></i>
<span aria-describedby="linkedin-description">
<span class="off-screen" id="linkedin-description">Taylor and Francis Group Linkedin page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
<div class="pull-left links">
<a href="https://www.youtube.com/user/TaylorandFrancis">
<i class="fa fa-youtube-square" title="Taylor and Francis Group YouTube page" aria-hidden="true" role="button"></i>
<span aria-describedby="youtube-description">
<span class="off-screen" id="youtube-description">Taylor and Francis Group Youtube page</span>
</span>
</a>
</div>
<div class="pull-left links">
<a href="http://www.weibo.com/tandfchina">
<i class="fa fa-weibo" title="Taylor and Francis Group Weibo page" aria-hidden="true" role="button"></i>
<span aria-describedby="weibo-description">
<span class="off-screen" id="weibo-description">Taylor and Francis Group Weibo page</span>
</span>
</a>
</div>
<div class="clearfix"></div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget responsive-layout none  widget-none  widget-compact-horizontal" id="8d803f96-081d-4768-ab7d-280a77af723b">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-horizontal"><div class="container">
<div class="row row-sm  ">
<div class="col-sm-3-4 ">
<div class="contents" data-pb-dropzone="contents0">
<div class="widget general-html none footer-info-container widget-none  widget-compact-vertical" id="b247ecb9-84c9-4762-b270-20f8be1f0ae4">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><div class="informa-group-info">
<span>Copyright © 2020 Informa UK Limited</span>
<span><a href="https://informa.com/privacy-policy/">Privacy policy</a></span>
<span><a href="/cookies">Cookies</a></span>
<span><a href="/terms-and-conditions">Terms & conditions</a></span>
<span><a href="/accessibility">Accessibility</a></span>
<p>Registered in England & Wales No. 3099067<br />
5 Howick Place | London | SW1P 1WG</p>
</div></div>
</div>
</div>
</div>
</div>
<div class="col-sm-1-4 footer_tandf_logo">
<div class="contents" data-pb-dropzone="contents1">
<div class="widget general-image none  widget-none  widget-compact-vertical" id="b6bde365-079b-454f-94f6-1841291656a1">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-vertical"><a href="http://taylorandfrancis.com/" title="Taylor and Francis Group">
<img src="/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png" alt="Taylor and Francis Group" />
</a></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
</div>
</div>
<div class="widget cookiePolicy none  widget-none  widget-compact-all" id="cea739ac-da2c-4d77-9cf1-cb3e0da7e31e">
<div class="wrapped ">
<div class="widget-body body body-none  body-compact-all"><div class="banner">
<a href="#" class="btn">Accept</a>
<p class="message">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href="/cookies">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>
</div></div>
</div>
</div>
</div>
</footer></div>
</div>
</div>
</div>
</div>
</body>
</html>
