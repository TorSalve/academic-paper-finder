{"papers": [{"title": "Gaze-Contingent Auditory Displays for Improved Spatial Attention in Virtual Reality", "authors": ["Margarita Vinnikov", "Robert S. Allison", "Suzette Fernandes"], "abstract": "Virtual reality simulations of group social interactions are important for many applications, including the virtual treatment of social phobias, crowd and group simulation, collaborative virtual environments (VEs), and entertainment. In such scenarios, when compared to the real world, audio cues are often impoverished. As a result, users cannot rely on subtle spatial audio-visual cues that guide attention and enable effective social interactions in real-world situations. We explored whether gaze-contingent audio enhancement techniques driven by inferring audio-visual attention in virtual displays could be used to enable effective communication in cluttered audio VEs. In all of our experiments, we hypothesized that visual attention could be used as a tool to modulate the quality and intensity of sounds from multiple sources to efficiently and naturally select spatial sound sources. For this purpose, we built a gaze-contingent display (GCD) that allowed tracking of a user\u2019s gaze in real-time and modifying the volume of the speakers\u2019 voices contingent on the current region of overt attention. We compared six different techniques for sound modulation with a base condition providing no attentional modulation of sound. The techniques were compared in terms of source recognition and preference in a set of user studies. Overall, we observed that users liked the ability to control the sounds with their eyes. They felt that a rapid change in attenuation with attention but not the elimination of competing sounds (partial rather than absolute selection) was most natural. In conclusion, audio GCDs offer potential for simulating rich, natural social, and other interactions in VEs. They should be considered for improving both performance and fidelity in applications related to social behaviour scenarios or when the user needs to work with multiple audio sources of information.", "keywords": ["sound modulation", "user experience", "visual-audio attention", "Gaze-contingent displays"], "published_in": "", "publication_date": "", "citations": "2", "isbn": "", "doi": "10.1145/3086563", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3067822", "paper_url": "https://dl.acm.org/doi/10.1145/3067822"}, {"title": "Entanglement HCI The Next Wave?", "authors": ["Christopher Frauenberger"], "abstract": "This article argues that our intimate entanglement with digital technologies is challenging the foundations of current HCI research and practice. Our relationships to virtual realities, artificial intelligence, neuro-implants or pervasive, cyberphysical systems generate ontological uncertainties, epistemological diffusion and ethical conundrums that require us to consider evolving the current research paradigm. I look to post-humanism and relational ontologies to sketch what I call Entanglement HCI in response. I review selected theories\u2014Actor-Network Theory, Post-Phenomenology, Object-Oriented Ontology, Agential Realism\u2014and their existing influences on HCI literature. Against this background, I develop Entanglement HCI from the following four perspectives: (a) the performative relationship between humans and technology; (b) the re-framing of knowledge generation processes around phenomena; (c) the tracing of accountabilities, responsibilities and ethical encounters; and (d) the practices of design and mattering that move beyond user-centred design.", "keywords": ["new materialism", "Entanglement", "philosophy", "posthumanism"], "published_in": "", "publication_date": "", "citations": "1", "isbn": "", "doi": "10.1145/3372746", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3364998", "paper_url": "https://dl.acm.org/doi/10.1145/3364998"}, {"title": "Fast and Precise Touch-Based Text Entry for Head-Mounted Augmented Reality with Variable Occlusion", "authors": ["John J. Dudley", "Keith Vertanen", "Per Ola Kristensson"], "abstract": "We present the VISAR keyboard: An augmented reality (AR) head-mounted display (HMD) system that supports text entry via a virtualised input surface. Users select keys on the virtual keyboard by imitating the process of single-hand typing on a physical touchscreen display. Our system uses a statistical decoder to infer users\u2019 intended text and to provide error-tolerant predictions. There is also a high-precision fall-back mechanism to support users in indicating which keys should be unmodified by the auto-correction process. A unique advantage of leveraging the well-established touch input paradigm is that our system enables text entry with minimal visual clutter on the see-through display, thus preserving the user\u2019s field-of-view. We iteratively designed and evaluated our system and show that the final iteration of the system supports a mean entry rate of 17.75wpm with a mean character error rate less than 1%. This performance represents a 19.6% improvement relative to the state-of-the-art baseline investigated: A gaze-then-gesture text entry technique derived from the system keyboard on the Microsoft HoloLens. Finally, we validate that the system is effective in supporting text entry in a fully mobile usage scenario likely to be encountered in industrial applications of AR HMDs.", "keywords": ["text entry", "Augmented reality"], "published_in": "", "publication_date": "", "citations": "6", "isbn": "", "doi": "10.1145/3300063", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3232163", "paper_url": "https://dl.acm.org/doi/10.1145/3232163"}], "total_number_of_results": 3}