{
  "papers": [
    {
      "title": "VR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools",
      "authors": [
        "Jackie (Junrui) Yang",
        "Hiroshi Horii",
        "Alexander Thayer",
        "Rafael Ballagas"
      ],
      "abstract": "Haptic feedback in VR is important for realistic simulation in virtual reality. However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system. We propose Ungrounded Haptic Retargeting, an interaction technique that provides a realistic haptic experience for grabbing tools using only passive mechanisms. This technique leverages the ungrounded feedback inherent in grabbing tools combined with dynamic visual adjustments of their position in virtual reality to create an illusion of physical presence for virtual objects. To demonstrate the capabilities of this technique, we created VR Grabbers, an exemplary passive VR controller, similar to training chopsticks, with haptic feedback for precise object selection and manipulation. We conducted two user studies based on VR Grabbers. The first study probed the perceptual limits of the illusion; we found that the maximum position difference between the virtual and physical world acceptable to the user is (-1.48, 1.95) cm. The second study showed that task performance of the VR Grabbers controller with Ungrounded Haptic Retargeting enabled outperforms the same controller with Ungrounded Haptic Retargeting disabled.",
      "keywords": [
        "grabbing tools",
        "virtual reality",
        "passive haptics",
        "haptic illusions"
      ],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "4",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242643",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242643"
    },
    {
      "title": "PuPoP: Pop-up Prop on Palm for Virtual Reality",
      "authors": [
        "Shan-Yuan Teng",
        "Tzu-Sheng Kuo",
        "Chi Wang",
        "Chi-Huan Chiang",
        "Da-Yuan Huang",
        "Liwei Chan",
        "Bing-Yu Chen"
      ],
      "abstract": "The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.",
      "keywords": ["shape-proxy", "haptics", "airbag", "virtual reality"],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "17",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242628",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242628"
    },
    {
      "title": "MagicalHands: Mid-Air Hand Gestures for Animating in VR",
      "authors": [
        "Rahul Arora",
        "Rubaiat Habib Kazi",
        "Danny M. Kaufman",
        "Wilmot Li",
        "Karan Singh"
      ],
      "abstract": "We explore the use of hand gestures for authoring animations in virtual reality (VR). We first perform a gesture elicitation study to understand user preferences for a spatiotemporal, bare-handed interaction system in VR. Specifically, we focus on creating and editing dynamic, physical phenomena (e.g., particle systems, deformations, coupling), where the mapping from gestures to animation is ambiguous and indirect. We present commonly observed mid-air gestures from the study that cover a wide range of interaction techniques, from direct manipulation to abstract demonstrations. To this end, we extend existing gesture taxonomies to the rich spatiotemporal interaction space of the target domain and distill our findings into a set of guidelines that inform the design of natural user interfaces for VR animation. Finally, based on our guidelines, we develop a proof-of-concept gesture-based VR animation system, MagicalHands. Our results, as well as feedback from user evaluation, suggest that the expressive qualities of hand gestures help users animate more effectively in VR.",
      "keywords": [
        "gesture elicitation",
        "virtual reality",
        "animation",
        "hand gestures"
      ],
      "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "17 October 2019",
      "citations": "4",
      "isbn": "9781450368162",
      "doi": "10.1145/3332165",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347942",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347942"
    },
    {
      "title": "Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality",
      "authors": [
        "Inrak Choi",
        "Heather Culbertson",
        "Mark R. Miller",
        "Alex Olwal",
        "Sean Follmer"
      ],
      "abstract": "Ungrounded haptic devices for virtual reality (VR) applications lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangential to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.",
      "keywords": [
        "grasp",
        "mass perception",
        "weight force",
        "haptics",
        "virtual reality"
      ],
      "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "20 October 2017",
      "citations": "53",
      "isbn": "9781450349819",
      "doi": "10.1145/3126594",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126599",
      "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126599"
    },
    {
      "title": "Redirected Jumping: Perceptual Detection Rates for Curvature Gains",
      "authors": [
        "Sungchul Jung",
        "Christoph W. Borst",
        "Simon Hoermann",
        "Robert W. Lindeman"
      ],
      "abstract": "Redirected walking (RDW) techniques provide a way to explore a virtual space that is larger than the available physical space by imperceptibly manipulating the virtual world view or motions. These manipulations may introduce conflicts between real and virtual cues (e.g., visual-vestibular conflicts), which can be disturbing when detectable by users. The empirically established detection thresholds of rotation manipulation for RDW still require a large physical tracking space and are therefore impractical for general-purpose Virtual Reality (VR) applications. We investigate Redirected Jumping (RDJ) as a new locomotion metaphor for redirection to partially address this limitation, and because jumping is a common interaction for environments like games. We investigated the detection rates for different curvature gains during RDJ. The probability of users detecting RDJ appears substantially lower than that of RDW, meaning designers can get away with greater manipulations with RDJ than with RDW. We postulate that the substantial vertical (up/down) movement present when jumping introduces increased vestibular noise compared to normal walking, thereby supporting greater rotational manipulations. Our study suggests that the potential combination of metaphors (e.g., walking and jumping) could further reduce the required physical space for locomotion in VR. We also summarize some differences in user jumping approaches and provide motion sickness measures in our study.",
      "keywords": [
        "redirected jumping",
        "walking",
        "sensory thresholds",
        "virtual reality",
        "user-computer interface",
        "psychomotor performance"
      ],
      "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "17 October 2019",
      "citations": "2",
      "isbn": "9781450368162",
      "doi": "10.1145/3332165",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347868",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347868"
    },
    {
      "title": "Virtual Replicas for Remote Assistance in Virtual and Augmented Reality",
      "authors": [
        "Ohan Oda",
        "Carmine Elvezio",
        "Mengu Sukan",
        "Steven Feiner",
        "Barbara Tversky"
      ],
      "abstract": "In many complex tasks, a remote subject-matter expert may need to assist a local user to guide actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We introduce two approaches that use Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display. Both approaches allow the expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. In one approach, the expert points in 3D to portions of virtual replicas to annotate them. In another approach, the expert demonstrates actions in 3D by manipulating virtual replicas, supported by constraints and annotations. We performed a user study of a 6DOF alignment task, a key operation in many physical task domains, comparing both approaches to an approach in which the expert uses a 2D tablet-based drawing system similar to ones developed for prior work on remote assistance. The study showed the 3D demonstration approach to be faster than the others. In addition, the 3D pointing approach was faster than the 2D tablet in the case of a highly trained expert.",
      "keywords": [
        "maintenance and repair",
        "assembly",
        "3D referencing techniques",
        "remote task assistance",
        "collaborative mixed/augmented reality",
        "remote guidance"
      ],
      "published_in": "UIST '15: Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology",
      "publication_date": "5 November 2015",
      "citations": "50",
      "isbn": "9781450337793",
      "doi": "10.1145/2807442",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2807442.2807497",
      "paper_url": "https://dl.acm.org/doi/10.1145/2807442.2807497"
    },
    {
      "title": "INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments",
      "authors": [
        "Anil \u00c7amc\u0131",
        "Kristine Lee",
        "Cody J. Roberts",
        "Angus G. Forbes"
      ],
      "abstract": "The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes.",
      "keywords": [
        "design environment",
        "virtual sonic environments",
        "browser-based interface",
        "virtual reality",
        "3D audio"
      ],
      "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "20 October 2017",
      "citations": "4",
      "isbn": "9781450349819",
      "doi": "10.1145/3126594",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126644",
      "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126644"
    },
    {
      "title": "FaceTouch: Enabling Touch Interaction in Display Fixed UIs for Mobile Virtual Reality",
      "authors": [
        "Jan Gugenheimer",
        "David Dobbelstein",
        "Christian Winkler",
        "Gabriel Haas",
        "Enrico Rukzio"
      ],
      "abstract": "We present FaceTouch, a novel interaction concept for mobile Virtual Reality (VR) head-mounted displays (HMDs) that leverages the backside as a touch-sensitive surface. With FaceTouch, the user can point at and select virtual content inside their field-of-view by touching the corresponding location at the backside of the HMD utilizing their sense of proprioception. This allows for rich interaction (e.g. gestures) in mobile and nomadic scenarios without having to carry additional accessories (e.g. a gamepad). We built a prototype of FaceTouch and conducted two user studies. In the first study we measured the precision of FaceTouch in a display-fixed target selection task using three different selection techniques showing a low error rate of 2% indicate the viability for everyday usage. To asses the impact of different mounting positions on the user performance we conducted a second study. We compared three mounting positions of the touchpad (face, hand and side) showing that mounting the touchpad at the back of the HMD resulted in a significantly lower error rate, lower selection time and higher usability. Finally, we present interaction techniques and three example applications that explore the FaceTouch design space.",
      "keywords": [
        "virtual reality",
        "back-of-device interaction",
        "vr interaction",
        "vr input",
        "nomadic vr",
        "mobile vr"
      ],
      "published_in": "UIST '16: Proceedings of the 29th Annual Symposium on User Interface Software and Technology",
      "publication_date": "16 October 2016",
      "citations": "43",
      "isbn": "9781450341899",
      "doi": "10.1145/2984511",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2984511.2984576",
      "paper_url": "https://dl.acm.org/doi/10.1145/2984511.2984576"
    },
    {
      "title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments",
      "authors": [
        "Roberto A. Montano Murillo",
        "Sriram Subramanian",
        "Diego Martinez Plasencia"
      ],
      "abstract": "Interaction in VR involves large body movements, easily inducing fatigue and discomfort. We propose Erg-O, a manipulation technique that leverages visual dominance to maintain the visual location of the elements in VR, while making them accessible from more comfortable locations. Our solution works in an open-ended fashion (no prior knowledge of the object the user wants to touch), can be used with multiple objects, and still allows interaction with any other point within user's reach. We use optimization approaches to compute the best physical location to interact with each visual element, and space partitioning techniques to distort the visual and physical spaces based on those mappings and allow multi-object retargeting. In this paper we describe the Erg-O technique, propose two retargeting strategies and report the results from a user study on 3D selection under different conditions, elaborating on their potential and application to specific usage scenarios.",
      "keywords": ["optimization", "virtual reality", "ergonomics"],
      "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "20 October 2017",
      "citations": "11",
      "isbn": "9781450349819",
      "doi": "10.1145/3126594",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126605",
      "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126605"
    },
    {
      "title": "HairBrush for Immersive Data-Driven Hair Modeling",
      "authors": [
        "Jun Xing",
        "Koki Nagano",
        "Weikai Chen",
        "Haotian Xu",
        "Li-Yi Wei",
        "Yajie Zhao",
        "Jingwan Lu",
        "Byungmoon Kim",
        "Hao Li"
      ],
      "abstract": "While hair is an essential component of virtual humans, it is also one of the most challenging digital assets to create. Existing automatic techniques lack the generality and flexibility to create rich hair variations, while manual authoring interfaces often require considerable artistic skills and efforts, especially for intricate 3D hair structures that can be difficult to navigate. We propose an interactive hair modeling system that can help create complex hairstyles in minutes or hours that would otherwise take much longer with existing tools. Modelers, including novice users, can focus on the overall hairstyles and local hair deformations, as our system intelligently suggests the desired hair parts. Our method combines the flexibility of manual authoring and the convenience of data-driven automation. Since hair contains intricate 3D structures such as buns, knots, and strands, they are inherently challenging to create using traditional 2D interfaces. Our system provides a new 3D hair authoring interface for immersive interaction in virtual reality (VR). Users can draw high-level guide strips, from which our system predicts the most plausible hairstyles via a deep neural network trained from a professionally curated dataset. Each hairstyle in our dataset is composed of multiple variations, serving as blend-shapes to fit the user drawings via global blending and local deformation. The fitted hair models are visualized as interactive suggestions that the user can select, modify, or ignore. We conducted a user study to confirm that our system can significantly reduce manual labor while improve the output quality for modeling a variety of head and facial hairstyles that are challenging to create via existing techniques.",
      "keywords": [
        "machine learning",
        "user interface",
        "hair",
        "data-driven",
        "modeling",
        "virtual reality"
      ],
      "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "17 October 2019",
      "citations": "0",
      "isbn": "9781450368162",
      "doi": "10.1145/3332165",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347876",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347876"
    },
    {
      "title": "Demonstration of TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction",
      "authors": [
        "Jaeyeon Lee",
        "Mike Sinclair",
        "Mar Gonzalez-Franco",
        "Eyal Ofek",
        "Christian Holz"
      ],
      "abstract": "Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. We demonstrate the TORC interaction scenarios for a virtual object in hand.",
      "keywords": [
        "haptic texture",
        "haptics",
        "haptic compliance",
        "vr object manipulation"
      ],
      "published_in": "UIST '19: The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "14 October 2019",
      "citations": "2",
      "isbn": "9781450368179",
      "doi": "10.1145/3332167",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332167.3356898",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332167.3356898"
    },
    {
      "title": "Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift",
      "authors": ["Tiare Feuchtner", "J\u00f6rg M\u00fcller"],
      "abstract": "We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.",
      "keywords": ["virtual hand illusion", "body ownership"],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "2",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242594",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242594"
    },
    {
      "title": "DodecaPen: Accurate 6DoF Tracking of a Passive Stylus",
      "authors": [
        "Po-Chen Wu",
        "Robert Wang",
        "Kenrick Kin",
        "Christopher Twigg",
        "Shangchen Han",
        "Ming-Hsuan Yang",
        "Shao-Yi Chien"
      ],
      "abstract": "We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications. Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment. We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems. We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.",
      "keywords": [
        "6DoF pose tracking",
        "binary square markers",
        "mixed reality"
      ],
      "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "20 October 2017",
      "citations": "12",
      "isbn": "9781450349819",
      "doi": "10.1145/3126594",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126664",
      "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126664"
    },
    {
      "title": "Collaborative Virtual Reality for Low-Latency Interaction",
      "authors": [
        "Carmine Elvezio",
        "Frank Ling",
        "Jen-Shuo Liu",
        "Steven Feiner"
      ],
      "abstract": "In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.",
      "keywords": [
        "collaboration",
        "rehabilitation",
        "virtual reality",
        "games"
      ],
      "published_in": "UIST '18 Adjunct: The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings",
      "publication_date": "11 October 2018",
      "citations": "2",
      "isbn": "9781450359498",
      "doi": "10.1145/3266037",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3266037.3271643",
      "paper_url": "https://dl.acm.org/doi/10.1145/3266037.3271643"
    },
    {
      "title": "TurkDeck: Physical Virtual Reality Based on People",
      "authors": [
        "Lung-Pan Cheng",
        "Thijs Roumen",
        "Hannes Rantzsch",
        "Sven K\u00f6hler",
        "Patrick Schmidt",
        "Robert Kovacs",
        "Johannes Jasper",
        "Jonas Kemper",
        "Patrick Baudisch"
      ],
      "abstract": "TurkDeck is an immersive virtual reality system that reproduces not only what users see and hear, but also what users feel. TurkDeck produces the haptic sensation using props, i.e., when users touch or manipulate an object in the virtual world, they simultaneously also touch or manipulate a corresponding object in the physical world. Unlike previous work on prop-based virtual reality, however, TurkDeck allows creating arbitrarily large virtual worlds in finite space and using a finite set of physical props. The key idea behind TurkDeck is that it creates these physical representations on the fly by making a group of human workers present and operate the props only when and where the user can actually reach them. TurkDeck manages these so-called \"human actuators\" by displaying visual instructions that tell the human actuators when and where to place props and how to actuate them. We demonstrate TurkDeck at the example of an immersive 300m2 experience in 25m2 physical space. We show how to simulate a wide range of physical objects and effects, including walls, doors, ledges, steps, beams, switches, stompers, portals, zip lines, and wind. In a user study, participants rated the realism/immersion of TurkDeck higher than a traditional prop-less baseline condition (4.9 vs. 3.6 on 7 item Likert).",
      "keywords": ["passive virtual reality", "prop-based virtual reality"],
      "published_in": "UIST '15: Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology",
      "publication_date": "5 November 2015",
      "citations": "71",
      "isbn": "9781450337793",
      "doi": "10.1145/2807442",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2807442.2807463",
      "paper_url": "https://dl.acm.org/doi/10.1145/2807442.2807463"
    },
    {
      "title": "Virtual shelves: interactions with orientation aware devices",
      "authors": ["Frank Chun Yat Li", "David Dearman", "Khai N. Truong"],
      "abstract": "Triggering shortcuts or actions on a mobile device often requires a long sequence of key presses. Because the functions of buttons are highly dependent on the current application's context, users are required to look at the display during interaction, even in many mobile situations when eyes-free interactions may be preferable. We present Virtual Shelves, a technique to trigger programmable shortcuts that leverages the user's spatial awareness and kinesthetic memory. With Virtual Shelves, the user triggers shortcuts by orienting a spatially-aware mobile device within the circular hemisphere in front of her. This space is segmented into definable and selectable regions along the phi and theta planes. We show that users can accurately point to 7 regions on the theta and 4 regions on the phi plane using only their kinesthetic memory. Building upon these results, we then evaluate a proof-of-concept prototype of the Virtual Shelves using a Nokia N93. The results show that Virtual Shelves is faster than the N93's native interface for common mobile phone tasks.",
      "keywords": [
        "mobile computing",
        "spatial memory",
        "kinesthetic memory",
        "spatially aware devices"
      ],
      "published_in": "UIST '09: Proceedings of the 22nd annual ACM symposium on User interface software and technology",
      "publication_date": "4 October 2009",
      "citations": "66",
      "isbn": "9781605587455",
      "doi": "10.1145/1622176",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1622176.1622200",
      "paper_url": "https://dl.acm.org/doi/10.1145/1622176.1622200"
    },
    {
      "title": "Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality",
      "authors": [
        "Haijun Xia",
        "Sebastian Herscher",
        "Ken Perlin",
        "Daniel Wigdor"
      ],
      "abstract": "Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users' agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.",
      "keywords": [
        "computer-supported collaborative work",
        "interaction techniques",
        "virtual reality"
      ],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "6",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242597",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242597"
    },
    {
      "title": "Portal-ble: Intuitive Free-hand Manipulation in Unbounded Smartphone-based Augmented Reality",
      "authors": [
        "Jing Qian",
        "Jiaju Ma",
        "Xiangyu Li",
        "Benjamin Attal",
        "Haoming Lai",
        "James Tompkin",
        "John F. Hughes",
        "Jeff Huang"
      ],
      "abstract": "Smartphone augmented reality (AR) lets users interact with physical and virtual spaces simultaneously. With 3D hand tracking, smartphones become apparatus to grab and move virtual objects directly. Based on design considerations for interaction, mobility, and object appearance and physics, we implemented a prototype for portable 3D hand tracking using a smartphone, a Leap Motion controller, and a computation unit. Following an experience prototyping procedure, 12 researchers used the prototype to help explore usability issues and define the design space. We identified issues in perception (moving to the object, reaching for the object), manipulation (successfully grabbing and orienting the object), and behavioral understanding (knowing how to use the smartphone as a viewport). To overcome these issues, we designed object-based feedback and accommodation mechanisms and studied their perceptual and behavioral effects via two tasks: picking up distant objects, and assembling a virtual house from blocks. Our mechanisms enabled significantly faster and more successful user interaction than the initial prototype in picking up and manipulating stationary and moving objects, with a lower cognitive load and greater user preference. The resulting system---Portal-ble---improves user intuition and aids free-hand interactions in mobile situations.",
      "keywords": [
        "augmented reality",
        "mid-air gesture",
        "3d hand manipulation",
        "smartphone"
      ],
      "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "17 October 2019",
      "citations": "4",
      "isbn": "9781450368162",
      "doi": "10.1145/3332165",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347904",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347904"
    },
    {
      "title": "Haptic Feedback to the Palm and Fingers for Improved Tactile Perception of Large Objects",
      "authors": ["Bukun Son", "Jaeyoung Park"],
      "abstract": "When one manipulates a large or bulky object, s/he utilizes tactile information at both fingers and the palm. Our goal is to efficiently convey contact information to a user's hand during interaction with a virtual object. We propose a haptic system that can provide haptic feedback to thumb/middle finger/index finger and on a palm. Our interface design utilizes a novel compact mechanism to provide haptic information to the palm. Also, we propose a haptic rendering strategy to calculate haptic feedback continuously. We demonstrate that cutaneous feedback on the palm improves the haptic perception of a large virtual object compared to when there is only kinesthetic feedback to the fingers.",
      "keywords": ["haptics", "wearable interface", "global shape perception"],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "7",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242656",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242656"
    },
    {
      "title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints",
      "authors": [
        "Devamardeep Hayatpur",
        "Seongkook Heo",
        "Haijun Xia",
        "Wolfgang Stuerzlinger",
        "Daniel Wigdor"
      ],
      "abstract": "We present Plane, Ray, and Point, a set of interaction techniques that utilizes shape constraints to enable quick and precise object alignment and manipulation in virtual reality. Users create the three types of shape constraints, Plane, Ray, and Point, by using symbolic gestures. The shape constraints are used like scaffoldings and limit and guide the movement of virtual objects that collide or intersect with them. The same set of gestures can be performed with the other hand, which allow users to further control the degrees of freedom for precise and constrained manipulation. The combination of shape constraints and bimanual gestures yield a rich set of interaction techniques to support object transformation. An exploratory study conducted with 3D design experts and novice users found the techniques to be useful in 3D scene design workflows and easy to learn and use.",
      "keywords": [
        "precise object manipulation",
        "3d object manipulation",
        "shape gestures",
        "constraints separation"
      ],
      "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "17 October 2019",
      "citations": "1",
      "isbn": "9781450368162",
      "doi": "10.1145/3332165",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347916",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347916"
    },
    {
      "title": "3D puppetry: a kinect-based interface for 3D animation",
      "authors": [
        "Robert Held",
        "Ankit Gupta",
        "Brian Curless",
        "Maneesh Agrawala"
      ],
      "abstract": "We present a system for producing 3D animations using physical objects (i.e., puppets) as input. Puppeteers can load 3D models of familiar rigid objects, including toys, into our system and use them as puppets for an animation. During a performance, the puppeteer physically manipulates these puppets in front of a Kinect depth sensor. Our system uses a combination of image-feature matching and 3D shape matching to identify and track the physical puppets. It then renders the corresponding 3D models into a virtual set. Our system operates in real time so that the puppeteer can immediately see the resulting animation and make adjustments on the fly. It also provides 6D virtual camera \\\\rev{and lighting} controls, which the puppeteer can adjust before, during, or after a performance. Finally our system supports layered animations to help puppeteers produce animations in which several characters move at the same time. We demonstrate the accessibility of our system with a variety of animations created by puppeteers with no prior animation experience.",
      "keywords": ["object tracking", "animation", "tangible user interface"],
      "published_in": "UIST '12: Proceedings of the 25th annual ACM symposium on User interface software and technology",
      "publication_date": "7 October 2012",
      "citations": "68",
      "isbn": "9781450315807",
      "doi": "10.1145/2380116",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2380116.2380170",
      "paper_url": "https://dl.acm.org/doi/10.1145/2380116.2380170"
    },
    {
      "title": "Evaluation of Interaction Techniques for a Virtual Reality Reading Room in Diagnostic Radiology",
      "authors": [
        "Markus Wirth",
        "Stefan Gradl",
        "Jan Sembdner",
        "Soeren Kuhrt",
        "Bjoern M. Eskofier"
      ],
      "abstract": "Today, radiologists diagnose three dimensional medical data using two dimensional displays. When designing environments with optimal conditions for such a process various aspects like contrast, screen reflection and background light have to be considered. As shown in previous research, applying virtual environments in combination with a Head-Mounted Display for diagnostic imaging provides potential benefits to reduce issues of bad posture and diagnostic mistakes. However, there is little research in exploring the usability and user experience of such beneficial environments. In this work we designed and evaluated different means of interaction to increase radiologists' performance. Therefore we created a virtual reality radiology reading room and employed it to evaluate three different interaction techniques. These allow a direct, semi-direct and indirect manipulation for performing scrolling- and windowing- tasks which are the most important for a radiologist. A study including nine radiologists was conducted and evaluated using the User Experience Questionnaire. Results indicate that direct manipulation is the preferred interaction technique, it outscored the other two control possibilities in attractiveness and pragmatic quality.",
      "keywords": [
        "virtual environment",
        "direct interaction",
        "diagnostic radiology",
        "interaction techniques",
        "indirect interaction",
        "virtual reality"
      ],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "4",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242636",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242636"
    },
    {
      "title": "Mutual Human Actuation",
      "authors": ["Lung-Pan Cheng", "Sebastian Marwecki", "Patrick Baudisch"],
      "abstract": "Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1) offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2) synchronizing the two users' timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.",
      "keywords": ["haptic turk", "virtual reality", "haptics", "immersion"],
      "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "20 October 2017",
      "citations": "24",
      "isbn": "9781450349819",
      "doi": "10.1145/3126594",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126667",
      "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126667"
    },
    {
      "title": "ILoveSketch: as-natural-as-possible sketching system for creating 3d curve models",
      "authors": ["Seok-Hyung Bae", "Ravin Balakrishnan", "Karan Singh"],
      "abstract": "We present ILoveSketch, a 3D curve sketching system that captures some of the affordances of pen and paper for professional designers, allowing them to iterate directly on concept 3D curve models. The system coherently integrates existing techniques of sketch-based interaction with a number of novel and enhanced features. Novel contributions of the system include automatic view rotation to improve curve sketchability, an axis widget for sketch surface selection, and implicitly inferred changes between sketching techniques. We also improve on a number of existing ideas such as a virtual sketchbook, simplified 2D and 3D view navigation, multi-stroke NURBS curve creation, and a cohesive gesture vocabulary. An evaluation by a professional designer shows the potential of our system for deployment within a real design process.",
      "keywords": [
        "sketchability",
        "implicit mode change",
        "axis widget",
        "sketch-based modeling",
        "product design",
        "3d curve"
      ],
      "published_in": "UIST '08: Proceedings of the 21st annual ACM symposium on User interface software and technology",
      "publication_date": "19 October 2008",
      "citations": "150",
      "isbn": "9781595939753",
      "doi": "10.1145/1449715",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1449715.1449740",
      "paper_url": "https://dl.acm.org/doi/10.1145/1449715.1449740"
    },
    {
      "title": "Mixed-Reality for Object-Focused Remote Collaboration",
      "authors": ["Martin Feick", "Anthony Tang", "Scott Bateman"],
      "abstract": "In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators' perspectives on the object as well as understand one another's perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other's perspective; (2) explore objects independently from one another, and (3) render gestures in the remote's workspace. In this work, we focus on the expert's role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.",
      "keywords": [
        "ar",
        "mixed reality",
        "vr",
        "object-focused remote collaboration"
      ],
      "published_in": "UIST '18 Adjunct: The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings",
      "publication_date": "11 October 2018",
      "citations": "0",
      "isbn": "9781450359498",
      "doi": "10.1145/3266037",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3266037.3266102",
      "paper_url": "https://dl.acm.org/doi/10.1145/3266037.3266102"
    },
    {
      "title": "Eye&amp;Head: Synergetic Eye and Head Movement for Gaze Pointing and Selection",
      "authors": ["Ludwig Sidenmark", "Hans Gellersen"],
      "abstract": "Eye gaze involves the coordination of eye and head movement to acquire gaze targets, but existing approaches to gaze pointing are based on eye-tracking in abstraction from head motion. We propose to leverage the synergetic movement of eye and head, and identify design principles for Eye&amp;Head gaze interaction. We introduce three novel techniques that build on the distinction of head-supported versus eyes-only gaze, to enable dynamic coupling of gaze and pointer, hover interaction, visual exploration around pre-selections, and iterative and fast confirmation of targets. We demonstrate Eye&amp;Head interaction on applications in virtual reality, and evaluate our techniques against baselines in pointing and confirmation studies. Our results show that Eye&amp;Head techniques enable novel gaze behaviours that provide users with more control and flexibility in fast gaze pointing and selection.",
      "keywords": [
        "eye tracking",
        "virtual reality",
        "3d interaction",
        "targetselection",
        "gaze interaction",
        "eye-head coordination"
      ],
      "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "17 October 2019",
      "citations": "6",
      "isbn": "9781450368162",
      "doi": "10.1145/3332165",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347921",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347921"
    },
    {
      "title": "Hands-on math: a page-based multi-touch and pen desktop for technical work and problem solving",
      "authors": [
        "Robert Zeleznik",
        "Andrew Bragdon",
        "Ferdi Adeputra",
        "Hsu-Sheng Ko"
      ],
      "abstract": "Students, scientists and engineers have to choose between the flexible, free-form input of pencil and paper and the computational power of Computer Algebra Systems (CAS) when solving mathematical problems. Hands-On Math is a multi-touch and pen-based system which attempts to unify these approaches by providing virtual paper that is enhanced to recognize mathematical notations as a means of providing in situ access to CAS functionality. Pages can be created and organized on a large pannable desktop, and mathematical expressions can be computed, graphed and manipulated using a set of uni- and bi-manual interactions which facilitate rapid exploration by eliminating tedious and error prone transcription tasks. Analysis of a qualitative pilot evaluation indicates the potential of our approach and highlights usability issues with the novel techniques used.",
      "keywords": [
        "pages",
        "gestures",
        "math",
        "stylus",
        "multi-touch",
        "paper"
      ],
      "published_in": "UIST '10: Proceedings of the 23nd annual ACM symposium on User interface software and technology",
      "publication_date": "3 October 2010",
      "citations": "41",
      "isbn": "9781450302715",
      "doi": "10.1145/1866029",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1866029.1866035",
      "paper_url": "https://dl.acm.org/doi/10.1145/1866029.1866035"
    },
    {
      "title": "Estimating Focused Object using Smooth Pursuit Eye Movements and Interest Points in the Real World",
      "authors": ["Yuto Tamura", "Kentaro Takemura"],
      "abstract": "User calibration is a significant problem in eye-based interaction. To overcome this, several solutions, such as the calibration-free method and implicit user calibration, have been proposed. Pursuits-based interaction is another such solution that has been studied for public screens and virtual reality. It has been applied to select graphical user interfaces (GUIs) because the movements in a GUI can be designed in advance. Smooth pursuit eye movements (smooth pursuits) occur when a user looks at objects in the physical space as well and thus, we propose a method to identify the focused object by using smooth pursuits in the real world. We attempted to determine the focused objects without prior information under several conditions by using the pursuits-based approach and confirmed the feasibility and limitations of the proposed method through experimental evaluations.",
      "keywords": [
        "eye tracking",
        "real world",
        "smooth pursuit eye movement",
        "focused object"
      ],
      "published_in": "UIST '19: The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "14 October 2019",
      "citations": "0",
      "isbn": "9781450368179",
      "doi": "10.1145/3332167",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332167.3357102",
      "paper_url": "https://dl.acm.org/doi/10.1145/3332167.3357102"
    },
    {
      "title": "Imaginary reality gaming: ball games without a ball",
      "authors": [
        "Patrick Baudisch",
        "Henning Pohl",
        "Stefanie Reinicke",
        "Emilia Wittmers",
        "Patrick L\u00fchne",
        "Marius Knaust",
        "Sven K\u00f6hler",
        "Patrick Schmidt",
        "Christian Holz"
      ],
      "abstract": "We present imaginary reality games, i.e., games that mimic the respective real world sport, such as basketball or soccer, except that there is no visible ball. The ball is virtual and players learn about its position only from watching each other act and a small amount of occasional auditory feed-back, e.g., when a person is receiving the ball. Imaginary reality games maintain many of the properties of physical sports, such as unencumbered play, physical exertion, and immediate social interaction between players. At the same time, they allow introducing game elements from video games, such as power-ups, non-realistic physics, and player balancing. Most importantly, they create a new game dynamic around the notion of the invisible ball. To allow players to successfully interact with the invisible ball, we have created a physics engine that evaluates all plausible ball trajectories in parallel, allowing the game engine to select the trajectory that leads to the most enjoyable game play while still favoring skillful play.",
      "keywords": [
        "motion capture",
        "probabilistic",
        "physical gaming",
        "augmented reality gaming",
        "imaginary interfaces"
      ],
      "published_in": "UIST '13: Proceedings of the 26th annual ACM symposium on User interface software and technology",
      "publication_date": "8 October 2013",
      "citations": "26",
      "isbn": "9781450322683",
      "doi": "10.1145/2501988",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2501988.2502012",
      "paper_url": "https://dl.acm.org/doi/10.1145/2501988.2502012"
    },
    {
      "title": "MobiLimb: Augmenting Mobile Devices with a Robotic Limb",
      "authors": [
        "Marc Teyssier",
        "Gilles Bailly",
        "Catherine Pelachaud",
        "Eric Lecolinet"
      ],
      "abstract": "In this paper, we explore the interaction space of MobiLimb, a small 5-DOF serial robotic manipulator attached to a mobile device. It (1) overcomes some limitations of mobile devices (static, passive, motionless); (2) preserves their form factor and I/O capabilities; (3) can be easily attached to or removed from the device; (4) offers additional I/O capabilities such as physical deformation and (5) can support various modular elements such as sensors, lights or shells. We illustrate its potential through three classes of applications: As a tool, MobiLimb offers tangible affordances and an expressive controller that can be manipulated to control virtual and physical objects. As a partner, it reacts expressively to users' actions to foster curiosity and engagement or assist users. As a medium, it provides rich haptic feedback such as strokes, pat and other tactile stimuli on the hand or the wrist to convey emotions during mediated multimodal communications.",
      "keywords": [
        "actuated device",
        "mobile device",
        "mobile augmentation",
        "robotic limb",
        "robotics"
      ],
      "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",
      "publication_date": "11 October 2018",
      "citations": "4",
      "isbn": "9781450359481",
      "doi": "10.1145/3242587",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242626",
      "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242626"
    },
    {
      "title": "Combining multiple depth cameras and projectors for interactions on, above and between surfaces",
      "authors": ["Andrew D. Wilson", "Hrvoje Benko"],
      "abstract": "Instrumented with multiple depth cameras and projectors, LightSpace is a small room installation designed to explore a variety of interactions and computational strategies related to interactive displays and the space that they inhabit. LightSpace cameras and projectors are calibrated to 3D real world coordinates, allowing for projection of graphics correctly onto any surface visible by both camera and projector. Selective projection of the depth camera data enables emulation of interactive displays on un-instrumented surfaces (such as a standard table or office desk), as well as facilitates mid-air interactions between and around these displays. For example, after performing multi-touch interactions on a virtual object on the tabletop, the user may transfer the object to another display by simultaneously touching the object and the destination display. Or the user may \"pick up\" the object by sweeping it into their hand, see it sitting in their hand as they walk over to an interactive wall display, and \"drop\" the object onto the wall by touching it with their other hand. We detail the interactions and algorithms unique to LightSpace, discuss some initial observations of use and suggest future directions.",
      "keywords": [
        "surface computing",
        "augmented reality",
        "depth cameras",
        "ubiquitous computing",
        "interactive spaces"
      ],
      "published_in": "UIST '10: Proceedings of the 23nd annual ACM symposium on User interface software and technology",
      "publication_date": "3 October 2010",
      "citations": "203",
      "isbn": "9781450302715",
      "doi": "10.1145/1866029",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1866029.1866073",
      "paper_url": "https://dl.acm.org/doi/10.1145/1866029.1866073"
    },
    {
      "title": "Interactions in the air: adding further depth to interactive tabletops",
      "authors": [
        "Otmar Hilliges",
        "Shahram Izadi",
        "Andrew D. Wilson",
        "Steve Hodges",
        "Armando Garcia-Mendoza",
        "Andreas Butz"
      ],
      "abstract": "Although interactive surfaces have many unique and compelling qualities, the interactions they support are by their very nature bound to the display surface. In this paper we present a technique for users to seamlessly switch between interacting on the tabletop surface to above it. Our aim is to leverage the space above the surface in combination with the regular tabletop display to allow more intuitive manipulation of digital content in three-dimensions. Our goal is to design a technique that closely resembles the ways we manipulate physical objects in the real-world; conceptually, allowing virtual objects to be 'picked up' off the tabletop surface in order to manipulate their three dimensional position or orientation. We chart the evolution of this technique, implemented on two rear projection-vision tabletops. Both use special projection screen materials to allow sensing at significant depths beyond the display. Existing and new computer vision techniques are used to sense hand gestures and postures above the tabletop, which can be used alongside more familiar multi-touch interactions. Interacting above the surface in this way opens up many interesting challenges. In particular it breaks the direct interaction metaphor that most tabletops afford. We present a novel shadow-based technique to help alleviate this issue. We discuss the strengths and limitations of our technique based on our own observations and initial user feedback, and provide various insights from comparing, and contrasting, our tabletop implementations",
      "keywords": [
        "3D",
        "surfaces",
        "tabletop",
        "depth-sensing cameras",
        "interactive surfaces",
        "computer vision",
        "switchable diffusers",
        "3D graphics",
        "holoscreen"
      ],
      "published_in": "UIST '09: Proceedings of the 22nd annual ACM symposium on User interface software and technology",
      "publication_date": "4 October 2009",
      "citations": "151",
      "isbn": "9781605587455",
      "doi": "10.1145/1622176",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1622176.1622203",
      "paper_url": "https://dl.acm.org/doi/10.1145/1622176.1622203"
    }
  ],
  "total_results": 32,
  "total_filtered_results": 32,
  "total_pages": 1
}
