{"papers": [{"title": "VR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools", "authors": ["Jackie (Junrui) Yang", "Hiroshi Horii", "Alexander Thayer", "Rafael Ballagas"], "abstract": "Haptic feedback in VR is important for realistic simulation in virtual reality. However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system. We propose Ungrounded Haptic Retargeting, an interaction technique that provides a realistic haptic experience for grabbing tools using only passive mechanisms. This technique leverages the ungrounded feedback inherent in grabbing tools combined with dynamic visual adjustments of their position in virtual reality to create an illusion of physical presence for virtual objects. To demonstrate the capabilities of this technique, we created VR Grabbers, an exemplary passive VR controller, similar to training chopsticks, with haptic feedback for precise object selection and manipulation. We conducted two user studies based on VR Grabbers. The first study probed the perceptual limits of the illusion; we found that the maximum position difference between the virtual and physical world acceptable to the user is (-1.48, 1.95) cm. The second study showed that task performance of the VR Grabbers controller with Ungrounded Haptic Retargeting enabled outperforms the same controller with Ungrounded Haptic Retargeting disabled.                     References                 2017. TomorrowTodayLabs/NewtonVR: A virtual reality interaction system for unity based on physics. https://github.com/TomorrowTodayLabs/NewtonVR. (2017). (Accessed on 09/18/2017).Google Scholar2017. VIVE | Discover Virtual Reality Beyond Imagination. https://www.vive.com/us/. (2017). (Accessed on 09/18/2017).Google ScholarMerwan Achibet, Benoit Le Gouis, Maud Marchal, Pierre-Alexandre Leziart, Ferran Argelaguet, Adrien Girard, Anatole Lecuyer, and Hiroyuki Kajimoto. 2017. FlexiFingers: Multi-finger interaction in VR combining passive haptics and pseudo-haptics. In 2017 IEEE Symposium on 3D User Interfaces (3DUI). IEEE.Google ScholarMerwan Achibet, Maud Marchal, Ferran Argelaguet, and Anatole Lecuyer. 2014. The Virtual Mitten: A novel interaction paradigm for visuo-haptic manipulation of objects using grip force. In 2014 IEEE Symposium on 3D User Interfaces (3DUI). IEEE.Google ScholarRyan Arisandi, Mai Otsuki, Asako Kimura, Fumihisa Shibata, and Hideyuki Tamura. 2014. Virtual Handcrafting: Building Virtual Wood Models Using ToolDevice . Proc. IEEE 102, 2 (feb 2014), 185--195.Google ScholarMahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D. Wilson. 2016. Haptic Retargeting. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI textquotesingle16. ACM Press.Google ScholarYuki Ban, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose. 2012. Magic pot. ACM SIGGRAPH 2012 Posters on - SIGGRAPH '12 (2012).Google ScholarYuki Ban, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose. 2015. Air haptics. ACM SIGGRAPH 2015 Emerging Technologies on - SIGGRAPH '15 (2015).Google ScholarLung-Pan Cheng, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D. Wilson. 2017. Sparse Haptic Proxy. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17. ACM Press.Google ScholarInrak Choi, Elliot W. Hawkes, David L. Christensen, Christopher J. Ploch, and Sean Follmer. 2016. Wolverine: A wearable haptic interface for grasping in virtual reality. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE.Google ScholarJohannes Hummel, Janki Dodiya, Robin Wolff, Andreas Gerndt, and Torsten Kuhlen. 2013. An evaluation of two simple methods for representing heaviness in immersive virtual environments. 2013 IEEE Symposium on 3D User Interfaces (3DUI) (Mar 2013).Google ScholarGinga Kato, Yoshihiro Kuroda, Ilana Nisky, Kiyoshi Kiyokawa, and Haruo Takemura. 2017. Design and Psychophysical Evaluation of the HapSticks: A Novel Non-Grounded Mechanism for Presenting Tool-Mediated Vertical Forces. IEEE Transactions on Haptics 10, 3 (Jul 2017), 338--349.Google ScholarY. Kitamura, T. Higashi, T. Masaki, and F. Kishino. Virtual chopsticks: object manipulation using multiple exact interactions. In Proceedings IEEE Virtual Reality (Cat. No. 99CB36316). IEEE Comput. Soc. Google ScholarLuv Kohli. 2010. Redirected touching: Warping space to remap passive haptics. In 2010 IEEE Symposium on 3D User Interfaces (3DUI). IEEE. Google ScholarL. Kohli, M. C. Whitton, and F. P. Brooks. 2012. Redirected touching: The effect of warping space on task performance. In 2012 IEEE Symposium on 3D User Interfaces (3DUI). IEEE.Google ScholarLuv Kohli, Mary C. Whitton, and Frederick P. Brooks. 2013. Redirected Touching: Training and adaptation in warped virtual spaces. In 2013 IEEE Symposium on 3D User Interfaces (3DUI). IEEE.Google ScholarT. Koyama, I. Yamano, K. Takemura, and T. Maeno. Multi-fingered exoskeleton haptic device using passive force feedback for dexterous teleoperation. In IEEE/RSJ International Conference on Intelligent Robots and System. IEEE.Google ScholarA. Lecuyer, S. Coquillart, A. Kheddar, P. Richard, and P. Coiffet. Pseudo-haptic feedback: can isometric input devices simulate force feedback?. In Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048). IEEE Comput. Soc. Google ScholarCyberGlove System LLC. 2017. CyberGrasp. http://www.cyberglovesystems.com/cybergrasp/. (2017). (Accessed on 09/13/2017).Google ScholarDaichi Matsumoto, Keisuke Hasegawa, Yasutoshi Makino, and Hiroyuki Shinoda. 2017. Displaying variable stiffness by passive nonlinear spring using visuo-haptic interaction. In 2017 IEEE World Haptics Conference (WHC). IEEE.Google ScholarYoky Matsuoka, Sonya J Allin, and Roberta L Klatzky. 2002. The tolerance for visual feedback distortions in a virtual environment. Physiology &amp; Behavior 77, 4--5 (Dec 2002), 651--655.Google ScholarNami Ogawa, Takuji Narumi, and Michitaka Hirose. 2017. Distortion in perceived size and body-based scaling in virtual environments. Proceedings of the 8th Augmented Human International Conference on - AH '17 (2017).Google ScholarChristopher Richard and Mark R Cutkosky. 1997. Contact force perception with an ungrounded haptic interface. In Proceedings of the ASME Dynamic Systems and Control Division, Vol. 181. American Society of Mechanical Engineers, 187.Google ScholarThomas B. Sheridan. 1992. Musings on Telepresence and Virtual Presence. Presence: Teleoperators and Virtual Environments 1, 1 (jan 1992), 120--126.Google ScholarF. Steinicke, G. Bruder, J. Jerald, H. Frenz, and M. Lappe. 2010. Estimation of Detection Thresholds for Redirected Walking Techniques. IEEE Transactions on Visualization and Computer Graphics 16, 1 (Jan 2010), 17--27.  Google ScholarKenji Sugihara, Mai Otsuki, Asako Kimura, Fumihisa Shibata, and Hideyuki Tamura. 2011. MAI painting brush++. In Proceedings of the 24th annual ACM symposium adjunct on User interface software and technology - UIST textquotesingle11 Adjunct. ACM Press.Google ScholarYusuke Takami, Mai Otsuki, Asako Kimura, Fumihisa Shibata, and Hideyuki Tamura. 2009. Daichitextquotesingles artworking. In ACM SIGGRAPH ASIA 2009 Art Gallery &amp; Emerging Technologies: Adaptation on - SIGGRAPH ASIA textquotesingle09. ACM Press.Google ScholarA Uesaka, K Fukuda, A Kimura, and F Shibata. 2008. TweezersDevice: A device facilitating pick and move manipulation in spatial works. Adjunct Proc. UIST (2008), 55--56.Google ScholarJulie M. Walker, Michael Raitor, Alex Mallery, Heather Culbertson, Philipp Stolka, and Allison M. Okamura. 2016. A dual-flywheel ungrounded haptic feedback system provides single-axis moment pulses for clear direction signals. In 2016 IEEE Haptics Symposium (HAPTICS). IEEE.Google ScholarBob G. Witmer and Michael J. Singer. 1998. Measuring Presence in Virtual Environments: A Presence Questionnaire. Presence: Teleoperators and Virtual Environments 7, 3 (jun 1998), 225--240.  Google ScholarH. Yano, M. Yoshie, and H. Iwata. Development of a non-grounded haptic interface using the gyro effect. In 11th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2003. HAPTICS 2003. Proceedings. IEEE Comput. Soc. Google ScholarRui Zhang, A. Kunz, P. Lochmatter, and G. Kovacs. Dielectric Elastomer Spring Roll Actuators for a Portable Force Feedback Device. In 2006 14th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. IEEE. Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242643/bf2b3622-343e-4392-aff2-9bdd5882632d/ufp1388p.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc04507775e60bcb84ed1d38e43d0202f2e0e125d9261aa36bfd81988ffd56893ff49c1d8ace0f918a5d84b8669b8b3330c2aaf3caa1202313cbdf0cb79b41application/x-mpegurlmp48.1 MB", "keywords": ["grabbing tools", "virtual reality", "passive haptics", "haptic illusions"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "4", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242643", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242643"}, {"title": "PuPoP: Pop-up Prop on Palm for Virtual Reality", "authors": ["Shan-Yuan Teng", "Tzu-Sheng Kuo", "Chi Wang", "Chi-Huan Chiang", "Da-Yuan Huang", "Liwei Chan", "Bing-Yu Chen"], "abstract": "The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.                     References                 2018. Cybergrasp, CyberGlove Systems Inc. http://www.cyberglovesystems.com/cybergrasp/. (2018). Accessed: March 19, 2018.Google ScholarMahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D. Wilson. 2016. Haptic Retargeting: Dynamic Repurposing of Passive Haptics for Enhanced Virtual Reality Experiences. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 1968--1979.  Google ScholarY. Ban, T. Kajinami, T. Narumi, T. Tanikawa, and M. Hirose. 2012a. Modifying an identified curved surface shape using pseudo-haptic effect. In 2012 IEEE Haptics Symposium (HAPTICS). 211--216.Google ScholarYuki Ban, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose. 2012b. Modifying an Identified Position of Edged Shapes Using Pseudo-haptic Effects. In Proceedings of the 18th ACM Symposium on Virtual Reality Software and Technology (VRST '12). ACM, New York, NY, USA, 93--96.  Google ScholarHrvoje Benko, Christian Holz, Mike Sinclair, and Eyal Ofek. 2016. NormalTouch and TextureTouch: High-fidelity 3D Haptic Shape Rendering on Handheld Virtual Reality Controllers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 717--728.  Google ScholarM. Bouzit, G. Burdea, G. Popescu, and R. Boian. 2002. The Rutgers Master II-new design force-feedback glove. IEEE/ASME Transactions on Mechatronics 7, 2 (June 2002), 256--263.Google ScholarBrent Edward Insko. 2001. Passive haptics significantly enhances virtual environments. Ph.D. Dissertation. http://www.cs.unc.edu/techreports/01-017.pdfGoogle ScholarLung-Pan Cheng, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D. Wilson. 2017. Sparse Haptic Proxy: Touch Feedback in Virtual Environments Using a General Passive Prop. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 3718--3728.  Google ScholarInrak Choi, Heather Culbertson, Mark R. Miller, Alex Olwal, and Sean Follmer. 2017. Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (UIST '17). ACM, New York, NY, USA, 119--130.  Google ScholarInrak Choi and Sean Follmer. 2016. Wolverine: A Wearable Haptic Interface for Grasping in VR. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16 Adjunct). ACM, New York, NY, USA, 117--119.  Google ScholarNeil A. Dodgson. 2004. Variation and extrema of human interpupillary distance. In Stereoscopic Displays and Virtual Reality Systems XI, Vol. 5291. International Society for Optics and Photonics, 36--47.Google ScholarSean Follmer, Daniel Leithinger, Alex Olwal, Nadia Cheng, and Hiroshi Ishii. 2012. Jamming user interfaces: programmable particle stiffness and sensing for malleable and shape-changing devices. In Proceedings of the 25th annual ACM symposium on User interface software and technology. ACM, 519--528.  Google ScholarEisuke Fujinawa, Shigeo Yoshida, Yuki Koyama, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose. 2017. Computational Design of Hand-held VR Controllers Using Haptic Shape Illusion. In Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology (VRST '17). ACM, New York, NY, USA, 28:1--28:10.  Google ScholarJames J. Gibson. 1986. The theory of affordances. The Ecological Approach To Visual Perception (1986), 127--143.Google ScholarChris Harrison and Scott E. Hudson. 2009. Providing Dynamically Changeable Physical Buttons on a Visual Display. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '09). ACM, New York, NY, USA, 299--308.  Google ScholarFabian Hemmert, Susann Hamann, Matthias L\u00f6we, Anne Wohlauf, and Gesche Joost. 2010a. Shape-changing Mobiles: Tapering in One-dimensional Deformational Displays in Mobile Phones. In Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction (TEI '10). ACM, New York, NY, USA, 249--252.  Google ScholarFabian Hemmert, Susann Hamann, Matthias L\u00f6we, Anne Wohlauf, Josefine Zeipelt, and Gesche Joost. 2010b. Take Me by the Hand: Haptic Compasses in Mobile Devices Through Shape Change and Weight Shift. In Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries (NordiCHI '10). ACM, New York, NY, USA, 671--674.  Google ScholarAnuruddha Hettiarachchi and Daniel Wigdor. 2016. Annexing Reality: Enabling Opportunistic Use of Everyday Objects As Tangible Proxies in Augmented Reality. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 1957--1967.  Google ScholarKen Hinckley, Randy Pausch, John C. Goble, and Neal F. Kassell. 1994. Passive Real-world Interface Props for Neurosurgical Visualization. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '94). ACM, New York, NY, USA, 452--458.  Google ScholarH. G. Hoffman. 1998. Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments. In Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No. 98CB36180). 59--63. Google ScholarDavid Holman and Roel Vertegaal. 2008. Organic User Interfaces: Designing Computers in Any Way, Shape, or Form . Commun. ACM 51, 6 (June 2008), 48--55.  Google ScholarDa-Yuan Huang, Ruizhen Guo, Jun Gong, Jingxian Wang, John Graham, De-Nian Yang, and Xing-Dong Yang. 2017. RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (UIST '17). ACM, New York, NY, USA, 539--551.  Google ScholarHiroo Iwata, Hiroaki Yano, and Naoto Ono. 2005. Volflex. In ACM SIGGRAPH 2005 Emerging Technologies (SIGGRAPH '05). ACM, New York, NY, USA.  Google ScholarSeoktae Kim, Hyunjung Kim, Boram Lee, Tek-Jin Nam, and Woohun Lee. 2008. Inflatable Mouse: Volume-adjustable Mouse with Air-pressure-sensitive Input and Haptic Feedback. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08). ACM, New York, NY, USA, 211--224.  Google ScholarE. Kwon, G. J. Kim, and S. Lee. 2009. Effects of sizes and shapes of props in tangible augmented reality. In 2009 8th IEEE International Symposium on Mixed and Augmented Reality. 201--202.  Google ScholarLi-Ke Ma, Yizhonc Zhang, Yang Liu, Kun Zhou, and Xin Tong. 2017. Computational Design and Fabrication of Soft Pneumatic Objects with Desired Deformations . ACM Trans. Graph. 36, 6 (Nov. 2017), 239:1--239:12.  Google ScholarC. L. MacKenzie and T. Iberall. 1994. The Grasping Hand . Elsevier.Google ScholarRyuma Niiyama, Xu Sun, Lining Yao, Hiroshi Ishii, Daniela Rus, and Sangbae Kim. 2015. Sticky Actuator: Free-Form Planar Actuators for Animated Objects. In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction (TEI '15). ACM, New York, NY, USA, 77--84.  Google ScholarJifei Ou, M\u00e9lina Skouras, Nikolaos Vlavianos, Felix Heibeck, Chin-Yi Cheng, Jannik Peters, and Hiroshi Ishii. 2016. aeroMorph - Heat-sealing Inflatable Shape-change Materials for Interaction Design (UIST '16). ACM Press, 121--132.  Google ScholarIrvin Rock and Jack Victor. 1964. Vision and Touch: An Experimentally Created Conflict between the Two Senses . Science 143, 3606 (Feb. 1964), 594--596.Google ScholarHarpreet Sareen, Udayan Umapathi, Patrick Shin, Yasuaki Kakehi, Jifei Ou, Hiroshi Ishii, and Pattie Maes. 2017. Printflatables: Printing Human-Scale, Functional and Dynamic Inflatable Objects. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 3669--3680.  Google ScholarSamuel B. Schorr and Allison M. Okamura. 2017. Fingertip Tactile Devices for Virtual Object Manipulation and Exploration. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 3115--3119.  Google ScholarJia Sheng, Ravin Balakrishnan, and Karan Singh. 2006. An Interface for Virtual 3D Sculpting via Physical Proxy. In Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia (GRAPHITE '06). ACM, New York, NY, USA, 213--220.  Google ScholarAdalberto L. Simeone, Eduardo Velloso, and Hans Gellersen. 2015. Substitutional Reality: Using the Physical Environment to Design Virtual Reality Experiences. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA, 3307--3316.  Google ScholarTimothy M. Simon, Ross T. Smith, and Bruce H. Thomas. 2014. Wearable Jamming Mitten for Virtual Environment Haptics. In Proceedings of the 2014 ACM International Symposium on Wearable Computers (ISWC '14). ACM, New York, NY, USA, 67--70.  Google ScholarM\u00e9lina Skouras, Bernhard Thomaszewski, Peter Kaufmann, Akash Garg, Bernd Bickel, Eitan Grinspun, and Markus Gross. 2014. Designing Inflatable Structures . ACM Trans. Graph. 33, 4 (July 2014), 63:1--63:10.  Google ScholarS. H. Winter and M. Bouzit. 2007. Use of Magnetorheological Fluid in a Force Feedback Glove . IEEE Transactions on Neural Systems and Rehabilitation Engineering 15, 1 (March 2007), 2--8.Google ScholarLining Yao, Ryuma Niiyama, Jifei Ou, Sean Follmer, Clark Della Silva, and Hiroshi Ishii. 2013. PneUI: pneumatically actuated soft composite materials for shape changing interfaces (UIST '13). ACM Press, 13--22.  Google ScholarA. Zenner and A. Kr\u00fcger. 2017. Shifty: A Weight-Shifting Dynamic Passive Haptic Proxy to Enhance Object Perception in Virtual Reality . IEEE Transactions on Visualization and Computer Graphics 23, 4 (April 2017), 1285--1294.  Google ScholarYiwei Zhao, Lawrence H. Kim, Ye Wang, Mathieu Le Goc, and Sean Follmer. 2017. Robotic Assembly of Haptic Proxy Objects for Tangible Interaction and Virtual Reality. In Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces (ISS '17). ACM, New York, NY, USA, 82--91.  Google ScholarIgor Zubrycki and Grzegorz Granosik. 2017. Novel Haptic Device Using Jamming Principle for Providing Kinaesthetic Feedback in Glove-Based Control Interface . Journal of Intelligent &amp; Robotic Systems 85, 3--4 (March 2017), 413--429.  Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242628/34876366-0a2f-4787-9457-041d210482c3/p5-teng.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc0450717ee65a998eb89ae74cacc300d3883ba4c9d18014ef45813aef55008ca97ce2c272e55fd2b578818d94011f855fba5cdbc84d74095fe1970ff3e1f7application/x-mpegurlmp4208.7 MB", "keywords": ["shape-proxy", "haptics", "airbag", "virtual reality"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "17", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242628", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242628"}, {"title": "MagicalHands: Mid-Air Hand Gestures for Animating in VR", "authors": ["Rahul Arora", "Rubaiat Habib Kazi", "Danny M. Kaufman", "Wilmot Li", "Karan Singh"], "abstract": "We explore the use of hand gestures for authoring animations in virtual reality (VR). We first perform a gesture elicitation study to understand user preferences for a spatiotemporal, bare-handed interaction system in VR. Specifically, we focus on creating and editing dynamic, physical phenomena (e.g., particle systems, deformations, coupling), where the mapping from gestures to animation is ambiguous and indirect. We present commonly observed mid-air gestures from the study that cover a wide range of interaction techniques, from direct manipulation to abstract demonstrations. To this end, we extend existing gesture taxonomies to the rich spatiotemporal interaction space of the target domain and distill our findings into a set of guidelines that inform the design of natural user interfaces for VR animation. Finally, based on our guidelines, we develop a proof-of-concept gesture-based VR animation system, MagicalHands. Our results, as well as feedback from user evaluation, suggest that the expressive qualities of hand gestures help users animate more effectively in VR.", "keywords": ["gesture elicitation", "virtual reality", "animation", "hand gestures"], "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "17 October 2019", "citations": "4", "isbn": "9781450368162", "doi": "10.1145/3332165", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347942", "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347942"}, {"title": "Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality", "authors": ["Inrak Choi", "Heather Culbertson", "Mark R. Miller", "Alex Olwal", "Sean Follmer"], "abstract": "Ungrounded haptic devices for virtual reality (VR) applications lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Grabity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangential to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.", "keywords": ["grasp", "mass perception", "weight force", "haptics", "virtual reality"], "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology", "publication_date": "20 October 2017", "citations": "53", "isbn": "9781450349819", "doi": "10.1145/3126594", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126599", "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126599"}, {"title": "Redirected Jumping: Perceptual Detection Rates for Curvature Gains", "authors": ["Sungchul Jung", "Christoph W. Borst", "Simon Hoermann", "Robert W. Lindeman"], "abstract": "Redirected walking (RDW) techniques provide a way to explore a virtual space that is larger than the available physical space by imperceptibly manipulating the virtual world view or motions. These manipulations may introduce conflicts between real and virtual cues (e.g., visual-vestibular conflicts), which can be disturbing when detectable by users. The empirically established detection thresholds of rotation manipulation for RDW still require a large physical tracking space and are therefore impractical for general-purpose Virtual Reality (VR) applications. We investigate Redirected Jumping (RDJ) as a new locomotion metaphor for redirection to partially address this limitation, and because jumping is a common interaction for environments like games. We investigated the detection rates for different curvature gains during RDJ. The probability of users detecting RDJ appears substantially lower than that of RDW, meaning designers can get away with greater manipulations with RDJ than with RDW. We postulate that the substantial vertical (up/down) movement present when jumping introduces increased vestibular noise compared to normal walking, thereby supporting greater rotational manipulations. Our study suggests that the potential combination of metaphors (e.g., walking and jumping) could further reduce the required physical space for locomotion in VR. We also summarize some differences in user jumping approaches and provide motion sickness measures in our study.", "keywords": ["redirected jumping", "walking", "sensory thresholds", "virtual reality", "user-computer interface", "psychomotor performance"], "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "17 October 2019", "citations": "2", "isbn": "9781450368162", "doi": "10.1145/3332165", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347868", "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347868"}, {"title": "Virtual Replicas for Remote Assistance in Virtual and Augmented Reality", "authors": ["Ohan Oda", "Carmine Elvezio", "Mengu Sukan", "Steven Feiner", "Barbara Tversky"], "abstract": "In many complex tasks, a remote subject-matter expert may need to assist a local user to guide actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We introduce two approaches that use Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display. Both approaches allow the expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. In one approach, the expert points in 3D to portions of virtual replicas to annotate them. In another approach, the expert demonstrates actions in 3D by manipulating virtual replicas, supported by constraints and annotations. We performed a user study of a 6DOF alignment task, a key operation in many physical task domains, comparing both approaches to an approach in which the expert uses a 2D tablet-based drawing system similar to ones developed for prior work on remote assistance. The study showed the 3D demonstration approach to be faster than the others. In addition, the 3D pointing approach was faster than the 2D tablet in the case of a highly trained expert.                     References                 M. Adcock, D. Ranatunga, R. Smith, and B.H. Thomas. 2014. Object-Based Touch Manipulation for Remote Guidance of Physical Tasks. In Proc. ACM SUI. 113--122.  Google ScholarDigital LibraryM. Bauer, G. Kortuem, and Z. Segall. 1999. \"Where Are You Pointing At?\" A Study of Remote Collaboration in a Wearable Video conference System. In Proc. IEEE ISWC. 151--158. Google ScholarDigital LibraryS. Bottecchia, J. Cieutat, and J. Jessel. 2010. T.A.C: Augmented Reality System for Collaborative Tele-assistance in the Field of Maintenance Through Internet. In Proc. ACM AH. 14:1--14:7.  Google ScholarDigital LibraryJ. Chastine, K. Nagel, Y. Zhu, and M. Hudachek-Buswell. 2008. Studies on the Effectiveness of Virtual Pointers in Collaborative Augmented Reality. In Proc. IEEE 3DUI. 117--124.  Google ScholarDigital LibraryS. Gauglitz, B. Nuernberger, M. Turk, and T. Hollerer. 2014. World-Stabilized Annotations And Virtual Scene Navigation For Remote Collaboration. In Proc. ACM UIST. 449--459.  Google ScholarDigital LibraryM. Goto, Y. Uematsu, H. Saito, S. Senda, and A Iketani. 2010. Task Support System by Displaying Instructional Video Onto AR Workspace. In Proc. IEEE ISMAR. 83--90.Google ScholarCross RefJ. Heiser, B. Tversky, and M. Silverman. 2004. Sketches for and from Collaboration. Visual and spatial reasoning in design III 3 (2004), 69--78.Google ScholarS. Henderson and S. Feiner. 2011. Augmented Reality in the Psychomotor Phase of a Procedural Task. In Proc. IEEE ISMAR. 191--200.  Google ScholarDigital LibraryS. Kim, G. Lee, N. Sakata, and M. Billinghurst. 2014. Improving Co-Presence with Augmented Visual Communication Cues for Sharing Experience through Video Conference. In Proc. IEEE ISMAR. 83--92.Google ScholarD.S. Kirk and D.S. Fraser. 2005. The Effects of Remote Gesturing on Distance Instruction. In Proc. CSCL. 301--310. Google ScholarDigital LibraryT. Kurata, N. Sakata, M. Kourogi, H. Kuzuoka, and M. Billinghurst. 2004. Remote Collaboration Using a Shoulder-Worn Active Camera/laser. In Proc. IEEE ISWC, Vol. 1. 62--69.  Google ScholarDigital LibraryG. Kurillo, R. Bajcsy, K. Nahrsted, and O. Kreylos. 2008. Immersive 3D Environment for Remote Collaboration and Training of Physical Activities. In Proc. IEEE VR. 269--270.Google ScholarH. Kuzuoka. 1992. Spatial Workspace Collaboration: A SharedView Video Support System for Remote Collaboration Capability. In Proc. ACM CHI. 533--540.  Google ScholarDigital LibraryJ. Lanir, R. Stone, B. Cohen, and P. Gurevich. 2013. Ownership and Control of Point of View in Remote Assistance. In Proc. ACM CHI. 2243--2252.  Google ScholarDigital LibraryR.A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A.J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. 2011. KinectFusion: Real-time Dense Surface Mapping and Tracking. In Proc. IEEE ISMAR. 127--136.  Google ScholarDigital LibraryO. Oda and S. Feiner. 2015. Goblin XNA Framework. (2015). http://goblinxna.codeplex.com/Google ScholarJ.S. Pierce, B.C. Stearns, and R. Pausch. 1999. Voodoo Dolls: Seamless Interaction at Multiple Scales in Virtual Environments. In Proc. ACM i3D. 141--145.  Google ScholarDigital LibraryT. Piumsomboon, D. Altimira, H. Kim, A. Clark, G. Lee, and M. Billinghurst. 2014. Grasp-Shell vs Gesture-Speech: A Comparison of Direct and Indirect Natural Interaction Techniques in Augmented Reality. In Proc. IEEE ISMAR. 73--82.Google ScholarR Core Team. 2015. R: A Language and Environment for Statistical Computing. http://www.R-project.org/Google ScholarG. Rizzolatti and L. Craighero. 2004. The Mirror-Neuron System. Annu. Rev. Neurosci. 27 (2004), 169--192.Google ScholarCross RefN. Sakata, T. Kurata, and H. Kuzuoka. 2006. Visual Assist with a Laser Pointer and Wearable Display for Remote Collaboration. In Proc CollabTech. 66--71.Google ScholarI. Sipiran and B. Bustos. 2011. Harris 3D: A Robust Extension of the Harris Operator for Interest Point Detection on 3D Meshes. Vis. Comput. 27, 11 (2011), 963--976.  Google ScholarDigital LibraryR.S. Sodhi, B.R. Jones, D. Forsyth, B.P. Bailey, and G. Maciocci. 2013. BeThere: 3D Mobile Collaboration with Spatial Input. In Proc. ACM CHI. 179--188.  Google ScholarDigital LibraryA. Stafford and W. Piekarski. 2008. User Evaluation of God-like Interaction Techniques. In Proc. AUIC. 19--27. Google ScholarDigital LibraryM. Tait and M. Billinghurst. 2014. View independence in remote collaboration using AR. In Proc. IEEE ISMAR. 309--310.Google ScholarL. Talmy. 2003. Toward a Cognitive Semantics. Vol. 1--2. MIT press.Google ScholarM. Tatzgern, R. Grasset, D. Kalkofen, and D. Schmalstieg. 2014. Transitional Augmented Reality Navigation for Live Captured Scenes. In Proc. IEEE VR. 21--26.Google ScholarF. Tecchia, L. Alem, and W. Huang. 2012. 3D Helping Hands: A Gesture Based MR System for Remote Collaboration. In Proc. ACM VRCAI. 323--328.  Google ScholarDigital LibraryS.G. Vandenberg and A.R. Kuse. 1978. Mental Rotations, a Group Test of Three-dimensional Spatial Visualization. Perceptual and motor skills 47, 2 (1978), 599--604.Google ScholarR. Wang, S. Paris, and J. Popovi'c. 2011. 6D Hands: Markerless Hand-tracking for Computer Aided Design. In Proc. ACM UIST. 549--558.  Google ScholarDigital LibraryM. Wexler, S.M. Kosslyn, and A. Berthoz. 1998. Motor Processes in Mental Rotation. Cognition 68, 1 (1998), 77--94.Google ScholarCross RefSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/2807442.2807497/71917494-b1b1-4b19-965e-22398743de54/p405.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ef0fe7e4ef58c68e280d19b32b13c337b50be581e7835caf2d0bb85c93e17df8b48fc4480c8f8ef0de5063b38bbb2a7048a40d53f3c2f52c619bf6203d0f00fc4f26a20331c7069a31application/x-mpegurlmp478 MB", "keywords": ["maintenance and repair", "assembly", "3D referencing techniques", "remote task assistance", "collaborative mixed/augmented reality", "remote guidance"], "published_in": "UIST '15: Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology", "publication_date": "5 November 2015", "citations": "50", "isbn": "9781450337793", "doi": "10.1145/2807442", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2807442.2807497", "paper_url": "https://dl.acm.org/doi/10.1145/2807442.2807497"}, {"title": "INVISO: A Cross-platform User Interface for Creating Virtual Sonic Environments", "authors": ["Anil \u00c7amc\u0131", "Kristine Lee", "Cody J. Roberts", "Angus G. Forbes"], "abstract": "The predominant interaction paradigm of current audio spatialization tools, which are primarily geared towards expert users, imposes a design process in which users are characterized as stationary, limiting the application domain of these tools. Navigable 3D sonic virtual realities, on the other hand, can support many applications ranging from soundscape prototyping to spatial data representation. Although modern game engines provide a limited set of audio features to create such sonic environments, the interaction methods are inherited from the graphical design features of such systems, and are not specific to the auditory modality. To address such limitations, we introduce INVISO, a novel web-based user interface for designing and experiencing rich and dynamic sonic virtual realities. Our interface enables both novice and expert users to construct complex immersive sonic environments with 3D dynamic sound components. INVISO is platform-independent and facilitates a variety of mixed reality applications, such as those where users can simultaneously experience and manipulate a virtual sonic environment. In this paper, we detail the interface design considerations for our audio-specific VR tool. To evaluate the usability of INVISO, we conduct two user studies: The first demonstrates that our visual interface effectively facilitates the generation of creative audio environments; the second demonstrates that both expert and non-expert users are able to use our software to accurately recreate complex 3D audio scenes.", "keywords": ["design environment", "virtual sonic environments", "browser-based interface", "virtual reality", "3D audio"], "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology", "publication_date": "20 October 2017", "citations": "4", "isbn": "9781450349819", "doi": "10.1145/3126594", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126644", "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126644"}, {"title": "FaceTouch: Enabling Touch Interaction in Display Fixed UIs for Mobile Virtual Reality", "authors": ["Jan Gugenheimer", "David Dobbelstein", "Christian Winkler", "Gabriel Haas", "Enrico Rukzio"], "abstract": "We present FaceTouch, a novel interaction concept for mobile Virtual Reality (VR) head-mounted displays (HMDs) that leverages the backside as a touch-sensitive surface. With FaceTouch, the user can point at and select virtual content inside their field-of-view by touching the corresponding location at the backside of the HMD utilizing their sense of proprioception. This allows for rich interaction (e.g. gestures) in mobile and nomadic scenarios without having to carry additional accessories (e.g. a gamepad). We built a prototype of FaceTouch and conducted two user studies. In the first study we measured the precision of FaceTouch in a display-fixed target selection task using three different selection techniques showing a low error rate of 2% indicate the viability for everyday usage. To asses the impact of different mounting positions on the user performance we conducted a second study. We compared three mounting positions of the touchpad (face, hand and side) showing that mounting the touchpad at the back of the HMD resulted in a significantly lower error rate, lower selection time and higher usability. Finally, we present interaction techniques and three example applications that explore the FaceTouch design space.                     References                 Argelaguet, F., and Andujar, C. A survey of 3d object selection techniques for virtual environments. Computers &amp; Graphics 37, 3 (2013), 121--136.  Google ScholarBaudisch, P., and Chu, G. Back-of-device interaction allows creating very small touch devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '09, ACM (New York, NY, USA, 2009), 1923--1932.  Google ScholarBoff, K. R., Kaufman, L., and Thomas, J. P. Handbook of perception and human performance.Google ScholarBowman, D. A., and Hodges, L. F. An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In Proceedings of the 1997 Symposium on Interactive 3D Graphics, I3D '97, ACM (New York, NY, USA, 1997), 35-ff.  Google ScholarBrooke, J., et al. Sus-a quick and dirty usability scale. Usability evaluation in industry 189, 194 (1996), 4--7.Google ScholarChan, L.-W., Kao, H.-S., Chen, M. Y., Lee, M.-S., Hsu, J., and Hung, Y.-P. Touching the void: Direct-touch interaction for intangible displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '10, ACM (New York, NY, USA, 2010), 2625--2634.  Google ScholarEbrahimi, E., Altenhoff, B., Pagano, C., and Babu, S. Carryover effects of calibration to visual and proprioceptive information on near field distance judgments in 3d user interaction. In 3D User Interfaces (3DUI), 2015 IEEE Symposium on (March 2015), 97--104.Google ScholarFeiner, S., MacIntyre, B., Haupt, M., and Solomon, E. Windows on the world: 2d windows for 3d augmented reality. In Proceedings of the 6th Annual ACM Symposium on User Interface Software and Technology, UIST '93, ACM (New York, NY, USA, 1993), 145--155.  Google ScholarGugenheimer, J., Wolf, D., Eyhtor, E., Maes, P., and Rukzio, E. Gyrovr: Simulating inertia in virtual reality using head worn flywheels. In Conditionally Accepted UIST '16, UIST '16, ACM (2016).  Google ScholarGugenheimer, J., Wolf, D., Haas, G., Krebs, S., and Rukzio, E. Swivrchair: A motorized swivel chair to nudge users' orientation for 360 degree storytelling in virtual reality. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI '16, ACM (New York, NY, USA, 2016), 1996--2000.  Google ScholarHarrison, C., Ramamurthy, S., and Hudson, S. E. On-body interaction: Armed and dangerous. In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction, TEI '12, ACM (New York, NY, USA, 2012), 69--76.  Google ScholarHart, S. G., and Staveland, L. E. Development of nasa-tlx (task load index): Results of empirical and theoretical research. Advances in psychology 52 (1988), 139--183. Google ScholarHincapi\u00e9-Ramos, J. D., Guo, X., Moghadasian, P., and Irani, P. Consumed endurance: A metric to quantify arm fatigue of mid-air interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '14, ACM (New York, NY, USA, 2014), 1063--1072.  Google ScholarHiraoka, S., Miyamoto, I., and Tomimatsu, K. Behind touch, a text input method for mobile phones by the back and tactile sense interface. Information Processing Society of Japan, Interaction 2003 (2003), 131--138.Google ScholarIto, M. Movement and thought: identical control mechanisms by the cerebellum. Trends in Neurosciences 16, 11 (1993), 448--450. Google ScholarKim, D. Development of method for quantification and analysis of simulator sickness in a driving simulation environment. Unpublished doctoral dissertation, Hanyang University, Seoul, South Korea (1999).Google ScholarKleinrock, L. Nomadic computing--an opportunity. SIGCOMM Comput. Commun. Rev. 25, 1 (Jan. 1995), 36--40.  Google ScholarLi, K. A., Baudisch, P., and Hinckley, K. Blindsight: Eyes-free access to mobile phones. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '08, ACM (New York, NY, USA, 2008), 1389--1398.  Google ScholarLindeman, R. W., Sibert, J. L., and Hahn, J. K. Towards usable vr: An empirical study of user interfaces for immersive virtual environments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '99, ACM (New York, NY, USA, 1999), 64--71.  Google ScholarLopes, P., Ion, A., M\u00fcller, W., Hoffmann, D., Jonell, P., and Baudisch, P. Proprioceptive interaction. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA '15, ACM (New York, NY, USA, 2015), 175--175.  Google ScholarLubos, P., Bruder, G., and Steinicke, F. Analysis of direct selection in head-mounted display environments. In 3D User Interfaces (3DUI), 2014 IEEE Symposium on (March 2014), 11--18. Google ScholarMacKenzie, I. S. Fitts' law as a research and design tool in human-computer interaction. Human-computer interaction 7, 1 (1992), 91--139.  Google ScholarMine, M. R. Virtual environment interaction techniques. Tech. rep., Chapel Hill, NC, USA, 1995. Google ScholarMine, M. R., Brooks, Jr., F. P., and Sequin, C. H. Moving objects in space: Exploiting proprioception in virtual-environment interaction. In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '97, ACM Press/Addison-Wesley Publishing Co. (New York, NY, USA, 1997), 19--26.  Google ScholarNguyen, A., and Banic, A. 3dtouch: A wearable 3d input device for 3d applications. In IEEE Virtual Reality 2015, IEEE (2015). Google ScholarPierce, J. S., Forsberg, A. S., Conway, M. J., Hong, S., Zeleznik, R. C., and Mine, M. R. Image plane interaction techniques in 3d immersive environments. In Proceedings of the 1997 Symposium on Interactive 3D Graphics, I3D '97, ACM (New York, NY, USA, 1997), 39--ff.  Google ScholarPoupyrev, I., Billinghurst, M., Weghorst, S., and Ichikawa, T. The go-go interaction technique: Non-linear mapping for direct manipulation in vr. In Proceedings of the 9th Annual ACM Symposium on User Interface Software and Technology, UIST '96, ACM (New York, NY, USA, 1996), 79--80.  Google ScholarPoupyrev, I., and Ichikawa, T. Manipulating objects in virtual worlds: Categorization and empirical evaluation of interaction techniques. Journal of Visual Languages &amp; Computing 10, 1 (1999), 19--35.  Google ScholarPoupyrev, I., Weghorst, S., Billinghurst, M., and Ichikawa, T. Egocentric object manipulation in virtual environments: Empirical evaluation of interaction techniques, 1998.Google ScholarPrto\u00e4rius, M., Valkov, D., Burgbacher, U., and Hinrichs, K. Digitap: An eyes-free vr/ar symbolic input device. In Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology, VRST '14, ACM (New York, NY, USA, 2014), 9--18.  Google ScholarSchwesig, C., Poupyrev, I., and Mori, E. Gummi: A bendable computer. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '04, ACM (New York, NY, USA, 2004), 263--270.  Google ScholarScott MacKenzie, I. Fitts throughput and the remarkable case of touch-based target selection. In Human-Computer Interaction: Interaction Technologies, M. Kurosu, Ed., vol. 9170 of Lecture Notes in Computer Science. Springer International Publishing, 2015, 238--249.Google ScholarSerrano, M., Ens, B. M., and Irani, P. P. Exploring the use of hand-to-face input for interacting with head-worn displays. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems, CHI '14, ACM (New York, NY, USA, 2014), 3181--3190.  Google ScholarSoukoreff, R. W., and MacKenzie, I. S. Towards a standard for pointing device evaluation, perspectives on 27 years of fitts' law research in hci. Int. J. Hum.-Comput. Stud. 61, 6 (Dec. 2004), 751--789.  Google ScholarSugimoto, M., and Hiroki, K. Hybridtouch: An intuitive manipulation technique for pdas using their front and rear surfaces. In Proceedings of the 8th Conference on Human-computer Interaction with Mobile Devices and Services, MobileHCI '06, ACM (New York, NY, USA, 2006), 137--140.  Google ScholarTanriverdi, V., and Jacob, R. J. K. Interacting with eye movements in virtual environments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '00, ACM (New York, NY, USA, 2000), 265--272.  Google ScholarWigdor, D., Forlines, C., Baudisch, P., Barnwell, J., and Shen, C. Lucid touch: A see-through mobile device. In Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology, UIST '07, ACM (New York, NY, USA, 2007), 269--278.  Google ScholarWigdor, D., Leigh, D., Forlines, C., Shipman, S., Barnwell, J., Balakrishnan, R., and Shen, C. Under the table interaction. In Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology, UIST '06, ACM (New York, NY, USA, 2006), 259--268.  Google ScholarWolf, K., M\u00fcller-Tomfelde, C., Cheng, K., and Wechsung, I. Does proprioception guide back-of-device pointing as well as vision' In CHI '12 Extended Abstracts on Human Factors in Computing Systems, CHI EA '12, ACM (New York, NY, USA, 2012), 1739--1744.  Google ScholarYao, R., Heath, T., Davies, A., Forsyth, T., Mitchell, N., and Hoberman, P. Oculus vr best practices guide. Oculus VR (2015).Google ScholarZelaznik, H. N., Mone, S., McCabe, G. P., and Thaman, C. Role of temporal and spatial precision in determining the nature of the speed-accuracy trade-off in aimed-hand movements. Journal of Experimental Psychology: Human Perception and Performance 14, 2 (1988), 221.Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/2984511.2984576/3839cee0-0ca8-45e6-a714-6addb9f8f4c8/p49-gugenheimer.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ef0eefe70a912b90dead24367f6ec0fb54d0839629954cd65d4157e40d91b85c8b525f8f9547e7e3ef881af0bbb9a0275822ad52b2ff99f6a6178dc36fae57f765cf0e7499db9388abapplication/x-mpegurlmp4266.2 MB", "keywords": ["virtual reality", "back-of-device interaction", "vr interaction", "vr input", "nomadic vr", "mobile vr"], "published_in": "UIST '16: Proceedings of the 29th Annual Symposium on User Interface Software and Technology", "publication_date": "16 October 2016", "citations": "43", "isbn": "9781450341899", "doi": "10.1145/2984511", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2984511.2984576", "paper_url": "https://dl.acm.org/doi/10.1145/2984511.2984576"}, {"title": "Erg-O: Ergonomic Optimization of Immersive Virtual Environments", "authors": ["Roberto A. Montano Murillo", "Sriram Subramanian", "Diego Martinez Plasencia"], "abstract": "Interaction in VR involves large body movements, easily inducing fatigue and discomfort. We propose Erg-O, a manipulation technique that leverages visual dominance to maintain the visual location of the elements in VR, while making them accessible from more comfortable locations. Our solution works in an open-ended fashion (no prior knowledge of the object the user wants to touch), can be used with multiple objects, and still allows interaction with any other point within user's reach. We use optimization approaches to compute the best physical location to interact with each visual element, and space partitioning techniques to distort the visual and physical spaces based on those mappings and allow multi-object retargeting. In this paper we describe the Erg-O technique, propose two retargeting strategies and report the results from a user study on 3D selection under different conditions, elaborating on their potential and application to specific usage scenarios.", "keywords": ["optimization", "virtual reality", "ergonomics"], "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology", "publication_date": "20 October 2017", "citations": "11", "isbn": "9781450349819", "doi": "10.1145/3126594", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126605", "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126605"}, {"title": "HairBrush for Immersive Data-Driven Hair Modeling", "authors": ["Jun Xing", "Koki Nagano", "Weikai Chen", "Haotian Xu", "Li-Yi Wei", "Yajie Zhao", "Jingwan Lu", "Byungmoon Kim", "Hao Li"], "abstract": "While hair is an essential component of virtual humans, it is also one of the most challenging digital assets to create. Existing automatic techniques lack the generality and flexibility to create rich hair variations, while manual authoring interfaces often require considerable artistic skills and efforts, especially for intricate 3D hair structures that can be difficult to navigate. We propose an interactive hair modeling system that can help create complex hairstyles in minutes or hours that would otherwise take much longer with existing tools. Modelers, including novice users, can focus on the overall hairstyles and local hair deformations, as our system intelligently suggests the desired hair parts. Our method combines the flexibility of manual authoring and the convenience of data-driven automation. Since hair contains intricate 3D structures such as buns, knots, and strands, they are inherently challenging to create using traditional 2D interfaces. Our system provides a new 3D hair authoring interface for immersive interaction in virtual reality (VR). Users can draw high-level guide strips, from which our system predicts the most plausible hairstyles via a deep neural network trained from a professionally curated dataset. Each hairstyle in our dataset is composed of multiple variations, serving as blend-shapes to fit the user drawings via global blending and local deformation. The fitted hair models are visualized as interactive suggestions that the user can select, modify, or ignore. We conducted a user study to confirm that our system can significantly reduce manual labor while improve the output quality for modeling a variety of head and facial hairstyles that are challenging to create via existing techniques.", "keywords": ["machine learning", "user interface", "hair", "data-driven", "modeling", "virtual reality"], "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "17 October 2019", "citations": "0", "isbn": "9781450368162", "doi": "10.1145/3332165", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347876", "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347876"}, {"title": "Demonstration of TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction", "authors": ["Jaeyeon Lee", "Mike Sinclair", "Mar Gonzalez-Franco", "Eyal Ofek", "Christian Holz"], "abstract": "Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. We demonstrate the TORC interaction scenarios for a virtual object in hand.", "keywords": ["haptic texture", "haptics", "haptic compliance", "vr object manipulation"], "published_in": "UIST '19: The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "14 October 2019", "citations": "2", "isbn": "9781450368179", "doi": "10.1145/3332167", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332167.3356898", "paper_url": "https://dl.acm.org/doi/10.1145/3332167.3356898"}, {"title": "Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift", "authors": ["Tiare Feuchtner", "J\u00f6rg M\u00fcller"], "abstract": "We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.                     References                 David Alais and David Burr. 2004. The ventriloquist effect results from near-optimal bimodal integration. Current biology 14, 3 (2004), 257--262.Google ScholarFerran Argelaguet, Ludovic Hoyet, Micha\u00ebl Trico, and Anatole L\u00e9cuyer. 2016. The role of interaction in virtual embodiment: Effects of the virtual hand representation. In Virtual Reality (VR), 2016 IEEE. IEEE, 3--10.Google ScholarMahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D Wilson. 2016. Haptic Retargeting: Dynamic Repurposing of Passive Haptics for Enhanced Virtual Reality Experiences. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 1968--1979.  Google ScholarMyroslav Bachynskyi, Gregorio Palmas, Antti Oulasvirta, J\u00fcrgen Steimle, and Tino Weinkauf. 2015b. Performance and ergonomics of touch surfaces: A comparative study using biomechanical simulation. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 1817--1826.  Google ScholarMyroslav Bachynskyi, Gregorio Palmas, Antti Oulasvirta, and Tino Weinkauf. 2015a. Informing the design of novel input methods with muscle coactivation clustering. ACM Transactions on Computer-Human Interaction (TOCHI) 21, 6 (2015), 30.  Google ScholarGunnar Borg. 1998. Borg's perceived exertion and pain scales. Human kinetics.Google ScholarMatthew Botvinick and Jonathan Cohen. 1998. Rubber hands 'feel'touch that eyes see. Nature 391, 6669 (1998), 756--756.Google ScholarNiclas Braun, Jeremy D Thorne, Helmut Hildebrandt, and Stefan Debener. 2014. Interplay of Agency and Ownership: The Intentional Binding and Rubber Hand Illusion Paradigm Combined. PloS one 9, 11 (2014), e111967.Google ScholarEric Burns, Sharif Razzaque, Abigail T Panter, Mary C Whitton, Matthew R McCallus, and Frederick P Brooks Jr. 2006. The hand is more easily fooled than the eye: Users are more sensitive to visual interpenetration than to visual-proprioceptive discrepancy. Presence: teleoperators &amp; virtual environments 15, 1 (2006), 1--15.  Google ScholarGemma Calvert, Charles Spence, and Barry E Stein. 2004. The handbook of multisensory processes. MIT press.Google ScholarEmilie A Caspar, Axel Cleeremans, and Patrick Haggard. 2015. The relationship between human agency and embodiment. Consciousness and cognition 33 (2015), 226--236.Google ScholarLung-Pan Cheng, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D Wilson. 2017. Sparse Haptic Proxy: Touch Feedback in Virtual Environments Using a General Passive Prop. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 3718--3728.  Google ScholarMarcello Costantini and Patrick Haggard. 2007. The rubber hand illusion: sensitivity and reference frame for body ownership. Consciousness and cognition 16, 2 (2007), 229--240.Google ScholarTimothy Dummer, Alexandra Picot-Annand, Tristan Neal, and Chris Moore. 2009. Movement and the rubber hand illusion. Perception 38, 2 (2009), 271.Google ScholarHH Ehrsson. 2012. The concept of body ownership and its relation to multisensory integration. The New Handbook of Multisensory Processes - B.E. Stein (Ed.) (2012), 775--792. MIT Press (Cambridge).Google ScholarH Henrik Ehrsson, Nicholas P Holmes, and Richard E Passingham. 2005. Touching a rubber hand: feeling of body ownership is associated with activity in multisensory brain areas. The Journal of Neuroscience 25, 45 (2005), 10564--10573.Google ScholarH Henrik Ehrsson, Charles Spence, and Richard E Passingham. 2004. That's my hand! Activity in premotor cortex reflects feeling of ownership of a limb. Science 305, 5685 (2004), 875--877.Google ScholarTiare Feuchtner and J\u00f6rg M\u00fcller. 2017. Extending the Body for Interaction with Reality. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 5145--5157.  Google ScholarDustin T Han, Mohamed Suhail, and Eric D Ragan. 2018. Evaluating Remapped Physical Reach for Hand Interactions with Passive Haptics in Virtual Reality. IEEE Transactions on Visualization and Computer Graphics (2018).  Google ScholarJuan David Hincapi\u00e9-Ramos, Xiang Guo, Paymahn Moghadasian, and Pourang Irani. 2014. Consumed endurance: a metric to quantify arm fatigue of mid-air interactions. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, 1063--1072.  Google ScholarAndreas Kalckert and H Henrik Ehrsson. 2014a. The moving rubber hand illusion revisited: Comparing movements and visuotactile stimulation to induce illusory ownership. Consciousness and cognition 26 (2014), 117--132.Google ScholarAndreas Kalckert and H Henrik Ehrsson. 2014b. The spatial distance rule in the moving and classical rubber hand illusions. Consciousness and cognition 30 (2014), 118--132.Google ScholarKonstantina Kilteni, Raphaela Groten, and Mel Slater. 2012a. The sense of embodiment in virtual reality. Presence: Teleoperators and Virtual Environments 21, 4 (2012), 373--387.  Google ScholarKonstantina Kilteni, Jean-Marie Normand, Maria V Sanchez-Vives, and Mel Slater. 2012b. Extending body space in immersive virtual reality: a very long arm illusion. PloS one 7, 7 (2012), e40867.Google ScholarLuv Kohli, Eric Burns, Dorian Miller, and Henry Fuchs. 2005. Combining passive haptics with redirected walking. In Proceedings of the 2005 international conference on Augmented tele-existence. ACM, 253--254.  Google ScholarLuv Kohli, Mary C Whitton, and Frederick P Brooks. 2012. Redirected touching: The effect of warping space on task performance. In 2012 IEEE Symposium on 3D User Interfaces (3DUI). IEEE, 105--112.Google ScholarElena Kokkinara and Mel Slater. 2014. Measuring the effects through time of the influence of visuomotor and visuotactile synchronous stimulation on a virtual body ownership illusion. Perception 43, 1 (2014), 43--58.Google ScholarLorraine Lin and Sophie J\u00f6rg. 2016. Need a Hand?: How Appearance Affects the Virtual Hand Illusion. In Proceedings of the ACM Symposium on Applied Perception (SAP '16). ACM, New York, NY, USA, 69--76.  Google ScholarDonna M Lloyd. 2007. Spatial limits on referred touch to an alien limb may reflect boundaries of visuo-tactile peripersonal space surrounding the hand. Brain and cognition 64, 1 (2007), 104--109.Google ScholarI Scott MacKenzie. 1992. Fitts' law as a research and design tool in human-computer interaction. Human-computer interaction 7, 1 (1992), 91--139.  Google ScholarAngelo Maravita, Charles Spence, and Jon Driver. 2003. Multisensory integration and the body schema: close to hand and within reach. Current Biology 13, 13 (2003), R531--R539.Google ScholarLynn McAtamney and E Nigel Corlett. 1993. RULA: a survey method for the investigation of work-related upper limb disorders. Applied ergonomics 24, 2 (1993), 91--99.Google ScholarRoberto A Montano Murillo, Sriram Subramanian, and Diego Martinez Plasencia. 2017. Erg-O: ergonomic optimization of immersive virtual environments. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology. ACM, 759--771.  Google ScholarBirgit Nierula, Matteo Martini, Marta Matamala-Gomez, Mel Slater, and Maria V Sanchez-Vives. 2017. Seeing an embodied virtual hand is analgesic contingent on colocation. The Journal of Pain 18, 6 (2017), 645--655.Google ScholarValeria I Petkova and H Henrik Ehrsson. 2008. If I were you: perceptual illusion of body swapping. PloS one 3, 12 (2008), e3832.Google ScholarEustace Christopher Poulton. 1974. Tracking skill and manual control. Academic press.Google ScholarIvan Poupyrev, Mark Billinghurst, Suzanne Weghorst, and Tadao Ichikawa. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in VR. In Proceedings of the 9th annual ACM symposium on User interface software and technology. ACM, 79--80.  Google ScholarSharif Razzaque, Zachariah Kohn, and Mary C Whitton. 2001. Redirected walking. In Proceedings of EUROGRAPHICS, Vol. 9. Citeseer, 105--106.Google ScholarMartin Riemer, Xaver Fuchs, Florian Bublatzky, Dieter Kleinb\u00f6hl, Rupert H\u00f6lzl, and J\u00f6rg Trojan. 2014. The rubber hand illusion depends on a congruent mapping between real and artificial fingers. Acta psychologica 152 (2014), 34--41.Google ScholarRichard A Schmidt, Tim Lee, Carolee Winstein, Gabriele Wulf, and Howard Zelaznik. 2018. Motor Control and Learning, 6E. Human kinetics.Google ScholarReza Shadmehr and Steven P Wise. 2005. The computational neurobiology of reaching and pointing: a foundation for motor learning. MIT press.Google ScholarSotaro Shimada, Kensuke Fukuda, and Kazuo Hiraki. 2009. Rubber hand illusion under delayed visual feedback. PloS one 4, 7 (2009), e6185.Google ScholarMel Slater, Daniel Perez-Marcos, H Henrik Ehrsson, and Maria V Sanchez-Vives. 2008. Towards a digital body: the virtual arm illusion. Frontiers in human neuroscience 2 (2008).Google ScholarG Tieri, E Tidoni, EF Pavone, and SM Aglioti. 2015. Mere observation of body discontinuity affects perceived ownership and vicarious agency over a virtual hand. Experimental brain research 233, 4 (2015), 1247--1259.Google ScholarManos Tsakiris, Lewis Carpenter, Dafydd James, and Aikaterini Fotopoulou. 2010. Hands only illusion: multisensory integration elicits sense of ownership for body parts but not for non-corporeal objects. Experimental Brain Research 204, 3 (2010), 343--352.Google ScholarManos Tsakiris, Gita Prabhu, and Patrick Haggard. 2006. Having a body versus moving your body: How agency structures body-ownership. Consciousness and cognition 15, 2 (2006), 423--432.Google ScholarLee D Walsh, G Lorimer Moseley, Janet L Taylor, and Simon C Gandevia. 2011. Proprioceptive signals contribute to the sense of body ownership. The Journal of physiology 589, 12 (2011), 3009--3021.Google ScholarJing Zhang, Wei Chen, Hengwei Li, Bernhard Hommel, and Tifei Yuan. 2014. Disentangling the sense of agency and the sense of ownership in the virtual hand illusion paradigm. Technical Report. PeerJ PrePrints.Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242594/92a932c5-75c6-4077-9626-76cf3ba919af/p31-feuchtner.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc04537a72e6509fd7b682415241f5bf14de4ad2c36db9d656c26a53d49e615097b41fa2f9e8e50dd6cfe0db981a2a6abb2e6f2066044ab9ee8b3e5b90de9fapplication/x-mpegurlmp4190.9 MB", "keywords": ["virtual hand illusion", "body ownership"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "2", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242594", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242594"}, {"title": "DodecaPen: Accurate 6DoF Tracking of a Passive Stylus", "authors": ["Po-Chen Wu", "Robert Wang", "Kenrick Kin", "Christopher Twigg", "Shangchen Han", "Ming-Hsuan Yang", "Shao-Yi Chien"], "abstract": "We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications. Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment. We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems. We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.", "keywords": ["6DoF pose tracking", "binary square markers", "mixed reality"], "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology", "publication_date": "20 October 2017", "citations": "12", "isbn": "9781450349819", "doi": "10.1145/3126594", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126664", "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126664"}, {"title": "Collaborative Virtual Reality for Low-Latency Interaction", "authors": ["Carmine Elvezio", "Frank Ling", "Jen-Shuo Liu", "Steven Feiner"], "abstract": "In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.                     References                 Mercury Messaging: An open-source framework to facilitate nonspatial communication in the Unity game engine. https://github.com/ColumbiaCGUI/MercuryMessaging (Last accessed: August 10, 2018).Google ScholarObi Rope. http://obi.virtualmethodstudio.com/ (Last accessed: July 11, 2018).Google ScholarOculus Networking. https://developer.oculus.com/blog/networked-physics-in-virtual-reality-networking-a-stack-of-cubes-with-unity-and-physx/ (Last accessed: July 11, 2018).Google ScholarRobert Anderson, David Gallup, Jonathan T. Barron, Janne Kontkanen, Noah Snavely, Carlos Hern\u00e1ndez, Sameer Agarwal, and Steven M. Seitz. 2016. Jump: Virtual Reality Video. ACM Trans. Graph. 35, 6, Article 198 (Nov. 2016), 13 pages.  Google ScholarJ. W. Burke, M. D. J. McNeill, D. K. Charles, P. J. Morrow, J. H. Crosbie, and S. M. McDonough. 2009. Optimising engagement for stroke rehabilitation using serious games. The Visual Computer 25, 12 (27 Aug 2009), 1085.  Google ScholarCarmine Elvezio, Mengu Sukan, and Steven Feiner. 2018. Mercury: A Messaging Framework for Modular UI Components. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA, Article 588, 12 pages.  Google ScholarMaureen K. Holden. 2005. Virtual Environments for Motor Rehabilitation: Review. CyberPsychology &amp; Behavior 8, 3 (2005), 187--211. PMID: 15971970.Google ScholarHannes Kaufmann and Dieter Schmalstieg. 2002. Mathematics and Geometry Education with Collaborative Augmented Reality. In ACM SIGGRAPH 2002 Conference Abstracts and Applications (SIGGRAPH '02). ACM, New York, NY, USA, 37--41.  Google ScholarJ. Mortensen, V. Vinayagamoorthy, M. Slater, A. Steed, B. Lok, and M. C. Whitton. 2002. Collaboration in Tele-immersive Environments. In Proceedings of the Workshop on Virtual Environments 2002 (EGVE '02). Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 93--101. http://dl.acm.org/citation.cfm?id=509709.509724 Google ScholarOhan Oda, Carmine Elvezio, Mengu Sukan, Steven Feiner, and Barbara Tversky. 2015. Virtual Replicas for Remote Assistance in Virtual and Augmented Reality. In Proceedings of the 28th Annual ACM Symposium on User Interface Software 38; Technology (UIST '15). ACM, New York, NY, USA, 405--415.  Google ScholarSergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L. Davidson, Sameh Khamis, Mingsong Dou, Vladimir Tankovich, Charles Loop, Qin Cai, Philip A. Chou, Sarah Mennicken, Julien Valentin, Vivek Pradeep, Shenlong Wang, Sing Bing Kang, Pushmeet Kohli, Yuliya Lutchyn, Cem Keskin, and Shahram Izadi. 2016. Holoportation: Virtual 3D Teleportation in Real-time. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 741--754.  Google ScholarChristian Sch\u00f6nauer, Thomas Pintaric, and Hannes Kaufmann. 2011. Full Body Interaction for Serious Games in Motor Rehabilitation. In Proceedings of the 2Nd Augmented Human International Conference (AH '11). ACM, New York, NY, USA, Article 4, 8 pages.  Google ScholarSung H You, Sung Ho Jang, Yun-Hee Kim, Yong-Hyun Kwon, Irene Barrow, and Mark Hallett. 2005. Cortical reorganization induced by virtual reality therapy in a child with hemiparetic cerebral palsy. Developmental Medicine &amp; Child Neurology 47, 9 (2005), 628--635.Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3266037.3271643/af97504f-88a6-4aa0-860d-ecc8198b3344/ude1062.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e1e5ff2e0dfa24a1f93e30b355b0c85a13c5282fd9bad72749d180b6efd44085e62ff892c0f7fb5114eb677fa5b7bb1f596830c06c5118cfee46abf953da434f2c84823ea05d11application/x-mpegurlmp424.6 MB", "keywords": ["collaboration", "rehabilitation", "virtual reality", "games"], "published_in": "UIST '18 Adjunct: The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings", "publication_date": "11 October 2018", "citations": "2", "isbn": "9781450359498", "doi": "10.1145/3266037", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3266037.3271643", "paper_url": "https://dl.acm.org/doi/10.1145/3266037.3271643"}, {"title": "TurkDeck: Physical Virtual Reality Based on People", "authors": ["Lung-Pan Cheng", "Thijs Roumen", "Hannes Rantzsch", "Sven K\u00f6hler", "Patrick Schmidt", "Robert Kovacs", "Johannes Jasper", "Jonas Kemper", "Patrick Baudisch"], "abstract": "TurkDeck is an immersive virtual reality system that reproduces not only what users see and hear, but also what users feel. TurkDeck produces the haptic sensation using props, i.e., when users touch or manipulate an object in the virtual world, they simultaneously also touch or manipulate a corresponding object in the physical world. Unlike previous work on prop-based virtual reality, however, TurkDeck allows creating arbitrarily large virtual worlds in finite space and using a finite set of physical props. The key idea behind TurkDeck is that it creates these physical representations on the fly by making a group of human workers present and operate the props only when and where the user can actually reach them. TurkDeck manages these so-called \"human actuators\" by displaying visual instructions that tell the human actuators when and where to place props and how to actuate them. We demonstrate TurkDeck at the example of an immersive 300m2 experience in 25m2 physical space. We show how to simulate a wide range of physical objects and effects, including walls, doors, ledges, steps, beams, switches, stompers, portals, zip lines, and wind. In a user study, participants rated the realism/immersion of TurkDeck higher than a traditional prop-less baseline condition (4.9 vs. 3.6 on 7 item Likert).                     References                 Alaraj, A., Lemole, M.G., Finkle, J.H., Yudkowsky, R., Wallace, A., Luciano, C., Banerjee, P.P., Rizzi, S.H., and Charbel, F.T. Virtual reality training in neurosurgery: review of current status and future applications. Surgical neurology international 2 (2011).Google ScholarAlvar http://virtual.vtt.fi/virtual/proj2/multimedia/alvarGoogle ScholarBergamasco, M. The GLAD-IN-ART Project. Virtual Reality SE-19. 251--258.Google ScholarBrooks Jr., and Frederick, P. What's real about virtual reality? Computer Graphics and Applications, IEEE 19.6 (1999): 16--27.  Google ScholarDigital LibraryCheng, L., L\u00fchne, P., Lopes, P., Sterz, C., &amp; Baudisch, P. Haptic Turk : a Motion Platform Based on People. Proc. CHI '14, 3463--3472.  Google ScholarDigital LibraryHinckley, K., Pausch, R., Goble, J.C., and Kassell, N.F. Passive Real-world Interface Props for Neurosurgical Visualization. Proc. CHI '94, 452--458.  Google ScholarDigital LibraryHoffmann, H. G. Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments. Proc. IEEE VRAIS '98, 59--63. Google ScholarDigital LibraryHollerbach, J.M. Torso Force Feedback Realistically Simulates Slope on Treadmill-Style Locomotion Interfaces. Int. J. Robotics Research 20, 12 (2001), 939--952.Google ScholarCross RefHughes, C.E., Stapleton, C.B., Hughes, D.E., Smith, E.M. Mixed reality in education, entertainment, and training. Computer Graphics and Applications 25.6 (2005): 24--30.  Google ScholarDigital LibraryInsko, B.E. Passive haptics significantly enhances virtual environments. Dissertation at University of North Carolina at Chapel Hill, 2001. Google ScholarDigital LibraryIwata, H., Yano, H., and Fukushima, H. CirculaFloor: A Locomotion Interface Using Circulation of Movable Tiles. In Proc. Virtual Reality'05, 223--230.  Google ScholarDigital LibraryIwata, H., Yano, H., and Nakaizumi, F. Gait Master: a Versatile Locomotion Interface for Uneven Virtual Terrain. Virtual Reality, (2001). Google ScholarDigital LibraryKohli, L., Burns, E., Miller, D., Fuchs, H. Combining passive haptics with redirected walking. Proc. ICAT'05, 253--254.  Google ScholarDigital LibraryLow, K-.L., Welch, G., Lastra, A., and Fuchs, H. Life-Size Projector-Based Dioramas. Proc. VRST'01, 93--101.  Google ScholarDigital LibraryMacKenzie, I.S. and Zhang, S.X. The Immediate Usability of Graffiti. Proc. GI '97, 129--137. Google ScholarDigital LibraryNASA Vertical Motion Simulator. http://www.nasa.gov/ centers/ames/research/technology-onepagers/vms.html.Google ScholarNoma, H., Sugihara, T., and Miyasato, T. Development of Ground Surface Simulator for Tel-E-Merge system. In Proc. VR'00, 217--224. Google ScholarDigital LibraryOculus Rift, www.oculusvr.com. Mar. 2014.Google ScholarOrtega, M. and Coquillart, S. Prop-based haptic interaction with co-location and immersion: an automotive application. Proc. HAVE'05.Google ScholarPair, J., Neumann, U., Piepol, D., and Swartout, W.R. FlatWorld: Combining Hollywood Set-Design Techniques with VR. CG&amp;A 23, 1 (2003), 12--15.  Google ScholarDigital LibraryPortal www.valvesoftware.com/games/portal.html.Google ScholarRazzaque, Sharif. Redirected walking. University of North Carolina at Chapel Hill, 2005.Google ScholarSasaki, N., Chen, H.-T., Sakamoto, D., and Igarashi, T. Facetons: face primitives with adaptive bounds for building 3D architectural models in virtual environment. Proc. VRST '13, 77--82.  Google ScholarDigital LibrarySchmidt, D., Kovacs, R., Mehta, V., Umapathi, U., K\u00f6hler, S., Cheng, L., Baudisch, P. Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality. Proc. CHI'15, to appear.  Google ScholarDigital LibrarySchmidt, H., Hesse, S., Bernhardt, R., and Kr\u00fcger, J. HapticWalker--a Novel Haptic Foot Device. Transactions on Applied Perception 2:2 2005, 166--180.  Google ScholarDigital LibrarySheng, J., Balakrishnan, R., and Singh, K. An interface for virtual 3D sculpting via physical proxy. Proc. GRAPHITE'06, 213--220.  Google ScholarDigital LibraryStewart, D. A Platform with Six Degrees of Freedom. Proc. Institution of Mechanical Engineers 180, 1 (1965), 371--386.Google ScholarCross RefSutherland, I.E. A Head-mounted Three Dimensional Display. Proc. AFIPS '68, 757--764.  Google ScholarDigital LibrarySzalav\u00e1ri, Z. and Gervautz, M. The Personal Interaction Panel - a Two-Handed Interface for Augmented Reality. Computer Graphics Forum 16, 3 (2008), 335--346.Google ScholarCross RefUsoh, M., Arthur, K., Whitton, M.C., et al. Walking &amp;gt; Walking-in-Place &amp;gt; Flying, in Virtual Environments. Proc. SIGGRAPH'99, 359--364.  Google ScholarDigital LibrarySupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/2807442.2807463/5a854eb8-b71f-40bd-8624-c4ca5aa3c736/p417.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ef0fe7e4ef58c68e280d19b32b1cc737b75be485fb4b83f271282a9b124cafdb96790f65f8f9ed95f832b20706265b0bfd926c53b50e2222093fd66319ef4c9b207bb495cb18fa054eapplication/x-mpegurlmp480 MB", "keywords": ["passive virtual reality", "prop-based virtual reality"], "published_in": "UIST '15: Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology", "publication_date": "5 November 2015", "citations": "71", "isbn": "9781450337793", "doi": "10.1145/2807442", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2807442.2807463", "paper_url": "https://dl.acm.org/doi/10.1145/2807442.2807463"}, {"title": "Virtual shelves: interactions with orientation aware devices", "authors": ["Frank Chun Yat Li", "David Dearman", "Khai N. Truong"], "abstract": "Triggering shortcuts or actions on a mobile device often requires a long sequence of key presses. Because the functions of buttons are highly dependent on the current application's context, users are required to look at the display during interaction, even in many mobile situations when eyes-free interactions may be preferable. We present Virtual Shelves, a technique to trigger programmable shortcuts that leverages the user's spatial awareness and kinesthetic memory. With Virtual Shelves, the user triggers shortcuts by orienting a spatially-aware mobile device within the circular hemisphere in front of her. This space is segmented into definable and selectable regions along the phi and theta planes. We show that users can accurately point to 7 regions on the theta and 4 regions on the phi plane using only their kinesthetic memory. Building upon these results, we then evaluate a proof-of-concept prototype of the Virtual Shelves using a Nokia N93. The results show that Virtual Shelves is faster than the N93's native interface for common mobile phone tasks.", "keywords": ["mobile computing", "spatial memory", "kinesthetic memory", "spatially aware devices"], "published_in": "UIST '09: Proceedings of the 22nd annual ACM symposium on User interface software and technology", "publication_date": "4 October 2009", "citations": "66", "isbn": "9781605587455", "doi": "10.1145/1622176", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1622176.1622200", "paper_url": "https://dl.acm.org/doi/10.1145/1622176.1622200"}, {"title": "Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality", "authors": ["Haijun Xia", "Sebastian Herscher", "Ken Perlin", "Daniel Wigdor"], "abstract": "Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users' agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.                     References                 Maneesh Agrawala, Andrew C. Beers, Ian McDowall, Bernd Fr\u00f6hlich, Mark Bolas, and Pat Hanrahan. 1997. The two-user Responsive Workbench: support for collaboration through individual views of a shared space. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques (SIGGRAPH '97). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 327--332.  Google ScholarThomas Baudel and Michel Beaudouin-Lafon. 1993. Charade: remote control of objects using free-hand gestures. Commun. ACM 36, 7 (July 1993), 28--35.  Google ScholarSteve Benford, Chris Greenhalgh, Tom Rodden, and James Pycock. 2001. Collaborative virtual environments. Commun. ACM 44, 7 (July 2001), 79--85.  Google ScholarHrvoje Benko, Edward W Ishak, and Steven Feiner. 2004. Collaborative mixed reality visualization of an archaeological excavation. In Mixed and Augmented Reality, 2004. ISMAR 2004. Third IEEE and ACM International Symposium on. IEEE, 132--140.  Google ScholarHrvoje Benko, Andrew D. Wilson, and Ravin Balakrishnan. 2008. Sphere: multi-touch interactions on a spherical display. In Proceedings of the 21st annual ACM symposium on User interface software and technology (UIST '08). ACM, New York, NY, USA, 77--86.  Google ScholarAnastasia Bezerianos and Ravin Balakrishnan. 2005. The vacuum: facilitating the manipulation of distant objects. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '05), 361- 370.  Google ScholarEric A. Bier, Maureen C. Stone, Ken Pier, Ken Fishkin, Thomas Baudel, Matt Conway, William Buxton, and Tony DeRose. 1994. Toolglass and magic lenses: the see-through interface. In Conference Companion on Human Factors in Computing Systems (CHI '94), Catherine Plaisant (Ed.). ACM, New York, NY, USA, 445--446.  Google ScholarEric A. Bier and Steven Freeman. 1991. MMM: a user interface architecture for shared editors on a single screen. In Proceedings of the 4th annual ACM symposium on User interface software and technology (UIST '91). ACM, New York, NY, USA, 79--86.  Google ScholarMark Billinghurst and Hirokazu Kato. 2002. Collaborative Augmented Reality. Commun. ACM 45, 7 (July 2002), 64--70.  Google ScholarRichard A. Bolt. 1980. \"Put-that-there\": Voice and gesture at the graphics interface. In Proceedings of the 7th annual conference on Computer graphics and interactive techniques (SIGGRAPH '80). ACM, New York, NY, USA, 262--270.  Google ScholarPeter Brandl, Clifton Forlines, Daniel Wigdor, Michael Haller, and Chia Shen. 2008. Combining and measuring the benefits of bimanual pen and direct-touch interaction on horizontal interfaces. In Proceedings of the working conference on Advanced visual interfaces (AVI '08). ACM, New York, NY, USA, 154--161.  Google ScholarDoug A. Bowman, David Koller, and Larry F. Hodges. Travel in immersive virtual environments: An evaluation of viewpoint motion control techniques. In Virtual Reality Annual International Symposium, 1997., IEEE 1997, pp. 45--52. IEEE, 1997. Google ScholarXiang Cao, Andrew D. Wilson, Ravin Balakrishnan, Ken Hinckley, and Scott E. Hudson. ShapeTouch: Leveraging contact shape on interactive surfaces. TABLETOP '08, 129--136.Google ScholarChrister Carlsson and Olof Hagsand. 1993. DIVE A multi-user virtual reality system. In Virtual Reality Annual International Symposium, 1993., 1993 IEEE. IEEE, 394--400.  Google ScholarElizabeth F Churchill and Dave Snowdon. 1998. Collaborative virtual environments: an introductory review of issues and systems. Virtual Reality 3, 1 (1998), 3--15.  Google ScholarMaxime Cordeil, Tim Dwyer, Karsten Klein, Bireswar Laha, Kim Marriott, and Bruce H. Thomas. 2017. Immersive collaborative analysis of network connectivity: CAVE-style or head-mounted display? IEEE Transactions on Visualization and Computer Graphics 23, 1 (Jan. 2017), 441--450.  Google ScholarJ. D. Foley, V. L. Wallace, P. Chan. 1984. The human factors of computer graphics interaction techniques. IEEE Computer Graphics and Applications 4, 11: 13- 48. Google ScholarMike Fraser, Steve Benford, Jon Hindmarsh, and Christian Heath. 1999. Supporting awareness and interaction through collaborative virtual interfaces. In Proceedings of the 12th annual ACM symposium on User interface software and technology (UIST '99). ACM, New York, NY, USA, 27--36.  Google ScholarDustin Freeman, Hrvoje Benko, Meredith Ringel Morris, and Daniel Wigdor. 2009. ShadowGuides: visualizations for in-situ learning of multi-touch and whole-hand gestures. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS '09). ACM, New York, NY, USA, 165--172.  Google ScholarWilliam W. Gaver, Abigail Sellen, Christian Heath, and Paul Luff. 1993. One is not enough: multiple views in a media space. In Proceedings of the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems (CHI '93). ACM, New York, NY, USA, 335--341.  Google ScholarGoogle Doc. https://www.google.com/docs/about/Google ScholarSaul Greenberg and David Marwood. 1994. Real time groupware as a distributed system: concurrency control and its effect on the interface. In Proceedings of the 1994 ACM conference on Computer supported cooperative work (CSCW '94). ACM, New York, NY, USA, 207--217.  Google ScholarTovi Grossman, Daniel Wigdor, and Ravin Balakrishnan. 2004. Multi-finger gestural interaction with 3d volumetric displays. In Proceedings of the 17th annual ACM symposium on User interface software and technology (UIST '04). ACM, New York, NY, USA, 61--70.  Google ScholarTovi Grossman and Ravin Balakrishnan. 2008. Collaborative interaction with volumetric displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '08). ACM, New York, NY, USA, 383--392.  Google ScholarJan Gugenheimer, Evgeny Stemasov, Julian Frommel, and Enrico Rukzio. 2017. ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 4021--4033.  Google ScholarYves Guiard. Asymmetric Division of Labor in Human Skilled Bimanual Action: The Kinematic Chain as a Model. In Journal of Motor Behavior 19 (1987), 486--517.Google ScholarCarl Gutwin and Saul Greenberg. 2002. A Descriptive Framework of Workspace Awareness for Real-Time Groupware. Comput. Supported Coop. Work 11, 3 (November 2002), 411--446.  Google ScholarJefferson Han and Brian Smith. 1997. CU-SeeMe VR immersive desktop teleconferencing. In Proceedings of the fourth ACM international conference on Multimedia (MULTIMEDIA '96). ACM, New York, NY, USA, 199--207.  Google ScholarKen Hinckley, Patrick Baudisch, Gonzalo Ramos, and Francois Guimbretiere. 2005. Design and analysis of delimiters for selection-action pen gesture phrases in scriboli. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '05). ACM, New York, NY, USA, 451--460.  Google ScholarKen Hinckley, Randy Pausch, John C. Goble, and Neal F. Kassell. 1994. A survey of design issues in spatial input. In Proceedings of the 7th annual ACM symposium on User interface software and technology (UIST '94). ACM, New York, NY, USA, 213--222.  Google ScholarKen Hinckley, Koji Yatani, Michel Pahud, Nicole Coddington, Jenny Rodenhouse, Andy Wilson, Hrvoje Benko, and Bill Buxton. 2010. Pen + touch = new tools. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10). ACM, New York, NY, USA, 27--36.  Google ScholarRoland Holm, Erwin Stauder, Roland Wagner, Markus Priglinger, and Jens Volkert. 2002. A combined immersive and desktop authoring tool for virtual environments. In Virtual Reality, 2002. Proceedings. IEEE. IEEE, 93--100. Google ScholarShahram Izadi, Harry Brignull, Tom Rodden, Yvonne Rogers, and Mia Underwood. 2003. Dynamo: a public interactive surface supporting the cooperative sharing and exchange of media. In Proceedings of the 16th annual ACM symposium on User interface software and technology (UIST '03). ACM, New York, NY, USA, 159--168.  Google ScholarBrett Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair MacIntyre, Nikunj Raghuvanshi, and Lior Shapira. 2014. RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-camera Units. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST '14). ACM, New York, NY, USA, 637--644.  Google ScholarAzam Khan, Justin Matejka, George Fitzmaurice, Gordon Kurtenbach. 2005. Spotlight: directing users' attention on large displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '05), 791--798.  Google ScholarRegis Kopper, Tao Ni, Doug A. Bowman, and Marcio Pinho. 2006. Design and Evaluation of Navigation Techniques for Multiscale Virtual Environments. In Proceedings of the IEEE conference on Virtual Reality (VR '06). IEEE Computer Society, Washington, DC, USA, 175--182.  Google ScholarAndr\u00e9 Kunert, Alexander Kulik, Stephan Beck, and Bernd Froehlich. 2014. Photoportals: shared references in space and time. In Proceedings of the 17th ACM conference on Computer supported cooperative work &amp; social computing (CSCW '14). ACM, New York, NY, USA, 1388--1399.  Google ScholarJoseph LaViola, Loring S. Holden, Andrew S. Forsberg, Dom S Bhuphaibool, and Robert C. Zeleznik, 1998. Collaborative conceptual modeling using the sketch framework. In Proceedings of the Third Conference on Computer-Aided Architectural Design Research in Asia, 154--158. CAADRIA. Osaka, Japan: Osaka University.Google ScholarPedro Lopes, Alexandra Ion, and Patrick Baudisch. 2015. Impacto: Simulating Physical Impact by Combining Tactile Stimulation with Electrical Muscle Stimulation. In Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology (UIST '15). ACM, New York, NY, USA, 11--19.  Google ScholarJock D. Mackinlay, Stuart K. Card, and George G. Robertson. 1990. Rapid controlled movement through a virtual 3D workspace. In Proceedings of the 17th annual conference on Computer graphics and interactive techniques (SIGGRAPH '90). ACM, New York, NY, USA, 171--176.  Google ScholarP McAvinney. The Sensor Frame-A Gesture-Based Device for the Manipulation of Graphic Objects. 1986Google ScholarMeredith Ringel Morris, Kathy Ryall, Chia Shen, Clifton Forlines, and Frederic Vernier. 2004. Beyond \"social protocols\": multi-user coordination policies for co-located groupware. In Proceedings of the 2004 ACM conference on Computer supported cooperative work (CSCW '04). ACM, New York, NY, USA, 262--265.  Google ScholarMeredith Ringel Morris, Jarrod Lombardo, and Daniel Wigdor. 2010. WeSearch: supporting collaborative search and sensemaking on a tabletop display. In Proceedings of the 2010 ACM conference on Computer supported cooperative work (CSCW '10). ACM, New York, NY, USA, 401--410.  Google ScholarThomas P. Moran, Patrick Chiu, and William van Melle. 1997. Pen-based interaction techniques for organizing material on an electronic whiteboard. In Proceedings of the 10th annual ACM symposium on User interface software and technology (UIST '97). ACM, New York, NY, USA, 45--54.  Google ScholarCuong Nguyen, Stephen DiVerdi, Aaron Hertzmann, and Feng Liu. 2017. CollaVR: Collaborative In-Headset Review for VR Video. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (UIST '17). ACM, New York, NY, USA, 267--277.  Google ScholarOculus Rift. https://www.oculus.com/rift/Google ScholarOhan Oda, Carmine Elvezio, Mengu Sukan, Steven Feiner, and Barbara Tversky. 2015. Virtual Replicas for Remote Assistance in Virtual and Augmented Reality. In Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology (UIST '15). ACM, New York, NY, USA, 405--415.  Google ScholarOliver Otto, Dave Roberts, and Robin Wolff. 2006. A review on effective closely-coupled collaboration using immersive CVE's. In Proceedings of the 2006 ACM international conference on Virtual reality continuum and its applications (VRCIA '06). ACM, New York, NY, USA, 145--154.  Google ScholarTomislav Pejsa, Julian Kantor, Hrvoje Benko, Eyal Ofek, and Andrew Wilson. 2016. Room2Room: Enabling Life-Size Telepresence in a Projected Augmented Reality Environment. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing (CSCW '16). ACM, New York, NY, USA, 1716--1725.  Google ScholarKen Perlin and David Fox. 1993. Pad: an alternative approach to the computer interface. SIGGRAPH '93, 57--64.  Google ScholarJeffrey S. Pierce, Brian C. Stearns, and Randy Pausch. 1999. Voodoo dolls: seamless interaction at multiple scales in virtual environments. In Proceedings of the 1999 symposium on Interactive 3D graphics (I3D '99). ACM, New York, NY, USA, 141--145.  Google ScholarIvan Poupyrev, Mark Billinghurst, Suzanne Weghorst, and Tadao Ichikawa. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in VR. In Proceedings of the 9th annual ACM symposium on User interface software and technology (UIST '96). ACM, New York, NY, USA, 79--80.  Google ScholarJames T Reason and Joseph John Brand. 1975. Motion sickness. Academic press.Google ScholarRandall B. Smith, Ranald Hixon, and Bernard Horan. 1998. Supporting flexible roles in a shared space. In Proceedings of the 1998 ACM conference on Computer supported cooperative work (CSCW '98). ACM, New York, NY, USA, 197--206.  Google ScholarDave Snowdon, Elizabeth F Churchill, and Alan J Munro. 2000. Collaborative virtual environments: digital spaces and places for CSCW. Collaborative Virtual Environments (2000), 1--34.Google ScholarAaron Stafford, Wayne Piekarski, and Bruce Thomas. 2006. Implementation of God-like Interaction Techniques for Supporting Collaboration Between Outdoor AR and Indoor Tabletop Users. In Proceedings of the 5th IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR '06). IEEE Computer Society, Washington, DC, USA, 165--172.  Google ScholarRichard Stoakley, Matthew J. Conway, and Randy Pausch. 1995. Virtual reality on a WIM: interactive worlds in miniature. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems CHI '95, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 265--272.  Google ScholarIvan E. Sutherland. 1964. Sketch pad a man-machine graphical communication system. In Proceedings of the SHARE design automation workshop (DAC '64). ACM, New York, NY, USA, 6.329--6.346.  Google ScholarIvan E. Sutherland. 1968. A head-mounted three dimensional display. In Proceedings of the December 9--11, 1968, fall joint computer conference, part I (AFIPS '68 (Fall, part I)). ACM, New York, NY, USA, 757--764.  Google ScholarPierre Wellner. 1991. The DigitalDesk calculator: tangible manipulation on a desk top display. In Proceedings of the 4th annual ACM symposium on User interface software and technology (UIST '91). ACM, New York, NY, USA, 27--33.  Google ScholarJacob O. Wobbrock, Meredith Ringel Morris, and Andrew D. Wilson. 2009. User-defined gestures for surface computing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '09). ACM, New York, NY, USA, 1083--1092.  Google ScholarMike Wu and Ravin Balakrishnan. 2003. Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays. In Proceedings of the 16th annual ACM symposium on User interface software and technology (UIST '03). ACM, New York, NY, USA, 193--202.  Google ScholarHaijun Xia, Bruno Araujo, Tovi Grossman, and Daniel Wigdor. 2016. Object-Oriented Drawing. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 4610--4621.  Google ScholarHaijun Xia, Bruno Araujo, and Daniel Wigdor. 2017. Collection Objects: Enabling Fluid Formation and Manipulation of Aggregate Selections. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 5592--5604.  Google ScholarHaijun Xia, Ken Hinckley, Michel Pahud, Xiao Tu, and Bill Buxton. 2017. WritLarge: Ink Unleashed by Unified Scope, Action, &amp; Zoom. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 3227--3240.  Google ScholarXiaolong Zhang and George W. Furnas. 2002. Social interactions in multiscale CVEs. In Proceedings of the 4th international conference on Collaborative virtual environments (CVE '02). ACM, New York, NY, USA, 31--38.  Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242597/7a53e5e8-4a60-46a1-9d14-9a26442b3a80/ufp1069p.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc04537a71e65ecc83bc78a5ebf3a9560f09533676cb17ad97025d641350980cedc9d856869681001492d8237971b3bd18cd387e77e760e58fb9afc455d9d5application/x-mpegurlmp415.9 MB", "keywords": ["computer-supported collaborative work", "interaction techniques", "virtual reality"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "6", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242597", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242597"}, {"title": "Portal-ble: Intuitive Free-hand Manipulation in Unbounded Smartphone-based Augmented Reality", "authors": ["Jing Qian", "Jiaju Ma", "Xiangyu Li", "Benjamin Attal", "Haoming Lai", "James Tompkin", "John F. Hughes", "Jeff Huang"], "abstract": "Smartphone augmented reality (AR) lets users interact with physical and virtual spaces simultaneously. With 3D hand tracking, smartphones become apparatus to grab and move virtual objects directly. Based on design considerations for interaction, mobility, and object appearance and physics, we implemented a prototype for portable 3D hand tracking using a smartphone, a Leap Motion controller, and a computation unit. Following an experience prototyping procedure, 12 researchers used the prototype to help explore usability issues and define the design space. We identified issues in perception (moving to the object, reaching for the object), manipulation (successfully grabbing and orienting the object), and behavioral understanding (knowing how to use the smartphone as a viewport). To overcome these issues, we designed object-based feedback and accommodation mechanisms and studied their perceptual and behavioral effects via two tasks: picking up distant objects, and assembling a virtual house from blocks. Our mechanisms enabled significantly faster and more successful user interaction than the initial prototype in picking up and manipulating stationary and moving objects, with a lower cognitive load and greater user preference. The resulting system---Portal-ble---improves user intuition and aids free-hand interactions in mobile situations.", "keywords": ["augmented reality", "mid-air gesture", "3d hand manipulation", "smartphone"], "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "17 October 2019", "citations": "4", "isbn": "9781450368162", "doi": "10.1145/3332165", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347904", "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347904"}, {"title": "Haptic Feedback to the Palm and Fingers for Improved Tactile Perception of Large Objects", "authors": ["Bukun Son", "Jaeyoung Park"], "abstract": "When one manipulates a large or bulky object, s/he utilizes tactile information at both fingers and the palm. Our goal is to efficiently convey contact information to a user's hand during interaction with a virtual object. We propose a haptic system that can provide haptic feedback to thumb/middle finger/index finger and on a palm. Our interface design utilizes a novel compact mechanism to provide haptic information to the palm. Also, we propose a haptic rendering strategy to calculate haptic feedback continuously. We demonstrate that cutaneous feedback on the palm improves the haptic perception of a large virtual object compared to when there is only kinesthetic feedback to the fingers.                     References                 Massimo Bergamasco. 1995. Haptic interfaces: the study of force and tactile feedback systems. In Robot and Human Communication, 1995. RO-MAN'95 TOKYO, Proceedings., 4th IEEE International Workshop on. IEEE,15--20.Google ScholarWouter M Bergmann Tiest and Astrid M L Kappers. 2009. Cues for haptic perception of compliance. IEEE transactions on haptics 2, 4 (2009), 189--199.  Google ScholarKenny Erleben, Jon Sporring, Knud Henriksen, and Henrik Dohlmann. 2005. Physics-Based Animation. Charles River Media. Inc., June 1, 2 (2005), 3. Google ScholarMarc O Ernst and Martin S Banks. Humans integrate visual and haptic information in a statistically optimal fashion. 2002. Nature 415, 6810 (2002), 429--433.Google ScholarMarco Fontana, Andrea Dettori, Fabio Salsedo, and Massimo Bergamasco. 2009. Mechanical design of a novel hand exoskeleton for accurate force displaying. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on. IEEE, 1704--1709. Google ScholarAntonio Frizoli, Federico Barbagli, Sue L Wu, Emanuele Ruffaldi, Massimo Bergamasco,and Kenneth Salisbury. 2004. Evaluation of multipoint contact interfaces in haptic perception of shapes, Symposium of multipoint interaction. IEEE ICRA 2004, proc (2004), 177--188.Google ScholarAhmet Guzererler, William R Provancher, and Cagatay Basdogan. 2016. Perception of Skin Stretch Applied to Palm: Effects of Speed and Displacement. In International Conference on Human Haptic Sensing and Touch Enabled Computer Applications. Springer, 180--189.  Google ScholarC Jacobson and L Sperling. 1976. Classification of theHand-Grip: A Preliminary Study. Journal ofOccupational and Environmental Medicine 18, 6 (1976), 395--398.Google ScholarGunnar Jansson and Linda Monaci. 2006. Identification of real objects under conditions similar to those in haptic displays: providing spatially distributed information at the contact areas is more important than increasing the number of areas. Virtual Reality 9, 4 (2006), 243--249 Google ScholarRebecca P Khurshid, Naomi T Fitter, Elizabeth AFedalei, and Katherine J Kuchenbecker. 2017. Effects ofgrip-force, contact, and acceleration feedback on ateleoperated pick-and-place task. IEEE transactions on haptics 10, 1 (2017), 40--53.  Google ScholarTatsuya Koyama, Ikuo Yamano, Kenjiro Takemura, and Takashi Maeno. 2002. Multi-fingered exoskeleton haptic device using passive force feedback for dexterous teleoperation. In Intelligent Robots and Systems, 2002.IEEE/RSJ International Conference on, Vol. 3. IEEE,2905--2910.Google ScholarSusan J Lederman and Roberta L Klatzky. 1987. Hand movements: A window into haptic object recognition. Cognitive psychology 19, 3 (1987), 342--368.Google ScholarNeil A Macmillan and C Douglas Creelman. 2004. Detection theory: A user's guide. Psychology pressGoogle ScholarKouta Minamizawa, Sho Kamuro, Naoki Kawakami, and Susumu Tachi. 2008. A palm-worn haptic display for bimanual operations in virtual environments. Haptics: Perception, devices and scenarios (2008), 458--463.  Google ScholarDiederick C Niehorster, Li Li, and Markus Lappe. 2017.The accuracy and precision of position and orientation tracking in the HTC vive virtual reality system for scientific research. i-Perception 8, 3 (2017), 2041669517708205.Google ScholarClaudio Pacchierotti, Leonardo Meli, Francesco Chinello, Monica Malvezzi, and Domenico Prattichizzo. 2015. Cutaneous haptic feedback to ensure the stability of robotic teleoperation systems. The International Journal of Robotics Research 34, 14 (2015), 1773--1787.  Google ScholarDigital LibraryClaudio Pacchierotti, Stephen Sinclair, Massimiliano Solazzi, Antonio Frisoli, Vincent Hayward, and Domenico Prattichizzo. 2017. Wearable haptic systems for the fingertip and the hand: taxonomy, review, and perspectives. IEEE transactions on haptics 10, 4, (2017), 580--600.  Google ScholarRebecca M Pierce, Elizabeth A Fedalei, and Katherine JKuchenbecker. 2014. A wearable device for controlling a robot gripper with fingertip contact, pressure, vibrotactile, and grip force feedback. In Haptics Symposium(HAPTICS), 2014 IEEE. IEEE, 19--25.Google ScholarJanet L Poole. 1994. Grasp pattern variations seen in thescleroderma hand. American Journal of Occupational Therapy 48, 1 (1994), 46--54.Google ScholarWilliam R Provancher, Katherine J Kuchenbecker, G\u00fcnter Niemeyer, and Mark R Cutkosky. 2005. Perception of curvature and object motion via contactlocation feedback. In Robotics Research. The Eleventh International Symposium. Springer, 456--465.Google ScholarScott L Springer and Nicola J Ferrier. 2002. Design andcontrol of a force-reflecting haptic interface forteleoperational grasping. Journal of Mechanical Design 124, 2 (2002), 277--283.Google ScholarHong Z Tan, Nathaniel I Durlach, G Lee Beauregard, Madayam A Srinivasan. 2017. Manual discrimination of compliance using active pinch grasp; The roles of force and work cues. Perception and psychophysics 57, 4 (1995), 498--510Google ScholarJianyu Yang, Hualong Xie, and Jiashun Shi. 2016. Anovel motion-coupling design for a jointlesstendon-driven finger exoskeleton for rehabilitation. Mechanism and Machine Theory 99 (2016), 83--102.Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242656/17b302ca-67d2-46fc-bb91-20a31164ff4a/ufp1472p.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc04507670e6589ad4bcd91f6c8dcb94053ec7d9994bc904f6ee43991d3409b23e195cdd608b110cb89bf9779d50260723a2f98a76a67d23451b60fe55ea17application/x-mpegurlmp411.8 MB", "keywords": ["haptics", "wearable interface", "global shape perception"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "7", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242656", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242656"}, {"title": "Plane, Ray, and Point: Enabling Precise Spatial Manipulations with Shape Constraints", "authors": ["Devamardeep Hayatpur", "Seongkook Heo", "Haijun Xia", "Wolfgang Stuerzlinger", "Daniel Wigdor"], "abstract": "We present Plane, Ray, and Point, a set of interaction techniques that utilizes shape constraints to enable quick and precise object alignment and manipulation in virtual reality. Users create the three types of shape constraints, Plane, Ray, and Point, by using symbolic gestures. The shape constraints are used like scaffoldings and limit and guide the movement of virtual objects that collide or intersect with them. The same set of gestures can be performed with the other hand, which allow users to further control the degrees of freedom for precise and constrained manipulation. The combination of shape constraints and bimanual gestures yield a rich set of interaction techniques to support object transformation. An exploratory study conducted with 3D design experts and novice users found the techniques to be useful in 3D scene design workflows and easy to learn and use.", "keywords": ["precise object manipulation", "3d object manipulation", "shape gestures", "constraints separation"], "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "17 October 2019", "citations": "1", "isbn": "9781450368162", "doi": "10.1145/3332165", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347916", "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347916"}, {"title": "3D puppetry: a kinect-based interface for 3D animation", "authors": ["Robert Held", "Ankit Gupta", "Brian Curless", "Maneesh Agrawala"], "abstract": "We present a system for producing 3D animations using physical objects (i.e., puppets) as input. Puppeteers can load 3D models of familiar rigid objects, including toys, into our system and use them as puppets for an animation. During a performance, the puppeteer physically manipulates these puppets in front of a Kinect depth sensor. Our system uses a combination of image-feature matching and 3D shape matching to identify and track the physical puppets. It then renders the corresponding 3D models into a virtual set. Our system operates in real time so that the puppeteer can immediately see the resulting animation and make adjustments on the fly. It also provides 6D virtual camera \\\\rev{and lighting} controls, which the puppeteer can adjust before, during, or after a performance. Finally our system supports layered animations to help puppeteers produce animations in which several characters move at the same time. We demonstrate the accessibility of our system with a variety of animations created by puppeteers with no prior animation experience.", "keywords": ["object tracking", "animation", "tangible user interface"], "published_in": "UIST '12: Proceedings of the 25th annual ACM symposium on User interface software and technology", "publication_date": "7 October 2012", "citations": "68", "isbn": "9781450315807", "doi": "10.1145/2380116", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2380116.2380170", "paper_url": "https://dl.acm.org/doi/10.1145/2380116.2380170"}, {"title": "Evaluation of Interaction Techniques for a Virtual Reality Reading Room in Diagnostic Radiology", "authors": ["Markus Wirth", "Stefan Gradl", "Jan Sembdner", "Soeren Kuhrt", "Bjoern M. Eskofier"], "abstract": "Today, radiologists diagnose three dimensional medical data using two dimensional displays. When designing environments with optimal conditions for such a process various aspects like contrast, screen reflection and background light have to be considered. As shown in previous research, applying virtual environments in combination with a Head-Mounted Display for diagnostic imaging provides potential benefits to reduce issues of bad posture and diagnostic mistakes. However, there is little research in exploring the usability and user experience of such beneficial environments. In this work we designed and evaluated different means of interaction to increase radiologists' performance. Therefore we created a virtual reality radiology reading room and employed it to evaluate three different interaction techniques. These allow a direct, semi-direct and indirect manipulation for performing scrolling- and windowing- tasks which are the most important for a radiologist. A study including nine radiologists was conducted and evaluated using the User Experience Questionnaire. Results indicate that direct manipulation is the preferred interaction technique, it outscored the other two control possibilities in attractiveness and pragmatic quality.                     References                 Mike Alger. 2015. Visual design methods for virtual reality. Ravensbourne. http://aperturesciencellc. com/vr/VisualDesignMethodsforVR_MikeAlger. pdf (2015).Google ScholarDoug A. Bowman and Larry F. Hodges. 1997. An Evaluation of Techniques for Grabbing and Manipulating Remote Objects in Immersive Virtual Environments. In Proceedings of the 1997 Symposium on Interactive 3D Graphics (I3D '97). ACM, New York, NY, USA, 35--ff.  Google ScholarSteve Bryson. 1996. Virtual reality in scientific visualization. Commun. ACM 39, 5 (1996), 62--71.  Google ScholarStuart K Card, Jock D Mackinlay, and George G Robertson. 1990. The design space of input devices. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 117--124.  Google ScholarAnthony M Codd and Bipasha Choudhury. 2011. Virtual reality anatomy: is it comparable with traditional methods in the teaching of human forearm musculoskeletal anatomy? Anatomical sciences education 4, 3 (2011), 119--125.Google ScholarDane Coffey, Nicholas Malbraaten, Trung Bao Le, Iman Borazjani, Fotis Sotiropoulos, Arthur G Erdman, and Daniel F Keefe. 2012. Interactive slice WIM: Navigating and interrogating volume data sets using a multisurface, multitouch VR interface. IEEE Transactions on Visualization and Computer Graphics 18, 10 (2012), 1614--1626.  Google ScholarLindo Duratti, Fei Wang, Evren Samur, and Hannes Bleuler. 2008. A real-time simulator for interventional radiology. In Proceedings of the 2008 ACM symposium on Virtual reality software and technology. ACM, 105--108.  Google ScholarAnthony G Gallagher, E Matt Ritter, Howard Champion, Gerald Higgins, Marvin P Fried, Gerald Moses, C Daniel Smith, and Richard M Satava. 2005. Virtual reality simulation for the operating room: proficiency-based training as a paradigm shift in surgical skills training. Annals of surgery 241, 2 (2005), 364.Google ScholarSujin Jang, Wolfgang Stuerzlinger, Satyajit Ambike, and Karthik Ramani. 2017. Modeling Cumulative Arm Fatigue in Mid-Air Interaction based on Perceived Exertion and Kinetics of Arm Motion. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 3328--3339.  Google ScholarJason Jerald. 2015. The VR book: human-centered design for virtual reality. Morgan &amp; Claypool. Google ScholarFranklin King, Jagadeesan Jayender, Sharath K Bhagavatula, Paul B Shyn, Steve Pieper, Tina Kapur, Andras Lasso, and Gabor Fichtinger. 2016. An immersive virtual reality environment for diagnostic imaging. Journal of Medical Robotics Research 1, 01 (2016), 1640003.Google ScholarRalf A Kockro, Christina Amaxopoulou, Tim Killeen, Wolfgang Wagner, Robert Reisch, Eike Schwandt, Angelika Gutenberg, Alf Giese, Eckart Stofft, and Axel T Stadie. 2015. Stereoscopic neuroanatomy lectures using a three-dimensional virtual reality environment. Annals of Anatomy-Anatomischer Anzeiger 201 (2015), 91--98.Google ScholarBireswar Laha, Doug A Bowman, and James D Schiffbauer. 2013. Validation of the MR simulation approach for evaluating the effects of immersion on visual analysis of volume data. IEEE transactions on visualization and computer graphics 19, 4 (2013), 529--538.  Google ScholarBireswar Laha, Kriti Sensharma, James D Schiffbauer, and Doug A Bowman. 2012. Effects of immersion on visual analysis of volume data. IEEE transactions on visualization and computer graphics 18, 4 (2012), 597--606.  Google ScholarBettina Laugwitz, Theo Held, and Martin Schrepp. 2008. Construction and evaluation of a user experience questionnaire. In Symposium of the Austrian HCI and Usability Engineering Group. Springer, 63--76.  Google ScholarKate Laver, Stacey George, Susie Thomas, Judith E. Deutsch, and Maria Crotty. 2012. Virtual Reality for Stroke Rehabilitation. Stroke 43, 2 (2012), e20--e21.Google ScholarJonathan Lazar, Jinjuan Heidi Feng, and Harry Hochheiser. 2017. Research methods in human-computer interaction. Morgan Kaufmann. Google ScholarAnne Collins McLaughlin, Wendy A. Rogers, and Arthur D. Fisk. 2009. Using Direct and Indirect Input Devices: Attention Demands and Age-related Differences. ACM Trans. Comput.-Hum. Interact. 16, 1, Article 2 (April 2009), 15 pages.  Google ScholarKensaku Mori, Akihiro Urano, Jun-ichi HASEGAWA, Jun-ichiro TORIWAKI, Hirofumi Anno, and Kazuhiro Katada. 1996. Virtualized Endoscope System--An Application of Virtual Reality Technology to Diagnostic Aid--. IEICE TRANSACTIONS on Information and Systems 79, 6 (1996), 809--819.Google ScholarIvan Poupyrev, Mark Billinghurst, Suzanne Weghorst, and Tadao Ichikawa. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in VR. In Proceedings of the 9th annual ACM symposium on User interface software and technology. ACM, 79--80.  Google ScholarBruce I Reiner, Eliot L Siegel, and Bill Rostenberg. 1999. Redesigning the PACS reading room: optimizing monitor and room lighting. In Medical Imaging 1999: PACS design and evaluation: engineering and clinical issues, Vol. 3662. International Society for Optics and Photonics, 276--281.Google ScholarNeal E Seymour, Anthony G Gallagher, Sanziana A Roman, Michael K O'brien, Vipin K Bansal, Dana K Andersen, and Richard M Satava. 2002. Virtual reality training improves operating room performance: results of a randomized, double-blinded study. Annals of surgery 236, 4 (2002), 458.Google ScholarAdalberto L Simeone. 2016. Indirect touch manipulation for interaction with stereoscopic displays. In 3D User Interfaces (3DUI), 2016 IEEE Symposium on. IEEE, 13--22.Google ScholarMaur'\u0131cio Sousa, Daniel Mendes, Soraia Paulo, Nuno Matela, Joaquim Jorge, and Daniel Sim oes Lopes. 2017. VRRRRoom: Virtual Reality for Radiologists in the Reading Room. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 4057--4062.  Google ScholarL\u00e1szl\u00f3 Szil\u00e1gyi and Zolt\u00e1n Beny\u00f3. 2010. Development of a virtual reality guided diagnostic tool based on magnetic resonance imaging. Acta Physiologica Hungarica 97, 3 (2010), 267--280.Google ScholarDavid J Vining, Kun Liu, Robert H Choplin, and Edward F Haponik. 1996. Virtual bronchoscopy: relationships of virtual reality endobronchial simulations to actual bronchoscopic findings. Chest 109, 2 (1996), 549--553.Google ScholarKirby G Vosburgh, Alexandra Golby, and Steven D Pieper. 2013. Surgery, Virtual Reality, and the Future. Studies in health technology and informatics 184 (2013), vii.Google ScholarKarel J Zuiderveld. 1996. VR in radiology-first experiences at University Hospital Utrecht. ACM SIGGRAPH Computer Graphics 30, 4 (1996), 47--48.  Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242636/2b4c1441-1c7c-4c03-b1fa-8197dec29893/ufp1329p.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc04507070e65bcf82ec10a7f14509d0fa7a707e746166242bfea99435d9c73376ef1bc48c34ae9342aed3d4a6c8d4f9eba55b38c352634dc4366764fb79aeapplication/x-mpegurlmp411.2 MB", "keywords": ["virtual environment", "direct interaction", "diagnostic radiology", "interaction techniques", "indirect interaction", "virtual reality"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "4", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242636", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242636"}, {"title": "Mutual Human Actuation", "authors": ["Lung-Pan Cheng", "Sebastian Marwecki", "Patrick Baudisch"], "abstract": "Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1) offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2) synchronizing the two users' timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.", "keywords": ["haptic turk", "virtual reality", "haptics", "immersion"], "published_in": "UIST '17: Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology", "publication_date": "20 October 2017", "citations": "24", "isbn": "9781450349819", "doi": "10.1145/3126594", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3126594.3126667", "paper_url": "https://dl.acm.org/doi/10.1145/3126594.3126667"}, {"title": "ILoveSketch: as-natural-as-possible sketching system for creating 3d curve models", "authors": ["Seok-Hyung Bae", "Ravin Balakrishnan", "Karan Singh"], "abstract": "We present ILoveSketch, a 3D curve sketching system that captures some of the affordances of pen and paper for professional designers, allowing them to iterate directly on concept 3D curve models. The system coherently integrates existing techniques of sketch-based interaction with a number of novel and enhanced features. Novel contributions of the system include automatic view rotation to improve curve sketchability, an axis widget for sketch surface selection, and implicitly inferred changes between sketching techniques. We also improve on a number of existing ideas such as a virtual sketchbook, simplified 2D and 3D view navigation, multi-stroke NURBS curve creation, and a cohesive gesture vocabulary. An evaluation by a professional designer shows the potential of our system for deployment within a real design process.", "keywords": ["sketchability", "implicit mode change", "axis widget", "sketch-based modeling", "product design", "3d curve"], "published_in": "UIST '08: Proceedings of the 21st annual ACM symposium on User interface software and technology", "publication_date": "19 October 2008", "citations": "150", "isbn": "9781595939753", "doi": "10.1145/1449715", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1449715.1449740", "paper_url": "https://dl.acm.org/doi/10.1145/1449715.1449740"}, {"title": "Mixed-Reality for Object-Focused Remote Collaboration", "authors": ["Martin Feick", "Anthony Tang", "Scott Bateman"], "abstract": "In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators' perspectives on the object as well as understand one another's perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other's perspective; (2) explore objects independently from one another, and (3) render gestures in the remote's workspace. In this work, we focus on the expert's role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.", "keywords": ["ar", "mixed reality", "vr", "object-focused remote collaboration"], "published_in": "UIST '18 Adjunct: The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings", "publication_date": "11 October 2018", "citations": "0", "isbn": "9781450359498", "doi": "10.1145/3266037", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3266037.3266102", "paper_url": "https://dl.acm.org/doi/10.1145/3266037.3266102"}, {"title": "Eye&amp;Head: Synergetic Eye and Head Movement for Gaze Pointing and Selection", "authors": ["Ludwig Sidenmark", "Hans Gellersen"], "abstract": "Eye gaze involves the coordination of eye and head movement to acquire gaze targets, but existing approaches to gaze pointing are based on eye-tracking in abstraction from head motion. We propose to leverage the synergetic movement of eye and head, and identify design principles for Eye&amp;Head gaze interaction. We introduce three novel techniques that build on the distinction of head-supported versus eyes-only gaze, to enable dynamic coupling of gaze and pointer, hover interaction, visual exploration around pre-selections, and iterative and fast confirmation of targets. We demonstrate Eye&amp;Head interaction on applications in virtual reality, and evaluate our techniques against baselines in pointing and confirmation studies. Our results show that Eye&amp;Head techniques enable novel gaze behaviours that provide users with more control and flexibility in fast gaze pointing and selection.", "keywords": ["eye tracking", "virtual reality", "3d interaction", "targetselection", "gaze interaction", "eye-head coordination"], "published_in": "UIST '19: Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "17 October 2019", "citations": "6", "isbn": "9781450368162", "doi": "10.1145/3332165", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332165.3347921", "paper_url": "https://dl.acm.org/doi/10.1145/3332165.3347921"}, {"title": "Hands-on math: a page-based multi-touch and pen desktop for technical work and problem solving", "authors": ["Robert Zeleznik", "Andrew Bragdon", "Ferdi Adeputra", "Hsu-Sheng Ko"], "abstract": "Students, scientists and engineers have to choose between the flexible, free-form input of pencil and paper and the computational power of Computer Algebra Systems (CAS) when solving mathematical problems. Hands-On Math is a multi-touch and pen-based system which attempts to unify these approaches by providing virtual paper that is enhanced to recognize mathematical notations as a means of providing in situ access to CAS functionality. Pages can be created and organized on a large pannable desktop, and mathematical expressions can be computed, graphed and manipulated using a set of uni- and bi-manual interactions which facilitate rapid exploration by eliminating tedious and error prone transcription tasks. Analysis of a qualitative pilot evaluation indicates the potential of our approach and highlights usability issues with the novel techniques used.", "keywords": ["pages", "gestures", "math", "stylus", "multi-touch", "paper"], "published_in": "UIST '10: Proceedings of the 23nd annual ACM symposium on User interface software and technology", "publication_date": "3 October 2010", "citations": "41", "isbn": "9781450302715", "doi": "10.1145/1866029", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1866029.1866035", "paper_url": "https://dl.acm.org/doi/10.1145/1866029.1866035"}, {"title": "Estimating Focused Object using Smooth Pursuit Eye Movements and Interest Points in the Real World", "authors": ["Yuto Tamura", "Kentaro Takemura"], "abstract": "User calibration is a significant problem in eye-based interaction. To overcome this, several solutions, such as the calibration-free method and implicit user calibration, have been proposed. Pursuits-based interaction is another such solution that has been studied for public screens and virtual reality. It has been applied to select graphical user interfaces (GUIs) because the movements in a GUI can be designed in advance. Smooth pursuit eye movements (smooth pursuits) occur when a user looks at objects in the physical space as well and thus, we propose a method to identify the focused object by using smooth pursuits in the real world. We attempted to determine the focused objects without prior information under several conditions by using the pursuits-based approach and confirmed the feasibility and limitations of the proposed method through experimental evaluations.", "keywords": ["eye tracking", "real world", "smooth pursuit eye movement", "focused object"], "published_in": "UIST '19: The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology", "publication_date": "14 October 2019", "citations": "0", "isbn": "9781450368179", "doi": "10.1145/3332167", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3332167.3357102", "paper_url": "https://dl.acm.org/doi/10.1145/3332167.3357102"}, {"title": "Imaginary reality gaming: ball games without a ball", "authors": ["Patrick Baudisch", "Henning Pohl", "Stefanie Reinicke", "Emilia Wittmers", "Patrick L\u00fchne", "Marius Knaust", "Sven K\u00f6hler", "Patrick Schmidt", "Christian Holz"], "abstract": "We present imaginary reality games, i.e., games that mimic the respective real world sport, such as basketball or soccer, except that there is no visible ball. The ball is virtual and players learn about its position only from watching each other act and a small amount of occasional auditory feed-back, e.g., when a person is receiving the ball. Imaginary reality games maintain many of the properties of physical sports, such as unencumbered play, physical exertion, and immediate social interaction between players. At the same time, they allow introducing game elements from video games, such as power-ups, non-realistic physics, and player balancing. Most importantly, they create a new game dynamic around the notion of the invisible ball. To allow players to successfully interact with the invisible ball, we have created a physics engine that evaluates all plausible ball trajectories in parallel, allowing the game engine to select the trajectory that leads to the most enjoyable game play while still favoring skillful play.", "keywords": ["motion capture", "probabilistic", "physical gaming", "augmented reality gaming", "imaginary interfaces"], "published_in": "UIST '13: Proceedings of the 26th annual ACM symposium on User interface software and technology", "publication_date": "8 October 2013", "citations": "26", "isbn": "9781450322683", "doi": "10.1145/2501988", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2501988.2502012", "paper_url": "https://dl.acm.org/doi/10.1145/2501988.2502012"}, {"title": "MobiLimb: Augmenting Mobile Devices with a Robotic Limb", "authors": ["Marc Teyssier", "Gilles Bailly", "Catherine Pelachaud", "Eric Lecolinet"], "abstract": "In this paper, we explore the interaction space of MobiLimb, a small 5-DOF serial robotic manipulator attached to a mobile device. It (1) overcomes some limitations of mobile devices (static, passive, motionless); (2) preserves their form factor and I/O capabilities; (3) can be easily attached to or removed from the device; (4) offers additional I/O capabilities such as physical deformation and (5) can support various modular elements such as sensors, lights or shells. We illustrate its potential through three classes of applications: As a tool, MobiLimb offers tangible affordances and an expressive controller that can be manipulated to control virtual and physical objects. As a partner, it reacts expressively to users' actions to foster curiosity and engagement or assist users. As a medium, it provides rich haptic feedback such as strokes, pat and other tactile stimuli on the hand or the wrist to convey emotions during mediated multimodal communications.                     References                 Sigurdur O Adalgeirsson and Cynthia Breazeal. 2010. MeBot: a robotic platform for socially embodied presence. In Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction. IEEE Press, 15--22. Google ScholarJason Alexander, Anne Roudaut, J\u00fcrgen Steimle, Kasper Hornb\u00e6k, Miguel Bruns Alonso, Sean Follmer, and Timothy Merritt. 2018. Grand Challenges in Shape-Changing Interface Research. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA, Article 299, 14 pages.  Google ScholarBruno Araujo, Ricardo Jota, Varun Perumal, Jia Xian Yao, Karan Singh, and Daniel Wigdor. 2016. Snake Charmer: Physically Enabling Virtual Objects. In Proceedings of the TEI'16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction. ACM, 218--226.  Google ScholarGilles Bailly, Sidharth Sahdev, Sylvain Malacria, and Thomas Pietrzak. 2016. LivingDesktop: Augmenting Desktop Workstation with Actuated Devices. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 5298--5310.  Google ScholarPatrick Baudisch and Gerry Chu. 2009. Back-of-device interaction allows creating very small touch devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1923--1932.  Google ScholarMichel Beaudouin-Lafon. 2004. Designing Interaction, Not Interfaces. In Proceedings of the Working Conference on Advanced Visual Interfaces (AVI '04). ACM, New York, NY, USA, 15--22.  Google ScholarAndrea Bianchi and Ian Oakley. 2013. Designing tangible magnetic appcessories. In Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction. ACM, 255--258.  Google ScholarAlex Butler and others. 2008. SideSight: Multi-\" touch\" Interaction around Small Devices\", In the proceedings of the 21st annual ACM symposium on User interface software and technology. (2008).  Google ScholarLiwei Chan, Stefanie M\u00fcller, Anne Roudaut, and Patrick Baudisch. 2012. CapStones and ZebraWidgets: sensing stacks of building blocks, dials and sliders on capacitive touch screens. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2189--2192.  Google ScholarNele Dael, Marcello Mortillaro, and Klaus R Scherer. 2012. Emotion expression in body action and posture. Emotion 12, 5 (2012), 1085.Google ScholarRaphael Deimel and Oliver Brock. 2013. A compliant hand based on a novel pneumatic actuator. In Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2047--2053.Google ScholarBrian R Duffy. 2003. Anthropomorphism and the social robot. Robotics and autonomous systems 42, 3--4 (2003), 177--190.Google ScholarRoberta Etzi, Charles Spence, and Alberto Gallace. 2014. Textures that we like to touch: An experimental study of aesthetic preferences for tactile stimuli. Consciousness and Cognition 29 (2014), 178--188.Google ScholarSean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii. 2013. inFORM: dynamic physical affordances and constraints through shape and object actuation.. In Uist, Vol. 13. 417--426.  Google ScholarYona Falinie A Gaus, Temitayo Olugbade, Asim Jan, Rui Qin, Jingxin Liu, Fan Zhang, Hongying Meng, and Nadia Bianchi-Berthouze. 2015. Social touch gesture recognition using random forest and boosting on distinct feature sets. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. ACM, 399--406.  Google ScholarAntonio Gomes, Andrea Nesbitt, and Roel Vertegaal. 2013. MorePhone: a study of actuated shape deformations for flexible thin-film smartphone notifications. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 583--592.  Google ScholarGoogle. 2018. ARCore Platform. Website. (01 April 2018). Retrieved April 01, 2018 from https://developers.google.com/ar/discover/.Google ScholarAntal Haans and Wijnand IJsselsteijn. 2006. Mediated social touch: a review of current research and future directions. Virtual Reality 9, 2--3 (2006), 149--159. Google ScholarFabian Hemmert. 2009. Life in the Pocket--The Ambient Life Project: Life-Like Movements in Tactile Ambient. International Journal of Ambient Computing and Intelligence (IJACI) 1, 2 (2009), 13--19.Google ScholarFabian Hemmert, Matthias L\u00f6we, Anne Wohlauf, and Gesche Joost. 2013. Animate mobiles: proxemically reactive posture actuation as a means of relational interaction with mobile phones. In Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction. ACM, 267--270.  Google ScholarMatthew J Hertenstein, Rachel Holmes, Margaret McCullough, and Dacher Keltner. 2009. The communication of emotion via touch. Emotion 9, 4 (2009), 566.Google ScholarMatthew J Hertenstein, Dacher Keltner, Betsy App, Brittany a Bulleit, and Ariane R Jaskolka. 2006. Touch communicates distinct emotions. Emotion (2006).Google ScholarKen Hinckley, Morgan Dixon, Raman Sarin, Francois Guimbretiere, and Ravin Balakrishnan. 2009. Codex: a dual screen tablet computer. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1933--1942.  Google ScholarGuy Hoffman. 2012. Dumb robots, smart phones: A case study of music listening companionship. In RO-MAN, 2012 IEEE. IEEE, 358--363.Google ScholarGuy Hoffman and Wendy Ju. 2014. Designing robots with movement in mind. Journal of Human-Robot Interaction 3, 1 (2014), 89--122.  Google ScholarYuhan Hu, Sang-won Leigh, and Pattie Maes. 2017. Hand Development Kit: Soft Robotic Fingers as Prosthetic Augmentation of the Hand. In Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology. ACM, 27--29.  Google ScholarGijs Huisman, Adu\u00e9n Darriba Frederiks, and Dirk Heylen. 2013. Affective touch at a distance. In Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on. IEEE, 701--702.  Google ScholarGijs Huisman, Jan Kolkmeier, and Dirk Heylen. 2014. With us or against us: simulated social touch by virtual agents in a cooperative or competitive setting. In International Conference on Intelligent Virtual Agents. Springer, 204--213.Google ScholarIrfan Hussain, Gionata Salvietti, Giovanni Spagnoletti, and Domenico Prattichizzo. 2016. The soft-sixthfinger: a wearable emg controlled robotic extra-finger for grasp compensation in chronic stroke patients. IEEE Robotics and Automation Letters 1, 2 (2016), 1000--1006.Google ScholarSungjae Hwang, Myungwook Ahn, and Kwang-yun Wohn. 2013. MagGetz: customizable passive tangible controllers on and around conventional mobile devices. In Proceedings of the 26th annual ACM symposium on User interface software and technology. ACM, 411--416.  Google ScholarHiroshi Ishii, D\u00e1vid Lakatos, Leonardo Bonanni, and Jean-Baptiste Labrune. 2012. Radical atoms: beyond tangible bits, toward transformable materials. interactions 19, 1 (2012), 38--51.  Google ScholarBaha Jabarin, James Wu, Roel Vertegaal, and Lenko Grigorov. 2003. Establishing remote conversations through eye contact with physical awareness proxies. In CHI'03 Extended Abstracts on Human Factors in Computing Systems. ACM, 948--949.  Google ScholarDigital LibraryAlec Jacobson, Daniele Panozzo, Oliver Glauser, C\u00e9dric Pradalier, Otmar Hilliges, and Olga Sorkine-Hornung. 2014. Tangible and modular input device for character articulation. ACM Transactions on Graphics (TOG) 33, 4 (2014), 82.  Google ScholarSungjune Jang, Lawrence H Kim, Kesler Tanner, Hiroshi Ishii, and Sean Follmer. 2016. Haptic edge display for mobile tactile interaction. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3706--3716.  Google ScholarTadakazu Kashiwabara, Hirotaka Osawa, Kazuhiko Shinozawa, and Michita Imai. 2012. TEROOS: a wearable avatar to enhance joint activities. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2001--2004.  Google ScholarGierad Laput, Eric Brockmeyer, Scott E Hudson, and Chris Harrison. 2015. Acoustruments: Passive, acoustically-driven, interactive controls for handheld devices. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 2161--2170.  Google ScholarMathieu Le Goc, Lawrence H Kim, Ali Parsaei, Jean-Daniel Fekete, Pierre Dragicevic, and Sean Follmer. 2016. Zooids: Building blocks for swarm user interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 97--109.  Google ScholarSang-won Leigh, Timothy Denton, Kush Parekh, William Peebles, Magnus Johnson, and Pattie Maes. 2018. Morphology Extension Kit: A Modular Robotic Platform for Physically Reconfigurable Wearables. In Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction. ACM, 11--18.  Google ScholarPaul Lemmens, Floris Crompvoets, Dirk Brokken, Jack Van Den Eerenbeemd, and Gert-Jan de Vries. 2009. A body-conforming tactile jacket to enrich movie viewing. In EuroHaptics Conference, 2009 and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics 2009. Third Joint. IEEE, 7--12.  Google ScholarRong-Hao Liang, Kai-Yin Cheng, Liwei Chan, Chuan-Xhyuan Peng, Mike Y Chen, Rung-Huei Liang, De-Nian Yang, and Bing-Yu Chen. 2013. GaussBits: magnetic tangible bits for portable and occlusion-free near-surface interactions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1391--1400.  Google ScholarNatan Linder and Pattie Maes. 2010. LuminAR: portable robotic augmented reality interface design and prototype. In Adjunct proceedings of the 23nd annual ACM symposium on User interface software and technology. ACM, 395--396.  Google ScholarRafael Morales Gonz\u00e1lez, Caroline Appert, Gilles Bailly, and Emmanuel Pietriga. 2017. Passive Yet Expressive TouchTokens. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 3741--3745.  Google ScholarMasahiro Mori. 1970. The uncanny valley. Energy 7, 4 (1970), 33--35.Google ScholarKen Nakagaki, Artem Dementyev, Sean Follmer, Joseph A Paradiso, and Hiroshi Ishii. 2016. ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 87--96.  Google ScholarKen Nakagaki, Sean Follmer, and Hiroshi Ishii. 2015. Lineform: Actuated curve interfaces for display, interaction, and constraint. In Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology. ACM, 333--339.  Google ScholarMasaru Ohkubo, Shuhei Umezu, and Takuya Nojima. 2016. Come alive! Augmented Mobile Interaction with Smart Hair. In Proceedings of the 7th Augmented Human International Conference 2016. ACM, 32.  Google ScholarH\u00e5kan Olausson, Johan Wessberg, Francis McGlone, and \u00c5ke Vallbo. 2010. The neurophysiology of unmyelinated tactile afferents. Neuroscience &amp; Biobehavioral Reviews 34, 2 (2010), 185--191.Google ScholarJoohee Park, Young-Woo Park, and Tek-Jin Nam. 2014. Wrigglo: shape-changing peripheral for interpersonal mobile communication. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 3973--3976.  Google ScholarYoung-Woo Park, Joohee Park, and Tek-Jin Nam. 2015. The trial of bendi in a coffeehouse: use of a shape-changing device for a tactile-visual phone conversation. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 2181--2190.  Google ScholarEsben W Pedersen, Sriram Subramanian, and Kasper Hornb\u00e6k. 2014. Is my phone alive?: a large-scale study of shape change in handheld devices using videos. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems. ACM, 2579--2588.  Google ScholarLazlo Ring, Timothy Bickmore, and Paola Pedrelli. 2016. An affectively aware virtual therapist for depression counseling. In ACM SIGCHI Conference on Human Factors in Computing Systems (CHI) workshop on Computing and Mental Health.Google ScholarAlbert Rizzo, Russell Shilling, Eric Forbell, Stefan Scherer, Jonathan Gratch, and Louis-Philippe Morency. 2016. Autonomous virtual human agents for healthcare information support and clinical interviewing. In Artificial intelligence in behavioral and mental health care. Elsevier, 53--79.Google ScholarE Rocon, AF Ruiz, JL Pons, Jos\u00e9 M Belda-Lois, and JJ S\u00e1nchez-Lacuesta. 2005. Rehabilitation robotics: a wearable exo-skeleton for tremor assessment and suppression. In Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on. IEEE, 2271--2276.Google ScholarAnne Roudaut, Abhijit Karnik, Markus L\u00f6chtefeld, and Sriram Subramanian. 2013. Morphees: toward high shape resolution in self-actuated flexible mobile devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 593--602.  Google ScholarAnne Roudaut, Diana Krusteva, Mike McCoy, Abhijit Karnik, Karthik Ramani, and Sriram Subramanian. 2016. Cubimorph: designing modular interactive devices. In Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 3339--3345.Google ScholarA.F. Rovers and H.A. van Essen. 2004. HIM: A Framework for Haptic Instant Messaging. In CHI '04 Extended Abstracts on Human Factors in Computing Systems (CHI EA '04). ACM, New York, NY, USA, 1313--1316.  Google ScholarKatri Salminen, Veikko Surakka, Jukka Raisamo, Jani Lylykangas, Johannes Pystynen, Roope Raisamo, Kalle M\u00e4kel\u00e4, and Teemu Ahmaniemi. 2011. Emotional responses to thermal stimuli. In Proceedings of the 13th international conference on multimodal interfaces. ACM, 193--196.  Google ScholarJulian Seifert, Sebastian Boring, Christian Winkler, Florian Schaub, Fabian Schwab, Steffen Herrdum, Fabian Maier, Daniel Mayer, and Enrico Rukzio. 2014. Hover Pad: interacting with autonomous and self-actuated displays in space. In Proceedings of the 27th annual ACM symposium on User interface software and technology. ACM, 139--147.  Google ScholarPaul Strohmeier, Juan Pablo Carrascal, Bernard Cheng, Margaret Meban, and Roel Vertegaal. 2016. An Evaluation of Shape Changes for Conveying Emotions. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 3781--3792.  Google ScholarIvan E Sutherland. 1965. The ultimate display. Multimedia: From Wagner to virtual reality (1965), 506--508.Google ScholarJan BF Van Erp and Alexander Toet. 2015. Social touch in human--computer interaction. Frontiers in digital humanities 2 (2015), 2.Google ScholarFaye Y Wu and Harry Asada. 2014. Bio-artificial synergies for grasp posture control of supernumerary robotic fingers. (2014).Google ScholarFaye Y Wu and H Harry Asada. 2015. \"Hold-and-manipulate\" with a single hand being assisted by wearable extra fingers. In Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 6205--6212.Google ScholarSteve Yohanan, Mavis Chan, Jeremy Hopkins, Haibo Sun, and Karon MacLean. 2005. Hapticat: exploration of affective touch. In Proceedings of the 7th international conference on Multimodal interfaces. ACM, 222--229.  Google ScholarYongjae Yoo, Taekbeom Yoo, Jihyun Kong, and Seungmoon Choi. 2015. Emotional responses of tactile icons: Effects of amplitude, frequency, duration, and envelope. In World Haptics Conference (WHC), 2015 IEEE. IEEE, 235--240.Google ScholarRan Zhao, Alexandros Papangelis, and Justine Cassell. 2014. Towards a dyadic computational model of rapport management for human-virtual agent interaction. In Intelligent Virtual Agents. Springer, 514--527.Google ScholarSupplemental Materialvideohttps://acm-prod-streaming.literatumonline.com/3242587.3242626/c5ac2f3f-a39f-46c2-8e70-c305104dd4eb/p53-teyssier.,180,300,750,964,1500,.mp4.m3u8?b92b4ad1b4f274c70877518315abb28be831d54738a81f1de54388f7ee05e3e1e72b3e6cec97dc04507170e60a98d7ec1901e6df794d22964d0c7e1070239f68848e66fbf4d73bfb9b2c449754bca7cbdf6143dc0f01d5fc5b57300cfb110f871ac9eec986application/x-mpegurlmp4374.4 MB", "keywords": ["actuated device", "mobile device", "mobile augmentation", "robotic limb", "robotics"], "published_in": "UIST '18: Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology", "publication_date": "11 October 2018", "citations": "4", "isbn": "9781450359481", "doi": "10.1145/3242587", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242626", "paper_url": "https://dl.acm.org/doi/10.1145/3242587.3242626"}, {"title": "Combining multiple depth cameras and projectors for interactions on, above and between surfaces", "authors": ["Andrew D. Wilson", "Hrvoje Benko"], "abstract": "Instrumented with multiple depth cameras and projectors, LightSpace is a small room installation designed to explore a variety of interactions and computational strategies related to interactive displays and the space that they inhabit. LightSpace cameras and projectors are calibrated to 3D real world coordinates, allowing for projection of graphics correctly onto any surface visible by both camera and projector. Selective projection of the depth camera data enables emulation of interactive displays on un-instrumented surfaces (such as a standard table or office desk), as well as facilitates mid-air interactions between and around these displays. For example, after performing multi-touch interactions on a virtual object on the tabletop, the user may transfer the object to another display by simultaneously touching the object and the destination display. Or the user may \"pick up\" the object by sweeping it into their hand, see it sitting in their hand as they walk over to an interactive wall display, and \"drop\" the object onto the wall by touching it with their other hand. We detail the interactions and algorithms unique to LightSpace, discuss some initial observations of use and suggest future directions.", "keywords": ["surface computing", "augmented reality", "depth cameras", "ubiquitous computing", "interactive spaces"], "published_in": "UIST '10: Proceedings of the 23nd annual ACM symposium on User interface software and technology", "publication_date": "3 October 2010", "citations": "203", "isbn": "9781450302715", "doi": "10.1145/1866029", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1866029.1866073", "paper_url": "https://dl.acm.org/doi/10.1145/1866029.1866073"}, {"title": "Interactions in the air: adding further depth to interactive tabletops", "authors": ["Otmar Hilliges", "Shahram Izadi", "Andrew D. Wilson", "Steve Hodges", "Armando Garcia-Mendoza", "Andreas Butz"], "abstract": "Although interactive surfaces have many unique and compelling qualities, the interactions they support are by their very nature bound to the display surface. In this paper we present a technique for users to seamlessly switch between interacting on the tabletop surface to above it. Our aim is to leverage the space above the surface in combination with the regular tabletop display to allow more intuitive manipulation of digital content in three-dimensions. Our goal is to design a technique that closely resembles the ways we manipulate physical objects in the real-world; conceptually, allowing virtual objects to be 'picked up' off the tabletop surface in order to manipulate their three dimensional position or orientation. We chart the evolution of this technique, implemented on two rear projection-vision tabletops. Both use special projection screen materials to allow sensing at significant depths beyond the display. Existing and new computer vision techniques are used to sense hand gestures and postures above the tabletop, which can be used alongside more familiar multi-touch interactions. Interacting above the surface in this way opens up many interesting challenges. In particular it breaks the direct interaction metaphor that most tabletops afford. We present a novel shadow-based technique to help alleviate this issue. We discuss the strengths and limitations of our technique based on our own observations and initial user feedback, and provide various insights from comparing, and contrasting, our tabletop implementations", "keywords": ["3D", "surfaces", "tabletop", "depth-sensing cameras", "interactive surfaces", "computer vision", "switchable diffusers", "3D graphics", "holoscreen"], "published_in": "UIST '09: Proceedings of the 22nd annual ACM symposium on User interface software and technology", "publication_date": "4 October 2009", "citations": "151", "isbn": "9781605587455", "doi": "10.1145/1622176", "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1622176.1622203", "paper_url": "https://dl.acm.org/doi/10.1145/1622176.1622203"}], "total_number_of_results": 32}