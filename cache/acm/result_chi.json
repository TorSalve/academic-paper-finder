{
  "papers": [
    {
      "title": "Selection-based Text Entry in Virtual Reality",
      "authors": [
        "Marco Speicher",
        "Anna Maria Feit",
        "Pascal Ziegler",
        "Antonio Kr\u00fcger"
      ],
      "abstract": "In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.",
      "keywords": [
        "user experience",
        "virtual reality",
        "task performance",
        "mid-air",
        "text entry",
        "pointing"
      ],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "21 April 2018",
      "citations": "14",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3174221",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3174221"
    },
    {
      "title": "Exploring Interaction Fidelity in Virtual Reality: Object Manipulation and Whole-Body Movements",
      "authors": [
        "Katja Rogers",
        "Jana Funke",
        "Julian Frommel",
        "Sven Stamm",
        "Michael Weber"
      ],
      "abstract": "High degrees of interaction fidelity (IF) in virtual reality (VR) are said to improve user experience and immersion, but there is also evidence of low IF providing comparable experiences. VR games are now increasingly prevalent, yet we still do not fully understand the trade-off between realism and abstraction in this context. We conducted a lab study comparing high and low IF for object manipulation tasks in a VR game. In a second study, we investigated players' experiences of IF for whole-body movements in a VR game that allowed players to crawl underneath virtual boulders and \"dangle'' along monkey bars. Our findings show that high IF is preferred for object manipulation, but for whole-body movements, moderate IF can suffice, as there is a trade-off with usability and social factors. We provide guidelines for the development of VR games based on our results.",
      "keywords": [
        "virtual objects",
        "virtual reality",
        "whole body interaction",
        "games",
        "interaction fidelity",
        "player experience"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "7",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300644",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300644"
    },
    {
      "title": "Crossing-Based Selection with Virtual Reality Head-Mounted Displays",
      "authors": [
        "Huawei Tu",
        "Susu Huang",
        "Jiabin Yuan",
        "Xiangshi Ren",
        "Feng Tian"
      ],
      "abstract": "This paper presents the first investigation into using the goal-crossing paradigm for object selection with virtual reality (VR) head-mounted displays. Two experiments were carried out to evaluate ray-casting crossing tasks with target discs in 3D space and goal lines on 2D plane respectively in comparison to ray-casting pointing tasks. Five factors, i.e. task difficulty, the direction of movement constraint (collinear vs. orthogonal), the nature of the task (discrete vs. continuous), field of view of VR devices and target depth, were considered in both experiments. Our findings are: (1) crossing generally had shorter or no longer time, and higher or similar accuracy than pointing, indicating crossing can complement or substitute pointing; (2) crossing tasks can be well modelled with Fitts' Law; (3) crossing performance depended on target depth; (4) crossing target discs in 3D space differed from crossing goal lines on 2D plane in many aspects such as time and error performance, the effects of target depth and the parameters of Fitts' models. Based on these findings, we formulate a number of design recommendations for crossing-based interaction in VR.",
      "keywords": [
        "virtual reality head-mounted displays",
        "crossing",
        "pointing",
        "ray-casting selection",
        "fitts' law"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "9",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300848",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300848"
    },
    {
      "title": "VirtualBricks: Exploring a Scalable, Modular Toolkit for Enabling Physical Manipulation in VR",
      "authors": [
        "Jatin Arora",
        "Aryan Saini",
        "Nirmita Mehra",
        "Varnit Jain",
        "Shwetank Shrey",
        "Aman Parnami"
      ],
      "abstract": "Often Virtual Reality (VR) experiences are limited by the design of standard controllers. This work aims to liberate a VR developer from these limitations in the physical realm to provide an expressive match to the limitless possibilities in the virtual realm. VirtualBricks is a LEGO based toolkit that enables construction of a variety of physical-manipulation enabled controllers for VR, by offering a set of feature bricks that emulate as well as extend the capabilities of default controllers. Based on the LEGO platform, the toolkit provides a modular, scalable solution for enabling passive haptics in VR. We demonstrate the versatility of our designs through a rich set of applications including re-implementations of artifacts from recent research. We share a VR Integration package for integration with Unity VR IDE, the CAD models for the feature bricks, for easy deployment of VirtualBricks within the community.",
      "keywords": [
        "construction toolkit",
        "physical manipulation",
        "passive haptics"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "1",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300286",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300286"
    },
    {
      "title": "A handle bar metaphor for virtual object manipulation with mid-air interaction",
      "authors": [
        "Peng Song",
        "Wooi Boon Goh",
        "William Hutama",
        "Chi-Wing Fu",
        "Xiaopei Liu"
      ],
      "abstract": "Commercial 3D scene acquisition systems such as the Microsoft Kinect sensor can reduce the cost barrier of realizing mid-air interaction. However, since it can only sense hand position but not hand orientation robustly, current mid-air interaction methods for 3D virtual object manipulation often require contextual and mode switching to perform translation, rotation, and scaling, thus preventing natural continuous gestural interactions. A novel handle bar metaphor is proposed as an effective visual control metaphor between the user's hand gestures and the corresponding virtual object manipulation operations. It mimics a familiar situation of handling objects that are skewered with a bimanual handle bar. The use of relative 3D motion of the two hands to design the mid-air interaction allows us to provide precise controllability despite the Kinect sensor's low image resolution. A comprehensive repertoire of 3D manipulation operations is proposed to manipulate single objects, perform fast constrained rotation, and pack/align multiple objects along a line. Three user studies were devised to demonstrate the efficacy and intuitiveness of the proposed interaction techniques on different virtual manipulation scenarios.",
      "keywords": ["user interaction", "3d manipulation", "bimanual gestures"],
      "published_in": "CHI '12: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "5 May 2012",
      "citations": "84",
      "isbn": "9781450310154",
      "doi": "10.1145/2207676",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2207676.2208585",
      "paper_url": "https://dl.acm.org/doi/10.1145/2207676.2208585"
    },
    {
      "title": "Object Manipulation in Virtual Reality Under Increasing Levels of Translational Gain",
      "authors": [
        "Graham Wilson",
        "Mark Mcgill",
        "Matthew Jamieson",
        "Julie R. Williamson",
        "Stephen A. Brewster"
      ],
      "abstract": "Room-scale Virtual Reality (VR) has become an affordable consumer reality, with applications ranging from entertainment to productivity. However, the limited physical space available for room-scale VR in the typical home or office environment poses a significant problem. To solve this, physical spaces can be extended by amplifying the mapping of physical to virtual movement (translational gain). Although amplified movement has been used since the earliest days of VR, little is known about how it influences reach-based interactions with virtual objects, now a standard feature of consumer VR. Consequently, this paper explores the picking and placing of virtual objects in VR for the first time, with translational gains of between 1x (a one-to-one mapping of a 3.5m*3.5m virtual space to the same sized physical space) and 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physical). Results show that reaching accuracy is maintained for up to 2x gain, however going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We suggest gain levels of 1.5x to 1.75x can be utilized without compromising the usability of a VR task, significantly expanding the bounds of interactive room-scale VR.",
      "keywords": [
        "translational gain",
        "virtual reality",
        "redirected walking",
        "object manipulation",
        "amplified movement"
      ],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "19 April 2018",
      "citations": "10",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3173673",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3173673"
    },
    {
      "title": "Pseudo-Haptic Weight: Changing the Perceived Weight of Virtual Objects By Manipulating Control-Display Ratio",
      "authors": [
        "Majed Samad",
        "Elia Gatti",
        "Anne Hermes",
        "Hrvoje Benko",
        "Cesare Parise"
      ],
      "abstract": "In virtual reality, the lack of kinesthetic feedback often prevents users from experiencing the weight of virtual objects. Control-to-display (C/D) ratio manipulation has been proposed as a method to induce weight perception without kinesthetic feedback. Based on the fact that lighter (heavier) objects are easier (harder) to move, this method induces an illusory perception of weight by manipulating the rendered position of users' hands---increasing or decreasing their displayed movements. In a series of experiments we demonstrate that C/D-ratio induces a genuine perception of weight, while preserving ownership over the virtual hand. This means that such a manipulation can be easily introduced in current VR experiences without disrupting the sense of presence. We discuss these findings in terms of estimation of physical work needed to lift an object. Our findings provide the first quantification of the range of C/D-ratio that can be used to simulate weight in virtual reality.",
      "keywords": [
        "pseudo-haptics",
        "virtual weight",
        "multisensory integration"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "14",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300550",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300550"
    },
    {
      "title": "RealityCheck: Blending Virtual Environments with Situated Physical Reality",
      "authors": [
        "Jeremy Hartmann",
        "Christian Holz",
        "Eyal Ofek",
        "Andrew D. Wilson"
      ],
      "abstract": "Today's virtual reality (VR) systems offer chaperone rendering techniques that prevent the user from colliding with physical objects. Without a detailed geometric model of the physical world, these techniques offer limited possibility for more advanced compositing between the real world and the virtual. We explore this using a realtime 3D reconstruction of the real world that can be combined with a virtual environment. RealityCheck allows users to freely move, manipulate, observe, and communicate with people and objects situated in their physical space without losing the sense of immersion or presence inside their virtual world. We demonstrate RealityCheck with seven existing VR titles, and describe compositing approaches that address the potential conflicts when rendering the real world and a virtual environment together. A study with frequent VR users demonstrate the affordances provided by our system and how it can be used to enhance current VR experiences.",
      "keywords": ["depth cameras", "3d compositing", "virtual reality"],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "4",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300577",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300577"
    },
    {
      "title": "FRVRIT: A Tool for Full Body Virtual Reality Game Evaluation",
      "authors": [
        "Daniel Maccormick",
        "Alain Sangalang",
        "Jackson Rushing",
        "Ravnik Singh Jagpal",
        "Pejman Mirza-Babaei",
        "Loutfouz Zaman"
      ],
      "abstract": "Virtual reality (VR) games continue to grow in popularity with the advancement of commercial VR capabilities such as the inclusion of full body tracking. This means game developers can design for novel interactions involving a player's full body rather than solely relying on controller input. However, existing research on evaluating player interaction in VR games primarily focuses on game content and inputs from game controllers or player hands. Current approaches for evaluating player full body interactions are limited to simple qualitative observation which makes evaluation difficult and time-consuming. We present a Full Room Virtual Reality Investigation Tool (FRVRIT) which combines data recording and visualization to provide a quantitative solution for evaluating player movement and interaction in VR games. The tool facilitates objective data observation through multiple visualization methods that can be manipulated, allowing developers to better observe and record player movements in the VR space to improve and iterate on the desired interactions in their games.",
      "keywords": [
        "full body",
        "analytics",
        "visualization",
        "tool",
        "GUR",
        "virtual reality",
        "full room"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "0",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3312825",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3312825"
    },
    {
      "title": "A Dose of Reality: Overcoming Usability Challenges in VR Head-Mounted Displays",
      "authors": [
        "Mark Mcgill",
        "Daniel Boland",
        "Roderick Murray-Smith",
        "Stephen Brewster"
      ],
      "abstract": "We identify usability challenges facing consumers adopting Virtual Reality (VR) head-mounted displays (HMDs) in a survey of 108 VR HMD users. Users reported significant issues in interacting with, and being aware of their real-world context when using a HMD. Building upon existing work on blending real and virtual environments, we performed three design studies to address these usability concerns. In a typing study, we show that augmenting VR with a view of reality significantly corrected the performance impairment of typing in VR. We then investigated how much reality should be incorporated and when, so as to preserve users' sense of presence in VR. For interaction with objects and peripherals, we found that selectively presenting reality as users engaged with it was optimal in terms of performance and users' sense of presence. Finally, we investigated how this selective, engagement-dependent approach could be applied in social environments, to support the user's awareness of the proximity and presence of others.",
      "keywords": ["augmented virtuality", "virtual reality", "engagement"],
      "published_in": "CHI '15: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems",
      "publication_date": "18 April 2015",
      "citations": "51",
      "isbn": "9781450331456",
      "doi": "10.1145/2702123",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2702123.2702382",
      "paper_url": "https://dl.acm.org/doi/10.1145/2702123.2702382"
    },
    {
      "title": "Beyond Horror and Fear: Exploring Player Experience Invoked by Emotional Challenge in VR Games",
      "authors": [
        "Xiaolan Peng",
        "Jin Huang",
        "Linghan Li",
        "Chen Gao",
        "Hui Chen",
        "Feng Tian",
        "Hongan Wang"
      ],
      "abstract": "Digital gameplay experience depends not only on the type of challenge that the game provides, but also on how the challenge be presented. With the introduction of a novel type of emotional challenge and the increasing popularity of virtual reality (VR), there is a need to explore player experience invoked by emotional challenge in VR games. We selected two games that provides emotional challenge and conducted a 24-subject experiment to compare the impact of a VR and monitor-display version of each game on multiple player experiences. Preliminary results show that many positive emotional experiences have been enhanced significantly with VR while negative emotional experiences such as horror and fear have less been influenced; participants' perceived immersion and presence were higher when using VR than using monitor-display. Our finding of VR's expressive capability in emotional experiences may encourage more design and research with regard to emotional challenge in VR games.",
      "keywords": [
        "games",
        "virtual reality",
        "emotion",
        "emotional challenge",
        "player experience"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "3",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3312832",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3312832"
    },
    {
      "title": "Emotional Beasts: Visually Expressing Emotions through Avatars in VR",
      "authors": ["Guillermo Bernal", "Pattie Maes"],
      "abstract": "With the advances in Virtual Reality (VR) and physiological sensing technology, even more immersive computer-mediated communication through life-like characteristics is now possible. In response to the current lack of culture, expression and emotions in VR avatars, we propose a two-fold solution. First, integration of bio-signal sensors into the HMD and techniques to detect aspects of the emotional state of the user. Second, the use of this data to generate expressive avatars which we refer to as Emotional Beasts. The creation of Emotional Beasts will allow us to experiment with the manipulation of a user's self-expression in VR space and as well as the perception of others in it, providing some valuable tools to evoke a desired emotional reaction. As this medium moves forward, this and other tools are what will help the field of virtual reality expand from a medium of surface-level experience to one of deep, emotionally compelling human-to-human connection.",
      "keywords": [
        "social virtual reality",
        "bio-signals",
        "affective computing",
        "self-expression",
        "avatars"
      ],
      "published_in": "CHI EA '17: Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems",
      "publication_date": "6 May 2017",
      "citations": "6",
      "isbn": "9781450346566",
      "doi": "10.1145/3027063",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3027063.3053207",
      "paper_url": "https://dl.acm.org/doi/10.1145/3027063.3053207"
    },
    {
      "title": "Vremiere: In-Headset Virtual Reality Video Editing",
      "authors": [
        "Cuong Nguyen",
        "Stephen Diverdi",
        "Aaron Hertzmann",
        "Feng Liu"
      ],
      "abstract": "Creative professionals are creating Virtual Reality (VR) experiences today by capturing spherical videos, but video editing is still done primarily in traditional 2D desktop GUI applications such as Premiere. These interfaces provide limited capabilities for previewing content in a VR headset or for directly manipulating the spherical video in an intuitive way. As a result, editors must alternate between editing on the desktop and previewing in the headset, which is tedious and interrupts the creative process. We demonstrate an application that enables a user to directly edit spherical video while fully immersed in a VR headset. We first interviewed professional VR filmmakers to understand current practice and derived a suitable workflow for in-headset VR video editing. We then developed a prototype system implementing this new workflow. Our system is built upon a familiar timeline design, but is enhanced with custom widgets to enable intuitive editing of spherical video inside the headset. We conducted an expert review study and found that with our prototype, experts were able to edit videos entirely within the headset. Experts also found our interface and widgets useful, providing intuitive controls for their editing needs.",
      "keywords": ["video editing", "virtual reality"],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "15",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025675",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025675"
    },
    {
      "title": "SeeingVR: A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision",
      "authors": [
        "Yuhang Zhao",
        "Edward Cutrell",
        "Christian Holz",
        "Meredith Ringel Morris",
        "Eyal Ofek",
        "Andrew D. Wilson"
      ],
      "abstract": "Current virtual reality applications do not support people who have low vision, i.e., vision loss that falls short of complete blindness but is not correctable by glasses. We present SeeingVR, a set of 14 tools that enhance a VR application for people with low vision by providing visual and audio augmentations. A user can select, adjust, and combine different tools based on their preferences. Nine of our tools modify an existing VR application post hoc via a plugin without developer effort. The rest require simple inputs from developers using a Unity toolkit we created that allows integrating all 14 of our low vision support tools during development. Our evaluation with 11 participants with low vision showed that SeeingVR enabled users to better enjoy VR and complete tasks more quickly and accurately. Developers also found our Unity toolkit easy and convenient to use.",
      "keywords": ["unity", "accessibility", "low vision", "virtual reality"],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "10",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300341",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300341"
    },
    {
      "title": "Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories",
      "authors": [
        "Markus Funk",
        "Florian M\u00fcller",
        "Marco Fendrich",
        "Megan Shene",
        "Moritz Kolvenbach",
        "Niclas Dobbertin",
        "Sebastian G\u00fcnther",
        "Max M\u00fchlh\u00e4user"
      ],
      "abstract": "Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.",
      "keywords": [
        "orientation indication",
        "locomotion",
        "teleportation",
        "point & teleport",
        "virtual environments",
        "virtual reality"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "6",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300377",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300377"
    },
    {
      "title": "TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction",
      "authors": [
        "Jaeyeon Lee",
        "Mike Sinclair",
        "Mar Gonzalez-Franco",
        "Eyal Ofek",
        "Christian Holz"
      ],
      "abstract": "Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. Our evaluation showed that using TORC, participants could manipulate virtual objects more precisely (e.g., position and rotate objects in 3D) than when using a conventional VR controller.",
      "keywords": [
        "vr object manipulation",
        "haptic texture",
        "haptics",
        "haptic compliance"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "18",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300301",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300301"
    },
    {
      "title": "WatchVR: Exploring the Usage of a Smartwatch for Interaction in Mobile Virtual Reality",
      "authors": [
        "Teresa Hirzle",
        "Jan Rixen",
        "Jan Gugenheimer",
        "Enrico Rukzio"
      ],
      "abstract": "Mobile virtual reality (VR) head-mounted displays (HMDs) are steadily becoming part of people's everyday life. Most current interaction approaches rely either on additional hardware (e.g. Daydream Controller) or offer only a liMassachusetts Institute of Technologyed interaction concept (e.g. Google Cardboard). We explore a solution where a conventional smartwatch, a device users already carry around with them, is used to enable short interactions but also allows for longer complex interactions with mobile VR. To explore the possibilities of a smartwatch for interaction, we conducted a user study in which we compared two variables with regard to user performance: interaction method (touchscreen vs inertial sensors) and wearing method (hand-held vs wrist-worn). We found that selection time and error rate were lowest when holding the smartwatch in one hand using its inertial sensors for interaction (hand-held).",
      "keywords": [
        "nomadic virtual reality",
        "mobile virtual reality",
        "3d pointing",
        "smartwatch"
      ],
      "published_in": "CHI EA '18: Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "20 April 2018",
      "citations": "8",
      "isbn": "9781450356213",
      "doi": "10.1145/3170427",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3170427.3188629",
      "paper_url": "https://dl.acm.org/doi/10.1145/3170427.3188629"
    },
    {
      "title": "Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality",
      "authors": ["Hemant Bhaskar Surale", "Fabrice Matulic", "Daniel Vogel"],
      "abstract": "We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard \"subtraction method\" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.",
      "keywords": ["controlled experiment", "interaction techniques"],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "7",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300426",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300426"
    },
    {
      "title": "A Dose of Reality: Overcoming Usability Challenges in VR Head-Mounted Displays",
      "authors": [
        "Mark Mcgill",
        "Roderick Murray-Smith",
        "Daniel Boland",
        "Stephen A. Brewster"
      ],
      "abstract": "This video presents insights into the usability challenges present in consumer VR Head-Mounted Displays regarding a users' capability to interact with and be aware of reality. We demonstrate how these issues can be overcome through selectively incorporating necessary elements of reality into VR, as a user engages with reality. We term this approach Engagement-Dependent Augmented Virtuality.",
      "keywords": ["virtual reality", "augmented virtuality", "engagement"],
      "published_in": "CHI EA '15: Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems",
      "publication_date": "18 April 2015",
      "citations": "1",
      "isbn": "9781450331463",
      "doi": "10.1145/2702613",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2702613.2732491",
      "paper_url": "https://dl.acm.org/doi/10.1145/2702613.2732491"
    },
    {
      "title": "\"When the Elephant Trumps\": A Comparative Study on Spatial Audio for Orientation in 360&#xba; Videos",
      "authors": ["Paulo Bala", "Raul Masu", "Valentina Nisi", "Nuno Nunes"],
      "abstract": "Orientation is an emerging issue in cinematic Virtual Reality (VR), as viewers may fail in locating points of interest. Recent strategies to tackle this research problem have investigated the role of cues, specifically diegetic sound effects. In this paper, we examine the use of sound spatialization for orientation purposes, namely by studying different spatialization conditions (\"none\", \"partial\", and \"full\" spatial manipulation) of multitrack soundtracks. We performed a between-subject mixed-methods study with 36 participants, aided by Cue Control, a tool we developed for dynamic spatial sound editing and data collection/analysis. Based on existing literature on orientation cues in 360\u00ba and theories on human listening, we discuss situations in which the spatialization was more effective (namely, \"full\" spatial manipulation both when using only music and when combining music and diegetic effects), and how this can be used by creators of 360\u00ba videos.",
      "keywords": [
        "cinematic virtual reality",
        "virtual audio spaces",
        "spatial audio",
        "360\u00ba video",
        "orientation"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "1",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300925",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300925"
    },
    {
      "title": "ARPen: Mid-Air Object Manipulation Techniques for a Bimanual AR System with Pen &amp; Smartphone",
      "authors": [
        "Philipp Wacker",
        "Oliver Nowak",
        "Simon Voelker",
        "Jan Borchers"
      ],
      "abstract": "Modeling in Augmented Reality (AR) lets users create and manipulate virtual objects in mid-air that are aligned to their real environment. We present ARPen, a bimanual input technique for AR modeling that combines a standard smartphone with a 3D-printed pen. Users sketch with the pen in mid-air, while holding their smartphone in the other hand to see the virtual pen traces in the live camera image. ARPen combines the pen's higher 3D input precision with the rich interactive capabilities of the smartphone touchscreen. We studied subjective preferences for this bimanual input technique, such as how people hold the smartphone while drawing, and analyzed the performance of different bimanual techniques for selecting and moving virtual objects. Users preferred a bimanual technique casting a ray through the pen tip for both selection and translation. We provide initial design guidelines for this new class of bimanual AR modeling systems.",
      "keywords": [
        "translation",
        "smartphone ar",
        "selection",
        "mid-air interaction",
        "immersive modeling",
        "pen",
        "bimanual interaction",
        "augmented reality"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "7",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300849",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300849"
    },
    {
      "title": "Select ahead: efficient object selection technique using the tendency of recent cursor movements",
      "authors": ["Soonchan Park", "Seokyeol Kim", "Jinah Park"],
      "abstract": "Virtual hand is one of the most intuitive metaphors for object selection in the virtual environment because of its natural mapping between the user action input and the cursor. However, it has a limitation of lengthy cursor manipulation for object selection task which is directly related to the level of workload of the user in performing object selection. In this paper, we propose 'Select Ahead' as a new object selection technique that improves efficiency by reducing the physical workload. Select Ahead guides the user to select the distant object along the estimated tendency of the recent cursor movements. We evaluate the relative performance of Select Ahead through the experiments in the 3D virtual environment with various object densities. The results show that Select Ahead significantly reduces the length of the cursor movements compared to those of the 3D point cursor and the 3D bubble cursor regardless of the object density. In the aspect of the total duration time for selection, Select Ahead outperforms the 3D point cursor and has no significant difference compared to that of the 3D bubble cursor.",
      "keywords": [
        "3d interaction",
        "virtual hand",
        "object selection",
        "virtual reality",
        "3d cursor"
      ],
      "published_in": "APCHI '12: Proceedings of the 10th asia pacific conference on Computer human interaction",
      "publication_date": "28 August 2012",
      "citations": "2",
      "isbn": "9781450314961",
      "doi": "10.1145/2350046",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2350046.2350060",
      "paper_url": "https://dl.acm.org/doi/10.1145/2350046.2350060"
    },
    {
      "title": "Virtual Hubs: Understanding Relational Aspects and Remediating Incubation",
      "authors": ["Jandy Luik", "Jenna Ng", "Jonathan Hook"],
      "abstract": "We have recently seen the emergence of new platforms that aim to provide remotely located entrepreneurs and startup companies with support analogous to that found within traditional incubation or acceleration spaces. This paper offers an understanding of these 'virtual hubs', and the inherently socio-technical interactions that occur between their members. Our study analyzes a sample of existing virtual hubs in two stages. First, we contribute broader insight into the current landscape of virtual hubs by documenting and categorizing 25 hubs regarding their form, support offered and a selection of further qualities. Second, we contribute detailed insight into the operation and experience of such hubs, from an analysis of 10 semi-structured interviews with organizers and participants of virtual hubs. We conclude by analyzing our findings in terms of relational aspects of non-virtual hubs from the literature and remediation theory, and propose opportunities for advancing the design of such platforms.",
      "keywords": [
        "incubators and accelerators",
        "virtual hubs",
        "relational aspects",
        "digital incubation",
        "remediation"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "0",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300471",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300471"
    },
    {
      "title": "shapeShift: 2D Spatial Manipulation and Self-Actuation of Tabletop Shape Displays for Tangible and Haptic Interaction",
      "authors": [
        "Alexa F. Siu",
        "Eric J. Gonzalez",
        "Shenli Yuan",
        "Jason B. Ginsberg",
        "Sean Follmer"
      ],
      "abstract": "We explore interactions enabled by 2D spatial manipulation and self-actuation of a tabletop shape display. To explore these interactions, we developed shapeShift, a compact, high-resolution (7 mm pitch), mobile tabletop shape display. shapeShift can be mounted on passive rollers allowing for bimanual interaction where the user can freely manipulate the system while it renders spatially relevant content. shapeShift can also be mounted on an omnidirectional-robot to provide both vertical and lateral kinesthetic feedback, display moving objects, or act as an encountered-type haptic device for VR. We present a study on haptic search tasks comparing spatial manipulation of a shape display for egocentric exploration of a map versus exploration using a fixed display and a touch pad. Results show a 30% decrease in navigation path lengths, 24% decrease in task time, 15% decrease in mental demand and 29% decrease in frustration in favor of egocentric navigation.",
      "keywords": [
        "shape-changing user interfaces",
        "shape displays",
        "actuated tangibles"
      ],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "21 April 2018",
      "citations": "23",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3173865",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3173865"
    },
    {
      "title": "Mailing Archived Emails as Postcards: Probing the Value of Virtual Collections",
      "authors": [
        "David B. Gerritsen",
        "Dan Tasse",
        "Jennifer K. Olsen",
        "Tatiana A. Vlahovic",
        "Rebecca Gulotta",
        "William Odom",
        "Jason Wiese",
        "John Zimmerman"
      ],
      "abstract": "People accumulate huge assortments of virtual possessions, but it is not yet clear how systems and system designers can help people make meaning from these large archives. Early research in HCI has suggested that people generally appear to value their virtual things less than their material things, but theory on material possessions does not entirely explain this difference. To investigate if changes to the form and behavior of virtual things may surface valued elements of a virtual archive, we designed a technology probe that selected snippets from old emails and mailed them as physical postcards to participating households. The probe uncovered features of emails that trigger meaningful reflection, and how contextual information can help people engage in reminiscence. Our study revealed insights about how materializing virtual possessions influences factors shaping how people draw on, understand, and value those possessions. We conclude with implication and strategies for aimed at supporting people in having more meaningful interactions and experiences with their virtual possessions.",
      "keywords": [
        "revisitation",
        "virtual possessions",
        "design",
        "technology probes",
        "self-reflection"
      ],
      "published_in": "CHI '16: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "7 May 2016",
      "citations": "7",
      "isbn": "9781450333627",
      "doi": "10.1145/2858036",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2858036.2858541",
      "paper_url": "https://dl.acm.org/doi/10.1145/2858036.2858541"
    },
    {
      "title": "Haptic Retargeting: Dynamic Repurposing of Passive Haptics for Enhanced Virtual Reality Experiences",
      "authors": [
        "Mahdi Azmandian",
        "Mark Hancock",
        "Hrvoje Benko",
        "Eyal Ofek",
        "Andrew D. Wilson"
      ],
      "abstract": "Manipulating a virtual object with appropriate passive haptic cues provides a satisfying sense of presence in virtual reality. However, scaling such experiences to support multiple virtual objects is a challenge as each one needs to be accompanied with a precisely-located haptic proxy object. We propose a solution that overcomes this limitation by hacking human perception. We have created a framework for repurposing passive haptics, called haptic retargeting, that leverages the dominance of vision when our senses conflict. With haptic retargeting, a single physical prop can provide passive haptics for multiple virtual objects. We introduce three approaches for dynamically aligning physical and virtual objects: world manipulation, body manipulation and a hybrid technique which combines both world and body manipulation. Our study results indicate that all our haptic retargeting techniques improve the sense of presence when compared to typical wand-based 3D control of virtual objects. Furthermore, our hybrid haptic retargeting achieved the highest satisfaction and presence scores while limiting the visible side-effects during interaction.",
      "keywords": ["perception", "haptics", "virtual reality"],
      "published_in": "CHI '16: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "7 May 2016",
      "citations": "114",
      "isbn": "9781450333627",
      "doi": "10.1145/2858036",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2858036.2858226",
      "paper_url": "https://dl.acm.org/doi/10.1145/2858036.2858226"
    },
    {
      "title": "pCubee: a perspective-corrected handheld cubic display",
      "authors": ["Ian Stavness", "Billy Lam", "Sidney Fels"],
      "abstract": "In this paper, we describe the design of a personal cubic display that offers novel interaction techniques for static and dynamic 3D content. We extended one-screen Fish Tank VR by arranging five small LCD panels into a box shape that is light and compact enough to be handheld. The display uses head-coupled perspective rendering and a real-time physics simulation engine to establish an interaction metaphor of having real objects inside a physical box that a user can hold and manipulate. We evaluated our prototype as a visualization tool and as an input device by comparing it with a conventional LCD display and mouse for a 3D tree-tracing task. We found that bimanual interaction with pCubee and a mouse offered the best performance and was most preferred by users. pCubee has potential in 3D visualization and interactive applications such as games, storytelling and education, as well as viewing 3D maps, medical and architectural data.",
      "keywords": [
        "handheld device",
        "physical interaction",
        "user interface",
        "multi-screen display",
        "user evaluation",
        "fish tank vr",
        "3d visualization"
      ],
      "published_in": "CHI '10: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "10 April 2010",
      "citations": "52",
      "isbn": "9781605589299",
      "doi": "10.1145/1753326",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1753326.1753535",
      "paper_url": "https://dl.acm.org/doi/10.1145/1753326.1753535"
    },
    {
      "title": "Extending the Body for Interaction with Reality",
      "authors": ["Tiare Feuchtner", "J\u00f6rg M\u00fcller"],
      "abstract": "In this paper, we explore how users can control remote devices with a virtual long arm, while preserving the perception that the artificial arm is actually part of their own body. Instead of using pointing, speech, or a remote control, the users' arm is extended in augmented reality, allowing access to devices that are out of reach. Thus, we allow users to directly manipulate real-world objects from a distance using their bare hands. A core difficulty we focus on is how to maintain ownership for the unnaturally long virtual arm, which is the strong feeling that one's limbs are actually part of the own body. Fortunately, what the human brain experiences as being part of the own body is very malleable and we find that during interaction the user's virtual arm can be stretched to more than twice its real length, without breaking the user's sense of ownership for the virtual limb.",
      "keywords": [
        "virtual hand illusion",
        "ubiquitous computing",
        "ownership",
        "augmented reality"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "12",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025689",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025689"
    },
    {
      "title": "Direct Finger Manipulation of 3D Object Image with Ultrasound Haptic Feedback",
      "authors": [
        "Atsushi Matsubayashi",
        "Yasutoshi Makino",
        "Hiroyuki Shinoda"
      ],
      "abstract": "In this study, we prototype and examine a system that allows a user to manipulate a 3D virtual object with multiple fingers without wearing any device. An autostereoscopic display produces a 3D image and a depth sensor measures the movement of the fingers. When a user touches a virtual object, haptic feedback is provided by ultrasound phased arrays. By estimating the cross section of the finger in contact with the virtual object and by creating a force pattern around it, it is possible for the user to recognize the position of the surface relative to the finger. To evaluate our system, we conducted two experiments to show that the proposed feedback method is effective in recognizing the object surface and thereby enables the user to grasp the object quickly without seeing it.",
      "keywords": [
        "touch/haptic/pointing/gesture",
        "virtual/augmented reality"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "3",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300317",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300317"
    },
    {
      "title": "Quantitative measurement of virtual vs. physical object embodiment through kinesthetic figural after effects",
      "authors": ["Ayman Alzayat", "Mark Hancock", "Miguel Nacenta"],
      "abstract": "Over the past decade, multi-touch surfaces have become commonplace, with many researchers and practitioners describing the benefits of their natural, physical-like interactions. We present a pair of studies that empirically investigates the psychophysical effects of direct interaction with both physical and virtual artefacts. We use the phenomenon of Kinesthetic Figural After Effects-a change in understanding of the physical size of an object after a period of exposure to an object of different size. Our studies show that, while this effect is robustly reproducible when using physical artefacts, this same effect does not manifest when manipulating virtual artefacts on a direct, multi-touch tabletop display. We contribute quantitative evidence suggesting a psychophysical difference in our response to physical vs. virtual objects, and discuss future research directions to explore measurable phenomena to evaluate the presence of physical-like changes from virtual on-screen objects.",
      "keywords": [
        "embodied interaction",
        "tabletop displays",
        "multi-touch",
        "tangible user interfaces",
        "physical interaction"
      ],
      "published_in": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "26 April 2014",
      "citations": "5",
      "isbn": "9781450324731",
      "doi": "10.1145/2556288",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2556288.2557282",
      "paper_url": "https://dl.acm.org/doi/10.1145/2556288.2557282"
    },
    {
      "title": "On the Shoulder of the Giant: A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible Interaction",
      "authors": [
        "Thammathip Piumsomboon",
        "Gun A. Lee",
        "Andrew Irlitti",
        "Barrett Ens",
        "Bruce H. Thomas",
        "Mark Billinghurst"
      ],
      "abstract": "We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction.",
      "keywords": [
        "tangible user interface",
        "remote collaboration",
        "mixed reality",
        "wearable interface",
        "live panorama sharing",
        "multi-scale"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "15",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300458",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300458"
    },
    {
      "title": "Imperceptible depth shifts for touch interaction with stereoscopic objects",
      "authors": ["Dimitar Valkov", "Alexander Giesler", "Klaus H. Hinrichs"],
      "abstract": "While touch technology has proven its usability for 2D interaction and has already become a standard input modality for many devices, the challenges to exploit its applicability with stereoscopically rendered content have barely been studied. In this paper we exploit the properties of the visual perception to allow users to touch stereoscopically displayed objects when the input is constrained to a 2D surface. Therefore, we have extended and generalized recent evaluations on the user's ability to discriminate small induced object shifts while reaching out to touch a virtual object, and we propose a practical interaction technique, the attracting shift technique, suitable for numerous application scenarios where shallow depth interaction is sufficient. In addition, our results indicate that slight object shifts during touch interaction make the virtual scene appear perceptually more stable compared to a static scene. As a consequence, applications have to manipulate the virtual objects to make them appear static for the user.",
      "keywords": ["human factors", "experimentation"],
      "published_in": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "26 April 2014",
      "citations": "7",
      "isbn": "9781450324731",
      "doi": "10.1145/2556288",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2556288.2557134",
      "paper_url": "https://dl.acm.org/doi/10.1145/2556288.2557134"
    },
    {
      "title": "Uvmode: usability verification mixed reality system for mobile devices",
      "authors": ["Ungyeon Yang", "Dongsik Jo", "Wooho Son"],
      "abstract": "UVMODE is a mixed reality based usability evaluation system for mobile information device development. The system contributes to increasing efficiency of the usability evaluation process by replacing real products with virtual models. With our system, users can change the design of a virtual product easily, and investigate how it affects its usability. While users can review and test the virtual product by manipulating it with MR interfaces, the system also provides evaluation tools for measuring objective usability measures, including estimated design quality and users' hand load. In this paper, we present the system design and implementation details of our system, and discuss how it could improve the current usability evaluation processes held in mobile information device industry.",
      "keywords": [
        "usability test",
        "design reviewing",
        "product-life-cycle",
        "operation simulation",
        "mixed reality",
        "tangible interface",
        "affective engineering",
        "photo-realistic rendering"
      ],
      "published_in": "CHI EA '08: CHI '08 Extended Abstracts on Human Factors in Computing Systems",
      "publication_date": "5 April 2008",
      "citations": "6",
      "isbn": "9781605580128",
      "doi": "10.1145/1358628",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1358628.1358893",
      "paper_url": "https://dl.acm.org/doi/10.1145/1358628.1358893"
    },
    {
      "title": "Annexing Reality: Enabling Opportunistic Use of Everyday Objects as Tangible Proxies in Augmented Reality",
      "authors": ["Anuruddha Hettiarachchi", "Daniel Wigdor"],
      "abstract": "Advances in display and tracking technologies hold the promise of increasingly immersive augmented-reality experiences. Unfortunately, the on-demand generation of haptic experiences is lagging behind these advances in other feedback channels. We present Annexing Reality; a system that opportunistically annexes physical objects from a user's current physical environment to provide the best-available haptic sensation for virtual objects. It allows content creators to a priori specify haptic experiences that adapt to the user's current setting. The system continuously scans user's surrounding, selects physical objects that are similar to given virtual objects, and overlays the virtual models on to selected physical ones reducing the visual-haptic mismatch. We describe the developer's experience with the Annexing Reality system and the techniques utilized in realizing it. We also present results of a developer study that validates the usability and utility of our method of defining haptic experiences.",
      "keywords": [
        "augmented reality",
        "opportunistic tangible interfaces",
        "augmented reality content authoring"
      ],
      "published_in": "CHI '16: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "7 May 2016",
      "citations": "52",
      "isbn": "9781450333627",
      "doi": "10.1145/2858036",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2858036.2858134",
      "paper_url": "https://dl.acm.org/doi/10.1145/2858036.2858134"
    },
    {
      "title": "BeThere: 3D mobile collaboration with spatial input",
      "authors": [
        "Rajinder S. Sodhi",
        "Brett R. Jones",
        "David Forsyth",
        "Brian P. Bailey",
        "Giuliano Maciocci"
      ],
      "abstract": "We present BeThere, a proof-of-concept system designed to explore 3D input for mobile collaborative interactions. With BeThere, we explore 3D gestures and spatial input which allow remote users to perform a variety of virtual interactions in a local user's physical environment. Our system is completely self-contained and uses depth sensors to track the location of a user's fingers as well as to capture the 3D shape of objects in front of the sensor. We illustrate the unique capabilities of our system through a series of interactions that allow users to control and manipulate 3D virtual content. We also provide qualitative feedback from a preliminary user study which confirmed that users can complete a shared collaborative task using our system.",
      "keywords": [
        "around device interaction",
        "augmented reality",
        "collaboration",
        "depth sensors"
      ],
      "published_in": "CHI '13: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "27 April 2013",
      "citations": "81",
      "isbn": "9781450318990",
      "doi": "10.1145/2470654",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2470654.2470679",
      "paper_url": "https://dl.acm.org/doi/10.1145/2470654.2470679"
    },
    {
      "title": "Sparse Haptic Proxy: Touch Feedback in Virtual Environments Using a General Passive Prop",
      "authors": [
        "Lung-Pan Cheng",
        "Eyal Ofek",
        "Christian Holz",
        "Hrvoje Benko",
        "Andrew D. Wilson"
      ],
      "abstract": "We propose a class of passive haptics that we call Sparse Haptic Proxy: a set of geometric primitives that simulate touch feedback in elaborate virtual reality scenes. Unlike previous passive haptics that replicate the virtual environment in physical space, a Sparse Haptic Proxy simulates a scene's detailed geometry by redirecting the user's hand to a matching primitive of the proxy. To bridge the divergence of the scene from the proxy, we augment an existing Haptic Retargeting technique with an on-the-fly target remapping: We predict users' intentions during interaction in the virtual space by analyzing their gaze and hand motions, and consequently redirect their hand to a matching part of the proxy. We conducted three user studies on haptic retargeting technique and implemented a system from three main results: 1) The maximum angle participants found acceptable for retargeting their hand is 40\u00b0, with an average rating of 4.6 out of 5. 2) Tracking participants' eye gaze reliably predicts their touch intentions (97.5%), even while simultaneously manipulating the user's hand-eye coordination for retargeting. 3) Participants preferred minimized retargeting distances over better-matching surfaces of our Sparse Haptic Proxy when receiving haptic feedback for single-finger touch input. We demonstrate our system with two virtual scenes: a flight cockpit and a room quest game. While their scene geometries differ substantially, both use the same sparse haptic proxy to provide haptic feedback to the user during task completion.",
      "keywords": [
        "virtual reality",
        "passive haptics",
        "retargeting",
        "perception"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "40",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025753",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025753"
    },
    {
      "title": "The Object Inside: Assessing 3D Examination with a Spherical Handheld Perspective-Corrected Display",
      "authors": ["Francois Berard", "Thibault Louis"],
      "abstract": "Handheld Perspective Corrected Displays (HPCDs) can create the feeling of holding a virtual 3D object. They offer a direct interaction that is isomorphic to the manipulation of physical objects. This illusion depends on the ability to provide a natural visuomotor coupling. High performances systems are thus required to evaluate the fundamental merits of HPCDs. We built a spherical HPCD using external projection. The system offers a lightweight wireless seamless display with head-coupled stereo, robust tracking, and low latency. We compared users' performances with this HPCD and two other interactions that used a fixed planar display and either a touchpad or the spherical display as an indirect input. The task involved the inspection of complex virtual 3D puzzles. Physical puzzles were also tested as references. Contrary to expectations, all virtual interactions were found to be more efficient than a more \"natural\" physical puzzle. The HPCD yielded lower performances than the touchpad. This study indicates that the object examination task did not benefit from the accurate and precise rotations offered by the HPCD, but benefited from the high C/D gain of the touchpad.",
      "keywords": [
        "handheld perspective corrected display (hpcd)",
        "depth perception",
        "isomorphic rotation",
        "object examination",
        "3d display",
        "evaluation"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "9",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025806",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025806"
    },
    {
      "title": "Prospective motor control on tabletops: planning grasp for multitouch interaction",
      "authors": [
        "Halla B. Olafsdottir",
        "Theophanis Tsandilas",
        "Caroline Appert"
      ],
      "abstract": "Substantial amount of research in Psychology has studied how people manipulate objects in the physical world. This work has unveiled that people show strong signs of prospective motor planning, i.e., they choose initial grasps that avoid uncomfortable end postures and facilitate object manipulation. Interactive tabletops allow their users great flexibility in the manipulation of virtual objects but to our knowledge previous work has never examined whether prospective motor control takes place in this context. To test this, we ran three experiments. We systematically studied how users adapt their grasp when asked to translate and rotate virtual objects on a multitouch tabletop. Our results demonstrate that target position and orientation significantly affect the orientation of finger placement on the object. We analyze our results in the light of the most recent model of planning for manipulating physical objects and identify their implications for the design of tabletop interfaces. \\",
      "keywords": [
        "multitouch",
        "acquisition and manipulation",
        "tabletops",
        "movement planning",
        "end-state comfort effect",
        "range of motion"
      ],
      "published_in": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "26 April 2014",
      "citations": "9",
      "isbn": "9781450324731",
      "doi": "10.1145/2556288",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2556288.2557029",
      "paper_url": "https://dl.acm.org/doi/10.1145/2556288.2557029"
    },
    {
      "title": "The Effect of Offset Correction and Cursor on Mid-Air Pointing in Real and Virtual Environments",
      "authors": [
        "Sven Mayer",
        "Valentin Schwind",
        "Robin Schweigert",
        "Niels Henze"
      ],
      "abstract": "Pointing at remote objects to direct others' attention is a fundamental human ability. Previous work explored methods for remote pointing to select targets. Absolute pointing techniques that cast a ray from the user to a target are affected by humans' limited pointing accuracy. Recent work suggests that accuracy can be improved by compensating systematic offsets between targets a user aims at and rays cast from the user to the target. In this paper, we investigate mid-air pointing in the real world and virtual reality. Through a pointing study, we model the offsets to improve pointing accuracy and show that being in a virtual environment affects how users point at targets. In the second study, we validate the developed model and analyze the effect of compensating systematic offsets. We show that the provided model can significantly improve pointing accuracy when no cursor is provided. We further show that a cursor improves pointing accuracy but also increases the selection time.",
      "keywords": [
        "modeling",
        "virtual environment",
        "offset correction",
        "cursor",
        "mid-air pointing",
        "ray casting"
      ],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "21 April 2018",
      "citations": "15",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3174227",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3174227"
    },
    {
      "title": "Design and Evaluation of a Handheld-based 3D User Interface for Collaborative Object Manipulation",
      "authors": [
        "Jer\u00f4nimo Gustavo Grandi",
        "Henrique Galvan Debarba",
        "Luciana Nedel",
        "Anderson Maciel"
      ],
      "abstract": "Object manipulation in 3D virtual environments demands a combined coordination of rotations, translations and scales, as well as the camera control to change the user's viewpoint. Then, for many manipulation tasks, it would be advantageous to share the interaction complexity among team members. In this paper we propose a novel 3D manipulation interface based on a collaborative action coordination approach. Our technique explores a smartphone -- the touchscreen and inertial sensors -- as input interface, enabling several users to collaboratively manipulate the same virtual object with their own devices. We first assessed our interface design on a docking and an obstacle crossing tasks with teams of two users. Then, we conducted a study with 60 users to understand the influence of group size in collaborative 3D manipulation. We evaluated teams in combinations of one, two, three and four participants. Experimental results show that teamwork increases accuracy when compared with a single user. The accuracy increase is correlated with the number of individuals in the team and their work division strategy.",
      "keywords": [
        "3D user interfaces",
        "collaborative manipulation",
        "user studies"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "11",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025935",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025935"
    },
    {
      "title": "Eden: a professional multitouch tool for constructing virtual organic environments",
      "authors": [
        "Kenrick Kin",
        "Tom Miller",
        "Bj\u00f6rn Bollensdorff",
        "Tony Derose",
        "Bj\u00f6rn Hartmann",
        "Maneesh Agrawala"
      ],
      "abstract": "Set construction is the process of selecting and positioning virtual geometric objects to create a virtual environment used in a computer-animated film. Set construction artists often have a clear mental image of the set composition, but find it tedious to build their intended sets with current mouse and keyboard interfaces. We investigate whether multitouch input can ease the process of set construction. Working with a professional set construction artist at Pixar Animation Studios, we designed and developed Eden, a fully functional multitouch set construction application. In this paper, we describe our design process and how we balanced the advantages and disadvantages of multitouch input to develop usable gestures for set construction. Based on our design process and the user experiences of two set construction artists, we present a general set of lessons we learned regarding the design of a multitouch interface.",
      "keywords": [
        "Eden",
        "camera control",
        "object manipulation",
        "gestures",
        "set construction",
        "multitouch",
        "gesture design"
      ],
      "published_in": "CHI '11: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "7 May 2011",
      "citations": "31",
      "isbn": "9781450302289",
      "doi": "10.1145/1978942",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1978942.1979141",
      "paper_url": "https://dl.acm.org/doi/10.1145/1978942.1979141"
    },
    {
      "title": "The Effect of Stereo Display Deficiencies on Virtual Hand Pointing",
      "authors": ["Mayra Donaji Barrera Machuca", "Wolfgang Stuerzlinger"],
      "abstract": "The limitations of stereo display systems affect depth perception, e.g., due to the vergence-accommodation conflict or diplopia. We performed three studies to understand how stereo display deficiencies impact 3D pointing for targets in front of a screen and close to the user, i.e., in peripersonal space. Our first two experiments compare movements with and without a change in visual depth for virtual respectively physical targets. Results indicate that selecting targets along the depth axis is slower and has less throughput for virtual targets, while physical pointing demonstrates the opposite result. We then propose a new 3D extension for Fitts' law that models the effect of stereo display deficiencies. Next, our third experiment verifies the model and measures more broadly how the change in visual depth between targets affects pointing performance in peripersonal space and confirms significant effects on time and throughput. Finally, we discuss implications for 3D user interface design.",
      "keywords": ["fitt's law", "selection", "3d pointing", "cursor"],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "12",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300437",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300437"
    },
    {
      "title": "RayCursor: A 3D Pointing Facilitation Technique based on Raycasting",
      "authors": ["Marc Baloup", "Thomas Pietrzak", "G\u00e9ry Casiez"],
      "abstract": "Raycasting is the most common target pointing technique in virtual reality environments. However, performance on small and distant targets is impacted by the accuracy of the pointing device and the user's motor skills. Current pointing facilitation techniques are currently only applied in the context of the virtual hand, i.e. for targets within reach. We propose enhancements to Raycasting: filtering the ray, and adding a controllable cursor on the ray to select the nearest target. We describe a series of studies for the design of the visual feedforward, filtering technique, as well as a comparative study between different 3D pointing techniques. Our results show that highlighting the nearest target is one of the most efficient visual feedforward technique. We also show that filtering the ray reduces error rate in a drastic way. Finally we show the benefits of RayCursor compared to Raycasting and another technique from the literature.",
      "keywords": [
        "virtual reality",
        "pointing technique",
        "visual feedforward"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "12",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300331",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300331"
    },
    {
      "title": "Designing effective gaze mechanisms for virtual agents",
      "authors": [
        "Sean Andrist",
        "Tomislav Pejsa",
        "Bilge Mutlu",
        "Michael Gleicher"
      ],
      "abstract": "Virtual agents hold great promise in human-computer interaction with their ability to afford embodied interaction using nonverbal human communicative cues. Gaze cues are particularly important to achieve significant high-level outcomes such as improved learning and feelings of rapport. Our goal is to explore how agents might achieve such outcomes through seemingly subtle changes in gaze behavior and what design variables for gaze might lead to such positive outcomes. Drawing on research in human physiology, we developed a model of gaze behavior to capture these key design variables. In a user study, we investigated how manipulations in these variables might improve affiliation with the agent and learning. The results showed that an agent using affiliative gaze elicited more positive feelings of connection, while an agent using referential gaze improved participants' learning. Our model and findings offer guidelines for the design of effective gaze behaviors for virtual agents.",
      "keywords": [
        "gaze",
        "nonverbal behavior",
        "affiliation",
        "learning",
        "virtual agents"
      ],
      "published_in": "CHI '12: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "5 May 2012",
      "citations": "31",
      "isbn": "9781450310154",
      "doi": "10.1145/2207676",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2207676.2207777",
      "paper_url": "https://dl.acm.org/doi/10.1145/2207676.2207777"
    },
    {
      "title": "TouchTools: leveraging familiarity and skill with physical tools to augment touch interaction",
      "authors": [
        "Chris Harrison",
        "Robert Xiao",
        "Julia Schwarz",
        "Scott E. Hudson"
      ],
      "abstract": "The average person can skillfully manipulate a plethora of tools, from hammers to tweezers. However, despite this remarkable dexterity, gestures on today's touch devices are simplistic, relying primarily on the chording of fingers: one-finger pan, two-finger pinch, four-finger swipe and similar. We propose that touch gesture design be inspired by the manipulation of physical tools from the real world. In this way, we can leverage user familiarity and fluency with such tools to build a rich set of gestures for touch interaction. With only a few minutes of training on a proof-of-concept system, users were able to summon a variety of virtual tools by replicating their corresponding real-world grasps.",
      "keywords": [
        "surface computing",
        "gesture design",
        "multitouch",
        "tangible computing",
        "capacitive sensing",
        "touchscreen"
      ],
      "published_in": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "26 April 2014",
      "citations": "21",
      "isbn": "9781450324731",
      "doi": "10.1145/2556288",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2556288.2557012",
      "paper_url": "https://dl.acm.org/doi/10.1145/2556288.2557012"
    },
    {
      "title": "Haptic-Enabled Handheld Mobile Robots: Design and Analysis",
      "authors": [
        "Ayberk \u00d6zg\u00fcr",
        "Wafa Johal",
        "Francesco Mondada",
        "Pierre Dillenbourg"
      ],
      "abstract": "The Cellulo robots are small tangible robots that are designed to represent virtual interactive point-like objects that reside on a plane within carefully designed learning activities. In the context of these activities, our robots not only display autonomous motion and act as tangible interfaces, but are also usable as haptic devices in order to exploit, for instance, kinesthetic learning. In this article, we present the design and analysis of the haptic interaction module of the Cellulo robots. We first detail our hardware and controller design that is low-cost and versatile. Then, we describe the task-based experimental procedure to evaluate the robot's haptic abilities. We show that our robot is usable in most of the tested tasks and extract perceptive and manipulative guidelines for the design of haptic elements to be integrated in future learning activities. We conclude with limitations of the system and future work.",
      "keywords": ["handheld robots", "haptic interaction", "mobile robots"],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "9",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025994",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025994"
    },
    {
      "title": "Interaction with magic lenses: real-world validation of a Fitts' Law model",
      "authors": ["Michael Rohs", "Antti Oulasvirta", "Tiia Suomalainen"],
      "abstract": "Rohs and Oulasvirta (2008) proposed a two-component Fitts' law model for target acquisition with magic lenses in mobile augmented reality (AR) with 1) a physical pointing phase, in which the target can be directly observed on the background surface, and 2) a virtual pointing phase, in which the target can only be observed through the device display. The model provides a good fit (R2=0.88) with laboratory data, but it is not known if it generalizes to real-world AR tasks. In the present outdoor study, subjects (N=12) did building-selection tasks in an urban area. The differences in task characteristics to the laboratory study are drastic: targets are three-dimensional and they vary in shape, size, z-distance, and visual context. Nevertheless, the model yielded an R2 of 0.80, and when using effective target width an R2 of 0.88 was achieved.",
      "keywords": [
        "Fitt's Law",
        "human-performance modeling",
        "augmented reality",
        "magic lens pointing",
        "field experiment",
        "target acquisition"
      ],
      "published_in": "CHI '11: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "7 May 2011",
      "citations": "20",
      "isbn": "9781450302289",
      "doi": "10.1145/1978942",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1978942.1979343",
      "paper_url": "https://dl.acm.org/doi/10.1145/1978942.1979343"
    },
    {
      "title": "Modeling Error Rates in Spatiotemporal Moving Target Selection",
      "authors": ["Jin Huang", "Byungjoo Lee"],
      "abstract": "When we try to acquire a moving target such as hitting a virtual tennis in a computer game, we must hit the target instantly when it flies over our hitting range. In other words, we have to acquire the target in spatial and temporal domains simultaneously. We call this type of task spatiotemporal moving target selection, which we find is common yet less studied in HCI. This paper presents a tentative model for predicting the error rates in spatiotemporal moving target selection. Our model integrates two latest models, the Ternary-Gaussian model and the Temporal Pointing model, to explain the influence of spatial and temporal constraints on pointing errors. In a 12-subject pointing experiment with a computer mouse, our model shows high fitting results with 0.904 R2. We discuss future research directions on this topic and how it could potentially help the design in dynamical user interfaces.",
      "keywords": [
        "moving target selection",
        "error rates",
        "spatial pointing",
        "temporal pointing"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "1",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3313077",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3313077"
    },
    {
      "title": "ForceRay: Extending Thumb Reach via Force Input Stabilizes Device Grip for Mobile Touch Input",
      "authors": [
        "Christian Corsten",
        "Marcel Lahaye",
        "Jan Borchers",
        "Simon Voelker"
      ],
      "abstract": "Smartphones are used predominantly one-handed, using the thumb for input. Many smartphones, however, have grown beyond 5\". Users cannot tap everywhere on these screens without destabilizing their grip. ForceRay (FR) lets users aim at an out-of-reach target by applying a force touch at a comfortable thumb location, casting a virtual ray towards the target. Varying pressure moves a cursor along the ray. When reaching the target, quickly lifting the thumb selects it. In a first study, FR was 195 ms slower and had a 3% higher selection error than the best existing technique, BezelCursor (BC), but FR caused significantly less device movement than all other techniques, letting users maintain a steady grip and removing their concerns about device drops. A second study showed that an hour of training speeds up both BC and FR, and that both are equally fast for targets at the screen border.",
      "keywords": [
        "touch",
        "mobile",
        "force",
        "one-handed",
        "pressure",
        "reachability"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "5",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300442",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300442"
    },
    {
      "title": "Hybrid-Brailler: Combining Physical and Gestural Interaction for Mobile Braille Input and Editing",
      "authors": [
        "Daniel Trindade",
        "Andr\u00e9 Rodrigues",
        "Tiago Guerreiro",
        "Hugo Nicolau"
      ],
      "abstract": "Braille input enables fast nonvisual entry speeds on mobile touchscreen devices. Yet, the lack of tactile cues commonly results in typing errors, which are hard to correct. We propose Hybrid-Brailler, an input solution that combines physical and gestural interaction to provide fast and accurate Braille input. We use the back of the device for physical chorded input while freeing the touchscreen for gestural interaction. Gestures are used in editing operations, such as caret movement, text selection, and clipboard control, enhancing the overall text entry experience. We conducted two user studies to assess both input and editing performance. Results show that Hybrid-Brailler supports fast entry rates as its virtual counterpart, while significantly increasing input accuracy. Regarding editing performance, when compared with the mainstream technique, Hybrid-Brailler shows performance benefits of 21% in speed and increased editing accuracy. We finish with lessons learned for designing future nonvisual input and editing techniques.",
      "keywords": [
        "blind",
        "mobile",
        "text entry",
        "touchscreen",
        "braille",
        "editing"
      ],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "19 April 2018",
      "citations": "4",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3173601",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3173601"
    },
    {
      "title": "DMove: Directional Motion-based Interaction for Augmented Reality Head-Mounted Displays",
      "authors": [
        "Wenge Xu",
        "Hai-Ning Liang",
        "Yuxuan Zhao",
        "Difeng Yu",
        "Diego Monteiro"
      ],
      "abstract": "We present DMove, directional motion-based interaction for Augmented Reality (AR) Head-Mounted Displays (HMDs) that is both hands- and device-free. It uses directional walk-ing as a way to interact with virtual objects. To use DMove, a user needs to perform directional motions such as mov-ing one foot forward or backward. In this research, we first investigate the recognition accuracy of the motion direc-tions of our method and the social acceptance of this type of interactions together with users' comfort rating for each direction. We then optimize its design and conduct a sec-ond study to compare DMove in task performance and user preferences (workload, motion sickness, user experience), with two approaches-Hand interaction (Meta 2-like) and Head+Hand interaction (HoloLens-like) for menu selection tasks. Based on the results of these two studies, we provide a set of guidelines for DMove and further demonstrate two applications that utilize directional motions.",
      "keywords": [
        "augmented reality",
        "motion direction",
        "head-mounted display",
        "menu selection"
      ],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "4",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300674",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300674"
    },
    {
      "title": "Spring: a solution for managing the third DOF with tactile interface",
      "authors": ["Robin Vivian", "J\u00e9r\u00f4me Dinet", "David Bertolo"],
      "abstract": "Tablets with touch-screens are one of the most widely used interfaces for three main reasons: the effectiveness and efficiency and fun side interactions. Associated with significant increases in power, the multi-touch touchscreen terminals are able to manipulate real-time application of virtual worlds for entertainment or learning. This raises a question that was already current with conventional interfaces (mouse for example) how to define, with 2D input interface, designation, orientation and move actions (simply and intuitively) on objects in 3D space that require at least 6 degrees of freedom (6 DOF for the object and sometimes six other for the camera)? Our study provides a way of managing the depth dimension with the principle of universal interaction: to screw / unscrew. The first part of this paper presents the theoretical framework based on prior studies and the second part describes the formalization of a grammar of gesture allowing the intuitive interaction management of depth component.",
      "keywords": [
        "interactions",
        "tactile device",
        "gestural grammar",
        "tablets touch-screen",
        "graphical user interfaces",
        "3dof",
        "grammar of gesture"
      ],
      "published_in": "APCHI '12: Proceedings of the 10th asia pacific conference on Computer human interaction",
      "publication_date": "28 August 2012",
      "citations": "3",
      "isbn": "9781450314961",
      "doi": "10.1145/2350046",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2350046.2350097",
      "paper_url": "https://dl.acm.org/doi/10.1145/2350046.2350097"
    },
    {
      "title": "An interaction system using mixed hand gestures",
      "authors": [
        "Zhong Yang",
        "Yi Li",
        "Yang Zheng",
        "Weidong Chen",
        "Xiaoxiang Zheng"
      ],
      "abstract": "This paper presents a mixed hand gesture interaction system in virtual environment, in which \"mixed\" means static and dynamic hand gestures are combined for both navigation and object manipulation. Firstly, a simple average background model and skin color are used for hand area segmentation. Then a state-based spotting algorithm is employed to automatically identify two types of hand gestures. A voting-based method is used for quick classification of static gestures. And we use the hidden Markov model (HMM) to recognize dynamic gestures. Since the training of HMM requires the consistency of the training data, outputted by the feature extraction, a data aligning algorithm is raised. Through our mixed hand gesture system, users can perform complicated operating commands in a natural way. The experimental results demonstrate that our methods are effective and accurate.",
      "keywords": [
        "hand gesture recognition",
        "data aligning",
        "hidden markov model (hmm)",
        "mixed hand gesture",
        "spotting algorithm"
      ],
      "published_in": "APCHI '12: Proceedings of the 10th asia pacific conference on Computer human interaction",
      "publication_date": "28 August 2012",
      "citations": "4",
      "isbn": "9781450314961",
      "doi": "10.1145/2350046",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2350046.2350074",
      "paper_url": "https://dl.acm.org/doi/10.1145/2350046.2350074"
    },
    {
      "title": "COMPASS: Rotational Keyboard on Non-Touch Smartwatches",
      "authors": [
        "Xin Yi",
        "Chun Yu",
        "Weijie Xu",
        "Xiaojun Bi",
        "Yuanchun Shi"
      ],
      "abstract": "Entering text is very challenging on smartwatches, especially on non-touch smartwatches where virtual keyboards are unavailable. In this paper, we designed and implemented COMPASS, a non-touch bezel-based text entry technique. COMPASS positions multiple cursors on a circular keyboard, with the location of each cursor dynamically optimized during typing to minimize rotational distance. To enter text, a user rotates the bezel to select keys with any nearby cursors. The design of COMPASS was justified by an iterative design process and user studies. Our evaluation showed that participants achieved a pick-up speed around 10 WPM and reached 12.5 WPM after 90-minute practice. COMPASS allows users to enter text on non-touch smartwatches, and also serves as an alternative for entering text on touch smartwatches when touch is unavailable (e.g., wearing gloves).",
      "keywords": [
        "circular keyboard",
        "text entry",
        "smartwatch",
        "non-touch",
        "multiple cursors"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "21",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025454",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025454"
    },
    {
      "title": "Mouse, Tactile, and Tangible Input for 3D Manipulation",
      "authors": [
        "Lonni Besan\u00e7on",
        "Paul Issartel",
        "Mehdi Ammi",
        "Tobias Isenberg"
      ],
      "abstract": "We evaluate the performance and usability of mouse-based, touch-based, and tangible interaction for manipulating objects in a 3D virtual environment. This comparison is a step toward a better understanding of the limitations and benefits of these existing interaction techniques, with the ultimate goal of facilitating an easy transition between the different 3D data exploration environments. For this purpose we analyze participants' performance in 3D manipulation using a docking task. We measured completion times, docking accuracy, as well as subjective criteria such as fatigue, workload, and preference. Our results show that the three input modalities provide similar levels of precision but require different completion times. We also discuss our qualitative observations as well as people's preferences and put our findings into context of the application domain of 3D data analysis environments.",
      "keywords": [
        "mouse",
        "usability study",
        "3D interaction",
        "tactile interaction",
        "TUI",
        "tangible interaction"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "20",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025863",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025863"
    },
    {
      "title": "Handsaw: tangible exploration of volumetric data by direct cut-plane projection",
      "authors": [
        "Leonardo Bonanni",
        "Jason Alonso",
        "Neil Chao",
        "Greg Vargas",
        "Hiroshi Ishii"
      ],
      "abstract": "Tangible User Interfaces are well-suited to handling three-dimensional data sets by direct manipulation of real objects in space, but current interfaces can make it difficult to look inside dense volumes of information. This paper presents the Handsaw, a system that detects a virtual cut-plane projected by an outstretched hand or laser-line directly on an object or space and reveals sectional data on an adjacent display. By leaving the hands free and using a remote display, these techniques can be shared between multiple users and integrated into everyday practice. The Handsaw has been prototyped for scientific visualizations in medicine, engineering and urban design. User evaluations suggest that using a hand is more intuitive while projected light is more precise than keyboard and mouse control, and the Handsaw system has the potential to be used effectively by novices and in groups.",
      "keywords": [
        "product design",
        "scientific visualization",
        "volumetric data",
        "tangible user interface",
        "cross-section",
        "interior design",
        "medical visualization"
      ],
      "published_in": "CHI '08: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "6 April 2008",
      "citations": "7",
      "isbn": "9781605580111",
      "doi": "10.1145/1357054",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1357054.1357098",
      "paper_url": "https://dl.acm.org/doi/10.1145/1357054.1357098"
    },
    {
      "title": "Robots, Pancakes, and Computer Games: Designing Serious Games for Robot Imitation Learning",
      "authors": [
        "Benjamin Walther-Franks",
        "Jan Smeddinck",
        "Peter Szmidt",
        "Andrei Haidu",
        "Michael Beetz",
        "Rainer Malaka"
      ],
      "abstract": "Autonomous manipulation robots can be valuable aids as interactive agents in the home, yet it has proven extremely difficult to program their behavior. Imitation learning uses data on human demonstrations to build behavioral models for robots. In order to cover a wide range of action strategies, data from many individuals is needed. Acquiring such large amounts of data can be a challenge. Tools for data capturing in this domain must thus implement a good user experience. We propose to use human computation games in order to gather data on human manual behavior. We demonstrate the idea with a strategy game that is operated via a natural user interface. A comparison between using the game for action execution and demonstrating actions in a virtual environment shows that people interact longer and have a better experience when playing the game.",
      "keywords": ["human computation games", "programming by demonstration"],
      "published_in": "CHI '15: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems",
      "publication_date": "18 April 2015",
      "citations": "4",
      "isbn": "9781450331456",
      "doi": "10.1145/2702123",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2702123.2702552",
      "paper_url": "https://dl.acm.org/doi/10.1145/2702123.2702552"
    },
    {
      "title": "iGYM: A Wheelchair-Accessible Interactive Floor Projection System for Co-located Physical Play",
      "authors": [
        "Roland Graf",
        "Sun Young Park",
        "Emma Shpiz",
        "Hun Seok Kim"
      ],
      "abstract": "Physical play opportunities for people with motor disabilities typically do not include co-located play with peers without disabilities in traditional sport settings. In this paper, we present a prototype of a wheelchair-accessible interactive floor projection system, iGYM, designed to enable people with motor disabilities to compete on par with, and in the same environment as, peers without disabilities. iGYM provides two key system features-peripersonal circle interaction and adjustable game mechanic (physics)-that enable individualized game calibration and wheelchair-accessible manipulation of virtual targets on the floor. Preliminary findings from our pilot study with people with motor disabilities using power wheelchairs, manual wheelchairs, and people without disabilities showed that the prototype system was accessible for all participants at higher than anticipated target speeds. Our work has implications for designing novel, physical play opportunities in inclusive traditional sport settings.",
      "keywords": [
        "interactive floor",
        "game accessibility",
        "co-located play",
        "adaptive sport",
        "peripersonal space"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "2",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3312792",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3312792"
    },
    {
      "title": "Crowdsourcing Interface Feature Design with Bayesian Optimization",
      "authors": ["John J. Dudley", "Jason T. Jacques", "Per Ola Kristensson"],
      "abstract": "Designing novel interfaces is challenging. Designers typically rely on experience or subjective judgment in the absence of analytical or objective means for selecting interface parameters. We demonstrate Bayesian optimization as an efficient tool for objective interface feature refinement. Specifically, we show that crowdsourcing paired with Bayesian optimization can rapidly and effectively assist interface design across diverse deployment environments. Experiment 1 evaluates the approach on a familiar 2D interface design problem: a map search and review use case. Adding a degree of complexity, Experiment 2 extends Experiment 1 by switching the deployment environment to mobile-based virtual reality. The approach is then demonstrated as a case study for a fundamentally new and unfamiliar interaction design problem: web-based augmented reality. Finally, we show how the model generated as an outcome of the refinement process can be used for user simulation and queried to deliver various design insights.",
      "keywords": ["optimization", "interface design", "crowdsourcing"],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "0",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300482",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300482"
    },
    {
      "title": "Augmented Reality Views for Occluded Interaction",
      "authors": [
        "Klemen Lilija",
        "Henning Pohl",
        "Sebastian Boring",
        "Kasper Hornb\u00e6k"
      ],
      "abstract": "We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.",
      "keywords": ["augmented reality", "finger-camera", "manipulation task"],
      "published_in": "CHI '19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "2",
      "isbn": "9781450359702",
      "doi": "10.1145/3290605",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290605.3300676",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290605.3300676"
    },
    {
      "title": "SIG: Spatiality of Augmented Reality User Interfaces",
      "authors": [
        "Alla Vovk",
        "Danilo Gasques Rodrigues",
        "Fridolin Wild",
        "Nadir Weibel"
      ],
      "abstract": "Augmented reality and spatial information manipulation is being increasingly used as part of environ- ment integrated form factors and wearable device such as head-mounted displays. The integration of this exciting technology in many aspects of peoples' lives is transforming the way we understand computing, pushing the boundaries of Spatial Interfaces into virtual but embedded environments. We think that the time is ripe for a renewed discussion about the role of Augmented Reality within Spatial Interfaces. With this SIG we want to expand the discussion related to Spatial Interfaces and the way they impact interaction with the world in two areas. First, we aim to critically discuss the definition of Spatial Interfaces and outline the common components that build such interfaces in today's world. Second, we would like the community to reflect on the path ahead and focus on the potential of what kind of experiences can Spatial Interfaces achieve today",
      "keywords": [
        "spatial computing",
        "spatial interface",
        "augmented reality",
        "interaction"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "0",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3311756",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3311756"
    },
    {
      "title": "Remixed Reality: Manipulating Space and Time in Augmented Reality",
      "authors": ["David Lindlbauer", "Andy D. Wilson"],
      "abstract": "We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.",
      "keywords": ["remixed reality", "augmented reality", "virtual reality"],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "19 April 2018",
      "citations": "14",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3173703",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3173703"
    },
    {
      "title": "High-precision pointing on large wall displays using small handheld devices",
      "authors": [
        "Mathieu Nancel",
        "Olivier Chapuis",
        "Emmanuel Pietriga",
        "Xing-Dong Yang",
        "Pourang P. Irani",
        "Michel Beaudouin-Lafon"
      ],
      "abstract": "Rich interaction with high-resolution wall displays is not limited to remotely pointing at targets. Other relevant types of interaction include virtual navigation, text entry, and direct manipulation of control widgets. However, most techniques for remotely acquiring targets with high precision have studied remote pointing in isolation, focusing on pointing efficiency and ignoring the need to support these other types of interaction. We investigate high-precision pointing techniques capable of acquiring targets as small as 4 millimeters on a 5.5 meters wide display while leaving up to 93 % of a typical tablet device's screen space available for task-specific widgets. We compare these techniques to state-of-the-art distant pointing techniques and show that two of our techniques, a purely relative one and one that uses head orientation, perform as well or better than the best pointing-only input techniques while using a fraction of the interaction resources.",
      "keywords": ["wall displays", "handheld devices", "pointing"],
      "published_in": "CHI '13: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "27 April 2013",
      "citations": "46",
      "isbn": "9781450318990",
      "doi": "10.1145/2470654",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2470654.2470773",
      "paper_url": "https://dl.acm.org/doi/10.1145/2470654.2470773"
    },
    {
      "title": "Outstanding: A Perspective-Switching Technique for Covering Large Distances in VR Games",
      "authors": ["Sebastian Cmentowski", "Andrey Krekhov", "Jens Krueger"],
      "abstract": "Room-scale virtual reality games allow players to experience an unmatched level of presence. A major reason is the natural navigation provided by physical walking. However, the tracking space is still limited, and viable alternatives or extensions are required to reach further virtual destinations. Our current work focuses on traveling over (very) large distances--an area where approaches such as teleportation are too exhausting and WIM teleportations potentially reduce presence. Our idea is to equip players with the ability to switch from first-person to a third-person god-mode perspective on demand. From above, players can command their avatar similar to a real-time strategy game and initiate travels over large distance. In our first exploratory evaluation, we learned that the proposed dynamic switching is intuitive, increases spatial orientation, and allows players to maintain a high degree of presence throughout the game. Based on the outcomes of a participatory design workshop, we also propose a set of extensions to our technique that should be considered in the future.",
      "keywords": [
        "world-in-miniature",
        "fast-travel",
        "presence",
        "navigation",
        "virtual reality games",
        "virtual avatar"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "1",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3312783",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3312783"
    },
    {
      "title": "SpaceTokens: Interactive Map Widgets for Location-centric Interactions",
      "authors": ["Daniel Miau", "Steven Feiner"],
      "abstract": "Map users often need to interact repetitively with multiple important locations. For example, a traveler may frequently check her hotel or a train station on a map, use them to localize an unknown location, or investigate routes involving them. Ironically, these location-centric tasks cannot be performed using locations directly; users must instead pan and zoom the map or use a menu to access locations. We propose SpaceTokens, interactive widgets that act as clones of locations, and which users can create and place on map edges like virtual whiteboard magnets. SpaceTokens make location a first-class citizen of map interaction. They empower users to rapidly perform location-centric tasks directly using locations: users can select combinations of on-screen locations and SpaceTokens to control the map window, or connect them to create routes. Participants in a study overwhelmingly preferred a SpaceTokens prototype over Google Maps on identical smartphones for the majority of tasks.",
      "keywords": [
        "navigation",
        "revisitation",
        "location-centric interaction",
        "maps",
        "bookmarks"
      ],
      "published_in": "CHI '18: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "21 April 2018",
      "citations": "1",
      "isbn": "9781450356206",
      "doi": "10.1145/3173574",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173574.3173822",
      "paper_url": "https://dl.acm.org/doi/10.1145/3173574.3173822"
    },
    {
      "title": "SharedSpaces Mingle",
      "authors": [
        "Leif Handberg",
        "Charlie Gullstrom",
        "Joke Kort",
        "Jimmy Nystr\u00f6m"
      ],
      "abstract": "SharedSpaces is a WebRTC design prototype that creates a virtual media space where people can mingle and interact. Although you are in different locations, you appear side by side in front of a chosen backdrop. This interactive installation addresses spatial and social connectedness, stressing the importance of integrating architectural and spatial features to support complex social dynamics in mediated interaction. The tool engages users in manipulating their real-time video- streams, creatively co-designing a shared mediated space that fits a contextual need. It supports social dynamics by allowing users to draw and paint together and to move and resize video streams. Further, it enhances grounding and social cues by merging video- streams and space, representing users as if they were in the same space. Standard and easily available equipment is used. Recent user studies show that a seamless integration of space, social dynamics and shared activity benefits the experience of presence, naturalness, immersion/engagement and social connectedness.",
      "keywords": [
        "webrtc",
        "spatial presence",
        "social",
        "mediated space",
        "naturalness",
        "media",
        "design",
        "spatial",
        "social connectedness",
        "architecture",
        "immersion",
        "audio/video communication"
      ],
      "published_in": "CHI EA '16: Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems",
      "publication_date": "7 May 2016",
      "citations": "1",
      "isbn": "9781450340823",
      "doi": "10.1145/2851581",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2851581.2889469",
      "paper_url": "https://dl.acm.org/doi/10.1145/2851581.2889469"
    },
    {
      "title": "Material Sketching: Towards the Digital Fabrication of Emergent Material Effects",
      "authors": [
        "Iremnur Tokac",
        "Herman Bruyninckx",
        "Corneel Cannaerts",
        "Andrew Vande Moere"
      ],
      "abstract": "Designing for digital or robotic fabrication typically involves a virtual model in order to determine and coordinate the required operations of its construction. As a result, its creative design space becomes constrained to material expressions that can be predicted through digital modeling. This paper describes our preliminary thinking and first empirical results when this digital modeling phase is skipped, and the designing occurs interactively 'with' the fabrication operations themselves. By analyzing the material responses of corrugated cardboard to simple linear cutting operations that are executed by a robotic arm, we demonstrate how emergent material effects can be discovered improvisationally. Such material effects cannot be virtually modeled, however, they can be recreated and controlled by the robotic manipulations. We believe this form of 'material sketching' broadens the advances in 'human-fabrication interaction' towards a novel and unforeseeable expressions of physical form that require a much more direct, yet still digital, relationship with materiality.",
      "keywords": [
        "craftsmanship",
        "robotic fabrication",
        "human-fabrication interaction",
        "material design",
        "digital fabrication"
      ],
      "published_in": "CHI EA '19: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2019",
      "citations": "0",
      "isbn": "9781450359719",
      "doi": "10.1145/3290607",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3290607.3313036",
      "paper_url": "https://dl.acm.org/doi/10.1145/3290607.3313036"
    },
    {
      "title": "Virtual projection: exploring optical projection as a metaphor for multi-device interaction",
      "authors": ["Dominikus Baur", "Sebastian Boring", "Steven Feiner"],
      "abstract": "Handheld optical projectors provide a simple way to overcome the limited screen real-estate on mobile devices. We present virtual projection (VP), an interaction metaphor inspired by how we intuitively control the position, size, and orientation of a handheld optical projector's image. VP is based on tracking a handheld device without an optical projector and allows selecting a target display on which to position, scale, and orient an item in a single gesture. By relaxing the optical projection metaphor, we can deviate from modeling perspective projection, for example, to constrain scale or orientation, create multiple copies, or offset the image. VP also supports dynamic filtering based on the projection frustum, creating overview and detail applications, and selecting portions of a larger display for zooming and panning. We show exemplary use cases implemented using our optical feature-tracking framework and present the results of a user study demonstrating the effectiveness of VP in complex interactions with large displays.",
      "keywords": [
        "interaction technique",
        "handheld projection",
        "mobile device"
      ],
      "published_in": "CHI '12: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "5 May 2012",
      "citations": "37",
      "isbn": "9781450310154",
      "doi": "10.1145/2207676",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2207676.2208297",
      "paper_url": "https://dl.acm.org/doi/10.1145/2207676.2208297"
    },
    {
      "title": "Don't just stare at me!",
      "authors": ["Ning Wang", "Jonathan Gratch"],
      "abstract": "Communication is more effective and persuasive when participants establish rapport. Tickle-Degnen and Rosenthal [57] argue rapport arises when participants exhibit mutual attentiveness, positivity and coordination. In this paper, we investigate how these factors relate to perceptions of rapport when users interact via avatars in virtual worlds. In this study, participants told a story to what they believed was the avatar of another participant. In fact, the avatar was a computer program that systematically manipulated levels of attentiveness, positivity and coordination. In contrast to Tickel-Degnen and Rosenthal's findings, high-levels of mutual attentiveness alone can dramatically lower perceptions of rapport in avatar communication. Indeed, an agent that attempted to maximize mutual attention performed as poorly as an agent that was designed to convey boredom. Adding positivity and coordination to mutual attentiveness, on the other hand, greatly improved rapport. This work unveils the dependencies between components of rapport and informs the design of agents and avatars in computer mediated communication.",
      "keywords": ["virtual agent", "gaze", "rapport"],
      "published_in": "CHI '10: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "10 April 2010",
      "citations": "30",
      "isbn": "9781605589299",
      "doi": "10.1145/1753326",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/1753326.1753513",
      "paper_url": "https://dl.acm.org/doi/10.1145/1753326.1753513"
    },
    {
      "title": "WireFab: Mix-Dimensional Modeling and Fabrication for 3D Mesh Models",
      "authors": [
        "Min Liu",
        "Yunbo Zhang",
        "Jing Bai",
        "Yuanzhi Cao",
        "Jeffrey M. Alperovich",
        "Karthik Ramani"
      ],
      "abstract": "Many rapid fabrication technologies are directed towards layer wise printing or laser based prototyping. We propose WireFab, a rapid modeling and prototyping system that uses bent metal wires as the structure framework. WireFab approximates both the skeletal articulation and the skin appearance of the corresponding virtual skin meshes, and it allows users to personalize the designs by (1) specifying joint positions and part segmentations, (2) defining joint types and motion ranges to build a wire-based skeletal model, and (3) abstracting the segmented meshes into mixed-dimensional appearance patterns or attachments.The WireFab is designed to allow the user to choose how to best preserve the fidelity of the topological structure and articulation motion while selectively maintaining the fidelity of the geometric appearance. Compared to 3D-printing based high-fidelity fabrication systems, WireFab increases prototyping speed by ignoring unnecessary geometric details while preserving structural integrity and articulation motion. In addition, other rapid or low-fidelity fabrication systems produce only static models, while WireFab produces posable articulated models and has the potential to enable personalized functional products larger than the machines that produce them.",
      "keywords": [
        "skeletal deformation",
        "physical prototyping",
        "interactive curve modeling",
        "shape abstraction",
        "mix-dimensional fabrication"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "4",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025619",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025619"
    },
    {
      "title": "Effects of display size and navigation type on a classification task",
      "authors": [
        "Can Liu",
        "Olivier Chapuis",
        "Michel Beaudouin-Lafon",
        "Eric Lecolinet",
        "Wendy E. Mackay"
      ],
      "abstract": "The advent of ultra-high resolution wall-size displays and their use for complex tasks require a more systematic analysis and deeper understanding of their advantages and drawbacks compared with desktop monitors. While previous work has mostly addressed search, visualization and sense-making tasks, we have designed an abstract classification task that involves explicit data manipulation. Based on our observations of real uses of a wall display, this task represents a large category of applications. We report on a controlled experiment that uses this task to compare physical navigation in front of a wall-size display with virtual navigation using pan-and-zoom on the desktop. Our main finding is a robust interaction effect between display type and task difficulty: while the desktop can be faster than the wall for simple tasks, the wall gains a sizable advantage as the task becomes more difficult. A follow-up study shows that other desktop techniques (overview+detail, lens) do not perform better than pan-and-zoom and are therefore slower than the wall for difficult tasks.",
      "keywords": [
        "classification task",
        "lenses",
        "pan-and-zoom",
        "overview+detail",
        "physical navigation",
        "wall-size display"
      ],
      "published_in": "CHI '14: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "26 April 2014",
      "citations": "43",
      "isbn": "9781450324731",
      "doi": "10.1145/2556288",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2556288.2557020",
      "paper_url": "https://dl.acm.org/doi/10.1145/2556288.2557020"
    },
    {
      "title": "Tap, Dwell or Gesture?: Exploring Head-Based Text Entry Techniques for HMDs",
      "authors": [
        "Chun Yu",
        "Yizheng Gu",
        "Zhican Yang",
        "Xin Yi",
        "Hengliang Luo",
        "Yuanchun Shi"
      ],
      "abstract": "Despite the increasing popularity of head mounted displays (HMDs), development of efficient text entry methods on these devices has remained under explored. In this paper, we investigate the feasibility of head-based text entry for HMDs, by which, the user controls a pointer on a virtual keyboard using head rotation. Specifically, we investigate three techniques: TapType, DwellType, and GestureType. Users of TapType select a letter by pointing to it and tapping a button. Users of DwellType select a letter by pointing to it and dwelling over it for a period of time. Users of GestureType perform word-level input using a gesture typing style. Two lab studies were conducted. In the first study, users typed 10.59 WPM, 15.58 WPM, and 19.04 WPM with DwellType, TapType, and GestureType, respectively. Users subjectively felt that all three of the techniques were easy to learn and considered the induced fatigue to be acceptable. In the second study, we further investigated GestureType. We improved its gesture-word recognition algorithm by incorporating the head movement pattern obtained from the first study. This resulted in users reaching 24.73 WPM after 60 minutes of training. Based on these results, we argue that head-based text entry is feasible and practical on HMDs, and deserves more attention.",
      "keywords": [
        "gesture keyboard",
        "hmd",
        "head-based text entry",
        "dwelling"
      ],
      "published_in": "CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "publication_date": "2 May 2017",
      "citations": "30",
      "isbn": "9781450346559",
      "doi": "10.1145/3025453",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025964",
      "paper_url": "https://dl.acm.org/doi/10.1145/3025453.3025964"
    },
    {
      "title": "Virtual birding: extending an environmental pastime into the virtual world for citizen science",
      "authors": ["Mark Cottman-Fields", "Margot Brereton", "Paul Roe"],
      "abstract": "This paper investigates engaging experienced birders, as volunteer citizen scientists, to analyze large recorded audio datasets gathered through environmental acoustic monitoring. Although audio data is straightforward to gather, automated analysis remains a challenging task; the existing expertise, local knowledge and motivation of the birder community can complement computational approaches and provide distinct benefits. We explored both the culture and practice of birders, and paradigms for interacting with recorded audio data. A variety of candidate design elements were tested with birders.This study contributes an understanding of how virtual interactions and practices can be developed to complement existing practices of experienced birders in the physical world. In so doing this study contributes a new approach to engagement in e-science. Whereas most citizen science projects task lay participants with discrete real world or artificial activities, sometimes using extrinsic motivators, this approach builds on existing intrinsically satisfying practices.",
      "keywords": [
        "bioacoustics",
        "birder",
        "domain-specific expertise",
        "bird watching",
        "citizen science",
        "biodiversity monitoring"
      ],
      "published_in": "CHI '13: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "publication_date": "27 April 2013",
      "citations": "17",
      "isbn": "9781450318990",
      "doi": "10.1145/2470654",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2470654.2466268",
      "paper_url": "https://dl.acm.org/doi/10.1145/2470654.2466268"
    }
  ],
  "total_results": 73,
  "total_filtered_results": 73,
  "total_pages": 1
}
