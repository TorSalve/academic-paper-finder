{"papers": [{"title": "Adjusting the difference between 3D spaces in VR systems and human perception through object manipulation", "authors": ["K. Kakusho", "J. Kitawaki", "S. Hagihara", "M. Minoh"], "abstract": "This paper describes a task-oriented approach to cope with the difference between the three dimensional (3D) space in a virtual reality (VR) system and that perceived by the user from its stereoscopic images with binocular disparity. This difference occurs for the reason that human visual perception is affected not only by binocular disparity but also by various kinds of visual cues. Although it is possible to set some of those cues consistent with each other when they are presented by a stereoscopic display, it is difficult to do the same for other cues especially vergence and focus. As the result, the users of VR systems often fail to manipulate virtual objects due to this difference. In this article, we propose to adjust the virtual space in a VR system through object manipulation by the user so that the user does not fail in each manipulation. We verified by experiments that this adjustment is effective to improve the rate of successful manipulation.", "keywords": ["Virtual reality", "Humans", "Visual perception", "Focusing", "Visualization", "Multimedia systems", "Three dimensional displays", "Eyes", "Layout", "Retina", "virtual reality", "visual perception", "three-dimensional displays", "stereo image processing", "3D spaces", "VR system", "human perception", "object manipulation", "task-oriented approach", "virtual reality system", "stereoscopic images", "binocular disparity", "visual cues"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 2, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840487", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840487", "paper_url": "https://ieeexplore.ieee.org/document/840487"}, {"title": "Selection and Manipulation Whole-Body Gesture Elicitation Study in Virtual Reality", "authors": ["Francisco R. Ortega", "Katherine Tarre", "Mathew Kress", "Adam S. Williams", "Armando B. Barreto", "Naphtali D. Rishe"], "abstract": "We present a whole-body gesture elicitation study using Head Mounted Displays, including a legacy bias reduction technique. The motivation for this study was to understand the type of gesture agreement rates for selection and manipulation interactions and to improve the user experience for whole-body interactions. We found that regardless of the production technique used to remove legacy bias, legacy bias was still found in some of the produced gestures.", "keywords": ["Wheels", "Three-dimensional displays", "Virtual reality", "User interfaces", "Resists", "Magnetic resonance imaging", "gesture recognition", "helmet mounted displays", "human computer interaction", "user experience", "virtual reality", "head mounted displays", "user experience", "production technique", "legacy bias", "whole-body gesture elicitation study", "virtual reality", "whole-body interactions", "manipulation interactions", "gesture agreement rates", "legacy bias reduction technique", "Gesture Elicitation", "Gestures", "Virtual Reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798182", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798182", "paper_url": "https://ieeexplore.ieee.org/document/8798182"}, {"title": "The Planar: an interdisciplinary approach to a VR enabled tool for generation and manipulation of 3D data in industrial environments", "authors": ["R. Schoenfelder", "F. Spenling"], "abstract": "We introduce the early prototype of the Planar, a novel input/output device designed for application in task areas focusing on the generation and manipulation of 3D data, e.g. CAD or styling. Technically, the Planar offers a spatially aware pen-sensitive display, mounted on an adjustable, scooter-like autonomous platform. The movable screen, with 6 degrees of freedom, can act like a window into 3D virtual environments and allows for efficient 2D and 3D interaction at the same time. Through our research, we have developed an interdisciplinary approach that aims to overcome a variety of drawbacks within many VR installations while extending the concepts of existing conventional systems. Our primary areas of focus include: working environments, forms of interaction, ergonomics, and process integration. In order to demonstrate the potential of the Planar we show a small review and a sketching/annotation application. The overall goal of this work is to contribute to the development of real VR applications.", "keywords": ["Virtual reality", "Displays", "Ergonomics", "User interfaces", "Prototypes", "Design automation", "Virtual environment", "User centered design", "Computer interfaces", "Graphics", "manufacturing data processing", "production engineering computing", "product design", "CAD", "touch sensitive screens", "user interfaces", "virtual reality", "human computer interaction", "Planar", "virtual reality", "data generation", "data manipulation", "3D data", "industrial environments", "input-output device", "CAD", "styling", "spatially aware pen-sensitive display", "adjustable scooter-like autonomous platform", "movable screen", "3D virtual environments", "2D interaction", "3D interaction", "working environments", "ergonomics", "process integration", "user interface", "product design"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 2, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492785", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492785", "paper_url": "https://ieeexplore.ieee.org/document/1492785"}, {"title": "Interactive Virtual Reality Simulation for Nanoparticle Manipulation and Nanoassembly using Optical Tweezers", "authors": ["Krishna C Bhavaraju"], "abstract": "Nanotechnology is one of the most promising technologies for future development. This paper proposes virtual reality (VR) as a tool to simulate nano particle manipulation using optical tweezers towards achieving nano-assembly and to handle effectively issues such as difficulty in viewing, perceiving and controlling the nano-scale objects. The simulation modeled using virtual reality displays all the forces acting on nanoparticle during the manipulation. The simulation is developed for particles that belong to the Rayleigh region and represents interactions of OT (a laser beam) with the nanoparticle. The laser beam aimed on to the nanoparticle traps the particle by applying optical forces. The trapped particle is then moved by moving the laser beam. The proposed VR based simulation tool with it capabilities can be easily extended and used for creating and open system framework by connecting it to a real OT setup to control nanoparticles manipulation. In addition, a feedback system can be build to increase of precision of movement.", "keywords": ["Virtual reality", "Optical feedback", "Particle beams", "Laser beams", "Laser modes", "Laser feedback", "Nanotechnology", "Optical control", "Displays", "Charge carrier processes", "digital simulation", "electronic engineering computing", "microassembling", "nanoparticles", "open systems", "optical instruments", "virtual reality", "interactive virtual reality simulation", "nanoparticle manipulation", "nanoassembly", "optical tweezers", "virtual reality displays", "Rayleigh region", "nanoparticle traps", "laser beam", "open system framework", "feedback system", "Virtual Reality", "Optical Tweezers", "I 6 [Simulation and Modeling]: Applications, Types of Simulation-Visual"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 1, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811040", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811040", "paper_url": "https://ieeexplore.ieee.org/document/4811040"}, {"title": "Smart Choices for Deviceless and Device-Based Manipulation in Immersive Virtual Reality", "authors": ["Fabio M. Caputo", "Daniel Mendes", "Alessia Bonetti", "Giacomo Saletti", "Andrea Giachetti"], "abstract": "The choice of a suitable method for object manipulation is one of the most critical aspects of virtual environment design. It has been shown that different environments or applications might benefit from direct manipulation approaches, while others might be more usable with indirect ones, exploiting, for example, three dimensional virtual widgets. When it comes to mid-air interactions, the success of a manipulation technique is not only defined by the kind of application but also by the hardware setup, especially when specific restrictions exist. In this paper we present an experimental evaluation of different techniques and hardware for mid-air object manipulation in immersive virtual environments (IVE). We compared task performances using both deviceless and device-based tracking solutions, combined with direct and widget-based approaches. We also tested, in the case of freehand manipulation, the effects of different visual feedback, comparing the use of a realistic virtual hand rendering with a simple cursor-like visualization.", "keywords": ["Task analysis", "Three-dimensional displays", "Visualization", "Virtual environments", "Pins", "Tracking", "data visualisation", "interactive systems", "virtual reality", "immersive virtual environments", "freehand manipulation", "realistic virtual hand", "smart choices", "immersive virtual reality", "critical aspects", "virtual environment design", "direct manipulation approaches", "indirect ones", "dimensional virtual widgets", "mid-air interactions", "manipulation technique", "hardware setup", "experimental evaluation", "mid-air object manipulation", "visual feedback", "specific restrictions", "deviceless tracking solutions", "device-based tracking solutions", "IVE", "realistic virtual hand rendering", "cursor-like visualization", "Human-centered computing-Interaction techniques Human-centered computing-User studies Human-centered computing-HCI design and evaluation methods Human-centered computing-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446598", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446598", "paper_url": "https://ieeexplore.ieee.org/document/8446598"}, {"title": "A Realtime Virtual Grasping System for Manipulating Complex Objects", "authors": ["Hao Tian", "Chanabo Wana", "Xinvu Zhana"], "abstract": "With the introduction of new VR/AR devices, realistic and fast interaction within virtual environments becomes more and more appealing. However, the challenge is to make interactions with virtual objects accurately reflect interactions with physical objects in realtime. In this paper, we present a virtual grasping system for multi-fingered hands when manipulating complex objects. Humanlike grasping postures and realistic grasping motions guarantee a physically plausible appearance for hand grasping. Our system does not require any pre-captured motion data. Our system is fast enough to allow realtime interaction during virtual grasping for complex objects.", "keywords": ["Grasping", "Virtual environments", "Indexes", "Task analysis", "Three-dimensional displays", "grippers", "virtual reality", "realtime virtual grasping system", "VR/AR devices", "realistic interaction", "virtual environments", "virtual objects", "physical objects", "multifingered hands", "humanlike grasping postures", "hand grasping", "complex object manipulation", "grasping motions", "Human-centered computing", "Human computer interaction (HCI)", "Interaction paradigms", "Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 1, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446538", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446538", "paper_url": "https://ieeexplore.ieee.org/document/8446538"}, {"title": "Virtual liquid manipulation using general shape vessel", "authors": ["K. Funahashi", "Y. Iwahori"], "abstract": "Describes a method to realize the interactive manipulation of a virtual liquid using virtual vessels which are expressed by a general convex-shape polyhedron. We propose a liquid manipulation model which has some functions to treat the relation between the volume of liquid in a vessel and the height level of the liquid surface in it while it is being tilted. For a general-shape vessel, a lookup table is implemented to calculate the above functions. Our system with this proposed model makes it possible to catch the liquid using the virtual vessel, to hold the liquid in it, then to spill the liquid by tilting it. Also, the system realizes a manipulation method to skim the liquid from another liquid vessel.", "keywords": ["Shape", "Virtual reality", "Space technology", "Computer graphics", "Computational modeling", "Computer simulation", "Virtual manufacturing", "Surface treatment", "Solids", "Layout", "liquids", "virtual reality", "table lookup", "virtual liquid manipulation", "general-shape virtual vessel", "interactive manipulation", "general convex-shape polyhedron", "liquid volume", "liquid surface height level", "vessel tilting", "lookup table", "liquid catching", "liquid holding", "liquid spilling", "liquid skimming"], "published_in": "Proceedings IEEE Virtual Reality 2001", "publication_date": "2001", "citations": 0, "isbn": ["0-7695-0948-7"], "doi": "10.1109/VR.2001.913798", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=913798", "paper_url": "https://ieeexplore.ieee.org/document/913798"}, {"title": "6 Degrees-of-freedom manipulation with a transparent, tangible object in world-fixed virtual reality displays", "authors": ["David J. Zielinski", "Derek Nankivil", "Regis Kopper"], "abstract": "We propose Specimen Box, an interaction technique that allows world-fixed display (such as CAVEs) users to naturally hold a plausible physical object while manipulating virtual content inside it. This virtual content is rendered based on the tracked position of the box. Specimen Box provides the weight and tactile feel of an actual object and does not occlude rendered objects in the scene. The end result is that the user sees the virtual content as if it exists inside the clear physical box. We conducted a user study which involved a cognitively loaded inspection task requiring extensive manipulation of the box. We compared Specimen Box to Grab-and-Twirl, a naturalistic bimanual manipulation technique that closely mimics the mechanics of our proposed technique. Results show that performance was significantly faster with Specimen Box. Further, performance of the control technique was positively affected by experience with Specimen Box.", "keywords": ["Haptic interfaces", "Inspection", "Face", "Three-dimensional displays", "Virtual environments", "Biomedical engineering", "computer displays", "virtual reality", "6 degrees-of-freedom manipulation", "transparent object", "tangible object", "world-fixed virtual reality displays", "Specimen Box", "interaction technique", "physical object", "virtual content manipulation", "cognitively loaded inspection task", "Grab-and-Twirl", "naturalistic bimanual manipulation technique", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 2, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892256", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892256", "paper_url": "https://ieeexplore.ieee.org/document/7892256"}, {"title": "Multimodal menu presentation and selection in immersive virtual environments", "authors": ["Namgyu Kim", "G.J. Kim", "Chan-Mo Park", "Inseok Lee", "S.H. Lim"], "abstract": "Usability has become one of the key ingredients in making virtual reality (VR) systems work, and a big part of a usable VR system is the design of effective interface/interaction schemes. We investigate the usability of various menu presentation and multi-modal selection schemes in immersive virtual environments. We have identified five major menu displays and 13 possible menu selection methods; among them, we have completed the usability testing for two interaction methods across the five display methods: (1) position control by tracking and command-giving by button-pressing, and (2) both position control and command-giving by gesture recognition. Other interaction methods, including by voice, remain to be investigated.", "keywords": ["Testing", "Displays", "Usability", "Virtual reality", "Computer science", "Design engineering", "Industrial engineering", "Nominations and elections", "Electrical capacitance tomography", "Ice", "virtual reality", "tracking", "computer displays", "user centred design", "human factors", "user interfaces", "multi-modal menu presentation", "multi-modal menu selection", "immersive virtual environments", "usability testing", "virtual reality", "user interface design", "interaction methods", "menu displays", "position control", "tracking", "command-giving", "button-pressing", "gesture recognition", "voice input"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 2, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840509", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840509", "paper_url": "https://ieeexplore.ieee.org/document/840509"}, {"title": "A multi-layer approach of interactive path planning for assisted manipulation in virtual reality", "authors": ["Simon Cailhol", "Philippe Fillatreau", "Jean-Yves Fourquet", "Yingshen Zhao"], "abstract": "This work considers Virtual Reality (VR) applications dealing with objects manipulation (such as industrial product assembly, disassembly or maintenance simulation). For such applications, the operator performing the simulation can be assisted by path planning techniques from the robotics research field. A novel automatic path planner involving geometrical, topological and semantic information of the environment is proposed for the guidance of the user through a haptic device. The interaction allows on one hand, the automatic path planner providing assistance to the human operator, and on the other hand, the human operator to reset the whole planning process suggesting a better suited path. Control sharing techniques are used to improve the assisted manipulation ergonomics by dynamically balancing the automatic path planner authority according to the operator involvement in the task, and by predicting user's intent to integrate it as early as possible in the planning process.", "keywords": ["Path planning", "Solid modeling", "Planning", "Semantics", "Assembly", "Virtual reality", "Robots", "haptic interfaces", "human computer interaction", "virtual reality", "interactive path planning", "virtual reality", "haptic device", "assisted manipulation ergonomics", "user intent prediction", "Virtual Reality", "Interactive path planning", "control sharing", "multi-layer architecture"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 0, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223344", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223344", "paper_url": "https://ieeexplore.ieee.org/document/7223344"}, {"title": "Flying robot manipulation system using a virtual plane", "authors": ["Kazuya Yonezawa", "Takefumi Ogawa"], "abstract": "The flexible movements of flying robots make it difficult for novices to manipulate them precisely with controllers such as a joystick and a proportional radio system. Moreover, the mapping of instructions between a robot and its reactions is not necessarily intuitive for users. We propose manipulation methods for flying robots using augmented reality technologies. In the proposed system, a virtual plane is superimposed on a flying robot and users control the robot by manipulating the virtual plane and drawing a moving path on it. We present the design and implementation of our system and describe experiments conducted to evaluate our methods.", "keywords": ["Prototypes", "Service robots", "Augmented reality", "Training", "Robot control", "Unmanned aerial vehicles", "aerospace robotics", "augmented reality", "control engineering computing", "flexible structures", "interactive devices", "manipulators", "flying robot manipulation system", "virtual plane", "flexible movement", "joystick", "proportional radio system", "augmented reality technology", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities", "I.2.9 [Artificial Intelligence]: Robotics \u2014 Operator interfaces"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 0, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223421", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223421", "paper_url": "https://ieeexplore.ieee.org/document/7223421"}, {"title": "Unified gesture-based interaction techniques for object manipulation and navigation in a large-scale virtual environment", "authors": ["Y. Tomozoe", "T. Machida", "K. Kiyokawa", "H. Takemura"], "abstract": "Manipulation of virtual objects and navigation are common operations in a large-scale virtual environment. In this paper, we propose a few gesture-based interaction techniques that can be used for both object manipulation and navigation. Unlike existing methods, our techniques enable a user to perform these two types of operations flexibly with a little practice in identical interaction manners by introducing a movability property attached to every virtual object.", "keywords": ["Navigation", "Large-scale systems", "Virtual environment", "Virtual reality", "Information science", "Real time systems", "Layout", "Books", "Keyboards", "Feedback", "gesture recognition", "virtual reality", "navigation", "gesture-based interaction", "object manipulation", "virtual environment", "virtual navigation", "virtual object"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 1, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310098", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310098", "paper_url": "https://ieeexplore.ieee.org/document/1310098"}, {"title": "The Influence of Visual Appearance of User's Avatar on the Manipulation of Objects in Virtual Environments", "authors": ["Abdelmajid Kadri", "Anatole Lecuyer", "Jean-Marie Burkhardt", "Simon Richir"], "abstract": "This paper describes an experiment conducted to study the influence of visual appearance of user's avatar (or 3D cursor) on the manipulation of virtual objects in virtual environments (VE). Participants were asked to pick up a virtual cube and place it at a random location in a VE. We found that one visual property of the avatar (the presence or absence of a directional cue) could influence the way participants picked up the cube in the VE. When using an avatar or a 3D cursor with a strong directional cue (e.g., arrows pointing to the left or right), participants generally picked up the cube by a specific side (e.g., right or left side). When using 3D cursors with no main directional cue, participants more frequently picked up the virtual cube by its front or top", "keywords": ["Avatars", "Virtual environment", "Imaging phantoms", "Keyboards", "Virtual reality", "Layout", "Extremities", "Pressing", "Chromium", "Multimedia systems", "avatars", "avatar", "object manipulation", "virtual environments", "virtual cube", "3D cursor", "avatar", "3D cursor", "visual appearance", "directional cue", "manipulation"], "published_in": "2007 IEEE Virtual Reality Conference", "publication_date": "March 2007", "citations": 0, "isbn": ["1-4244-0905-5", "1-4244-0906-3"], "doi": "10.1109/VR.2007.352507", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4161049", "paper_url": "https://ieeexplore.ieee.org/document/4161049"}, {"title": "Depth-based 3D gesture multi-level radial menu for virtual object manipulation", "authors": ["Matthew M. Davis", "Joseph L. Gabbard", "Doug A. Bowman", "Dennis Gracanin"], "abstract": "In this work, we present a depth-based solution to multi-level menus for selection and manipulation of virtual objects using freehand gestures. Navigation between and through menus is performed using three gesture states that utilize X, Y translations of the finger with boundary crossing. Although presented in a single context, this menu structure can be applied to a myriad of domains requiring several levels of menu data, and serves to supplement existing and emerging menu design for augmented, virtual, and mixed-reality applications.", "keywords": ["Thumb", "Navigation", "Virtual reality", "User interfaces", "Indexes", "Visualization", "augmented reality", "gesture recognition", "solid modelling", "depth-based 3D gesture multilevel radial menu", "virtual object manipulation", "freehand gestures", "boundary crossing", "menu design", "mixed-reality applications", "augmented reality applications", "virtual reality applications", "Natural user interface", "translation", "boundary crossing"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 7, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504707", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504707", "paper_url": "https://ieeexplore.ieee.org/document/7504707"}, {"title": "Effect of Proprioception Training of patient with Hemiplegia by Manipulating Visual Feedback using Virtual Reality: The Preliminary results", "authors": ["Sangwoo Cho", "Kiwan Han", "Hyeongrae Lee", "Jinsick Park", "In Young Kim", "Sun I. Kim", "Jeonghun Ku", "Youn Joo Kang"], "abstract": "In this study, we confirmed proprioception training effect of patients with hemiplegia by manipulating visual feedback. Six patients with hemiplegia were participated in the experiment. Patients have trained with the reaching task with visual feedback without visual feedback for two weeks. Patients were evaluated with pre-, middle test and post-test with the task with and without visual feedback. In the results, the first-click error distance after the training of the reaching task was reduced when they got the training with the task removed visual feedback. In addition, the performance velocity profile of reaching movement formed an inverse U shape after the training. In conclusion, visual feedback manipulation using virtual reality could provide a tool for training reaching movement by enforcing to use their proprioception, which enhances reaching movement skills for patients with hemiplegia.", "keywords": ["Virtual reality", "Force feedback", "Biomedical engineering", "Muscles", "Engine cylinders", "Sun", "Hospitals", "Testing", "Shape", "Computer applications", "patient treatment", "virtual reality", "visual perception", "patient proprioception training", "hemiplegia", "virtual reality", "visual feedback manipulation", "Virtual Reality", "Proprioception", "Visual Feedback", "J.3 [Computer applications]: LIFE AND MEDICAL SCIENCES\u00bfHealth"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 0, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811056", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811056", "paper_url": "https://ieeexplore.ieee.org/document/4811056"}, {"title": "Comparing three interaction methods for manipulating thin deformable virtual objects", "authors": ["Johannes Hummel", "Robin Wolff", "Andreas Gerndt", "Torsten Kuhlen"], "abstract": "We present results of a user study in which we compared three interaction methods for manipulating deformable objects in immersive virtual environments. The task was to control a virtual robot hand removing a thin foil cover from a satellite in an on-orbit servicing training simulator. The lack of haptic feedback placed a high challenge on the user when trying to apply the right force for grasping the foil without losing grip or damaging it. We compared the intuitiveness and effectiveness of using a tracked joystick, finger distance measurement, and a novel prototype enabling direct force input through pinching.", "keywords": ["Force", "Robots", "Satellites", "Prototypes", "Electronic mail", "Force measurement", "Training", "interactive devices", "mobile robots", "virtual reality", "interaction method", "thin deformable virtual object", "immersive virtual environment", "virtual robot hand", "on-orbit servicing training simulator", "haptic feedback", "tracked joystick", "finger distance measurement", "direct force input", "pinching", "B.4.2 [Input/Output Devices]: Channels and controllers", "H.3.4 [Systems and Software]: Performance evaluation H.5.2 [User Interfaces]: Interaction styles I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 4, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180920", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180920", "paper_url": "https://ieeexplore.ieee.org/document/6180920"}, {"title": "Effect based scene manipulation for multimodal VR systems", "authors": ["Matthias Haringer", "Steffi Beckhaus"], "abstract": "Games use high quality graphics and pre-crafted visual and auditive effects to create user experiences. Virtual environments offer new navigation and interaction methods, immersive installations, and haptic and olfactoric output. We introduce an extension to VR systems, which makes it possible to use a wide range of multimodal effects from gaming and VR to be activated and modified on a per object basis at runtime. To access, manipulate, and add effects to the objects of a scene intuitively, our extension realizes an abstract, hierarchical scene concept using multimodal objects. Multiple effects can be added to each object and the parameters of each effect can be manipulated online. Fading of effects and bundling of multiple effects for multiple objects are more advanced features of the system.", "keywords": ["Layout", "Virtual reality", "Virtual environment", "Mood", "Graphics", "Navigation", "Engines", "Runtime", "Tuners", "Haptic interfaces", "computer games", "computer graphics", "virtual reality", "effect based scene manipulation", "multimodal VR systems", "high quality graphics", "precrafted visual effects", "virtual environments", "interaction methods", "olfactoric output", "haptic output", "virtual environments", "shaders", "effects", "multimodality"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 1, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444814", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444814", "paper_url": "https://ieeexplore.ieee.org/document/5444814"}, {"title": "Hand motion calibration and retargeting for intuitive object manipulation in immersive virtual environments", "authors": ["Brandon Wilson", "Matthew Bounds", "Alireza Tavakkoli"], "abstract": "In this paper a system is proposed to combine small finger movements with the large scale body movements captured from a motion capture system. The strength of the proposed work over previous research is in the real-time and natural interactions that the virtual hands have with their environment. By being able to conform to physics, the virtual hands feel like virtual extensions of one's own hands. This provides a higher degree of immersion and interactivity when compared to more traditional virtual reality systems.", "keywords": ["Skeleton", "Calibration", "Virtual environments", "Silicon", "Real-time systems", "Tracking", "calibration", "image motion analysis", "interactive systems", "virtual reality", "hand motion calibration", "hand motion retargeting", "intuitive object manipulation", "immersive virtual environments", "motion capture system", "natural interactions", "real-time interactions", "virtual hands", "virtual extensions", "virtual reality systems", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Virtual Reality", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Animation", "I.3.8 [Computer Graphics]: Applications"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 0, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504779", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504779", "paper_url": "https://ieeexplore.ieee.org/document/7504779"}, {"title": "A Framework for Virtual 3D Manipulation of Face in Video", "authors": ["Jungsik Park", "Jong-Ii Park"], "abstract": "This paper presents a framework that enables a user to manipulate his/her face shape three-dimensionally in video. Existing face manipulation applications and methods have some limitations: single photo, manipulation on image domain, or limited deformation. In the proposed framework, face is tracked from video by using landmark tracking and fitting 3d morphable face model to image, and the face model is further deformed according to the user input with mesh deformation method and rendered with texture from the frame image onto the camera preview. Therefore, unlike conventional applications and researches for face manipulation, the proposed framework allows the user to perform free-form 3d deformation of face in video and to view the deformed face at various viewpoints.", "keywords": ["Face", "Three-dimensional displays", "Shape", "Solid modeling", "Deformable models", "Strain", "Cameras", "cameras", "deformation", "image texture", "object tracking", "mesh deformation method", "landmark tracking", "3D morphable face model", "3D face manipulation applications", "image domain manipulation", "face tracking", "image texture", "camera", "free-form 3D face deformation method", "Computing methodologies-Computer graphics-Graphics systems and interface-Mixed / augmented reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446445", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446445", "paper_url": "https://ieeexplore.ieee.org/document/8446445"}, {"title": "Redirected Jumping: Imperceptibly Manipulating Jump Motions in Virtual Reality", "authors": ["Daigo Hayashi", "Kazuyuki Fujita", "Kazuki Takashima", "Robert W. Lindeman", "Yoshifumi Kitamura"], "abstract": "Jumping is a fundamental movement in our daily lives that is often used in many video games. However, little research has been done on jumping and its possible use as a redirection technique in virtual reality (VR). In this study we explore Redirected Jumping, a novel redirection technique which enables us to purposefully manipulate the mapping of the user's physical jumping movements (e.g., distance and direction) to movement in the virtual space, allowing richer and more active physical VR experiences within a limited tracking area. To demonstrate the possibilities afforded by Redirected Jumping, we implemented a jump detection algorithm and jumping redirection methods for three basic jumping actions (i.e., horizontal, vertical, and rotational jumps) using common VR devices. We conducted three user studies to investigate the effective manipulation ranges, and the results revealed that our methods can manipulate a user's jumping movements without his/her noticing, similar to walking.", "keywords": ["Legged locomotion", "Virtual reality", "Meters", "Tracking", "Games", "Sports", "Foot", "virtual reality", "Redirected Jumping", "imperceptibly manipulating jump motions", "virtual reality", "fundamental movement", "virtual space", "jump detection algorithm", "jumping redirection methods", "basic jumping actions", "horizontal jumps", "rotational jumps", "VR experience", "vertical jumps", "effective manipulation", "redirection technique", "Virtual reality", "virtual locomotion", "redirected walking", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems\u2014Artificial, augmented, and virtual realities", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u2014Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797989", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797989", "paper_url": "https://ieeexplore.ieee.org/document/8797989"}, {"title": "Analysis of Proximity-Based Multimodal Feedback for 3D Selection in Immersive Virtual Environments", "authors": ["Oscar Ariza", "Gerd Bruder", "Nicholas Katzakis", "Frank Steinicke"], "abstract": "Interaction tasks in virtual reality (VR) such as three-dimensional (3D) selection or manipulation of objects often suffer from reduced performance due to missing or different feedback provided by VR systems than during corresponding realworld interactions. Vibrotactile and auditory feedback have been suggested as additional perceptual cues complementing the visual channel to improve interaction in VR. However, it has rarely been shown that multimodal feedback improves performance or reduces errors during 3D object selection. Only little research has been conducted in the area of proximity-based multimodal feedback, in which stimulus intensities depend on spatiotemporal relations between input device and the virtual target object. In this paper, we analyzed the effects of unimodal and bimodal feedback provided through the visual, auditory and tactile modalities, while users perform 3D object selections in VEs, by comparing both binary and continuous proximity-based feedback. We conducted a Fitts' Law experiment and evaluated the different feedback approaches. The results show that the feedback types affect ballistic and correction phases of the selection movement, and significantly influence the user performance.", "keywords": ["Three-dimensional displays", "Task analysis", "Visualization", "Haptic interfaces", "Feeds", "Performance evaluation", "Two dimensional displays", "feedback", "haptic interfaces", "human computer interaction", "virtual reality", "proximity-based multimodal feedback", "3D selection", "immersive virtual environments", "interaction tasks", "virtual reality", "missing feedback", "VR systems", "vibrotactile feedback", "auditory feedback", "3D object selection", "virtual target object", "unimodal feedback", "bimodal feedback", "visual modalities", "auditory modalities", "tactile modalities", "binary proximity-based feedback", "continuous proximity-based feedback", "selection movement", "user performance", "perceptual cues", "Fitts' Law experiment", "H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input Devices and Strategies", "Evaluation"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 1, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446317", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446317", "paper_url": "https://ieeexplore.ieee.org/document/8446317"}, {"title": "Selection and Manipulation Whole-Body Gesture Elicitation Study In Virtual Reality", "authors": ["Francisco R. Ortega", "Katherine Tarre", "Mathew Kress", "Adam S. Williams", "Armando B. Barreto", "Naphtali D. Rishe"], "abstract": "We present a whole-body gesture elicitation study using Head Mounted Displays, including a legacy bias reduction. The motivation for this study was to understand the type of gesture agreement rates for selection and manipulation interactions and to improve the user experience for whole-body interactions. We looked at 23 participants and 20 distinct referents (with multiple gestures per referent). We found that regardless of the production technique used to remove legacy bias, legacy bias was still found in some of the produced gestures. In some instances, gestures were derived from previous interactions but were still appropriate for the environment presented. This study provides a rich set of information and useful recommendations for future designers and/or developers.", "keywords": ["Production", "Virtual reality", "User interfaces", "Three-dimensional displays", "Resists", "Headphones", "User experience", "gesture recognition", "helmet mounted displays", "human computer interaction", "virtual reality", "whole-body interactions", "manipulation interactions", "gesture agreement rates", "legacy bias reduction", "whole-body gesture elicitation study", "virtual reality", "legacy bias", "head mounted displays", "Gesture Elicitation\u2014Gestures\u2014Virtual Reality\u2014Whole-Body"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798105", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798105", "paper_url": "https://ieeexplore.ieee.org/document/8798105"}, {"title": "Evaluation of Visual Perception Manipulation in Virtual Reality Training Environments to Improve Golf Performance", "authors": ["Anushka Godse", "Rajiv Khadka", "Amy Banic"], "abstract": "In the real world, prior research in the field of perception and action has shown that individuals visually perceive objects differently based on their actual performance of an action on those objects, especially for sporting activities. For example, a golfer who performs well on putting a ball into the hole, will perceive that ball as larger (easier to hit) and the hole as larger (easier to put the ball in). We asked the following research question, can manipulation of visual perception of objects influence actual performance in the real world? Virtual objects are easily manipulated in Virtual Reality environments, therefore we investigated the use of Virtual Reality training where the properties of objects, such as size, were manipulated to influence perception on a golf putting task. In this paper, we present the results of our experimental user study. Putting performance increased after virtual reality training exposure when virtual objects were larger (perceived as easier to hit) and decreased when virtual objects were smaller (more difficult to hit). Our research has the potential to broaden the study of how virtual reality training can be used to improve sports training in a unique way.", "keywords": ["Training", "Task analysis", "Sports", "Visualization", "Virtual environments", "Human computer interaction", "computer based training", "sport", "virtual reality", "visual perception", "virtual objects", "sports training", "visual perception manipulation", "golf putting task", "putting performance", "virtual reality training environments", "golf performance improvement", "Virtual Reality training", "perception and action", "visual manipulation", "virtual environments", "sports training", "change in size of virtual objects", "sports performance", "golf task", "CCS [Human-centered Computing]: Human-computer interaction (HCI)- Interaction paradigms-Virtual Reality", "[Human-centered Computing]: CCS Human-computer interaction (HCI)- Empirical studies in HCI", "CCS [Software and its engineering]: Software Organization and properties-Virtual worlds software-Virtual worlds training simulation"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798026", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798026", "paper_url": "https://ieeexplore.ieee.org/document/8798026"}, {"title": "Towards a linguistically motivated model for selection in virtual reality", "authors": ["Thies Pfeiffer"], "abstract": "Swiftness and robustness of natural communication is tied to the redundancy and complementarity found in our multimodal communication. Swiftness and robustness of human-computer interaction (HCI) is also a key to the success of a virtual reality (VR) environment. The interpretation of multimodal interaction signals has therefore been considered a high goal in VR research, e.g. following the visions of Bolt's put-that-there in 1980 [1]. It is our impression that research on user interfaces for VR systems has been focused primarily on finding and evaluating technical solutions and thus followed a technology-oriented approach to HCI. In this article, we argue to complement this by a human-oriented approach based on the observation of human-human interaction. The aim is to find models of human-human interaction that can be used to create user interfaces that feel natural. As the field of Linguistics is dedicated to the observation and modeling of human-human communication, it could be worthwhile to approach natural user interfaces from a linguistic perspective. We expect at least two benefits from following this approach. First, the human-oriented approach substantiates our understanding of natural human interactions. Second, it brings about a new perspective by taking the interaction capabilities of a human addressee into account, which are not often explicitly considered or compared with that of the system. As a consequence of following both approaches to create user interfaces, we expect more general models of human interaction to emerge.", "keywords": ["Pragmatics", "Humans", "Virtual reality", "Human computer interaction", "Solid modeling", "Context", "human computer interaction", "user interfaces", "virtual reality", "linguistically motivated model", "virtual reality environment", "natural communication", "multimodal communication", "human-computer interaction", "HCI", "multimodal interaction signals", "VR research", "Bolt put-that-there", "technology-oriented approach", "human-oriented approach", "human-human interaction", "human-human communication", "natural user interfaces", "human addressee", "H.5.2 [Information Interfaces and Presentation]: User Interfaces \u2014 Natural Language", "I.3.6 [Computer Graphics]: Methodology and Techniques \u2014 Interaction Techniques", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial", "Augmented and Virtual Realities"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 1, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180896", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180896", "paper_url": "https://ieeexplore.ieee.org/document/6180896"}, {"title": "Motivation to Select Point of View in Cinematic Virtual Reality", "authors": ["Andrea Stevenson Won", "Tanja Aitamurto", "Byungdoo Kim", "Sukolsak Sakshuwong", "Catherine Kircos", "Yasamin Sadeghi"], "abstract": "This paper examines the effects of participants' preferred point of view of two protagonists, and their motivation for this preference, on two viewings of a cinematic 360-degree video filmed from the first person perspective. Before watching the film, which dramatized gender bias in a STEM workplace, participants were asked to state whether they preferred to view the film from the point of view (POV) of a male protagonist, a female protagonist, or make no selection. They were then asked why they held this preference. Their answers were predictive. Participants' tracked head movements, and the events participants recalled from the film, differed according to their pre-stated preference and motivation.", "keywords": ["Avatars", "Ethics", "Computers", "Virtual environments", "Companies", "Behavioral sciences", "human factors", "virtual reality", "cinematic virtual reality", "protagonists", "person perspective", "gender bias", "STEM workplace", "male protagonist", "K.3.1 [Computers and Education]: Computer Uses in Education", "J.4. [Computer Applications]: Social and Behavioral Sciences"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798184", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798184", "paper_url": "https://ieeexplore.ieee.org/document/8798184"}, {"title": "Mitigating Incorrect Perception of Distance in Virtual Reality through Personalized Rendering Manipulation", "authors": ["Alex Peer", "Kevin Ponto"], "abstract": "Viewers of virtual reality appear to have an incorrect sense of space when performing blind directed-action tasks, such as blind walking or blind throwing. It has been shown that various manipulations can influence this incorrect sense of space, and that the degree of misperception varies by person. It follows that one could measure the degree of misperception an individual experiences and generate some manipulation to correct for it, though it is not clear that correct behavior in a specific blind directed action task leads to correct behavior in all tasks in general. In this work, we evaluate the effectiveness of correcting perceived distance in virtual reality by first measuring individual perceived distance through blind throwing, then manipulating sense of space using a vertex shader to make things appear more or less distant, to a degree personalized to the individual's perceived distance. Two variants of the manipulation are explored. The effects of these personalized manipulations are first evaluated when performing the same blind throwing task used to calibrate the manipulation. Then, in order to observe the effects of the manipulation on dissimilar tasks, participants perform two perceptual matching tasks which allow full visual feedback as objects, or the participants themselves, move through space.", "keywords": ["Task analysis", "Virtual environments", "Measurement uncertainty", "Atmospheric measurements", "Particle measurements", "Rendering (computer graphics)", "rendering (computer graphics)", "virtual reality", "performing blind directed-action tasks", "specific blind directed action task", "perceived distance", "virtual reality", "personalized manipulations", "blind throwing task", "dissimilar tasks", "perceptual matching tasks", "personalized rendering manipulation", "I.3.7 [Computing Methodologies]: Graphics Utilities\u2014Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797911", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797911", "paper_url": "https://ieeexplore.ieee.org/document/8797911"}, {"title": "Influence of control/display ratio on the perception of mass of manipulated objects in virtual environments", "authors": ["L. Dominjon", "A. Lecuyer", "J.-M. Burkhardt", "P. Richard", "S. Richir"], "abstract": "This paper describes two psychophysical experiments which were conducted to evaluate the influence of the control/display (C/D) ratio on the perception of mass of manipulated objects in virtual environments (VE). In both experiments, a discrimination task was used in which participants were asked to identify the heavier object between two virtual balls. Participants could weigh each ball via a haptic interface and look at its synthetic display on a computer screen. Unknown to the participants, two parameters varied between each trial: the difference of mass between the balls and the C/D ratio used in the visual display when weighing the comparison ball. The data collected demonstrated that the C/D ratio significantly influenced the result of the mass discrimination task and sometimes even reversed it. The absence of gravity force largely increased this effect. These results suggest that if the visual motion of a manipulated virtual object is amplified when compared to the actual motion of the user's hand (i.e. if the C/D ratio used is smaller than 1), the user tends to feel that the mass of the object decreases. Thus, decreasing or amplifying the motions of the user in a VE can strongly modify the perception of haptic properties of objects that he/she manipulates. Designers of virtual environments could use these results for simplification considerations and also to avoid potential perceptual aberrations.", "keywords": ["Weight control", "Virtual environment", "Haptic interfaces", "Computer displays", "Gravity", "Humans", "Psychology", "Computer interfaces", "Large screen displays", "Chromium", "virtual reality", "haptic interfaces", "user centred design", "human factors", "psychology", "control-display ratio", "object manipulation", "virtual environments", "psychophysical experiments", "object identification", "virtual balls", "haptic interface", "synthetic display", "computer screen", "object mass perception", "visual display", "visual motion", "virtual object", "perceptual aberrations", "human perception", "pseudohaptic", "visuo-haptic integration"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 31, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492749", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492749", "paper_url": "https://ieeexplore.ieee.org/document/1492749"}, {"title": "Precise and rapid interaction through scaled manipulation in immersive virtual environments", "authors": ["S. Frees", "G.D. Kessler"], "abstract": "A significant benefit of an immersive virtual environment is that it provides users the ability to interact with objects in a very natural, direct way; often realized by using a tracked, hand-held wand or stylus to \"grab\" and position objects. In the absence of force feedback or props, it is difficult and frustrating for users to move their arms, hands, or fingers to precise positions in 3D space, and more difficult to hold them at a constant position, or to move them in a uniform direction over time. The imprecision of user interaction in virtual environments is a fundamental problem that limits the complexity of the environment the user can interact with directly. We present PRISM (precise and rapid interaction through scaled manipulation), a novel interaction technique which acts on the user's behavior in the environment to determine whether they have precise or imprecise goals in mind. When precision is desired, PRISM dynamically adjusts the \"control/ display\" ratio which determines the relationship between physical hand movements and the motion of the controlled virtual object, making it less sensitive to the user's hand movement. In contrast to techniques like Go-Go, which scale up hand movement to allow \"long distance\" manipulation; PRISM scales the hand movement down to increase precision. We present the results of a user study which shows that PRISM significantly out-performs the more traditional direct manipulation approach.", "keywords": ["Virtual environment", "Arm", "Computer graphics", "Force feedback", "Virtual reality", "Haptic interfaces", "Fingers", "Motion control", "Chromium", "Computational modeling", "virtual reality", "human computer interaction", "solid modelling", "tracking", "haptic interfaces", "immersive virtual environments", "virtual objects", "tracked hand-held wand", "stylus", "object grabbing", "object positioning", "force feedback", "3D space", "user interaction", "environment complexity", "PRISM", "scaled manipulation", "user behavior", "control-display ratio", "controlled virtual object motion", "user hand movement", "direct manipulation", "precise manipulation"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 5, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492759", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492759", "paper_url": "https://ieeexplore.ieee.org/document/1492759"}, {"title": "A soft hand model for physically-based manipulation of virtual objects", "authors": ["Jan Jacobs", "Bernd Froehlich"], "abstract": "We developed a new hand model for increasing the robustness of finger-based manipulations of virtual objects. Each phalanx of our hand model consists of a number of deformable soft bodies, which dynamically adapt to the shape of grasped objects based on the applied forces. Stronger forces directly result in larger contact areas, which increase the friction between hand and object as would occur in reality. For a robust collision-based soft body simulation, we extended the lattice-shape matching algorithm to work with adaptive stiffness values, which are dynamically derived from force and velocity thresholds. Our implementation demonstrates that this approach allows very precise and robust grasping, manipulation and releasing of virtual objects and performs in real-time for a variety of complex scenarios. Additionally, laborious tuning of object and friction parameters is not necessary for the wide range of objects that we typically grasp with our hands.", "keywords": ["Grasping", "Robustness", "Force", "Friction", "Adaptation model", "Shape", "solid modelling", "virtual reality", "soft hand model", "virtual object manipulation", "finger-based manipulation", "soft body simulation", "lattice-shape matching algorithm", "adaptive stiffness value", "object grasping", "object manipulation", "object release", "friction parameter"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 19, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759430", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759430", "paper_url": "https://ieeexplore.ieee.org/document/5759430"}, {"title": "Efficient manipulation of object groups in virtual environments", "authors": ["W. Stuerzlinger", "G. Smith"], "abstract": "We describe simple techniques for object group manipulation, an important operation in user interaction with a virtual environment. All presented manipulation techniques exploit constraints to simplify user interaction. The techniques are based on how humans perceive groups and afford direct manipulation of such groups. Furthermore, we introduce two new intuitive ways to create a whole group of objects: drag-add and random drag-add. Finally, we present an evaluation of the presented techniques.", "keywords": ["Layout", "Virtual reality", "Packaging", "User interfaces", "Computer science", "Virtual environment", "Humans", "Floors", "Libraries", "virtual reality", "user interfaces", "human factors", "object group manipulation", "user interaction", "virtual environment", "group perception", "direct manipulation", "drag-add", "random drag-add", "virtual reality", "user interfaces"], "published_in": "Proceedings IEEE Virtual Reality 2002", "publication_date": "2002", "citations": 10, "isbn": ["0-7695-1492-8"], "doi": "10.1109/VR.2002.996529", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=996529", "paper_url": "https://ieeexplore.ieee.org/document/996529"}, {"title": "EyeSeeThrough: Unifying Tool Selection and Application in Virtual Environments", "authors": ["Diako Mardanbegi", "Benedikt Mayer", "Ken Pfeuffer", "Shahram Jalaliniya", "Hans Gellersen", "Alexander Perzl"], "abstract": "In 2D interfaces, actions are often represented by fixed tools arranged in menus, palettes, or dedicated parts of a screen, whereas 3D interfaces afford their arrangement at different depths relative to the user and the user can move them relative to each other. In this paper, we introduce EyeSeeThrough as a novel interaction technique that utilizes eye-tracking in VR. The user can apply an action to an intended object by visually aligning the object with the tool at the line-of-sight, and then issue a confirmation command. The underlying idea is to merge the two-step process of 1) selection of a mode in a menu and 2) applying it to a target, into one unified interaction. We present a user study where we compare the method to the baseline two-step selection. The results of our user study showed that our technique outperforms the two step selection in terms of speed and comfort. We further developed a prototype of a virtual living room to demonstrate the practicality of the proposed technique.", "keywords": ["Tools", "Visualization", "Three-dimensional displays", "Task analysis", "User interfaces", "Space exploration", "Virtual environments", "human computer interaction", "object tracking", "virtual reality", "EyeSeeThrough tool", "user study", "mode selection process", "application process", "VR", "virtual living room", "baseline two-step selection", "line-of-sight", "eye-tracking", "virtual environments", "Human-centered computing\u2014Human-centered-computing\u2014Gestural input"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797988", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797988", "paper_url": "https://ieeexplore.ieee.org/document/8797988"}, {"title": "Yea Big, Yea High: A 3D User Interface for Surface Selection by Progressive Refinement in Virtual Environments", "authors": ["Bret Jackson", "Brighten Jelke", "Gabriel Brown"], "abstract": "We present Yea Big, Yea High - a 3D user interface for surface selection in virtual environments. The interface extends previous selection interfaces that support exploratory visualization and 3D modeling. While these systems primarily focus on selecting single objects, Yea Big, Yea High allows users to select part of a surface mesh, a common task for data analysis, model editing, or annotation. The selection can be progressively refined by physically indicating a region of interest between a user's hands. We describe the design of the interface and key challenges we encountered. We present findings from a case study exploring design choices and use of the system.", "keywords": ["Three-dimensional displays", "Shape", "User interfaces", "Solid modeling", "Virtual environments", "Surface reconstruction", "Task analysis", "data analysis", "data visualisation", "user interfaces", "virtual reality", "3D user interface", "surface selection", "progressive refinement", "virtual environments", "surface mesh", "selection interfaces", "data analysis", "model editing", "annotation", "Human-centered computing-Human computer interaction-Interaction techniques-Gestural input", "Human-centered computing-Human computer interaction-Interaction paradigms-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 3, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8447559", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8447559", "paper_url": "https://ieeexplore.ieee.org/document/8447559"}, {"title": "Optimization-based virtual surface contact manipulation at force control rates", "authors": ["D.D. Nelson", "E. Cohen"], "abstract": "Previous interactive works have used springs, heuristics, and dynamics for surface placement applications. We present an analytical technique for kilohertz rate manipulation of CAD models with virtual surface and trimming constraints. The optimization approach allows best placement and sensitivity analysis for mechanical design objectives and parametric domain objectives. Such objectives are not readily incorporated into previous interactive methods. Force feedback is rendered to the user using previously developed haptics principles.", "keywords": ["Force control", "Jacobian matrices", "Force feedback", "Design automation", "Kinematics", "Surface treatment", "Prototypes", "Constraint optimization", "Animation", "Optimization methods", "CAD", "force control", "force feedback", "virtual reality", "haptic interfaces", "optimization based virtual surface contact manipulation", "force control rates", "interactive works", "surface placement applications", "analytical technique", "kilohertz rate manipulation", "CAD models", "virtual surface", "trimming constraints", "optimization approach", "best placement", "sensitivity analysis", "mechanical design objectives", "parametric domain objectives", "previous interactive methods", "force feedback", "haptics principles"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 6, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840361", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840361", "paper_url": "https://ieeexplore.ieee.org/document/840361"}, {"title": "A virtual reality system for the simulation and manipulation of wireless communication networks", "authors": ["Tobias Rick", "Anette Von Kapri", "Torsten Kuhlen"], "abstract": "The knowledge of the propagation behavior of radio waves is a fundamental prerequisite for planning and optimizing mobile radio networks. Propagation effects are usually simulated numerically, since real-world measurement campaigns are time-consuming and expensive. Automatic planning algorithms can explore a vast amount of network configurations to find good deployment schemes. However, complex urban scenarios demand for a great emphasis on site-specific details in the propagation environment which are often not covered by automatic approaches. Therefore, we have combined the simulation of radio waves with an interactive exploration and modification of the propagation environment in a virtual reality prototype application. By coupling real-time simulation and manipulation tasks we can provide an uninterrupted user-centered workflow.", "keywords": ["Solid modeling", "Computational modeling", "Buildings", "Cities and towns", "Real time systems", "Interference", "Virtual reality", "digital simulation", "mobile radio", "optimisation", "radio networks", "radiowave propagation", "telecommunication computing", "telecommunication network planning", "virtual reality", "wireless communication network", "radio waves propagation behavior", "optimisation", "mobile radio networks", "automatic planning algorithm", "network configuration", "interactive exploration", "virtual reality prototype application", "real time simulation", "real time manipulation", "user centered workflow"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 0, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759446", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759446", "paper_url": "https://ieeexplore.ieee.org/document/5759446"}, {"title": "Effective manipulation of virtual objects within arm's reach", "authors": ["Mathias Moehring", "Bernd Froehlich"], "abstract": "We present a study that compares finger-based direct interaction to controller-based ray interaction in a CAVE as well as in head-mounted displays. We focus on interaction tasks within reach of the users' arms and hands and explore various feedback methods including visual, pressure-based tactile and vibro-tactile feedback. Furthermore, we enhanced a precise finger tracking device with a direct pinch-detection mechanism to improve the robustness of grasp detection. Our results indicate that finger-based interaction is generally preferred if the functionality and ergonomics of manually manipulated virtual artifacts has to be assessed. However, controller-based interaction is often faster and more robust. In projection-based environments finger-based interaction almost reaches the task completion times and the subjective robustness of controller-based interaction if the grasping heuristics relies on our direct pinch detection. It also improves significantly by adding tactile feedback, while visual feedback proves sufficient in head-mounted displays. Our findings provide a guideline for the design of fine grain finger-based interfaces.", "keywords": ["Grasping", "Visualization", "Tactile sensors", "Robustness", "Performance evaluation", "Optical feedback", "ergonomics", "force feedback", "haptic interfaces", "helmet mounted displays", "virtual reality", "effective virtual object manipulation", "finger-based direct interaction", "controller-based ray interaction", "CAVE", "head-mounted display", "interaction task", "user arms", "user hands", "visual feedback", "pressure-based tactile feedback", "vibro-tactile feedback", "finger tracking device", "direct pinch-detection mechanism", "grasp detection", "ergonomics", "projection-based environment", "grasping heuristics", "fine grain finger-based interface"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 27, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759451", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759451", "paper_url": "https://ieeexplore.ieee.org/document/5759451"}, {"title": "GPU implementation of 3D object selection by conic volume techniques in virtual environments", "authors": ["Tobias Rick", "Anette Von Kapri", "Torsten Kuhlen"], "abstract": "In this paper we present a GPU implementation to accurately select 3D objects based on their silhouettes by a pointing device with six degrees of freedom (6DOF) in a virtual environment (VE). We adapt a 2D picking metaphor to 3D selection in VE's by changing the projection and view matrices according to the position and orientation of a 6DOF pointing device and rendering a conic selection volume to an off-screen pixel buffer. This method works for triangulated as well as volume rendered objects, no explicit geometric representation is required.", "keywords": ["Virtual environment", "Rendering (computer graphics)", "Computer graphics", "Virtual reality", "Mice", "Geometry", "Brain", "Shape", "Jitter", "Statistics", "computer graphic equipment", "coprocessors", "interactive systems", "virtual reality", "GPU implementation", "3D object selection", "conic selection volume techniques", "virtual environments", "six degrees of freedom", "I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 1, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444783", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444783", "paper_url": "https://ieeexplore.ieee.org/document/5444783"}, {"title": "Visual Manipulation for Underwater Drag Force Perception in Immersive Virtual Environments", "authors": ["Hyeongyeop Kang", "Geonsun Lee", "Junghyun Han"], "abstract": "In this paper, we propose to reproduce drag forces in a virtual underwater environment. To this end, we first compute the drag forces to be exerted on human limbs in a physically correct way. Adopting a pseudo-haptic approach that generates visual discrepancies between the real and virtual limb motions, we compute the extent of drag forces that are applied to the virtual limbs and can be naturally perceived. Through two tests, our drag force simulation method is compared with others. The results show that our method is effective in reproducing the sense of being immersed in water. Our study can be utilized for various types of virtual underwater applications such as scuba diving training and aquatic therapy.", "keywords": ["Drag", "Force", "Visualization", "Haptic interfaces", "Avatars", "Angular velocity", "Shoulder", "haptic interfaces", "virtual reality", "virtual underwater applications", "drag force simulation method", "virtual limb motions", "visual discrepancies", "virtual underwater environment", "drag forces", "immersive virtual environments", "underwater drag force perception", "visual manipulation", "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Treemaps", "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798300", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798300", "paper_url": "https://ieeexplore.ieee.org/document/8798300"}, {"title": "Can We Create Better Haptic Illusions by Reducing Body Information?", "authors": ["Yutaro Hirao", "Takashi Kawai"], "abstract": "In this paper, we propose a method of alleviating a sense of unnaturalness and individual differences in pseudo-haptic experience in VR. The method is to use a non-isomorphic manipulation of a virtual body, where the actual and virtual body movement is decoupled. As an evaluation of this approach, two manipulation methods are compared in the task of pulling a virtual object in VR. In the first one, the user physically performs a pulling gesture, and in the second one, the user pulls with a controller's analog stick (Fig. 1). A temporal delay is added to the virtual object's motion for the pseudo-haptic effect. The results suggest a tendency where the individual differences in pseudo-haptic experience and unnaturalness were smaller with the analog stick manipulation, even with intensive pseudo-haptic expressions.", "keywords": ["Haptic interfaces", "Delays", "Visualization", "Virtual reality", "Task analysis", "Art", "haptic interfaces", "interactive devices", "user experience", "virtual reality", "pseudohaptic experience", "VR", "nonisomorphic manipulation", "manipulation methods", "pseudohaptic effect", "analog stick manipulation", "intensive pseudohaptic expressions", "haptic IIIusions", "body information reduction", "Pseudo-haptics", "cross-modality", "virtual reality", "human information processing", "perception", "[Computer graphics]: Graphics systems and interfaces---Perception", "[Computer graphics]: Graphics systems and interfaces---virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798190", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798190", "paper_url": "https://ieeexplore.ieee.org/document/8798190"}, {"title": "Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities", "authors": ["Jer\u00f4nimo Gustavo Grandi", "Henrique Galvan Debarba", "Anderson Maciel"], "abstract": "We present an assessment of asymmetric interactions in Collaborative Virtual Environments (CVEs). In our asymmetric setup, two co-located users interact with virtual 3D objects, one in immersive Virtual Reality (VR) and the other in mobile Augmented Reality (AR). We conducted a study with 36 participants to evaluate performance and collaboration aspects of pair work, and compare it with two symmetric scenarios, either with both users in immersive VR or mobile AR. To perform this experiment, we adopt a collaborative AR manipulation technique from literature and develop and evaluate a VR manipulation technique of our own. Our results indicate that pairs in asymmetric VR-AR achieved significantly better performance than the AR symmetric condition, and similar performance to VR symmetric. Regardless of the condition, pairs had similar work participation indicating a high cooperation level even when there is a visualization and interaction asymmetry between the participants.", "keywords": ["Collaboration", "Three-dimensional displays", "Visualization", "Task analysis", "Augmented reality", "User interfaces", "augmented reality", "virtual 3D objects", "collaboration aspects", "collaborative AR manipulation technique", "VR manipulation technique", "asymmetric VR-AR", "AR symmetric condition", "visualization", "interaction asymmetry", "collaborative virtual environments", "immersive virtual reality", "mobile augmented reality", "Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction techniques", "Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Mixed/augmented reality Human-centered computing\u2014Collaborative and social computing"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798080", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798080", "paper_url": "https://ieeexplore.ieee.org/document/8798080"}, {"title": "MiddleVR a generic VR toolbox", "authors": ["Sebastien Kuntz"], "abstract": "Creating a virtual reality application requires the use of a 3D engine. Most 3D engine don't support virtual reality hardware or have the special features needed to render images for head-mounted displays, stereoscopic walls or CAVEs. MiddleVR is a generic VR toolbox with the following goals: - add support for VR in 3D engines - speed-up the application creation process with high-level building blocks such as interactions (navigations, selection of objects, manipulation), immersive menus and custom GUIs.", "keywords": ["Three-dimensional displays", "Hardware", "Engines", "Navigation", "Software", "Virtual reality", "Stereo image processing", "rendering (computer graphics)", "virtual reality", "MiddleVR toolbox", "generic VR toolbox", "virtual reality", "3D engine", "image rendering", "application creation process", "user interaction", "immersive menu", "custom GUI", "graphical user interface", "Virtual reality", "software", "middleware", "abstraction"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 6, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223460", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223460", "paper_url": "https://ieeexplore.ieee.org/document/7223460"}, {"title": "Full body interaction in virtual reality with affordable hardware", "authors": ["Tuukka M. Takala", "Mikael Matveinen"], "abstract": "Summary form only given. Recently a number of affordable game controllers have been adopted by virtual reality (VR) researchers [1][4]. We present a video\n1\n of a VR demo called TurboTuscany, where we employ such controllers; our demo combines a Kinect controlled full body avatar with Oculus Rift head-mounted-display [2]. We implemented three positional head tracking schemes that use Kinect, Razer Hydra, and PlayStation (PS) Move controllers. In the demo the Kinect tracked avatar can be used to climb ladders, play with soccer balls, and otherwise move or interact with physically simulated objects. PS Move or Razer Hydra controller is used to control locomotion, and for selecting and manipulating objects. Our subjective experience is that the best head tracking immersion is achieved by using Kinect together with PS Move, as the latter is more accurate and responsive while having a large tracking volume. We also noticed that Oculus Rift's orientation tracking has less latency than any of the positional trackers that we used, while Razer Hydra has less latency than PS Move, and Kinect has the largest latency. Besides positional tracking, our demo uses these three trackers to correct the yaw drift of Oculus Rift. TurboTuscany was developed by using our RUIS toolkit, which is a software platform for VR application development [3]. The demo and RUIS toolkit can be downloaded online\n2\n.", "keywords": ["Tracking", "Games", "Avatars", "Hardware", "Media", "avatars", "computer games", "helmet mounted displays", "motion control", "object tracking", "game controllers", "virtual reality", "VR demo", "TurboTuscany", "Kinect controlled full body avatar", "Oculus Rift head-mounted-display", "positional head tracking schemes", "soccer balls", "ladders", "physically simulated objects", "PS Move controller", "Razer Hydra controller", "locomotion control", "object selection", "object manipulation", "RUIS toolkit", "software platform", "VR application development", "PlayStation Move controller"], "published_in": "2014 IEEE Virtual Reality (VR)", "publication_date": "March 2014", "citations": 8, "isbn": ["978-1-4799-2871-2"], "doi": "10.1109/VR.2014.6802099", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6802099", "paper_url": "https://ieeexplore.ieee.org/document/6802099"}, {"title": "Conducting Human-Subject Experiments with Virtual and Augmented Reality", "authors": ["J.E. Swan", "S.R. Ellis", "B.D. Adelstein"], "abstract": "Summary form only for tutorial. As virtual and augmented reality hardware and software have grown more mature over the past decade, the focus of the field is shifting away from the basic engineering technology, and towards the science and applications of VR and AR techniques. Increasingly, virtual and augmented reality researchers are conducting human-subject experiments, both to understand the way humans perceive, manipulate, and cognate with VR and AR information, and to quantify the utility of VR and AR in different application contexts.", "keywords": ["Augmented reality", "Virtual reality", "Humans", "NASA", "Hardware", "Application software", "Design for experiments", "Performance analysis", "Anthropometry", "Psychology"], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 0, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.35", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667673", "paper_url": "https://ieeexplore.ieee.org/document/1667673"}, {"title": "Stand-alone, Wearable System for Full Body VR Avatars: Towards Physics-based 3D Interaction", "authors": ["Tuukka M. Takala", "Chen Chun Hsin", "Takashi Kawai"], "abstract": "We introduce a stand-alone, wearable system with full body and finger tracking for first-person virtual reality (VR) avatars. The system does not rely on any external trackers or components. It comprises of a head-mounted display, inertial motion capture suit, VR gloves, and VR backpack PC. Making use of the wearable system and RUIS toolkit [1], we present an example implementation of our vision for physics-based full body avatar interaction. This envisioned interaction involves three elements from the reality-based interaction framework of Jacob et al. [2]: na\u00efve physics, body awareness, and environment awareness. These elements lend common sense affordances within the virtual world and allow users to employ their everyday knowledge of the real world. We argue that when it comes to full body avatar interfaces, it is not only users, but also developers who benefit from utilizing physics simulation as the basis upon which different interaction techniques are built on. This physics-based approach provides intuitive manipulation and locomotion interactions without requiring individually crafted scripts. Our example implementation presents several such interactions. Furthermore, the many interaction techniques emerging from physical simulation are congruous with each other, which promotes user interface consistency. We also introduce the idea of using physics components (colliders, joints, materials, etc.) as 3D user interface building blocks, as opposed to scripting or visual programming.", "keywords": ["Avatars", "Three-dimensional displays", "Physics", "Wearable computers", "Tracking", "avatars", "helmet mounted displays", "human computer interaction", "user interfaces", "first-person virtual reality avatars", "external trackers", "head-mounted display", "inertial motion capture suit", "VR gloves", "VR backpack PC", "reality-based interaction framework", "na\u00efve physics", "body awareness", "virtual world", "body avatar interfaces", "physics-based approach", "intuitive manipulation", "locomotion interactions", "physical simulation", "physics components", "3D user interface building blocks", "full body VR avatars", "interaction techniques", "physics-based 3D interaction", "stand-alone wearable system", "user interface consistency"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798169", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798169", "paper_url": "https://ieeexplore.ieee.org/document/8798169"}, {"title": "iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking Interface for Study and Reflection in VR Learning Environments", "authors": ["Yi-Ting Chen", "Chi-Hsuan Hsu", "Chih-Han Chung", "Yu-Shuen Wang", "Sabarish V. Babu"], "abstract": "In this contribution, we design, implement and evaluate the pedagogical benefits of a novel interactive note taking interface (iVRNote) in VR for the purpose of learning and reflection lectures. In future VR learning environments, students would have challenges in taking notes when they wear a head mounted display (HMD). To solve this problem, we installed a digital tablet on the desk and provided several tools in VR to facilitate the learning experience. Specifically, we track the stylus' position and orientation in the physical world and then render a virtual stylus in VR. In other words, when students see a virtual stylus somewhere on the desk, they can reach out with their hand for the physical stylus. The information provided will also enable them to know where they will draw or write before the stylus touches the tablet. Since the presented iVRNote featuring our note taking system is a digital environment, we also enable students save efforts in taking extensive notes by providing several functions, such as post-editing and picture taking, so that they can pay more attention to lectures in VR. We also record the time of each stroke on the note to help students review a lecture. They can select a part of their note to revisit the corresponding segment in a virtual online lecture. Figures and the accompanying video demonstrate the feasibility of the presented iVRNote system. To evaluate the system, we conducted a user study with 20 participants to assess the preference and pedagogical benefits of the iVRNote interface. The feedback provided by the participants were overall positive and indicated that the iVRNote interface could be potentially effective in VR learning experiences.", "keywords": ["Three-dimensional displays", "Writing", "Tools", "Tracking", "Education", "Portable computers", "Task analysis", "computer aided instruction", "helmet mounted displays", "human computer interaction", "interactive video", "virtual reality", "virtual online lecture", "pedagogical benefits", "iVRNote interface", "VR learning experiences", "interactive note-taking interface", "learning reflection lectures", "digital tablet", "learning experience", "physical world", "virtual stylus", "digital environment", "iVRNote system", "VR learning environments", "head mounted display", "HMD", "physical stylus render", "Virtual reality", "classroom", "on-line learning", "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Education", "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 2, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798338", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798338", "paper_url": "https://ieeexplore.ieee.org/document/8798338"}, {"title": "What do we care 4? A virtual reality music video", "authors": ["Marieke Nooren", "Steye Hallema"], "abstract": "How to make a filmic language for VR is the issue filmmakers, artists and scientists are breaking their heads over. With the team of WildVreemd, Steye Hallema invented new ways of montage for this VR. Because how to edit in a movie where all the old film rules no longer apply? This resulted in \u2018What do we care 4\u2019, a fantastic journey through the vast and open plains of filmic language in VR-video. It's the first serious attempt to explore questions like: Where do you look when you can choose yourself? How do you manipulate the viewer in such a way that he/she focuses on what the storyteller wants? How do you go from one shot to another? Be prepared to be amazed by some creative solutions. The VR-video was nominated for the UK Music Video Awards 2015 in the category Best Interactive and innovative Video, next to international big names like Coldplay, Cee Lo Green and Years & Years. Director Steye Hallema already saw in 2009 the great potential of VR, especially for music. Especially for musicians it literally opens worlds to let fans \u2018disappear\u2019 into their musical world. \u2018VR is the headphones for the eyes\u2019.", "keywords": "", "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 0, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504792", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504792", "paper_url": "https://ieeexplore.ieee.org/document/7504792"}, {"title": "Investigation of Visual Self-Representation for a Walking-in-Place Navigation System in Virtual Reality", "authors": ["Chanho Park", "Kyungho Jang"], "abstract": "Walking-in-place (WIP) is one of the techniques for navigation in virtual reality (VR), and it can be configured in a limited space with a simple algorithm. Although WIP systems provide a sense of movement, it is important to deliver immersive VR experiences by providing information as similar as possible to walking in the real world. There have been many studies on the WIP technology, but it has rarely been done on visual self-representation of WIP in the virtual environment (VE). In this paper, we describe our investigation of virtual self-representation for application to a WIP navigation system using a HMD and full body motion capture system. Our system is designed to move in the pelvis direction by calculating the inertial sensor data, and a virtual body that is linked to the user's movement is seen from the first-person perspective (1PP) in two ways: (i) full body, and (ii) full body with natural walking. In (ii), when a step is detected, the motion of the lower part of the avatar is manipulated as if the user is performing real walking. We discuss the possibility of visual self-representation for the WIP system.", "keywords": ["Foot", "Legged locomotion", "Avatars", "Visualization", "Navigation", "Virtual environments", "avatars", "helmet mounted displays", "visual self-representation", "walking-in-place navigation system", "virtual reality", "immersive VR experiences", "virtual self-representation", "WIP navigation system", "virtual body", "natural walking", "HMD", "full body motion capture system", "pelvis direction", "inertial sensor data", "first-person perspective", "avatar", "Computing methodologies", "Computer graphics", "Graphics systems and interfaces", "Virtual reality", "Human-centered computing", "Human computer interaction (HCI)", "Interaction techniques"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798345", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798345", "paper_url": "https://ieeexplore.ieee.org/document/8798345"}, {"title": "Application of redirected walking in room-scale VR", "authors": ["Eike Langbehn", "Paul Lubos", "Gerd Bruder", "Frank Steinicke"], "abstract": "Redirected walking (RDW) promises to allow near-natural walking in an infinitely large virtual environment (VE) by subtle manipulations of the virtual camera. Previous experiments showed that a physical radius of at least 22 meters is required for undetectable RDW. However, we found that it is possible to decrease this radius and to apply RDW to room-scale VR, i. e., up to approximately 5m \u00d7 5m. This is done by using curved paths in the Ve instead of straight paths, and by coupling them together in a way that enables continuous walking. Furthermore, the corresponding paths in the real world are laid out in a way that fits perfectly into room-scale VR. In this research demo, users can experience RDW in a room-scale head-mounted display VR setup and explore a VE of approximately 25m \u00d7 25m.", "keywords": ["Legged locomotion", "Visualization", "Human computer interaction", "Virtual environments", "Sensitivity", "cameras", "gait analysis", "helmet mounted displays", "human factors", "virtual reality", "room-scale VR", "redirected walking", "RDW", "virtual environment", "VE", "virtual camera", "user experience", "head-mounted display", "Virtual Reality", "Redirected Walking", "Room-scale"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 10, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892373", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892373", "paper_url": "https://ieeexplore.ieee.org/document/7892373"}, {"title": "HOG on a WIM", "authors": ["Aaron Stafford", "Wayne Piekarski", "Bruce H. Thomas"], "abstract": "This paper presents a new interaction metaphor for mixed space collaboration: HOG on a WIM. Hand of god (HOG) on a world in miniature (WIM) is the first collaborative WIM. It enables a table-top display user to collaborate with a virtual reality (VR) user. The tabletop display user has a god's eye view of the virtual world and communicates with the VR user through natural gestures and speech. The VR user controls a WIM to navigate and manipulate the virtual world with the aid of the tabletop display user's guidance.", "keywords": ["Virtual reality", "Navigation", "Computer displays", "Collaboration", "Computer graphics", "Cameras", "Rendering (computer graphics)", "Wearable computers", "Speech", "Three dimensional displays", "groupware", "virtual reality", "mixed space collaboration", "hand of god", "world in miniature", "virtual reality", "virtual world", "I.3.6 [Computer Graphics]: Methodology and Techniques\u00bfInteraction techniques", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u00bfVirtual reality", "H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces\u00bfCollaborative computing"], "published_in": "2008 IEEE Virtual Reality Conference", "publication_date": "March 2008", "citations": 2, "isbn": ["978-1-4244-1971-5", "978-1-4244-1972-2"], "doi": "10.1109/VR.2008.4480805", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4480805", "paper_url": "https://ieeexplore.ieee.org/document/4480805"}, {"title": "Tendon Vibration Increases Vision-induced Kinesthetic Illusions in a Virtual Environment", "authors": ["Daiki Hagimori", "Shunsuke Yoshimoto", "Nobuchika Sakata", "Kiyoshi Kiyokawa"], "abstract": "In virtual reality (VR) systems, a user avatar is typically manipulated based on actual body motion in order to present natural immersive sensations resulted from proprioceptors. It is useful to be able to feel a large body motion in a virtual environment while it is actually smaller in the real environment. As a method to achieve this, we have been working on kinesthetic illusions induced by tendon vibration. In this article, we report on a user study focusing on the effects of a combination of visual stimulus and tendon vibration. In the user study, we measured subjects' perceived elbow angles while they observed the corresponding virtual arm bending to 90 degrees, with different rotation amplification ratios between 1.0 and 1.5, with and without tendon vibration on the wrist joint. The results show that the perceived angle is increased by up to 8 degrees when tendon vibration is applied, roughly in proportion to the rotation amplification ratio. Our study suggests that the vision-induced kinesthetic illusion is further increased by tendon vibration, and that our technique can be applied to VR applications to make users feel larger body motion than that in the real environment.", "keywords": ["Vibrations", "Tendons", "Elbow", "Visualization", "Muscles", "Virtual environments", "Wrist", "avatars", "haptic interfaces", "human factors", "virtual environment", "vision-induced kinesthetic illusion", "virtual reality systems", "user avatar", "virtual arm", "body motion", "vision-induced kinesthetic IIIusions", "VR systems", "Human-centered computing", "Human computer interaction (HCI)", "Interaction devices", "Haptic devices", "Computing methodologies", "Computer graphics", "Graphics systems and interfaces", "Perception", "Hardware validation", "Emerging technologies", "Emerging interfaces"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797832", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797832", "paper_url": "https://ieeexplore.ieee.org/document/8797832"}, {"title": "Demo of the Matrix Has You: Realizing Slow Motion in Full-Body Virtual Reality", "authors": ["Michael Rietzler", "Florian Geiselhart", "Julia Brich", "Enrico Rukzio"], "abstract": "We perceive the flow of time as a constant factor in the real world, but there are examples in media, like films or games, where time is being manipulated and slowed down. Manipulating temporal cues is simple in linear media by slowing down video and audio. Interactive media like VR however poses additional challenges, because user interaction speed is independent from media speed. While the speed of the environment can still be manipulated easily, interaction is a new aspect to consider. We implemented such manipulation by slowing down visual feedback of user movements. In prior experiments we slowed down the virtual representation of a user by applying a velocity based low pass filter and by visually redirecting the motion. We found such a manipulation to be even contributing to realism, enjoyment or presence as long as it is consistent with the experience.", "keywords": ["Media", "Visualization", "Games", "Tracking", "Virtual environments", "Hardware", "low-pass filters", "virtual reality", "interactive media", "VR", "user interaction speed", "visual feedback", "virtual representation", "low pass filter", "slow motion", "full-body virtual reality", "films", "games", "linear media", "matrix demo", "temporal cue manipulation", "Human-centered computing", "Human computer interaction (HCI)", "Interaction paradigms-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446136", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446136", "paper_url": "https://ieeexplore.ieee.org/document/8446136"}, {"title": "[DC] Designing VR for Teamwork: The Influence of HMD VR Communication Capabilities on Teamwork Competencies", "authors": ["Beata N. Balint"], "abstract": "With recent technological advancements, Virtual Reality (VR) has been advocated as an effective team training delivery method. However, to date, research has focused on demonstrating its effectiveness at the system level rather than isolating those factors within the system design (e.g. display size) that may influence the development of teamwork competencies. Communication is a significant component of teamwork. However, within Head Mounted Display (HMD)-based VR systems, communication is computer-mediated, resulting in the deprivation of important audio and visual cues. This research project therefore investigates the following question: \u201cHow do system design attributes that manipulate communication in HMD VR training systems affect the team's ability for effective performance?\u201d.", "keywords": ["Teamwork", "Training", "Resists", "Measurement", "Task analysis", "Solid modeling", "Virtual reality", "computer based training", "computer mediated communication", "helmet mounted displays", "team working", "virtual reality", "system design", "display size", "teamwork competencies", "visual cues", "research project", "HMD VR training systems", "HMD VR communication capabilities", "Virtual Reality", "system level", "audio cues", "team training delivery method", "head mounted display-based VR systems", "Head-mounted display", "teamwork", "communication", "design", "Software and its engineermg-Vlrtual worlds training simulations", "Human-centered computing-Virtual reality", "Human-centered computing-Collaborative interaction"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798147", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798147", "paper_url": "https://ieeexplore.ieee.org/document/8798147"}, {"title": "Rapid creation of photorealistic virtual reality content with consumer depth cameras", "authors": ["Chih-Fan Chen", "Mark Bolas", "Evan Suma Rosenberg"], "abstract": "Virtual objects are essential for building environments in virtual reality (VR) applications. However, creating photorealistic 3D models is not easy, and handcrafting the detailed 3D model from a real object can be time and labor intensive. An alternative way is to build a structured camera array such as a light-stage to reconstruct the model from a real object. However, these technologies are very expensive and not practical for most users. In this work, we demonstrate a complete end-to-end pipeline for the capture, processing, and rendering of view-dependent 3D models in virtual reality from a single consumer-grade RGB-D camera. The geometry model and the camera trajectories are automatically reconstructed from a RGB-D image sequence captured offline. Based on the HMD position, selected images are used for real-time model rendering. The result of this pipeline is a 3D mesh with view-dependent textures suitable for real-time rendering in virtual reality. Specular reflections and light-burst effects are especially noticeable when users view the objects from different perspectives in a head-tracked environment.", "keywords": ["Solid modeling", "Cameras", "Three-dimensional displays", "Image color analysis", "Virtual reality", "Computational modeling", "Image reconstruction", "image colour analysis", "image sequences", "image texture", "mesh generation", "rendering (computer graphics)", "stereo image processing", "virtual reality", "photorealistic virtual reality content", "consumer depth cameras", "virtual objects", "photorealistic 3D models", "structured camera array", "view-dependent 3D models", "consumer-grade RGB-D camera", "geometry model", "camera trajectories", "RGB-D image sequence", "HMD position", "real-time model rendering", "3D mesh", "view-dependent textures", "specular reflections", "light-burst effects", "head-tracked environment", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities", "I.3.7 [Computing Methodologies]: Three-Dimensional Graphics and Realism \u2014 Color, shading, shadowing, and texture", "I.2.10 [Computing Methodologies]: Vision and Scene Understanding \u2014 3D/stereo scene analysis"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 3, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892385", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892385", "paper_url": "https://ieeexplore.ieee.org/document/7892385"}, {"title": "Experiencing guidance in 3D spaces with a vibrotactile head-mounted display", "authors": ["Victor Adriel De Jesus Oliveira", "Luca Brayda", "Luciana Nedel", "Anderson Maciel"], "abstract": "Vibrotactile feedback is broadly used to support different tasks in virtual and augmented reality applications, such as navigation, communication, attentional redirection, or to enhance the sense of presence in virtual environments. Thus, we aim to include the haptic component to the most popular wearable used in VR applications: the VR headset. After studying the acuity around the head for vibrating stimuli, and trying different parameters, actuators, and configurations, we developed a haptic guidance technique to be used in a vibrotactile Head-mounted Display (HMD). Our vi-brotactile HMD was made to render the position of objects in a 3D space around the subject by varying both stimulus loci and vibration frequency. In this demonstration, the participants will interact with different scenarios where the mission is to select a number of predefined objects. However, instead of displaying occlusive graphical information to point to these objects, vibrotactile cues will provide guidance in the VR setup. Participants will see that our haptic guidance technique can be both easy to use and entertaining. (See Video: https://youtu.be/_H0MQy6QD7M).", "keywords": ["Resists", "Haptic interfaces", "Three-dimensional displays", "Headphones", "Vibrations", "Navigation", "Human computer interaction", "augmented reality", "computer graphics", "haptic interfaces", "helmet mounted displays", "3D spaces", "vibrotactile head-mounted display", "vibrotactile feedback", "virtual reality applications", "augmented reality applications", "VR applications", "VR headset", "vibrating stimuli", "haptic guidance technique", "HMD", "3D space", "vibration frequency", "occlusive graphical information", "VR setup", "H.5.1 [Human computer interaction (HCI)]: Interaction devices \u2014 Haptic devices", "H.5.2 [Human computer interaction (HCI)]: Interaction paradigms \u2014 Virtual reality"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 2, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892375", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892375", "paper_url": "https://ieeexplore.ieee.org/document/7892375"}, {"title": "Redirected Spaces: Going Beyond Borders", "authors": ["Eike Langbehn", "Paul Lubos", "Frank Steinicke"], "abstract": "Real walking in virtual reality (VR) is a promising locomotion technique since it offers multi-modal feedback to the user. Unfortunately, the virtual environment (VE) is limited by the available space in the physical world. So far, several techniques were developed to overcome this problem, e. g. redirected walking (RDW) and the use of impossible spaces. RDW subtly manipulates the viewpoint of the user to reorient her walking direction. Impossible spaces are based on subtle changes of the VE to reuse the same physical space for different virtual spaces. In this research demonstration, we show how these two approaches of redirected walking and impossible spaces can be combined. In particular, for our implementation we focus on the use of curved corridors that benefits both methods.", "keywords": ["Legged locomotion", "Virtual environments", "Visualization", "Electronic mail", "Three-dimensional displays", "User interfaces", "feedback", "virtual reality", "redirected walking", "redirected spaces", "virtual reality", "multimodal feedback", "physical world", "RDW", "walking direction", "physical space", "locomotion technique", "virtual spaces", "Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality", "Human-centered computing-Human computer interaction (HCI)-Interaction techniques"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 1, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446167", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446167", "paper_url": "https://ieeexplore.ieee.org/document/8446167"}, {"title": "Do Textures and Global Illumination Influence the Perception of Redirected Walking Based on Translational Gain?", "authors": ["Kristoffer Waldow", "Arnulph Fuhrmann", "Stefan M. Gr\u00fcnvogel"], "abstract": "For locomotion in virtual environments (VE) the method of redirected walking (RDW) enables users to explore large virtual areas within a restricted physical space by (almost) natural walking. The trick behind this method is to manipulate the virtual camera in an user-undetectable manner that leads to a change of his movements. If the virtual camera is manipulated too strong then the user recognizes this manipulation and reacts accordingly. We studied the effect of human perception of RDW under the influence of the level of realism in rendering the virtual scene.", "keywords": ["Legged locomotion", "Lighting", "Rendering (computer graphics)", "Virtual environments", "Cameras", "Tracking", "cameras", "gait analysis", "image recognition", "image texture", "natural walking", "human perception", "RDW method", "redirected walking method", "virtual camera manipulation", "virtual environment locomotion", "VE locomotion", "global illumination influence", "Virtual Reality", "Locomotion", "Human Perception"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446587", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446587", "paper_url": "https://ieeexplore.ieee.org/document/8446587"}, {"title": "Self-motion illusions in immersive virtual reality environments", "authors": ["Gerd Bruder", "Frank Steinicke", "Phil Wieland"], "abstract": "Motion perception in immersive virtual reality environments significantly differs from the real world. For example, previous work has shown that users tend to underestimate travel distances in immersive virtual environments (VEs). As a solution to this problem, some researchers propose to scale the mapped virtual camera motion relative to the tracked real-world movement of a user until real and virtual motion appear to match, i. e., real-world movements could be mapped with a larger gain to the VE in order to compensate for the underestimation. Although this approach usually results in more accurate self-motion judgments by users, introducing discrepancies between real and virtual motion can become a problem, in particular, due to misalignments of both worlds and distorted space cognition. In this paper we describe a different approach that introduces apparent self-motion illusions by manipulating optic flow fields during movements in VEs. These manipulations can affect self-motion perception in VEs, but omit a quantitative discrepancy between real and virtual motions. We introduce four illusions and show in experiments that optic flow manipulation can significantly affect users' self-motion judgments. Furthermore, we show that with such manipulation of optic flow fields the underestimation of travel distances can be compensated.", "keywords": ["Visualization", "Optical sensors", "Cameras", "Integrated optics", "Blindness", "Optical distortion", "Detectors", "cameras", "image sequences", "motion estimation", "virtual reality", "visual perception", "self-motion illusions", "immersive virtual reality environments", "mapped virtual camera motion", "motion perception", "tracked real-world movement", "virtual motion", "optic flow field manipulation", "self-motion judgment", "travel distance underestimation", "Self-motion perception", "visual illusions", "optic flow"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 13, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759434", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759434", "paper_url": "https://ieeexplore.ieee.org/document/5759434"}, {"title": "I Can See on My Feet While Walking: Sensitivity to Translation Gains with Visible Feet", "authors": ["Lucie Kruse", "Eike Langbehn", "Frank Steinicke"], "abstract": "Redirected walking allows users to explore immersive virtual environments by real walking even when the physical tracking space is limited. Redirected walking is usually implemented via translation gains, rotation gains, and curvature gains, while previous research was focused on identifying detection thresholds for such manipulations. To our knowledge, all previous experiments were conducted without a visual self-representation of the user in the virtual environment, in particular, without showing the user's feet. In this paper, we address the question if the virtual self-representation of the user's feet changes the detection thresholds for translation gains. Furthermore, we consider the influence of the holisticness of the visual stimulus, i. e., the type of virtual environment. Therefore, we conducted an experiment to identify detection thresholds for translation gains under three different conditions: (i) without visible virtual feet and (ii) with visible virtual feet both in a high fidelity visually rich virtual environment, and (iii) with visible virtual feet in a low cue virtual environment. The results revealed the range of detection thresholds for translations gains, which cannot be detected by the user when the feet are visible. Furthermore, the results show a significant difference between the two types of environment. Our findings suggest that the virtual environment is more important for manipulation detection than the visual self-representation of the user's feet.", "keywords": ["Legged locomotion", "Visualization", "Foot", "Virtual environments", "Tracking", "Cameras", "Avatars", "human computer interaction", "virtual reality", "redirected walking", "immersive virtual environments", "rotation gains", "curvature gains", "visible virtual feet", "high fidelity visually rich virtual environment", "low cue virtual environment", "feet virtual self-representation", "feet visual self-representation", "translation gains sensitivity", "Locomotion", "redirected walking", "translation gains.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems\u2015Artificial", "augmented", "and virtual realities", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u2015Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 3, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446216", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446216", "paper_url": "https://ieeexplore.ieee.org/document/8446216"}, {"title": "Climate Change on Your Plate: A VR Seafood Buffet Experience", "authors": ["Daniel Pimentel", "Ricardo Amaya", "Shiva Halan", "Sri Kalyanaraman", "Jeremy Bailenson"], "abstract": "The use of virtual reality (VR) to depict climate change impacts is a popular strategy used by environmental, news, and political organizations to encourage pro-environmental outcomes. However, despite widespread dissemination of immersive content, climate change mitigation efforts remain tepid. In response, we present a VR simulation conveying the adverse effects of climate change in a personally-relevant fashion. The \u201cVirtual Seafood Buffet\u201d experience allows users to select from dozens of lifelike virtual seafood items and experience the degradation of that particular species based on projected climate change impacts. The developed simulation is proposed as an intervention designed to encourage climate change mitigation efforts. An overview of the simulation, its purpose, and directions for future research are outlined herein.", "keywords": ["Degradation", "Virtual reality", "Oceans", "Psychology", "Visualization", "Solid modeling", "climate mitigation", "climatology", "environmental science computing", "virtual reality", "environmental news", "climate change mitigation efforts", "VR simulation", "lifelike virtual seafood items", "virtual reality", "virtual seafood buffet experience", "VR seafood buffet experience", "proenvironmental outcomes", "Climate change", "food", "virtual reality", "Human Computer Interaction (HCI)"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798076", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798076", "paper_url": "https://ieeexplore.ieee.org/document/8798076"}, {"title": "Fluid VR: Extended Object Associations for Automatic Mode Switching in Virtual Reality", "authors": ["Mayra Donaji Barrera Machuca", "Junwei Sun", "Due-Minh Pham", "Wolfgang Stuerzlinger"], "abstract": "Constrained interaction and navigation methods for virtual reality reduce the complexity of the interaction. Yet, with previously presented solutions, users need to learn new interaction tools or remember different actions for changing between different interaction methods. In this paper, we propose Fluid VR, a new 3D user interface for interactive virtual environments that lets users seamlessly transition between navigation and selection. Based on the selected object's properties, Fluid VR applies specific constraints to the interaction or navigation associated with the object. This way users have a better control of their actions, without having to change tools or activate different modes of interaction.", "keywords": ["Navigation", "Three-dimensional displays", "Fluids", "Orbits", "User interfaces", "Switches", "Virtual reality", "interactive systems", "user interfaces", "virtual reality", "Fluid VR", "3D user interface", "interactive virtual environments", "selected object", "extended object associations", "automatic mode switching", "virtual reality", "constrained interaction", "navigation methods", "interaction tools", "interaction methods", "Interaction techniques. 3D selection. 3D navigation. 3D interfaces", "Virtual reality.: H.5.2. Information interfaces and presentation (e.g", "HCI): Interaction styles"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446437", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446437", "paper_url": "https://ieeexplore.ieee.org/document/8446437"}, {"title": "The Effect of Hanger Reflex on Virtual Reality Redirected Walking", "authors": ["Chun Xie", "Chun Kwang Tan", "Taisei Sugiyama"], "abstract": "One of the major challenges in virtual reality (VR) is to create a perception of a large virtual space within a limited physical space. Here, we explore the effect of haptic-based navigation by Hanger Reflex (HR) on the perception in redirected walking (RDW) with visual manipulation. Seven individuals walked along a straight path in VR while, unbeknown to them, the visual scene was rotated with a curvature gain of \u03c0/36, forcing them to walk in a circular path in real space. HR rotation (in the left, right, and neutral direction) was induced by a wearable haptic device during each of the walking trials, and they reported their perceived walking direction and effort to walk along the path on visual analog scale. Experiment results showed that HR can influence the perception in RDW, but the effects may be complex and therefore require further investigation.", "keywords": ["Legged locomotion", "Visualization", "Haptic interfaces", "Trajectory", "Virtual reality", "Resists", "Face", "haptic interfaces", "virtual reality", "curvature gain", "visual scene", "visual manipulation", "haptic-based navigation", "VR", "virtual reality redirected walking", "Hanger Reflex", "RDW", "visual analog scale", "perceived walking direction", "wearable haptic device", "neutral direction", "Virtual reality", "redirected walking", "Hanger Reflex", "visual manipulation", "Human-centered computing~User studies", "Human-centered computing~Virtual reality", "Human-centered computing~Haptic devices"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798231", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798231", "paper_url": "https://ieeexplore.ieee.org/document/8798231"}, {"title": "Subjective Perception and Objective Measurements in Perceiving Object Softness for VR Surgical Systems", "authors": ["Antoine Widmer", "Yaoping Hu"], "abstract": "A critical issue of virtual reality (VR) surgical systems is to correctly represent both haptic and visual information for distinguishing the softness of organs/tissues. We investigated the relationship between subjective perception of object softness and objective measurements of haptic and visual information. On a co-location VR setup, human subjects pressed deformable balls (simulating organs/tissues) under the conditions of both haptic and visual information available and only haptic (or visual) information available. We recorded and analyzed the subject's selection (subjective perception) of the harder object between two balls and objective measurements of maximum force (haptic) and pressing depth (visual). The results preliminarily indicated that subjective perception behaves differently from objective measurements in perceiving object softness. This has implications for creating accurate simulation in VR surgical systems.", "keywords": ["Virtual reality", "Surgery", "Haptic interfaces", "Force measurement", "Humans", "Testing", "Analysis of variance", "Deformable models", "Pressing", "Computational modeling", "medical computing", "surgery", "virtual reality", "subjective perception", "objective measurements", "object softness", "VR surgical systems", "virtual reality surgical systems", "deformable balls", "Multimodal interaction", "haptics", "perception", "medicine", "H.1.2 [Models and Principles]: User/Machine Systems\u00bfHuman information processing", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u00bfVirtual reality", "J.3 [Computer Applications]: Life and Medical Sciences\u00bfHealth"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 1, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811048", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811048", "paper_url": "https://ieeexplore.ieee.org/document/4811048"}, {"title": "DepthText: Leveraging Head Movements towards the Depth Dimension for Hands-free Text Entry in Mobile Virtual Reality Systems", "authors": ["Xueshi Lu", "Difeng Yu", "Hai-Ning Liang", "Xiyu Feng", "Wenge Xu"], "abstract": "Text entry is a common activity in virtual reality (VR) systems. However, there is a limited number of approaches available for mobile VR systems, where it might be inconvenient for users to carry an input device. We propose a novel hands-free text entry technique we call DepthText which leverages the acceleration sensing abilities of built-in IMU sensors of mobile VR systems. Users are able to enter text by moving their head forward. The results of a 5-day study indicate that users can achieve an average of 10.76 words per minute (wpm) on the last day with low errors. This performance is comparable to the dwell-based technique which is the most common way of entering text that is hands-free. One advantage of DepthText over the dwell-based technique is that users can have more control of the pace of selecting characters, rather than being pushed by a pre-set dwell time.", "keywords": ["Acceleration", "Sensors", "Virtual reality", "Input devices", "Error analysis", "Mobile handsets", "Training", "inertial systems", "mobile computing", "text analysis", "virtual reality", "DepthText", "head movements", "depth dimension", "mobile virtual reality systems", "mobile VR systems", "hands-free text entry technique", "acceleration sensing abilities", "dwell-based technique", "built-in IMU sensors"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797901", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797901", "paper_url": "https://ieeexplore.ieee.org/document/8797901"}, {"title": "Explore the Weak Association between Flow and Performance Based on a Visual Search Task Paradigm in Virtual Reality", "authors": ["Yulong Bian", "Chao Zhou", "Yeqing Chen", "Yanshuai Zhao", "Juan Liu", "Chenglei Yang", "Xiangxu Meng"], "abstract": "Weak association model indicates that distraction caused by the disjunction between the primary task and interactive artifacts may be a key factor directly leading to weak association between flow and task performance in virtual reality (VR) activities. To test the idea, this paper proposed a VR visual search paradigm based on which we constructed a VR oceanic treasure hunting system. Experiment 1 proved that distraction caused by the incongruence was indeed a direct antecedent of weak association. Next, we slightly adjusted the system by providing visual cues to achieve task-oriented selective attention. Experiment 2 found this helped enhance the task performance without damaging flow experience.", "keywords": ["Task analysis", "Visualization", "Virtual reality", "Solid modeling", "Human factors", "Resource management", "Atmospheric measurements", "human computer interaction", "virtual reality", "VR visual search paradigm", "VR oceanic treasure hunting system", "visual cues", "task-oriented selective attention", "task performance", "flow experience", "visual search task paradigm", "weak association model", "virtual reality activities", "Flow experience", "weak association", "virtual reality", "visual search task design guideline", "H.1.2. [Models and Principles]: User/Machine Systems \u2014 Human factors; Introduction"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798237", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798237", "paper_url": "https://ieeexplore.ieee.org/document/8798237"}, {"title": "Challenges to Applying Virtual Reality Technology and Techniques to Visual Analytics", "authors": ["R. May", "P.K. Arya", "D.A. Bowman", "G. Schmidt", "A. Sullivan"], "abstract": "Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces. It has grown out of and is strongly related to information visualization. Visual analytic tools assist analysts in detecting the expected and discovering the unexpected from complex, noisy, incomplete, heterogeneous, and sometimes deliberately deceptive data. VR/AR research has claimed for years to provide the potential for more effective environments to understand and explore information spaces. This would seem to make VR technology a natural for application to visual analytics. However, visual analytics is focused on the analytical process not the tools and technology. As such, any new methods, techniques, and technologies need to show benefit to analysts working on real problems. Additionally, the majority of visual analytics research funding is not focused on disruptive physical interface technologies. Generally speaking, new technologies and techniques for visual analytics need to function within the current analytical environment. The purpose of this panel is to introduce the domain of visual analytics to the audience and explore how and where VR/AR research can be adapted for use in visual analytics. The panelists have been selected based on topic areas of research that have potential for near term insertion or impact on visual analytics. This panel will focus on the hard issues of defining what it will take to get VR technology introduced into the visual analytics research funding stream.", "keywords": ["Virtual reality", "Visual analytics", "Visualization", "Computer science", "Uncertainty", "Biomedical imaging", "User interfaces", "Three dimensional displays", "Computer displays", "Event detection"], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 1, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.32", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667670", "paper_url": "https://ieeexplore.ieee.org/document/1667670"}, {"title": "Worlds-in-Wedges: Combining Worlds-in-Miniature and Portals to Support Comparative Immersive Visualization of Forestry Data", "authors": ["Jung Who Nam", "Krista Mccullough", "Joshua Tveite", "Maria Molina Espinosa", "Charles H. Perry", "Barry T. Wilson", "Daniel F. Keefe"], "abstract": "Virtual reality (VR) environments are typically designed so users feel present in a single virtual world at a time, but this creates a problem for applications that require visual comparisons (e.g., forest scientists comparing multiple data-driven virtual forests). To address this, we present Worlds-in-Wedges, a 3D user interface and visualization technique that supports comparative immersive visualization by dividing the virtual space surrounding the user into volumetric wedges. There are three visual/interactive levels. The first, worlds-in-context, visualizes high-level relationships between the worlds (e.g., a map for worlds that are related in space). The second level, worlds-in-miniature, is a multi-instance implementation of the World-in-Miniature technique extended to support mutlivari-ate glyph visualization. The third level, worlds-in-wedges, displays multiple large-scale worlds in wedges that act as volumetric portals. The interface supports navigation, selection, and view manipulation. Since the techniques were inspired directly by problems facing forest scientists, the interface was evaluated by building a complete multivariate data visualization of the US Forest Service Forest Inventory and Analysis public dataset. Scientist user feedback and lessons from iterative design are reported.", "keywords": ["Data visualization", "Forestry", "Visualization", "Three-dimensional displays", "Portals", "Computer science", "Task analysis", "data visualisation", "forestry", "portals", "user interfaces", "virtual reality", "complete multivariate data visualization", "scientist user feedback", "worlds-in-wedges", "worlds-in-miniature", "virtual reality environments", "single virtual world", "forest scientists", "virtual space", "volumetric wedges", "worlds-in-context", "immersive visualization", "mutlivariate glyph visualization", "world-in-miniature technique", "US Forest Service Forest Inventory and Analysis public dataset", "worlds-in-miniature", "3D user interface", "presence", "comparative visualization", "Human-centered computing\u2014Virtual reality", "Human-centered computing\u2014Interaction techniques", "Human-centered computing\u2014Scientific visualization", "Human-centered computing\u2014Geographic visualization"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797871", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797871", "paper_url": "https://ieeexplore.ieee.org/document/8797871"}, {"title": "Coupled-clay: Physical-virtual 3D collaborative interaction environment", "authors": ["Kas\u0131m Ozacar", "Takuma Hagiwara", "Jiawei Huang", "Kazuki Takashima", "Yoshifumi Kitamura"], "abstract": "We propose Coupled-clay, a bi-directional 3D collaborative interactive environment that supports the 3D modeling work between groups of users at remote locations. Coupled-clay consists of two network-connected workspaces, the Physical Interaction Space and the Virtual Interaction Space. The physical interaction space allows a user to directly manipulate a physical object whose shape and position are precisely tracked. This tracked 3D information is transferred to the virtual interaction space in real time. The virtual interaction space is made of an interactive multi-user stereoscopic 3D tabletop, or other 3D displays with adequate interaction device. The users at the virtual interaction space observe the virtual 3D object which corresponds to the physical object and manipulate its geometrical attributes (e.g., translation, rotation and scaling). Additionally, they can control the graphical attributes of the virtual object such as color and texture. Information about changes in geometrical and graphical attributes are sent back to the physical interaction space in real time and reflected to the object in the physical interaction space by a robotic arm and a top-mounted projector. Coupled-clay can be used to remotely collaborate on 3D modeling tasks such as between a skilled designer and novice learners. This paper details our Coupled-clay implementation and presents its interaction capabilities.", "keywords": ["Three-dimensional displays", "Shape", "Real-time systems", "Collaboration", "Bidirectional control", "Electronic mail", "Robots", "interactive devices", "solid modelling", "stereo image processing", "three-dimensional displays", "virtual reality", "coupled-clay", "physical-virtual 3D collaborative interaction environment", "bidirectional 3D collaborative interactive environment", "remote location", "network-connected workspace", "physical interaction space", "virtual interaction space", "3D information", "multiuser stereoscopic 3D tabletop", "3D display", "interaction device", "virtual 3D object", "physical object", "geometrical attribute", "graphical attribute", "virtual object", "robotic arm", "top-mounted projector", "3D modeling task", "H.5.3 [INFORMATION INTERFACES AND PRESENTATION]: Group and Organization Interfaces \u2014 Collaborative computing", "I.3.7 [Computing Methodologies]", "COMPUTER GRAPHICS \u2014 Three-Dimensional Graphics and Realism"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 0, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223392", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223392", "paper_url": "https://ieeexplore.ieee.org/document/7223392"}, {"title": "Cutting, Deforming and Painting of 3D meshes in a Two Handed Viso-haptic VR System", "authors": ["Adam Faeth", "Michael Oren", "Jonathan Sheller", "Sean Godinez", "Chris Harding"], "abstract": "We describe M4, the multi-modal mesh manipulation system, which aims to provide a more intuitive desktop interface for freeform manipulation of 3D meshes. The system combines interactive 3D graphics with haptic force feedback and provide several virtual tools for the manipulation of 3D objects represented by irregular triangle meshes. The current functionality includes mesh painting with pressure dependent brush size and paint preview, mesh cutting via drawing a poly-line on the model and two types of mesh deformations. We use two phantoms, either in a co-located haptic/3D-stereo setup or as a fish tank VR setup with a 3D flat panel. In our system, the second hand assists the manipulation of the object, either by \";holding\"; the mesh or by affecting the manipulation directly. While the connection of 3D artists and designers to such a direct interaction system may be obvious, we are also investigating its potential benefits for landscape architects and other users of spatial geoscience data. Feedback from an upcoming user study will evaluate the benefits of this system and its tools for these different user groups.", "keywords": ["Painting", "Virtual reality", "Haptic interfaces", "Graphics", "Force feedback", "Brushes", "Paints", "Deformable models", "Imaging phantoms", "Marine animals", "computer graphics", "haptic interfaces", "virtual reality", "3D meshes", "two handed viso-haptic VR system", "multi-modal mesh manipulation system", "interactive 3D graphics", "haptic force feedback", "virtual tools", "H.5.2 Haptic I/O", "I.3.7 Three-Dimensional Graphics and Realism \u00bf Virtual Reality", "I.3.4 Graphics Utilities \u00bf Graphic Editors", "I.3.6 Methodology and Techniques - Interaction techniques", "Haptics", "H3D", "3D graphics", "X3D", "shaders", "digital shapes", "surface mesh", "geometric modeling", "deformation", "cutting"], "published_in": "2008 IEEE Virtual Reality Conference", "publication_date": "March 2008", "citations": 4, "isbn": ["978-1-4244-1971-5", "978-1-4244-1972-2"], "doi": "10.1109/VR.2008.4480776", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4480776", "paper_url": "https://ieeexplore.ieee.org/document/4480776"}, {"title": "Comparative study of input devices for a VR mine simulation", "authors": ["David Zielinski", "Brendan Macdonald", "Regis Kopper"], "abstract": "It has been shown that virtual reality (VR) can be used to train mine workers for safety in critical situations [4]. The National Institute for Occupational Safety and Health (NIOSH) has a virtual reality (VR) laboratory on its Pittsburgh campus. Currently, input devices for the system are an Xbox 360 game pad and an air mouse. Due to the high cost and added complexity of most 3D tracking systems, we wanted to first test to see if the mine safety application could benefit from an upgrade to a 6-DOF tracking system. Thus, we conducted a pilot study at Duke University's six-sided CAVE-type system, and collected performance and questionnaire data for three tasks (selection, navigation, and maneuvering) and three devices (gamepad, air mouse, 6-DOF wand). Results indicate that the wand allows users to complete tasks faster and is preferred by users. However, in certain situations its use led to more errors.", "keywords": ["Mice", "Virtual reality", "Navigation", "Performance evaluation", "Solid modeling", "Training", "Usability", "computer based training", "computer games", "industrial training", "mining", "occupational health", "occupational safety", "personnel", "virtual reality", "input devices", "VR mine simulation", "virtual reality", "mine worker training", "National Institute for Occupational Safety and Health", "NIOSH", "virtual reality laboratory", "Pittsburgh campus", "Xbox 360 game pad", "air mouse", "3D tracking systems", "mine safety application", "6-DOF tracking system", "Duke University six-sided CAVE-type system", "Virtual reality", "mine safety", "device comparison"], "published_in": "2014 IEEE Virtual Reality (VR)", "publication_date": "March 2014", "citations": 2, "isbn": ["978-1-4799-2871-2"], "doi": "10.1109/VR.2014.6802083", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6802083", "paper_url": "https://ieeexplore.ieee.org/document/6802083"}, {"title": "Object impersonation: Towards effective interaction in tablet- and HMD-based hybrid virtual environments", "authors": ["Jia Wang", "Robert W. Lindeman"], "abstract": "In virtual reality, hybrid virtual environment (HVE) systems provide the immersed user with multiple interactive representations of the virtual world, and can be effectively used for 3D interaction tasks with highly diverse requirements. We present a new HVE metaphor called Object Impersonation that allows the user to not only manipulate a virtual object from outside, but also become the object, and maneuver from inside. This approach blurs the line between travel and object manipulation, leading to efficient cross-task interaction in various task scenarios. Using a tablet- and HMD-based HVE system, two different designs of Object Impersonation were implemented, and compared to a traditional, non-hybrid 3D interface for three different object manipulation tasks. Results indicate improved task performance and enhanced user experience with the added orientation control from the object's point of view. However, they also revealed higher cognitive overhead to attend to both interaction contexts, especially without sufficient reference cues in the virtual environment.", "keywords": ["Three-dimensional displays", "Context", "Virtual environments", "Avatars", "Integrated circuits", "Space exploration", "helmet mounted displays", "human computer interaction", "notebook computers", "position control", "user interfaces", "virtual reality", "object impersonation", "effective interaction", "tablet-based hybrid virtual environment", "HMD-based hybrid virtual environment", "virtual reality", "HVE system", "multiple interactive representation", "virtual world", "3D interaction task", "virtual object manipulation", "cross-task interaction", "user experience", "orientation control", "cognitive overhead", "Hybrid virtual environments", "cross-task interaction", "3D user interface", "tablet interface", "virtual reality"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 6, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223332", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223332", "paper_url": "https://ieeexplore.ieee.org/document/7223332"}, {"title": "Effects of handling real objects and avatar fidelity on cognitive task performance in virtual environments", "authors": ["B. Lok", "S. Naik", "M. Whitton", "F.P. Brooks"], "abstract": "Immersive virtual environments (VEs) provide participants with computer-generated environments filled with virtual objects to assist in learning, training, and practicing dangerous and/or expensive tasks. But for certain tasks, does having every object being virtual inhibit the interactivity? Further, does the virtual object's visual fidelity affect performance? Overall VE effectiveness may be reduced if users spend most of their time and cognitive capacity learning how to interact and adapting to interacting with a purely virtual environment. We investigated how handling real objects and how self-avatar visual fidelity affects performance on a spatial cognitive task in an immersive VE. We compared participants' performance on a block arrangement task in both a real-space environment and several virtual and hybrid environments. The results showed that manipulating real objects in a VE brings task performance closer to that of real space, compared to manipulating virtual objects.", "keywords": ["Avatars", "Virtual environment", "Assembly", "Lubricating oils", "Engines", "Feedback", "Haptic interfaces", "Problem-solving", "Cognitive science", "Libraries", "virtual reality", "user interfaces", "human factors", "computer based training", "avatar fidelity", "cognitive task performance", "immersive virtual environments", "training", "visual fidelity", "performance", "cognitive capacity", "self-avatar visual fidelity", "immersive VE", "virtual objects"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 12, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191130", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191130", "paper_url": "https://ieeexplore.ieee.org/document/1191130"}, {"title": "Advancing Ethical Decision Making in Virtual Reality", "authors": ["Sinhwa Kang", "Jake Chanenson", "Pranav Ghate", "Peter Cowal", "Madeleine Weaver", "David M. Krum"], "abstract": "Virtual reality (VR) has been widely utilized for training and education purposes because of pedagogical, safety, and economic benefits. The investigation of moral judgment is a particularly interesting VR application, related to training. For this study, we designed a within-subject experiment manipulating the role of study participants in a Trolley Dilemma scenario: either victim or driver. We conducted a pilot study with four participants and describe preliminary results and implications in this poster.", "keywords": ["Ethics", "Virtual reality", "Vehicles", "Decision making", "Training", "Switches", "Three-dimensional displays", "decision making", "ethical aspects", "virtual reality", "ethical decision making", "virtual reality", "education purposes", "pedagogical safety", "economic benefits", "moral judgment", "within-subject experiment", "Trolley Dilemma scenario", "VR application", "Human-centered computing", "Human computer interaction (HCI)", "Interaction paradigms", "Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798151", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798151", "paper_url": "https://ieeexplore.ieee.org/document/8798151"}, {"title": "Visualizing Natural Environments from Data in Virtual Reality: Combining Realism and Uncertainty", "authors": ["Jiawei Huang", "Melissa S. Lucash", "Mark B. Simpson", "Casey Helgeson", "Alexander Klippel"], "abstract": "Understanding complex scientific data visualizations in 2D can be challenging. Virtual Reality (VR) provides an alternative, combining realistic 3D representations with intuitive, natural interactions with data through embodied experiences. However, realistic 3D representations and associated immersive experiences are prone to misrepresentations as they are selectively representative and often leave little room for abstraction. This is particularly challenging for topics such as modeling natural environments where users value realism. We discuss the causes and categories of potential misrepresentations in VR with a particular focus on scientific visualization. We contextualize our discussion by presenting an application prototype that translates ecological model output data into a high-fidelity VR experience that allows users to walk through forests of the future. We also designed and implemented two methods to display uncertainties in high-fidelity VR environments: A multi-scenarios approach to provide users access to alternative scenarios, and a slide-and-show approach to view the environment within the confidence interval.", "keywords": ["Data visualization", "Uncertainty", "Three-dimensional displays", "Forestry", "Two dimensional displays", "Solid modeling", "Biological system modeling", "data visualisation", "solid modelling", "virtual reality", "virtual reality", "realistic 3D representations", "intuitive interactions", "embodied experiences", "immersive experiences", "modeling natural environments", "high-fidelity VR experience", "high-fidelity VR environments", "scientific data visualizations", "ecological model output data", "slide-and-show approach", "Visualization", "scientific visualization", "virtual reality", "Computing methodologies", "Computer graphics", "Graphics systems and interfaces", "Human-centered computing", "Visualization application domains", "Visualization theory, concepts and paradigms", "Modeling and simulation", "Simulation types and techniques"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797996", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797996", "paper_url": "https://ieeexplore.ieee.org/document/8797996"}, {"title": "Effect of Sensory Conflict and Postural Instability on Cybersickness", "authors": ["Adrian K. T. Ng", "Leith K. Y. Chan", "Henry Y. K. Lau"], "abstract": "Sensory conflict theory and postural instability theory were often tested individually to explain cybersickness in VR systems, but they were seldom systematically compared. An earlier study evaluated them on a large screen using 2D videos. This study evaluated sensory conflict and postural instability on the discomfort in VR. Virtual visual locomotion were shown on an head-mounted display. A motion platform vibrated in low-frequency while the participant stood on top. Each factor was manipulated alone or in combination. Results showed that the visual motion only condition led to the highest miserable score, higher than the physical vibration only condition. This suggested that consistent with previous literature, sensory conflict may be a major contributing factor of cybersickness.", "keywords": ["Visualization", "Vibrations", "Virtual reality", "Navigation", "Resists", "Two dimensional displays", "Videos", "human factors", "virtual reality", "physical vibration only condition", "visual motion only condition", "virtual visual locomotion", "VR systems", "postural instability theory", "sensory conflict theory", "cybersickness", "Motion sickness", "VIMS", "motion platform", "HMD", "visual motion", "vestibular vibration", "Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Virtual reality", "Computing methodologies\u2014Computer graphics\u2014Graphics systems and interfaces\u2014Perception"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797781", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797781", "paper_url": "https://ieeexplore.ieee.org/document/8797781"}, {"title": "A Tangible User Interface System for CAVE Applicat", "authors": ["Ji-Sun Kim", "D. Gracanin", "H.L. Singh", "K. Matkovic", "J. Juric"], "abstract": "This paper describes a 3D, tangible user interface system and related interaction techniques for the CAVE environment. The developed system is based on off-the shelf software and hardware components to provide 3D input data for CAVE applications. A CAVE with four sides (three walls and a floor) is used as a display and interaction space. Interaction tasks (selection, manipulation, navigation) are performed using interaction techniques based on manipulation of physical objects (props). All virtual objects are directly manipulated using the corresponding props to which Augmented Reality physical markers are attached. Each physical marker corresponds to a specific virtual object. The floor projection (a white rectangle overlaid on top of the application generated video stream) or a directional light create illumination necessary for computer vision based marker detection using ARToolKit. Initial evaluation results are positive and provide directions for future research.", "keywords": ["User interfaces", "Application software", "Augmented reality", "Space technology", "Displays", "Streaming media", "Virtual reality", "Computer graphics", "Real time systems", "Military computing", "3D interaction", "mixed reality", "augmented reality", "3D interaction", "mixed reality", "augmented reality"], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 0, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.21", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667655", "paper_url": "https://ieeexplore.ieee.org/document/1667655"}, {"title": "Efficient Physics-Based Implementation for Realistic Hand-Object Interaction in Virtual Reality", "authors": ["Markus H\u00f6ll", "Markus Oberweger", "Clemens Arth", "Vincent Lepetit"], "abstract": "We propose an efficient physics-based method for dexterous `real hand' - `virtual object' interaction in Virtual Reality environments. Our method is based on the Coulomb friction model, and we show how to efficiently implement it in a commodity VR engine for realtime performance. This model enables very convincing simulations of many types of actions such as pushing, pulling, grasping, or even dexterous manipulations such as spinning objects between fingers without restrictions on the objects' shapes or hand poses. Because it is an analytic model, we do not require any prerecorded data, in contrast to previous methods. For the evaluation of our method, we conduction a pilot study that shows that our method is perceived more realistic and natural, and allows for more diverse interactions. Further, we evaluate the computational complexity of our method to show real-time performance in VR environments.", "keywords": ["Friction", "Computational modeling", "Grasping", "Solid modeling", "Three-dimensional displays", "Real-time systems", "dexterous manipulators", "friction", "human-robot interaction", "virtual reality", "virtual object interaction", "Virtual Reality environments", "Coulomb friction model", "commodity VR engine", "realtime performance", "convincing simulations", "dexterous manipulations", "diverse interactions", "VR environments", "hand-object interaction", "physics-based method", "physics-based implementation", "dexterous real hand-virtual object interaction", "I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling-Physically-based Modeling", "I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality", "H.5.2 [Information Interfaces and Presentation]: User Interfaces-Direct Manipulation"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 1, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8448284", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8448284", "paper_url": "https://ieeexplore.ieee.org/document/8448284"}, {"title": "Tension based 7-DOF force feedback device: SPIDAR-G", "authors": ["S. Kim", "S. Hasegawa", "Y. Koike", "M. Sato"], "abstract": "We demonstrate a new intuitive force feedback device for advanced VR applications. Force feedback for the device is tension based and is characterized by 7 degrees of freedom (DOF); 3 DOF for translation, 3 DOF for rotation, and 1 DOF for grasp. The SPIDAR-G (Space Interface Device for Artificial Reality with Grip) will allow users to interact with virtual objects naturally by manipulating two hemispherical grips located in the center of the device frame. We show how to connect the strings between each vertex of grip and each extremity of the frame in order to achieve force feedback. In addition, methodologies will be discussed for calculating translation, orientation and grasp using the length of 8 strings connected to the motors and encoders on the frame. The SPIDAR-G exhibits smooth force feedback, minimized inertia, no backlash, scalability and safety. Such features are attributed to strategic string arrangement and control that results in stable haptic rendering. The design and control of the SPIDAR-G is described in detail and the space graphic user interface system based on the proposed SPIDAR-G system is demonstrated. Experimental results validate the feasibility of the proposed device and reveal its application to virtual reality.", "keywords": ["Force feedback", "Virtual reality", "Extremities", "Scalability", "Safety", "Haptic interfaces", "Rendering (computer graphics)", "Control systems", "Graphics", "User interfaces", "force feedback", "virtual reality", "haptic interfaces", "interactive devices", "graphical user interfaces", "tension based force feedback device", "seven degrees of freedom", "Space Interface Device for Artificial Reality with Grip", "virtual objects", "hemispherical grips", "minimized inertia", "scalability", "safety", "haptic rendering", "space graphic user interface", "experimental results", "SPIDAR-G", "virtual reality"], "published_in": "Proceedings IEEE Virtual Reality 2002", "publication_date": "2002", "citations": 43, "isbn": ["0-7695-1492-8"], "doi": "10.1109/VR.2002.996540", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=996540", "paper_url": "https://ieeexplore.ieee.org/document/996540"}, {"title": "MIRAGE: A Touch Screen based Mixed Reality Interface for Space Planning Applications", "authors": ["Gun A. Lee", "Hyun Kang", "Wookho Son"], "abstract": "Space planning is one of the popular applications of VR technology including interior design, architecture design, and factory layout. In order to provide easier and efficient methods to accommodate physical objects into virtual space under plan, we suggest applying mixed reality (MR) interface. Our MR system consists of a video see-through display with a touch screen interface, mounted on a mobile platform, and we use screen space 3D manipulations to arrange virtual objects within the MR scene. Investigating the interface with our prototype implementation, we are convinced that our system will help users to design spaces in more easy and effective way.", "keywords": ["Virtual reality", "Space technology", "Production facilities", "Layout", "Virtual environment", "Computerized monitoring", "Technology planning", "Hardware", "Cameras", "Video sharing", "architectural CAD", "data visualisation", "planning", "touch sensitive screens", "user interfaces", "virtual reality", "MIRAGE", "mixed reality interface", "space planning", "virtual reality", "interior design", "architecture design", "factory layout", "video see-through display", "touch screen interface", "mobile platform", "screen space 3D manipulations", "mixed reality visualization", "Mixed reality", "augmented reality", "space planning", "touch interface", "plant layout", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u00bf Artificial, augmented, and virtual realities"], "published_in": "2008 IEEE Virtual Reality Conference", "publication_date": "March 2008", "citations": 1, "isbn": ["978-1-4244-1971-5", "978-1-4244-1972-2"], "doi": "10.1109/VR.2008.4480797", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4480797", "paper_url": "https://ieeexplore.ieee.org/document/4480797"}, {"title": "Im-O-Ret: Immersive object retrieval", "authors": ["Pedro B. Pascoal", "Alfredo Ferreira", "Joaquim Jorge"], "abstract": "The growing number of three-dimensional (3D) objects stored in digital libraries brought forth the challenge of search in 3D model collections. To address it, several approaches have been developed for 3D object retrieval. However, these approaches traditionally present query results as a list of thumbnails, and fail to take advantage of recent visualization and interaction technologies. In this paper, we propose an approach to 3D object retrieval using immersive VR for query result visualization. Query results are shown in a three-dimensional virtual space as 3D objects and users can explore these results by navigating in this virtual space and manipulating the scattered objects.", "keywords": ["Three dimensional displays", "Solid modeling", "Visualization", "Navigation", "Search engines", "Computational modeling", "data visualisation", "digital libraries", "image retrieval", "solid modelling", "Im-O-Ret", "immersive object retrieval", "three-dimensional objects", "digital libraries", "3D model collections", "3D object retrieval", "thumbnails", "visualization technologies", "interaction technologies", "query result visualization", "three-dimensional virtual space", "virtual space navigation", "scattered object manipulation", "Multimedia Information Retrieval", "3D Object Retrieval", "Immersive Virtual Environment"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 2, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180912", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180912", "paper_url": "https://ieeexplore.ieee.org/document/6180912"}, {"title": "Toward Intuitive 3D User Interfaces for Climbing, Flying and Stacking", "authors": ["Antonin Bernardin", "Guillaume Cortes", "Rebecca Fribourg", "Tiffany Luong", "Florian Nouviale", "Hakim Si-Mohammed"], "abstract": "In this paper, we propose 3D user interfaces (3DUI) that are adapted to specific Virtual Reality (VR) tasks: climbing a ladder using a puppet metaphor, piloting a drone thanks to a 3D virtual compass and stacking 3D objects with physics-based manipulation and time control. These metaphors have been designed to provide the user with an intuitive, playful and efficient way to perform each task.", "keywords": ["Three-dimensional displays", "Drones", "Compass", "Task analysis", "User interfaces", "Stacking", "Navigation", "user interfaces", "virtual reality", "physics-based manipulation", "toward intuitive 3D", "3DUI", "puppet metaphor", "Intuitive 3D User Interfaces", "Virtual Reality tasks", "3D virtual compass", "stacking 3D objects", "Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446047", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446047", "paper_url": "https://ieeexplore.ieee.org/document/8446047"}, {"title": "Effects of Stereoscopic Viewing and Haptic Feedback, Sensory-Motor Congruence and Calibration on Near-Field Fine Motor Perception-Action Coordination in Virtual Reality", "authors": ["David Brickler", "Matias Volonte", "Jeffrey W. Bertrand", "Andrew T. Duchowski", "Sabarish V. Babu"], "abstract": "We present an empirical evaluation on how stereoscopic viewing and haptic feedback deferentially affects fine motor perception-action coordination in a pick-and-place task in Virtual Reality (VR). The factors considered were stereoscopic viewing, haptic feedback, sensory-motor congruence and mismatch, and calibration on perception-action coordination in near field fine motor task performance in VR. Quantitative measures of placement error, distance, collision, and time to complete trials were recorded and analyzed. Overall, we found that participants' manual dexterous task performance was enhanced in the presence of both stereoscopic viewing and haptic feedback. However, we found that time to complete task was greatly enhanced by the presence of haptic feedback, and economy and efficiency of movement of the end effector as well as the manipulated object was enhanced by the presence of both haptic feedback and stereoscopic viewing. Whereas, number of collisions and placement accuracy were greatly enhanced by the presence of stereoscopic viewing in near-field fine motor perception-action coordination. Our research additionally shows that mismatch in sensory-motor stimuli can detrimentally affect the number of collisions, and efficiency of end effector and object movements in near-field fine motor activities, and can be further negatively affected by the absence of haptic feedback and stereoscopic viewing. In spite of reduced cue situations in VR, and the absence or presence of stereoscopic viewing and haptic feedback, we found that participants tend to calibrate or adapt their perception-action coordination rapidly with a set of at least 5 trials.", "keywords": ["Haptic interfaces", "Stereo image processing", "Task analysis", "Surgery", "Three-dimensional displays", "Training", "Solid modeling", "feedback", "haptic interfaces", "human factors", "stereo image processing", "virtual reality", "visual perception", "stereoscopic viewing", "sensory-motor congruence", "near-field fine motor perception-action coordination", "virtual reality", "field fine motor task performance", "sensory-motor stimuli", "near-field fine motor activities", "haptic feedback", "end effector", "object movement", "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797744", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797744", "paper_url": "https://ieeexplore.ieee.org/document/8797744"}, {"title": "Automatic Selective Disassembly and Path Planning for the Simulation of Maintenance Operations", "authors": ["Iker Aguinaga", "Diego Borro", "Luis Matey"], "abstract": "Maintenance operations have a great impact in the security and life expectancy of any product. Virtual Reality can help reducing costs and design time by moving testing from physical mock-ups to virtual ones. VR allows the application of additional tools not possible in a physical mock-up. The increase in power of current computers allows their use not only for visualization and interaction but also for geometrical analysis and reasoning. These capabilities provide designers with tools that can increase their productivity. This paper proposes methods for the generation of selective disassembly plans and the generation of collision free trajectories in the context of interactive simulation of maintenance operations. These methods provide a design team with additional tools to asses a design, prior to the construction of physical mock-ups and, therefore, reducing the cost of a project", "keywords": ["Path planning", "Assembly", "Virtual reality", "Costs", "Testing", "Application software", "Design engineering", "Computer graphics", "Visualization", "Productivity", "maintenance engineering", "path planning", "production engineering computing", "virtual reality", "automatic selective disassembly", "path planning", "maintenance operations", "virtual reality", "geometrical analysis", "geometrical reasoning", "collision free trajectories", "interactive simulation", "Disassembly Planning", "VR applications"], "published_in": "2007 IEEE Virtual Reality Conference", "publication_date": "March 2007", "citations": 2, "isbn": ["1-4244-0905-5", "1-4244-0906-3"], "doi": "10.1109/VR.2007.352503", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4161045", "paper_url": "https://ieeexplore.ieee.org/document/4161045"}, {"title": "A user-centered approach on combining realism and interactivity in virtual environments", "authors": ["M. Roussou", "G. Drettakis", "N. Tsingos", "A. Reche", "E. Gallo"], "abstract": "In this paper we describe a project that adopts a user-centered approach in the design of virtual environments (VEs) with enhanced realism and interactivity, guided by real-world applications in the areas of urban planning/architecture and cultural heritage education. In what concerns realism, we introduce an image-based 3D capture process, where realistic models are created from photographs and subsequently displayed in a VR system using a high-quality, view-dependent algorithm. The VE is further enhanced using advanced vegetation and shadow display algorithms as well as 3D sound. A high degree of interactivity is added, allowing users to build and manipulate elements of the VEs according to their needs, as specified through a user task analysis and scenario-based approach which is currently being evaluated. This work is developed as part of the Ell-funded research project CREATE.", "keywords": ["Virtual environment", "Virtual reality", "Rendering (computer graphics)", "Vegetation mapping", "Image reconstruction", "Computer displays", "Educational institutions", "Urban planning", "Cultural differences", "Three dimensional displays", "user centred design", "architectural CAD", "virtual reality", "town and country planning", "humanities", "stereo image processing", "user-centered design", "virtual environments", "urban planning", "urban architecture", "cultural heritage education", "3D capture", "VR system", "3D sound", "user task analysis"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 2, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310094", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310094", "paper_url": "https://ieeexplore.ieee.org/document/1310094"}, {"title": "NuNav3D: A touch-less, body-driven interface for 3D navigation", "authors": ["Charilaos Papadopoulos", "Daniel Sugarman", "Arie Kaufmant"], "abstract": "We introduce NuNav3D, a body-driven 3D navigation interface for large displays and immersive scenarios. While 3D navigation is a core component of VR applications, certain situations, like remote displays in public or large visualization environments, do not allow for using a navigation controller or prop. NuNav3D maps hand motions, obtained from a pose recognition framework which is driven by a depth sensor, to a virtual camera manipulator, allowing for direct control of 4 DOFs of navigation. We present the NuNav3D navigation scheme and our preliminary user study results under two scenarios, a path-following case with tight geometrical constraints and an open space exploration case, while comparing our method against a traditional joypad controller.", "keywords": ["Navigation", "Cameras", "Three dimensional displays", "Vectors", "Virtual environments", "Electronic mail", "navigation", "pose estimation", "user interfaces", "touchless interface", "body-driven 3D navigation interface", "large displays", "immersive scenario", "remote displays", "large visualization environments", "navigation controller", "hand motion", "pose recognition framework", "depth sensor", "virtual camera manipulator", "NuNav3D navigation", "path-following case", "geometrical constraints", "open space exploration case", "joypad controller", "H.5.2 [User Interfaces]: Evaluation/methodology \u2014 Interaction Styles", "I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 10, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180885", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180885", "paper_url": "https://ieeexplore.ieee.org/document/6180885"}, {"title": "Force Push: Exploring Expressive Gesture-to-Force Mappings for Indirect 3D Object Manipulation", "authors": ["Run Yu", "Doug A. Bowman"], "abstract": "We present Force Push, a hyper-natural gesture-to-action mapping for object manipulation in virtual reality (VR). It maps hand gestures derived from human-human interaction to physics-driven movement of an object and uses expressive features of gestures to enhance controllability. An initial user study shows both the performance and broader user experience qualities of Force Push as compared to a traditional direct control mapping.", "keywords": ["Force", "Three-dimensional displays", "Task analysis", "Drag", "Human computer interaction", "Controllability", "gesture recognition", "human computer interaction", "virtual reality", "expressive gesture-to-Force mappings", "Force Push", "hyper-natural gesture-to-action mapping", "human-human interaction", "physics-driven movement", "indirect 3D Object Manipulation", "hand gesture mapping", "direct control mapping", "Gesture", "object manipulation", "indirect mapping", "H.5.2 [Information Interfaces and Presentation]", "User Interfaces", "Interaction styles"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446453", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446453", "paper_url": "https://ieeexplore.ieee.org/document/8446453"}, {"title": "Mobius Walker: Pitch and Roll Redirected Walking", "authors": ["Tatsuki Yamamoto", "Jumpei Shimatani", "Isamu Ohashi", "Keigo Matsumoto", "Takuji Narumi", "Tomohiro Tanikawa", "Michitaka Hirose"], "abstract": "A redirected walking (RDW) techniques enable users to walk around infinite virtual environments (VEs) in a finite physical space. In previous studies on RDW, many researchers have discussed manipulations in the yaw direction, but few have tackled with redirection in pitch and roll directions. We propose a novel VR system, which realizes pitch and roll redirections and allows users to experience walking on the 3D model of Mobius Strip in the VE.", "keywords": ["Legged locomotion", "Strips", "Three-dimensional displays", "Electronic mail", "Solid modeling", "Aerospace electronics", "Acceleration", "virtual reality", "RDW", "roll redirections", "Mobius Strip", "redirected walking techniques", "finite physical space", "virtual environments", "pitch redirections", "3D model", "Mobius Walker", "Human-centered computing-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446263", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446263", "paper_url": "https://ieeexplore.ieee.org/document/8446263"}, {"title": "A Multidirectional Haptic Feedback Prototype for Experiencing Collisions Between Virtual and Real Objects", "authors": ["Li Zhang", "Weiping He", "Mengmeng Sun", "Xiaoliang Bai", "Shuxia Wang"], "abstract": "Haptic feedback has shown its great value in HCI research and applications for enhancing user experiences. Simulating people's tactile sensations of virtual objects is currently a primary research target. Different from wearing motors on fingers or hands, we attached vibration motors on a physical object to simulate the augmented sense of collision with virtual objects to bare hands. We developed a novel sensor-based proof-of-concept prototype that distributes multiple vibration motors around a physical object and provides vibrational sensations from the collision direction through combinations of motors. Users can obtain augmented haptic feedback when manipulating the augmented physical object to interact with virtual objects in an AR environment. We first studied the influence of the number and input voltage of motors for a correct judgment of different directions to identify the design parameters of the prototype. Then we investigated the effect of introducing the prototype in a typical manipulation task in AR. We found the prototype was efficient for enhancing human performance and collision experience with virtual objects together with visual feedback.", "keywords": ["Haptic interfaces", "Vibrations", "Prototypes", "Task analysis", "Visualization", "Force", "User interfaces", "haptic interfaces", "human computer interaction", "user experience", "vibrations", "multidirectional haptic feedback prototype", "virtual objects", "multiple vibration motors", "vibrational sensations", "collision direction", "augmented physical object", "collision experience", "human performance", "sensor-based proof-of-concept prototype", "augmented haptic feedback", "manipulation task", "visual feedback", "H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities", "H.5.2 [User Interfaces]: Haptic I/O"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797878", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797878", "paper_url": "https://ieeexplore.ieee.org/document/8797878"}, {"title": "Evaluation of Information Widgets for a Virtual Reality Serious Game", "authors": ["Joshua Salyers", "Daniel Cliburn", "Keely Canniff", "Stephany Barajas"], "abstract": "As K-12 humanities teachers increasingly use virtual reality programs as pedagogical tools, creators of virtual reality serious games should better understand the way that the design of 3D user interfaces influence learning and user engagement. This study explores user preferences towards selection technique, level of transparency, and positioning of 3D information widgets in an open-world virtual reality environment. Using a recently constructed virtual heritage game, Little Manila Recreated, this article adds to a growing body of user studies that attempt to establish best practices for the use of 3D information widgets in VR history projects designed for K-12 classrooms.", "keywords": ["Three-dimensional displays", "Games", "History", "Testing", "Virtual environments", "Tools", "computer aided instruction", "history", "serious games (computing)", "virtual reality", "information widgets", "virtual reality serious game", "K-12 humanities teachers", "virtual reality programs", "user engagement", "user preferences", "open-world virtual reality environment", "virtual heritage game", "Human-centered computing\u2014Virtual Reality", "Human-centered computing\u2014user studies"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798150", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798150", "paper_url": "https://ieeexplore.ieee.org/document/8798150"}, {"title": "Virtual Hand Realism Affects Object Size Perception in Body-Based Scaling", "authors": ["Nami Ogawa", "Takuji Narumi", "Michitaka Hirose"], "abstract": "How does the representation of an embodied avatar influence the way in which one perceives the scale of a virtual environment? In virtual reality, it is common to embody avatars of various appearances, from abstract to realistic. It is known that changes in the realism of virtual hands affect the self-body perception, including body ownership. However, the influence of self-avatar realism on the perception of non-body objects has not been investigated. Considering the theory that the scale of the external environment is perceived relative to the size of one's body (body-based scaling), it can be hypothesized that the realism of an avatar affects not only body ownership but also the fidelity of the avatar with respect to our own body as a metric. Therefore, this study examines how avatar realism affects perceived object sizes as the size of the virtual hand changes. In the experiment, we manipulate the level of realism (realistic, iconic, and abstract) and size (veridical and enlarged) of the virtual hand and measure the perceived size of a cube. The results show that the size of the cube is perceived to be smaller when the virtual hand is enlarged compared to when it is veridical, indicating that the participants perceive the sizes of objects based on the size of the avatar, only in the case of a highly realistic hand. Our findings indicate that the more realistic the avatar, the stronger is the sense of embodiment including body ownership, which fosters scaling the size of objects using the size of the body as a fundamental metric. This provides evidence that self-avatar appearances affect how we perceive not only virtual bodies but also virtual spaces.", "keywords": ["Avatars", "Estimation", "Visualization", "Measurement", "Three-dimensional displays", "Haptic interfaces", "avatars", "visual perception", "virtual bodies", "virtual spaces", "body-based scaling", "embodied avatar influence", "virtual environment", "virtual reality", "self-body perception", "self-avatar realism", "nonbody objects", "perceived size", "virtual hand realism", "object size perception", "Human-centered computing\u2014Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798040", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798040", "paper_url": "https://ieeexplore.ieee.org/document/8798040"}, {"title": "Camera-Based Selection with Cardboard HMDs", "authors": ["Siqi Luo", "Robert J. Teather"], "abstract": "We present a study of selection techniques for low-cost mobile VR devices, such as Google Cardboard, using the outward facing camera on modern smartphones. We compared three selection techniques, air touch, head ray, and finger ray. Initial evaluation indicates that hand-based selection (air touch) was the worst. A ray cast using the tracked finger position offered much higher selection performance. Our results suggest that camera-based mobile tracking is feasible with ray-based techniques.", "keywords": ["Three-dimensional displays", "Cameras", "Mobile handsets", "Task analysis", "Performance evaluation", "Virtual environments", "Two dimensional displays", "cameras", "helmet mounted displays", "smart phones", "virtual reality", "camera-based selection", "selection techniques", "low-cost mobile VR devices", "Google Cardboard", "outward facing camera", "modern smartphones", "air touch", "head ray", "finger ray", "hand-based selection", "tracked finger position", "camera-based mobile tracking", "ray-based techniques", "selection performance", "cardboard HMDs", "smartphones", "ray cast", "Mobile VR", "selection", "Google Cardboard", "Human-centered computing \u2192 Virtual Reality", "Human-centered computing \u2192 Pointing"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797772", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797772", "paper_url": "https://ieeexplore.ieee.org/document/8797772"}, {"title": "Soft Hand Simulation for Smooth and Robust Natural Interaction", "authors": ["Mickeal Verschoor", "Daniel Lobo", "Miguel A. Otaduy"], "abstract": "Natural hand-based interaction should feature hand motion that adapts smoothly to the tracked user's motion, reacts robustly to contact with objects in a virtual environment, and enables dexterous manipulation of these objects. In our work, we enable all these properties thanks to an efficient soft hand simulation model. This model integrates an articulated skeleton, nonlinear soft tissue and frictional contact, to provide the realism necessary for natural interaction. Robust and smooth interaction is made possible by simulating in a single energy minimization framework all the mechanical energy exchanges among elements of the hand: coupling between the hand's skeleton and the user's motion, constraints at skeletal joints, nonlinear soft skin deformation, coupling between the hand's skeleton and the soft skin, frictional contact between the skin and virtual objects, and coupling between a grasped object and other virtual objects. We have put our effort on describing all elements of the hand that provide for realism and natural interaction, while ensuring minimal and bounded computational cost, which is key for smooth and robust interaction. As a result, we accomplish hand simulation as an asset that can be connected to diverse input tracking devices, and seamlessly integrated in game engines for fast deployment in VR applications.", "keywords": ["Skin", "Skeleton", "Computational modeling", "Couplings", "Robustness", "Strain", "Tracking", "dexterous manipulators", "gesture recognition", "human computer interaction", "image motion analysis", "skin", "virtual reality", "natural hand-based interaction", "virtual environment", "dexterous manipulation", "articulated skeleton", "nonlinear soft tissue", "frictional contact", "smooth interaction", "single energy minimization framework", "nonlinear soft skin deformation", "virtual objects", "grasped object", "mechanical energy", "feature hand motion", "tracked user motion", "soft hand simulation model", "hand skeleton", "skeletal joints", "game engines", "VR applications", "Hand Interaction", "VR", "Simulation: Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality", "Human-centered computing-Human computer interaction (HCI)-Interaction techniques"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8447555", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8447555", "paper_url": "https://ieeexplore.ieee.org/document/8447555"}, {"title": "Batmen \u2014 Hybrid collaborative object manipulation using mobile devices", "authors": ["Marcio Cabral", "Gabriel Roque", "Mario Nagamura", "Andre Montes", "Eduardo Zilles Borba", "Celso Kurashima", "Marcelo Knorich Zuffo"], "abstract": "In this work we present an interactive and collaborative 3D object manipulation system using the shelf mobile devices coupled with Augmented Reality (AR) technology that allows multiple users to collaborate concurrently on a scene. Each user interested in participating in this collaboration uses both a mobile device running android and a desktop (or laptop) working in tandem. The 3D scene was visualized by the user in the desktop system. The changes in the scene viewpoint changes and the object manipulations were performed using a mobile device through the AR. The system leverages user's knowledge of common tasks performed on current mobile devices such as pinching for zooming in and out; swiping with one or two fingers for object rotation and press-and-hold for 2 seconds for object translation. As you will see in this video, we built a prototype system (in a maze style) and applied an informal user study with three experienced VR researchers. Users had to carry a 3D cube through three square rings along the maze. In resume, we diagnosed that working in a collaborative way (users A and B) was better and easier than individual one (user C). We registered more than 2 minutes late for the individual experience comparing to the teamwork. It may happen because the two player team shared information, functions and had a multi-perspective view during the task.", "keywords": ["Interaction techniques", "collaborative environment", "hybrid reality", "object manipulation", "computer graphics", "user experience"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 0, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504787", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504787", "paper_url": "https://ieeexplore.ieee.org/document/7504787"}, {"title": "Automatic generation of world in miniatures for realistic architectural immersive virtual environments", "authors": ["Andrea Bonsch", "Sebastian Freitag", "Torsten W. Kuhlen"], "abstract": "Orientation and wayfinding in architectural Immersive Virtual Environments (IVEs) are non-trivial, accompanying tasks which generally support the users' main task. World in Miniatures (WIMs) - essentially 3D maps containing a scene replica - are an established approach to gain survey knowledge about the virtual world, as well as information about the user's relation to it. However, for large-scale, information-rich scenes, scaling and occlusion issues result in diminishing returns. Since there typically is a lack of standardized information regarding scene decompositions, presenting the inside of self-contained scene extracts is challenging. Therefore, we present an automatic WIM generation workflow for arbitrary, realistic in- and outdoor IVEs in order to support users with meaningfully selected and scaled extracts of the IVE as well as corresponding context information. Additionally, a 3D user interface is provided to manually manipulate the represented extract.", "keywords": ["Three-dimensional displays", "Context", "Data mining", "Solid modeling", "Buildings", "Runtime", "Geometry", "graphical user interfaces", "virtual reality", "3D user interface", "indoor IVE", "outdoor IVE", "automatic WIM generation workflow", "self-contained scene extraction", "scene decompositions", "large-scale scenes", "information-rich scenes", "3D maps", "automatic world in miniature generation", "realistic architectural immersive virtual environments", "I.3.6 [Computer Graphics]: Methodology and Techniques \u2014 Interaction Techniques"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 2, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504700", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504700", "paper_url": "https://ieeexplore.ieee.org/document/7504700"}, {"title": "Entropy of Controller Movements Reflects Mental Workload in Virtual Reality", "authors": ["Daniel Reinhardt", "Steffen Haesler", "J\u00f6rn Hurtienne", "Carolin Wienrich"], "abstract": "Virtual Reality can impose cognitive demands on users and influence their task performance. These cognitive demands, however, have been difficult to measure precisely without inducing breaks of presence. Based on findings in psychological science on how motion trajectories reflect underlying cognitive processes, we investigated entropy (i.e. the degree of movement irregularity) as an unobtrusive measure of mental workload. Entropy values were obtained from a time-series history of controller movement data. Mental workload is considered high over a given time interval, when the measured entropy is high as well. By manipulating the difficulty of a simple rhythm game we could show that the results are comparable to the results of the NASA-TLX questionnaire, which is currently used as the gold standard in VR for measuring mental workload. Thus, our results pave the way for further investigating the entropy of controller movements as a precise measurement of mental workload in VR.", "keywords": ["Task analysis", "Entropy", "Physiology", "Games", "Current measurement", "Atmospheric measurements", "Particle measurements", "cognition", "cognitive systems", "computer games", "entropy", "human computer interaction", "human factors", "psychology", "virtual reality", "controller movement data", "mental workload", "measured entropy", "controller movements", "precise measurement", "cognitive demands", "cognitive processes", "movement irregularity", "unobtrusive measure", "entropy values", "virtual reality", "NASA-TLX questionnaire", "psychological science", "Sample Entropy", "entropy of controller movements", "virtual reality", "non-intrusive measure", "evaluation method", "mental workload", "H.5.2 [Information Interfaces and Presentation (e.g., HCI)]: User Interfaces\u2014Evaluation\\methodology"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797977", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797977", "paper_url": "https://ieeexplore.ieee.org/document/8797977"}, {"title": "Design of collaborative 3D user interfaces for virtual and augmented reality", "authors": ["Jeronimo G Grandi"], "abstract": "We explore design approaches for cooperative work in virtual manipulation tasks. We seek to understand the fundamental aspects of the human cooperation and design interfaces and manipulation actions to enhance the group's ability to solve complex manipulation tasks in various immersion scenarios.", "keywords": ["Three-dimensional displays", "Collaboration", "User interfaces", "Virtual environments", "Visualization", "Augmented reality", "augmented reality", "groupware", "user interfaces", "collaborative 3D user interfaces", "augmented reality", "virtual manipulation tasks", "virtual reality", "H.5.2. [Information Interfaces and Presentation]: User Interfaces \u2014 Input devices and strategies", "H.5.3. [Information Interfaces and Presentation]: Group and Organization Interfaces \u2014 Computer-supported cooperative work"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 1, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892355", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892355", "paper_url": "https://ieeexplore.ieee.org/document/7892355"}, {"title": "FAAST: The Flexible Action and Articulated Skeleton Toolkit", "authors": ["Evan A. Suma", "Belinda Lange", "Albert Skip Rizzo", "David M. Krum", "Mark Bolas"], "abstract": "The Flexible Action and Articulated Skeleton Toolkit (FAAST) is middleware to facilitate integration of full-body control with virtual reality applications and video games using OpenNI-compliant depth sensors (currently the PrimeSensor and the Microsoft Kinect). FAAST incorporates a VRPN server for streaming the user's skeleton joints over a network, which provides a convenient interface for custom virtual reality applications and games. This body pose information can be used for goals such as realistically puppeting a virtual avatar or controlling an on-screen mouse cursor. Additionally, the toolkit also provides a configurable input emulator that detects human actions and binds them to virtual mouse and keyboard commands, which are sent to the actively selected window. Thus, FAAST can enable natural interaction for existing off-the-shelf video games that were not explicitly developed to support input from motion sensors. The actions and input bindings are configurable at run-time, allowing the user to customize the controls and sensitivity to adjust for individual body types and preferences. In the future, we plan to substantially expand FAAST's action lexicon, provide support for recording and training custom gestures, and incorporate real-time head tracking using computer vision techniques.", "keywords": ["Sensors", "Games", "Virtual reality", "Mice", "Joints", "Software", "avatars", "computer games", "computer vision", "middleware", "FAAST", "flexible action", "articulated skeleton toolkit", "middleware", "virtual reality applications", "OpenNI-compliant depth sensors", "VRPN server", "realistic puppeting", "virtual avatar", "off-the-shelf video games", "configurable input emulator", "computer vision techniques", "depth-sensing cameras", "gestures", "video games", "middleware"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 98, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759491", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759491", "paper_url": "https://ieeexplore.ieee.org/document/5759491"}, {"title": "Using relative head and hand-target features to predict intention in 3D moving-target selection", "authors": ["Juan Sebastian Casallas", "James H. Oliver", "Jonathan W. Kelly", "Frederic Merienne", "Samir Garbaya"], "abstract": "Selection of moving targets is a common, yet complex task in human-computer interaction (HCI) and virtual reality (VR). Predicting user intention may be beneficial to address the challenges inherent in interaction techniques for moving-target selection. This article extends previous models by integrating relative head-target and hand-target features to predict intended moving targets. The features are calculated in a time window ending at roughly two-thirds of the total target selection time and evaluated using decision trees. With two targets, this model is able to predict user choice with up to ~ 72% accuracy on general moving-target selection tasks and up to ~ 78% by also including task-related target properties.", "keywords": ["Accuracy", "Predictive models", "Decision trees", "Three-dimensional displays", "Solid modeling", "Virtual reality", "Human computer interaction", "decision trees", "human computer interaction", "virtual reality", "3D moving-target selection", "human-computer interaction", "HCI", "virtual reality", "VR", "user intention prediction", "decision trees", "user choice prediction", "task-related target properties", "relative hand-target features", "relative head-target features", "H.5.2 [Information interfaces and presentation]: User Interfaces \u2014 Interaction Styles, Theory and methods", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Virtual Reality", "I.5.4 [Pattern Recognition]: Applications"], "published_in": "2014 IEEE Virtual Reality (VR)", "publication_date": "March 2014", "citations": 0, "isbn": ["978-1-4799-2871-2"], "doi": "10.1109/VR.2014.6802050", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6802050", "paper_url": "https://ieeexplore.ieee.org/document/6802050"}, {"title": "A comparison of methods for navigation and wayfinding in large virtual environments using walking", "authors": ["Richard A. Paris", "Timothy P. Mcnamara", "John J. Rieser", "Bobby Bodenheimer"], "abstract": "Interesting virtual environments that permit free exploration are rarely small. A number of techniques have been developed to allow people to walk in larger virtual spaces than permitted by physical extent of the virtual reality hardware, and in this paper we compare three such methods in terms of how they affect presence and spatial awareness. In our first psychophysical study, we compared two methods of reorientation and one method of redirected walking on subjects' presence and spatial memory while navigating a pre-specified path. We further compared the two reorientation methods in a second psychophysical study involving free exploration and navigation in a large virtual environment. Our results provide criteria by which the choice of a locomotion method for navigating large virtual environments may be selected.", "keywords": ["Legged locomotion", "Navigation", "Virtual environments", "Tracking", "Turning", "Extraterrestrial measurements", "Layout", "gait analysis", "human factors", "virtual reality", "wayfinding", "virtual environment navigation", "virtual reality hardware", "spatial awareness", "psychophysical study", "reorientation methods", "redirected walking method", "subjects presence", "spatial memory", "free exploration", "locomotion method", "Presence", "Spatial Awareness", "Reorientation", "Redirected Walking"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 1, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892276", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892276", "paper_url": "https://ieeexplore.ieee.org/document/7892276"}, {"title": "Underwater Manipulation Training Simulation System for Manned DeepSubmarine Vehicle", "authors": ["Xiaoxi Zhang", "Yong Yin", "Feifei Wan"], "abstract": "In order to solve the problems of high safety risk and low training efficiency in underwater operation of deep submersible vehicle. Taking China's first manned deep submarine vehicle \u201cjiaolong\u201d as the simulation prototype, an underwater training platform is developed, which is not limited by time and place. Using three-dimensional modeling technology to build the three-dimensional model of \u201cjiaolong\u201d A mathematical model of the motion of a manipulator is established to simulate the associated motion between the joints of the manipulator. The collision detection technique is used to determine whether there is interaction between the manipulator and the operated object, between the object and the sampling basket, and between the equipment and the scene. The system is based on 3D development engine, which includes cobalt-rich crust mining area, polymetallic sulphide area, shallow sea area and cold spring area and so on. With this system the operator can train the underwater operation process. The system uses virtual reality helmet as the final visual display mode and the line of sight collision detection based on helmet instead of mouse click to simulate the whole process of underwater operation of deep submersible.", "keywords": ["Solid modeling", "Manipulators", "Underwater vehicles", "Training", "Mathematical model", "Biological system modeling", "Three-dimensional displays", "cobalt", "computer based training", "manipulators", "mining", "virtual reality", "sight collision detection", "manipulation training simulation system", "high safety risk", "low training efficiency", "deep submersible vehicle", "China's first manned deep submarine vehicle", "simulation prototype", "underwater training platform", "three-dimensional modeling technology", "mathematical model", "manipulator", "associated motion", "collision detection technique", "operated object", "3D development engine", "cobalt-rich crust mining area", "polymetallic sulphide area", "shallow sea area", "cold spring area", "underwater operation process", "manned deep submersible vehicle", "immersive", "underwater operation", "motion model", "interaction"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797838", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797838", "paper_url": "https://ieeexplore.ieee.org/document/8797838"}, {"title": "Desk-sized immersive workplace using force feedback grid interface", "authors": ["T. Yamada", "D. Tsubouchi", "T. Ogi", "M. Hirose"], "abstract": "When a 3D object is designed in a virtual world, it is essential that the designer can observe and manipulate the object directly in 3D space. The aim of our study is to develop a virtual workplace in which 3D virtual objects can be observed from various directions and can be touched directly by utilising force feedback. In this paper, we propose a 3D modelling system that consists of an inclined three-screen display and a force feedback device. Using this system, we propose a force feedback grid that extends the concept of 2D CAD manipulation to a grid that can be used in a 3D space, and which integrates with force feedback in order to support modelling in the 3D space. Finally, we report the results of an experiment that we conducted in order to verify the effectiveness of the force feedback grid.", "keywords": ["Employment", "Force feedback", "Mirrors", "Three dimensional displays", "Space technology", "Large screen displays", "Computer displays", "Head", "Imaging phantoms", "Computer graphics", "force feedback", "virtual reality", "haptic interfaces", "interactive devices", "CAD", "solid modelling", "engineering graphics", "desk-sized immersive workplace", "force-feedback grid interface", "3D object design", "virtual world", "virtual object manipulation", "virtual workplace", "3D virtual object observation directions", "touching", "force feedback device", "3D modelling system", "inclined 3-screen display", "CAD manipulation"], "published_in": "Proceedings IEEE Virtual Reality 2002", "publication_date": "2002", "citations": 9, "isbn": ["0-7695-1492-8"], "doi": "10.1109/VR.2002.996516", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=996516", "paper_url": "https://ieeexplore.ieee.org/document/996516"}, {"title": "Automatic Generation of Dynamically Relightable Virtual Objects with Consumer-Grade Depth Cameras", "authors": ["Chih-Fan Chen", "Evan Suma Rosenberg"], "abstract": "This research demo showcases the results of novel approach for estimating the illumination and reflectance properties of virtual objects captured using consumer-grade RGB-D cameras. This method is implemented within a fully automatic content creation pipeline that generates photo realistic objects in real-time virtual reality scenes with dynamic lighting. The geometry of the target object is first reconstructed from depth images captured using a handheld camera. To get nearly drift-free texture maps of the virtual object, a set of selected images from the original color stream is used for camera pose optimization. Our approach further separates these images into diffuse (view-independent) and specular (view-dependent) components using low-rank decomposition. The lighting conditions during capture and reflectance properties of the virtual object are subsequently estimated from the specular maps. By combining these parameters with the diffuse texture, reconstructed objects are then rendered in a real-time virtual reality demo that plausibly replicates the real world illumination and showcases dynamic lighting with varying direction, intensity, and color.", "keywords": ["Lighting", "Image reconstruction", "Image color analysis", "Cameras", "Real-time systems", "Virtual environments", "cameras", "image capture", "image colour analysis", "image reconstruction", "image sensors", "image texture", "lighting", "optimisation", "pose estimation", "realistic images", "rendering (computer graphics)", "solid modelling", "virtual reality", "automatic generation", "dynamically relightable virtual objects", "consumer-grade depth cameras", "reflectance properties", "virtual object", "consumer-grade RGB-D cameras", "fully automatic content creation pipeline", "photo realistic objects", "real-time virtual reality scenes", "target object", "depth images", "handheld camera", "drift-free texture maps", "reconstructed objects", "real-time virtual reality demo", "dynamic lighting", "specular maps", "Computing methodologies", "Computer graphics", "Graphics systems and interfaces", "Virtual reality", "Rendering", "Reflectance modeling"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798037", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798037", "paper_url": "https://ieeexplore.ieee.org/document/8798037"}, {"title": "Simultaneous mapping and redirected walking for ad hoc free walking in virtual environments", "authors": ["Thomas Nescher", "Markus Zank", "Andreas Kunz"], "abstract": "Providing real walking in virtual environments remains a challenge because of space, setup, and tracking system requirements. With the help of redirected walking (RDW), natural and unconstrained walking in virtual environments has become possible - without using mechanical locomotion devices - by manipulating the user's real world trajectory such that he remains within the boundaries of the walkable space. Nevertheless, typical home or office environments do not provide sufficient space or a suitable room which is free of obstacles. This paper presents an approach that combines RDW with a low-cost and user-worn tracking approach based on simultaneous localization and mapping (SLAM). I.e. learning the environment with the walkable area, tracking the user's viewpoint, and RDW is done on the fly without any prior setup, without preparing a room, and without setting up a tracking system. This allows ad hoc free walking in virtual environments even within dynamic and cluttered physical rooms, where the walkable area is of arbitrary shape. Furthermore, by combining SLAM and a planning RDW controller, this approach has the potential to provide the best free walking experience for any given physical environment.", "keywords": ["Legged locomotion", "Simultaneous localization and mapping", "Virtual environments", "Cameras", "Three-dimensional displays", "human computer interaction", "SLAM (robots)", "virtual reality", "redirected walking", "user-worn tracking", "simultaneous localization and mapping", "ad hoc free walking", "virtual environments", "SLAM", "planning RDW controller", "SLAM", "redirected walking", "tracking", "head tracking", "virtual reality", "locomotion", "walkable area", "obstacle avoidance"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 9, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504742", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504742", "paper_url": "https://ieeexplore.ieee.org/document/7504742"}, {"title": "Estimation of detection thresholds for audiovisual rotation gains", "authors": ["Niels Christian Nilsson", "Evan Suma", "Rolf Nordahl", "Mark Bolas", "Stefania Serafin"], "abstract": "Redirection techniques allow users to explore large virtual environments on foot while remaining within a limited physical space. However, research has primarily focused on redirection through manipulation of the visuals used to represent the virtual environment. We describe a within-subjects study (n=31) exploring if participants' ability to detect differences between real and virtual rotations is influenced by the addition of sound that is spatially aligned with its virtual source. The results revealed similar detection thresholds for conditions involving moving audio, static audio, and no audio. This may be viewed as an indication of visual dominance during scenarios such as the one used for the current study.", "keywords": ["Visualization", "Virtual environments", "Electronic mail", "Estimation", "Legged locomotion", "audio-visual systems", "computer graphics", "object detection", "virtual reality", "visual dominance indication", "virtual source", "virtual environments", "redirection techniques", "audiovisual rotation gains", "detection threshold estimation", "H.1.2 [Information Systems]: User/Machine Systems \u2014 Human factors", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Virtual Reality"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 8, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504743", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504743", "paper_url": "https://ieeexplore.ieee.org/document/7504743"}, {"title": "Self-characterstics and sound in immersive virtual reality \u2014 Estimating avatar weight from footstep sounds", "authors": ["Erik Sikstr\u00f6m", "Amalia De G\u00f6tzen", "Stefania Serafin"], "abstract": "This experiment aimed to investigate whether a user controlling a full body avatar via real time motion tracking in an immersive virtual reality setup, would estimate the weight of the virtual avatar differently if the footstep sounds are manipulated using three different audio filter settings. The visual appearance of the avatar was available in two sizes. The subjects performed six walks with each audio configuration active once over two ground types. After completing each walk, the participants were asked to estimate the weight of the virtual avatar and the suitability of the audio feedback. The results indicate that the filters amplifying the two lower center frequencies altered the subjects estimates of the weight of the avatar body towards being heavier than when compared with the filter with the higher center frequency. There were no significant differences between the weight estimates of the two groups using the different avatar bodies.", "keywords": ["Avatars", "Legged locomotion", "Context", "Concrete", "Frequency estimation", "Tracking", "audio signal processing", "avatars", "hearing", "immersive virtual reality", "avatar weight estimation", "footstep sounds", "full body avatar", "real time motion tracking", "audio filter settings", "avatar visual appearance", "audio configuration", "virtual avatar", "audio feedback", "H.5.1 [Multimedia Information Systems]: Artificial", "virtual realities \u2014 Audio output"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 2, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223406", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223406", "paper_url": "https://ieeexplore.ieee.org/document/7223406"}, {"title": "Haptic Hybrid Rotations: Overcoming Hardware Angular Limitations of Force-Feedback Devices", "authors": ["L. Dominjon", "S. Richir", "A. Lecuyer", "J.-M. Burkhardt"], "abstract": "This paper describes a new interaction technique called haptic hybrid rotations to overcome the physical angular limitations of force-feedback devices when manipulating virtual objects. This technique is based on a hybrid control of the object manipulated with the device. When approaching the angular mechanical stops of the device, the control mode switches from angular position-control to rate-control. The forcefeedback of the device is used to simulate the use of an elastic device in the rate-control mode. An experiment was carried out to compare this technique with two other common alternatives that are used when manipulating virtual objects with force-feedback devices: rotations scaling, and the clutching technique. Our results showed that haptic hybrid rotations were both the fastest and the most appreciated technique for the proposed experiment.", "keywords": ["Haptic interfaces", "Hardware", "Virtual reality", "Laboratories", "Force control", "Switches", "Chromium", "User interfaces", "User centered design", "Multimedia systems", "interaction technique", "hybrid control", "position/rate control.", "interaction technique", "hybrid control", "position/rate control."], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 3, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.68", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667641", "paper_url": "https://ieeexplore.ieee.org/document/1667641"}, {"title": "The Perceptive Workbench: toward spontaneous and natural interaction in semi-immersive virtual environments", "authors": ["B. Leibe", "T. Starner", "W. Ribarsky", "Z. Wartell", "D. Krum", "B. Singletary", "L. Hodges"], "abstract": "The Perceptive Workbench enables a spontaneous, natural and unimpeded interface between the physical and virtual worlds. It uses vision-based methods for interaction that eliminate the need for wired input devices and wired tracking. Objects are recognized and tracked when placed on the display surface. Through the use of multiple light sources, the object's 3D shape can be captured and inserted into the virtual interface. This ability permits spontaneity since either preloaded objects or those objects selected on-the-spot by the user can become physical icons. Integrated into the same vision-based interface is the ability to identify 3D hand position, pointing direction and sweeping arm gestures. Such gestures can enhance selection, manipulation and navigation tasks. In this paper, the Perceptive Workbench is used for augmented reality gaming and terrain navigation applications, which demonstrate the utility and capability of the interface.", "keywords": ["Wires", "Shape", "Navigation", "Virtual environment", "Augmented reality", "Fingers", "Surface reconstruction", "Displays", "Light sources", "Application software", "augmented reality", "tracking", "object recognition", "light sources", "computer vision", "interactive devices", "computer games", "navigation", "gesture recognition", "Perceptive Workbench", "spontaneous natural interaction", "semi-immersive virtual environments", "unimpeded interface", "virtual world", "vision-based interaction methods", "object recognition", "object tracking", "object placement", "display surface", "multiple light sources", "3D shape capture", "virtual interface", "preloaded objects", "physical icons", "vision-based interface", "3D hand position", "pointing direction", "sweeping arm gestures", "object selection", "object manipulation", "navigation tasks", "augmented reality", "computer games", "terrain navigation applications"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 18, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840358", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840358", "paper_url": "https://ieeexplore.ieee.org/document/840358"}, {"title": "Exploration of an EEG-Based Cognitively Adaptive Training System in Virtual Reality", "authors": ["Arindam Dey", "Alex Chatburn", "Mark Billinghurst"], "abstract": "Virtual Reality (VR) is effective in various training scenarios across multiple domains, such as education, health and defense. However, most of those applications are not adaptive to the real-time cognitive or subjectively experienced load placed on the trainee. In this paper, we explore a cognitively adaptive training system based on real-time measurement of task related alpha activity in the brain. This measurement was made by a 32-channel mobile Electroencephalography (EEG) system, and was used to adapt the task difficulty to an ideal level which challenged our participants, and thus theoretically induces the best level of performance gains as a result of training. Our system required participants to select target objects in VR and the complexity of the task adapted to the alpha activity in the brain. A total of 14 participants undertook our training and completed 20 levels of increasing complexity. Our study identified significant differences in brain activity in response to increasing levels of task complexity, but response time did not alter as a function of task difficulty. Collectively, we interpret this to indicate the brain's ability to compensate for higher task load without affecting behaviourally measured visuomotor performance.", "keywords": ["Training", "Electroencephalography", "Task analysis", "Adaptive systems", "Real-time systems", "Frequency measurement", "Virtual reality", "bioelectric potentials", "cognition", "electroencephalography", "medical signal processing", "neurophysiology", "virtual reality", "EEG-based cognitively adaptive training system", "real-time cognitive measurement", "32-channel mobile electroencephalography system", "virtual reality", "brain activity", "task related alpha activity", "H.1.2 [Models and Principles]: User/Machine Systems\u2014Human Factors", "H.5.1 [Multimedia Information Systems]: Artificial\u2014Augmented and Virtual Realities", "Virtual Reality", "Cognitively Adaptive Training", "Electroencephalography", "Alpha Activity"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797840", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797840", "paper_url": "https://ieeexplore.ieee.org/document/8797840"}, {"title": "VR as a Content Creation Tool for Movie Previsualisation", "authors": ["Quentin Galvane", "I-Sheng Lin", "Fernando Argelaguet", "Tsai-Yen Li", "Marc Christie"], "abstract": "Creatives in animation and film productions have forever been exploring the use of new means to prototype their visual sequences before realizing them, by relying on hand-drawn storyboards, physical mockups or more recently 3D modelling and animation tools. However these 3D tools are designed in mind for dedicated animators rather than creatives such as film directors or directors of photography and remain complex to control and master. In this paper we propose a VR authoring system which provides intuitive ways of crafting visual sequences, both for expert animators and expert creatives in the animation and film industry. The proposed system is designed to reflect the traditional process through (i) a storyboarding mode that enables rapid creation of annotated still images, (ii) a previsualisation mode that enables the animation of the characters, objects and cameras, and (iii) a technical mode that enables the placement and animation of complex camera rigs (such as cameras cranes) and light rigs. Our methodology strongly relies on the benefits of VR manipulations to re-think how content creation can be performed in this specific context, typically how to animate contents in space and time. As a result, the proposed system is complimentary to existing tools, and provides a seamless back-and-forth process between all stages of previsualisation. We evaluated the tool with professional users to gather experts' perspectives on the specific benefits of VR in 3D content creation.", "keywords": ["Three-dimensional displays", "Cameras", "Tools", "Animation", "Motion pictures", "Layout", "Visualization", "authoring systems", "cameras", "cinematography", "computer animation", "data visualisation", "Internet", "solid modelling", "statistical analysis", "user interfaces", "virtual reality", "movie previsualisation", "film productions", "visual sequences", "hand-drawn storyboards", "3D modelling", "dedicated animators", "film directors", "VR authoring system", "expert animators", "expert creatives", "film industry", "storyboarding mode", "previsualisation mode", "complex camera rigs", "VR manipulations", "3D content creation tool", "Human-centered computing\u2014Visualization\u2014Visualization techniques\u2014Treemaps", "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798181", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798181", "paper_url": "https://ieeexplore.ieee.org/document/8798181"}, {"title": "The effect of environment characteristics and user interaction on levels of virtual environment sickness", "authors": ["R.A. Ruddle"], "abstract": "Data are reported for symptoms of virtual environment (VE) sickness that arose in 10 behavioral experiments. In total, 134 participants took part in the experiments and were immersed in VEs for approximately 150 hours. Nineteen of the participants reported major symptoms and two were physically sick. The tasks that participants ' performed ranged from manipulating virtual objects that they \"held\" in their hands, to traveling distances of 10 km or more while navigating virtual mazes. The data are interpreted within a framework provided by the virtual environment description and classification system. Environmental dimensions and visual complexity had little effect on the severity of participants ' symptoms. Long periods of immersion tended to produce major ocular-motor symptoms. Nausea was affected by the type of movement made to control participants ' view, and was particularly severe when participants had to spend substantial amounts of time (3%) looking steeply downwards at their virtual feet. Contrary to expectations, large rapid movements had little effect on most participants, and neither did movements that were not under participants ' direct control.", "keywords": ["Virtual environment", "Large screen displays", "Virtual reality", "Navigation", "Biomedical monitoring", "Capacitive sensors", "Proposals", "Performance evaluation", "Motion measurement", "Time measurement", "virtual reality", "medical computing", "interactive systems", "human factors", "user interaction", "virtual objects", "virtual mazes navigation", "visual complexity", "ocular-motor symptoms", "environment characteristics", "virtual environment sickness", "behavioral experiments", "physical sickness", "nausea", "10 Km"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 12, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310067", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310067", "paper_url": "https://ieeexplore.ieee.org/document/1310067"}, {"title": "HOMERE: a multimodal system for visually impaired people to explore virtual environments", "authors": ["A. Lecuyer", "P. Mobuchon", "C. Megard", "J. Perret", "C. Andriot", "J.-P. Colinot"], "abstract": "The paper describes the HOMERE system: a multimodal system dedicated to visually impaired people to explore and navigate inside virtual environments. The system addresses three main applications: preparation for the visit of an existing site, training for the use of a blind cane, and ludic exploration of virtual worlds. The HOMERE system provides the user with different sensations when navigating inside a virtual world: a force feedback corresponding to the manipulation of a virtual blind cane, a thermal feedback corresponding to the simulation of a virtual sun, and an auditory feedback in spatialized conditions corresponding to the ambient atmosphere and specific events in the simulation. A visual feedback of the scene is also provided to enable sighted people to follow the navigation of the main user. HOMERE has been tested by several visually impaired people who were all confident about the potential of this prototype.", "keywords": ["Virtual environment", "Navigation", "Force feedback", "Atmospheric modeling", "Discrete event simulation", "Thermal force", "Sun", "Atmosphere", "Layout", "Testing", "handicapped aids", "virtual reality", "force feedback", "digital simulation", "HOMERE", "multimodal system", "visually impaired people", "virtual environments", "ludic exploration", "virtual worlds", "force feedback", "virtual blind cane manipulation", "thermal feedback", "virtual sun simulation", "auditory feedback", "ambient atmosphere", "visual feedback"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 24, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191147", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191147", "paper_url": "https://ieeexplore.ieee.org/document/1191147"}, {"title": "Elucidating Factors that can Facilitate Veridical Spatial Perception in Immersive Virtual Environments", "authors": ["W.B. Thompson", "J.E. Swan", "D. Proffitt", "J.K. Kearney", "V. Interrante", "W.B. Thompson", "J.E. Swan", "D. Proffitt", "J.K. Kearney", "V. Interrante"], "abstract": "Enabling veridical spatial perception in immersive virtual environments (IVEs) is an important yet elusive goal, as even the factors implicated in the often-reported phenomenon of apparent distance compression in HMD-based IVEs have yet to be satisfactorily elucidated. In recent experiments (Interrante et al., 2006), we have found that participants appear less prone to significantly underestimate egocentric distances in HMD-based IVEs, relative to in the real world, in the special case that they unambiguously know, through first-hand observation, that the presented virtual environment is a high fidelity 3D model of their concurrently occupied real environment. We had hypothesized that this increased veridicality might be due to participants having a stronger sensation of 'presence' in the IVE under these conditions of co-location, which state of mind leads them to act on their visual input in the IVE similarly as they would in the real world (the presence hypothesis). However, alternative hypotheses are also possible. Primary among these is the visual calibration hypothesis: participants could be relying on metric information gleaned from their exposure to the real environment to calibrate their judgments of sizes and distances in the matched virtual environment. It is important to disambiguate between the presence and visual calibration hypotheses because they suggest different directions for efforts to facilitate veridical distance perception in general (non-co-located) IVEs. In this paper, we present the results of an experiment that seeks novel insight into this question. Using a mixed within- and between-subjects design, we compare participants' relative ability to accurately estimate egocentric distances in three different virtual environment models: one that is an identical match to the occupied real environment; one in which each of the walls in our virtual room model has been surreptitiously moved ~10% inward towards the center of the room; and one in which each of the walls has been surreptitiously moved ~10% outwards from the center of the room. If the visual calibration hypothesis holds, then we should expect to see a degradation in the accuracy of peoples' distance judgments in the surreptitiously modified models, manifested as an underestimation of distances when the IVE is actually larger than the real room and as an overestimation of distances when the IVE is smaller. However, what we found is that distances were significantly underestimated in the virtual environment relative to in the real world in each of the surreptitiously modified room environments, while remaining reasonably accurate (consistent with our previous findings) in the case of the faithfully size-matched room environment. In a post-test survey, participants in each of the three room size conditions reported equivalent subjective levels of presence and did not indicate any overt awareness of the room size manipulation", "keywords": ["Virtual environment", "Calibration", "Computer graphics", "Displays", "Computer science", "Computer architecture", "Degradation", "Chromium", "Virtual reality", "USA Councils", "augmented reality", "computer graphics", "visual perception", "spatial perception", "immersive virtual environments", "human perception", "3D spatial relationships", "augmented reality", "optical-motor information", "ocular-motor information", "egocentric distance perception", "immersive virtual environments"], "published_in": "2007 IEEE Virtual Reality Conference", "publication_date": "March 2007", "citations": 19, "isbn": ["1-4244-0905-5", "1-4244-0906-3"], "doi": "10.1109/VR.2007.352458", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4161000", "paper_url": "https://ieeexplore.ieee.org/document/4161000"}, {"title": "Perceiving mass in mixed reality through pseudo-haptic rendering of Newton's third law", "authors": ["Paul Issartel", "Florimond Gu\u00e9niat", "Sabine Coquillart", "Mehdi Ammi"], "abstract": "In mixed reality, real objects can be used to interact with virtual objects. However, unlike in the real world, real objects do not encounter any opposite reaction force when pushing against virtual objects. The lack of reaction force during manipulation prevents users from perceiving the mass of virtual objects. Although this could be addressed by equipping real objects with force-feedback devices, such a solution remains complex and impractical. In this work, we present a technique to produce an illusion of mass without any active force-feedback mechanism. This is achieved by simulating the effects of this reaction force in a purely visual way. A first study demonstrates that our technique indeed allows users to differentiate light virtual objects from heavy virtual objects. In addition, it shows that the illusion is immediately effective, with no prior training. In a second study, we measure the lowest mass difference (JND) that can be perceived with this technique. The effectiveness and ease of implementation of our solution provides an opportunity to enhance mixed reality interaction at no additional cost.", "keywords": ["Cloning", "Force", "Springs", "Visualization", "Virtual reality", "Haptic interfaces", "Damping", "force feedback", "haptic interfaces", "rendering (computer graphics)", "virtual reality", "mass perception", "pseudohaptic rendering", "Newton's third law", "virtual objects", "active force-feedback mechanism", "lowest mass difference", "JND", "mixed reality interaction", "Mass Perception", "Physically-Based Simulation", "Mixed Reality", "Pseudo-Haptics"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 5, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223322", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223322", "paper_url": "https://ieeexplore.ieee.org/document/7223322"}, {"title": "Remapped Physical-Virtual Interfaces with Bimanual Haptic Retargeting", "authors": ["Brandon J. Matthews", "Bruce H. Thomas", "Stewart Von Itzstein", "Ross T. Smith"], "abstract": "This paper proposes a novel interface for virtual reality in which physical interface components are mapped to multiple virtual counterparts using haptic retargeting illusions. This gives virtual reality interfaces the ability to have correct haptic sensations for many virtual buttons although in the physical space there is only one. This is a generic system that can be applied to areas including design, interaction tasks, product prototype development and interactive games in virtual reality. The system presented extends existing retargeting algorithms to support asymmetric bimanual interactions. A new warp technique, called interface warp, was developed to support remapped virtual reality user interfaces. Through an experimental user study, we explore the effects of bimanual retargeting and the interface warp technique on task response time, errors, presence, perceived manipulation compared to unimanual (single handed) retargeting and other existing warp techniques. The results demonstrated faster task response time and less errors for the interface warp technique and shows no significant effect of bimanual interactions.", "keywords": ["Haptic interfaces", "Virtual reality", "Task analysis", "User interfaces", "Mathematical model", "Visualization", "Shape", "haptic interfaces", "virtual reality", "physical-virtual interfaces", "bimanual haptic retargeting", "physical interface components", "haptic retargeting illusions", "virtual buttons", "asymmetric bimanual interactions", "virtual reality user interfaces", "interface warp technique", "virtual counterparts", "haptic sensations", "H.5.2 [Information Interfaces and Presentation]: User Interfaces\u2014Haptic I/O", "H.1.2 [Models and Principles]: User/Machine Systems\u2014Human Factors"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797974", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797974", "paper_url": "https://ieeexplore.ieee.org/document/8797974"}, {"title": "Inducing self-motion sensations in driving simulators using force-feedback and haptic motion", "authors": ["Guillaume Bouyer", "Amine Chellali", "Anatole L\u00e9cuyer"], "abstract": "Producing sensations of motion in driving simulators often requires using cumbersome and expensive motion platforms. In this article we present a novel and alternative approach for producing self-motion sensations in driving simulations by relying on haptic-feedback. The method consists in applying a force-feedback proportional to the acceleration of the virtual vehicle directly to the hands of the driver, by means of a haptic device attached to the manipulated controller (or a steering wheel). We designed a proof-of-concept based on a standard gamepad physically attached at the extremity of a standard 3DOF haptic display. Haptic effects were designed to match notably the acceleration/braking (longitudinal forces) and left/right turns (lateral forces) of the virtual vehicle. A preliminary study conducted with 23 participants, engaged in gamepad-based active VR navigations in a straight line, showed that haptic motion effects globally improved the involvement and realism of motion sensation for participants with prior experience with haptic devices. Taken together, our results suggest that our approach could be further tested and used in driving simulators in entertainment and/or professional contexts.", "keywords": ["Haptic interfaces", "Acceleration", "Three-dimensional displays", "Solid modeling", "Context", "Wheels", "Navigation", "computerised navigation", "digital simulation", "driver information systems", "force feedback", "haptic interfaces", "virtual reality", "driving simulators", "force feedback", "motion platforms", "self-motion sensations", "haptic feedback", "virtual vehicle", "standard gamepad", "standard 3DOF haptic display", "gamepad-based active VR navigations", "haptic motion effects", "Driving Simulation", "Self-motion", "Haptic", "Force-feedback"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 2, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892234", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892234", "paper_url": "https://ieeexplore.ieee.org/document/7892234"}, {"title": "Automatic 3D selection technique assignment using real-time scenario analysis", "authors": ["Jeffrey A. Cashion", "Chadwick Wingrave", "Joseph J. Laviola"], "abstract": "Selection in 3D virtual environments can vary wildly depending on the context of the selection. Scene attributes such as object velocity, scene density, and user's cursor velocity can impact the user's ability to accurately select an object. Many 3D selection techniques have been explored, and are usually optimal for a specific set of conditions. As a result, software developers must compromise by choosing a single technique that works well on average, but is lacking in at least one scenario. We present a preliminary study that explores the feasibility of new autoselection algorithms that automatically determines the most appropriate selection technique in real-time, thus leveraging the performance benefits of each technique. We evaluated the techniques across three levels of scene density and three levels of object velocity.", "keywords": ["Three-dimensional displays", "Algorithm design and analysis", "Virtual environments", "Educational institutions", "Software algorithms", "Accuracy", "Software", "virtual reality", "automatic 3D selection technique assignment", "real-time scenario analysis", "3D virtual environment", "scene attribute", "object velocity", "scene density", "cursor velocity", "autoselection algorithm", "Interaction techniques", "3D object selection", "dense and dynamic environments"], "published_in": "2013 IEEE Virtual Reality (VR)", "publication_date": "March 2013", "citations": 0, "isbn": ["978-1-4673-4796-9", "978-1-4673-4795-2"], "doi": "10.1109/VR.2013.6549383", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6549383", "paper_url": "https://ieeexplore.ieee.org/document/6549383"}, {"title": "3DTouch: A wearable 3D input device for 3D applications", "authors": ["Anh Nguyen", "Amy Banic"], "abstract": "3D applications appear in every corner of life in the current technology era. There is a need for an ubiquitous 3D input device that works with many different platforms, from head-mounted displays (HMDs) to mobile touch devices, 3DTVs, and even the Cave Automatic Virtual Environments. We present 3DTouch [1], a novel wearable 3D input device worn on the fingertip for 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D input device that is self-contained, mobile, and universally works across various 3D platforms. This video presents a working prototype of our solution, which is described in details in the paper [1]. Our approach relies on a relative positioning technique using an optical laser sensor (OPS) and a 9-DOF inertial measurement unit (IMU). The device employs touch input for the benefits of passive haptic feedback, and movement stability. On the other hand, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices. We propose a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. We envision that modular solutions like 3DTouch opens up a whole new design space for interaction techniques to further develop on.", "keywords": ["Three-dimensional displays", "Mobile communication", "Computer science", "Virtual environments", "Prototypes", "Optical feedback", "H.5.2 [Information interfaces and presentation]: User Interfaces \u2014 Graphical user interfaces \u2014 Input devices and strategies"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 3, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223451", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223451", "paper_url": "https://ieeexplore.ieee.org/document/7223451"}, {"title": "A virtual reality study on Santa Maria Crater on Mars", "authors": ["Jue Wang", "Keith J. Bennett"], "abstract": "This poster presents a study of virtual reality on selected sites on Mars. A Virtual Astronaut (VA) is created as an interactive virtual 3D environment for the planetary science community to support the Mars Exploration Rover (MER) mission. A prototype study was conducted based on orbital and Opportunity rover data covering Santa Maria Crater in Meridiani Planum. The prototype VA provides dynamic visual representations of the imaging, compositional, and mineralogical data for the rover operations. It lets one navigate through the scene and interact with the environment, such as viewing in-situ observations, feature measurement, and an animation of rover drives. The system is optimized based on a set of performance tests and user feedbacks.", "keywords": ["Mars", "Navigation", "Data models", "Random access memory", "Virtual reality", "Three-dimensional displays", "NASA", "astronomy computing", "Mars", "virtual reality", "mineralogical data", "compositional data", "imaging data", "dynamic visual representation", "Meridiani Planum", "Mars exploration rover", "MER mission", "planetary science community", "interactive virtual 3D environment", "virtual astronaut", "Mars", "Santa Maria Crater", "virtual reality study", "virtual astronaut", "Mars surface", "Unity", "data fusion", "navigation"], "published_in": "2013 IEEE Virtual Reality (VR)", "publication_date": "March 2013", "citations": 0, "isbn": ["978-1-4673-4796-9", "978-1-4673-4795-2"], "doi": "10.1109/VR.2013.6549384", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6549384", "paper_url": "https://ieeexplore.ieee.org/document/6549384"}, {"title": "Co-location and tactile feedback for 2D widget manipulation", "authors": ["A.J.F. Kok", "R. Van Liere"], "abstract": "This study investigated the effect of co-location and tactile feedback on 2D widget manipulation tasks in virtual environments. Task completion time and positioning accuracy during each task were measured for subjects under 4 situations (co-located vs no co-located and tactile feedback vs no tactile feedback). Performance results indicate that co-location and tactile feedback both significantly improve the performance of 2D widget manipulation in 3D virtual environments. Subjective results support these findings.", "keywords": ["Feedback", "Time measurement", "Virtual reality", "Timing", "Mathematics", "Computer science", "Virtual environment", "Space stations", "Mirrors", "Information systems", "virtual reality", "force feedback", "colocation", "tactile feedback", "2D widget manipulation", "virtual environments", "task completion time", "positioning accuracy"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 4, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310085", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310085", "paper_url": "https://ieeexplore.ieee.org/document/1310085"}, {"title": "I Got Your Point: An Investigation of Pointing Cues in a Spherical Fish Tank Virtual Reality Display", "authors": ["Fan Wu", "Qian Zhou", "Kyoungwon Seo", "Toshiro Kashiwagi", "Sidney Fels"], "abstract": "Pointing is a fundamental building block in human communication. While it is ubiquitous in our daily interactions within the real world, it is difficult to precisely interpret a virtual agent's pointing direction to the physical world, considering its complex and subtle gesture cues, such as the movements of the human hand and head. Fish Tank Virtual Reality (FTVR) display has the potential to provide accurate pointing cues as it creates a compelling 3D spatial effect by rendering perspective-corrected vision. In this paper, we conducted a study with pointing cues of three levels (Head-only, Hand-only, and Hand+Head) to evaluate how the head and hand gesture cues affect observers' performance in interpretation of where a virtual agent is pointing in a spherical FTVR display. The results showed that the hand gesture significantly helps people interpret the pointing both accurately and quickly for fine pointing (15\u00b0), with 19.4% higher accuracy and 1.42 seconds faster than the head cue. The combination of the head and hand yielded a small improvement on the accuracy (4.4%) with even slightly longer time (0.38 seconds) compared to the hand-only cue. However, for coarse pointing (30\u00b0), head cue appears to be sufficient with the accuracy of 90.2%. The result of this study provides guidelines on cues selection for designing pointing in the virtual environment.", "keywords": ["Three-dimensional displays", "Task analysis", "Virtual environments", "Fish", "Rendering (computer graphics)", "Observers", "computer displays", "gesture recognition", "rendering (computer graphics)", "virtual reality", "visual perception", "accurate pointing cues", "hand gesture cues", "virtual agent", "spherical FTVR display", "head cue", "hand-only cue", "coarse pointing", "cues selection", "virtual environment", "spherical fish tank virtual reality display", "fundamental building block", "human communication", "complex gesture cues", "subtle gesture cues", "3D spatial effect", "Human-Centered Computing", "Human computer interaction", "Interaction design", "Empirical studies in interaction design"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798063", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798063", "paper_url": "https://ieeexplore.ieee.org/document/8798063"}, {"title": "Simulation of Standard Control Actuators in Dynamic Virtual Environments", "authors": ["Frank Gommlich", "Guido Heumer", "Arnd Vitzthum", "Bernhard Jung"], "abstract": "Realistic behavior of control actuators is important for virtual proto-typing applications. We present a systematic approach for modeling such articulated components as described in the European Standard EN 894-3. Control actuators may have several rotational and translational degrees of freedom (DOFs), possibly with discrete lock states. During user interactions, information about the actuators' manipulation is collected and made available to the higher application layers in the form of interaction events. This allows for recording and playback of demonstrated manipulation sequences for many purposes, such as ergonomics evaluations involving virtual humans. The framework uses XML for declaration and is implemented using a freely available physics engine.", "keywords": ["Actuators", "Virtual environment", "Engines", "Virtual reality", "Physics", "Computer graphics", "Virtual prototyping", "Application software", "Ergonomics", "XML", "actuators", "control engineering computing", "manipulators", "virtual reality", "standard control actuators", "dynamic virtual environments", "virtual prototyping applications", "European Standard EN 894-3", "translational degrees of freedom", "DOF", "discrete lock states", "actuators manipulation", "manipulation sequences", "XML", "I.3.6 [Computer Graphics]: Methodology and Techniques\u00bfInteraction techniques", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u00bf[Virtual Reality]"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 2, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811049", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811049", "paper_url": "https://ieeexplore.ieee.org/document/4811049"}, {"title": "Biomechanical Parameters Under Curvature Gains and Bending Gains in Redirected Walking", "authors": ["Keigo Matsumoto", "Ayaka Yamada", "Anna Nakamura", "Yasushi Uchmura", "Keitaro Kawai", "Tomohiro Tanikawa"], "abstract": "In this study, we examined the effect of walking biomechanics, which occurs when the curvature of the walking path in virtual space is changed, while the actual walking path remains constant. Curvature gains and bending gains were used to change the virtual walking path. We found a significant difference in most biomechanical parameters when curvature manipulation and bending manipulation are applied compared with the case in which they are not applied. Some parameters were also suggested to depend on the visual sense or disagreement between the visual and other senses.", "keywords": ["Legged locomotion", "Biomechanics", "Visualization", "Electronic mail", "Education", "Virtual environments", "Cameras", "bending", "biomechanics", "gait analysis", "manipulators", "mobile robots", "virtual reality", "virtual walking path", "biomechanical parameters", "curvature manipulation", "bending manipulation", "curvature gains", "bending gains", "redirected walking", "walking biomechanics", "virtual space", "actual walking path", "Human-centered computing-Virtual reality", "Computing methodologies-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446062", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446062", "paper_url": "https://ieeexplore.ieee.org/document/8446062"}, {"title": "Adopting the Roll Manipulation for Redirected Walking", "authors": ["Tatsuki Yamamoto", "Keigo Matsumoto", "Takuji Narumi", "Tomohiro Tanikawa", "Michitaka Hirose"], "abstract": "The contribution of this paper is to propose a novel Redirected Walking (RDW) technique that adopts manipulation gain in the roll direction. RDW is a technique that enables users to explore a large Virtual Environment (VE) while walking within a physically limited space by manipulating their virtual vision. Thus far, studies have determined the detection thresholds for translation, rotation, and curvature gains, but the thresholds are limited in the yaw direction. In contrast, in other research areas, movements in the yaw and roll directions have sometimes been addressed at the same time. In the present study, we investigated the detection threshold of inclination gain in the roll direction. We applied an inclination gain that was increased gradually while the participants walked 3 meters straight ahead. The results showed that users can detect an inclination gain of 1.93\u00b0 on the left hand and 1.39\u00b0 on the right hand.", "keywords": ["Legged locomotion", "Electronic mail", "Virtual environments", "Visualization", "Foot", "Meters", "virtual reality", "roll manipulation", "RDW", "manipulation gain", "roll direction", "physically limited space", "virtual vision", "detection threshold", "inclination gain", "virtual environment", "redirected walking technique", "distance 3 m", "Human-centered computing-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446479", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446479", "paper_url": "https://ieeexplore.ieee.org/document/8446479"}, {"title": "Interactive cloth simulation in virtual environments", "authors": ["M. Keckeisen", "S.L. Stoev", "M. Feurer", "W. Strasser"], "abstract": "Current cloth simulation systems have become powerful enough to be used interactively, and applications relevant to textile industries like virtually tailoring real garments seem to be possible in the near future. We show that interactive cloth simulation can be improved by virtual reality techniques. We present the Virtual Dressmaker, an application for interactive assembly and physically based interactive simulation of cloth. In particular, the system allows us to select and drag parts of the clothes during the simulation. The proposed application consists of a VR-interface client that provides nearly live-size stereo projection combined with 6DOF interaction and a server for physically based cloth simulation. In order to verify the usability of our approach, we elaborate on usability studies, which show that our application allows fast, easy, and precise interaction compared to interaction with a traditional 3D-desktop application based on 2D input devices.", "keywords": ["Computational modeling", "Application software", "Clothing", "Assembly", "Virtual reality", "Computer simulation", "Textile industry", "Usability", "Computer graphics", "Manufacturing industries", "virtual reality", "digital simulation", "textile industry", "graphical user interfaces", "interactive cloth simulation", "virtual environments", "cloth simulation systems", "textile industries", "virtual tailoring", "real garments", "virtual reality techniques", "Virtual Dressmaker", "interactive assembly", "physically based interactive simulation", "VR-interface client", "nearly live-size stereo projection", "6DOF interaction", "physically based cloth simulation", "usability studies", "precise interaction"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 12, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191123", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191123", "paper_url": "https://ieeexplore.ieee.org/document/1191123"}, {"title": "Estimation of Detection Thresholds for Redirected Turning", "authors": ["Junya Mizutani", "Keigo Matsumoto", "Ryohei Nagao", "Takuji Narumi", "Tomohiro Tanikawa", "Michitaka Hirose"], "abstract": "Redirection makes it possible to walk around a vast virtual space in a limited real space while providing a natural walking sensation by applying a gain to the amount of movement in a real space. However, manipulating the walking path while keeping it and maintaining the naturalness of walking when turning at a corner cannot be achieved by the existing methods. To realize natural manipulation for turning at a corner, this study proposes novel \u201cturning gains\u201d, which refer to the increase in real and virtual turning degrees. The result of an experiment which aims to estimate the detection thresholds of turning gains indicated that when the turning radius is 0.5 m, discrimination is more difficult compared with the rotation gains (r = 0.0m).", "keywords": ["Turning", "Legged locomotion", "Resists", "Virtual environments", "Three-dimensional displays", "User interfaces", "virtual reality", "redirected turning", "natural walking sensation", "walking path", "natural manipulation", "turning gains", "virtual turning degrees", "turning radius", "rotation gains", "virtual space", "Human-centered computing", "Human computer interaction (HCI)", "Interaction paradigms", "Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797976", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797976", "paper_url": "https://ieeexplore.ieee.org/document/8797976"}, {"title": "High-definition digital display case with the image-based interaction", "authors": ["Yuki Ban", "Takashi Kajinami", "Takuji Narumi", "Tomohiro Tanikawa", "Michitaka Hirose"], "abstract": "This paper proposes a high-definition digital display case for manipulating a virtual exhibit that has linking mechanisms. This technique enhances the understanding of dynamic exhibits. It is difficult to construct interactive contents of dynamic virtual exhibits, because measuring the mechanism invokes the risk of an exhibit's deterioration, and it takes tremendous efforts to create a fine spun computer graphics (CG) model for mechanisms. Therefore, we propose an image-based interaction method that uses image-based rendering to construct interactive contents for dynamic virtual exhibits using the interpolation between exhibit pictures with a number of deformational conditions and viewpoints. Using this method, we construct a high-definition digital showcase and exhibit the interactive content at a museum to evaluate the availability of our system.", "keywords": ["Interpolation", "Solid modeling", "Computational modeling", "Rendering (computer graphics)", "Head", "Cameras", "museums", "rendering (computer graphics)", "virtual reality", "high-definition digital display case", "image-based interaction", "virtual exhibit", "linking mechanisms", "interactive contents", "computer graphics", "CG model", "image-based rendering", "museum", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities", "I.3.6 [Computer Graphics]: Methodology and Techniques \u2014 Interaction techniques"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 0, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223339", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223339", "paper_url": "https://ieeexplore.ieee.org/document/7223339"}, {"title": "Towards context-sensitive reorientation for real walking in virtual reality", "authors": ["Timofey Grechkin", "Mahdi Azmandian", "Mark Bolas", "Evan Suma"], "abstract": "Redirected walking techniques have been introduced to overcome physical limitations for natural locomotion in virtual reality. Although subtle perceptual manipulations are helpful to keep users within relatively small tracked spaces, it is inevitable that users will approach critical boundary limits. Current solutions to this problem involve breaks in presence by introducing distractors, or freezing the virtual world relative to the user's perspective. We propose an approach that integrates into the virtual world narrative to draw users' attention and to cause them to temporarily alter their course to avoid going off bounds. This method ties together unnoticeable translation, rotation, and curvature gains, efficiently reorienting the user while maintaining the user's sense of immersion. We also discuss how this new method can be effectively used in conjunction with other reorientation techniques.", "keywords": ["Virtual environments", "Target tracking", "Context", "Legged locomotion", "Visualization", "user interfaces", "virtual reality", "context-sensitive reorientation", "redirected walking techniques", "virtual reality", "perceptual manipulation", "virtual world", "user perspective", "translation gain", "rotation gain", "curvature gain", "user immersion sense", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities", "I.3.6 [Computer Graphics]: Methodology and Techniques \u2014 Interaction techniques", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Virtual reality"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 2, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223357", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223357", "paper_url": "https://ieeexplore.ieee.org/document/7223357"}, {"title": "Crossover Applications", "authors": ["Brian Wilke", "Jonathan Metzgar", "Keith Johnson", "Sudhanshu Semwal", "Bonnie Snyder", "Kachun Yu", "Dan Neafus"], "abstract": "VR applications provide an opportunity to study a variety of new applications. One of the focus areas of the media convergence, games and media integration (McGMI) program is to develop new media applications for the visually impaired population. We are particularly interested in developing applications which are at the same time interesting for the sighted population as well-hence the title-crossover applications. Bonnie Snyder, who has been working with the visually impaired population for more than twenty years, visited a group of students early in the Fall 2008 As many typical applications are geared toward sighted population, the cost of software and hardware systems tend to be a lot higher. In addition, several games, developed for primarily the sighted, provide minimal interaction for the blind. Although this issue remains a topic of discussion in both IEEE VR and ISMAR and related conferences, much more can be done. We used this as motivation and developed three applications for both the sighted and the visually impaired population (a) haptic chess program combines PHANTOM force feedback interaction with OpenAL audio; (b) Simple hand movement recognition on iPhone provides a hierarchical menu application; (c) Barnyard fun program uses interesting animal-sound feedback to facilitate spatial selection. In future, we expect to conduct testing of these applications in Denver Museum as possible.", "keywords": ["Virtual reality", "Application software", "Force feedback", "Convergence", "Costs", "Software systems", "Hardware", "Haptic interfaces", "Imaging phantoms", "Testing", "computer games", "handicapped aids", "virtual reality", "media convergence games and media integration", "title-crossover applications", "minimal interaction", "IEEE VR applications", "ISMAR", "haptic chess program", "PHANTOM force feedback interaction", "OpenAL audio", "Barnyard fun program", "iPhone", "Denver Museum", "Applications for visually impaired", "H.5 [Information interfaces and presentation]: (I.7).", "H.5.2 [User Interfaces Haptic I/O, Auditory (no-speech feedback]"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 4, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811068", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811068", "paper_url": "https://ieeexplore.ieee.org/document/4811068"}, {"title": "The effect of 3D widget representation and simulated surface constraints on interaction in virtual environments", "authors": ["R.W. Lindeman", "J.L. Sibert", "J.N. Templeman"], "abstract": "The paper reports empirical results from two studies of effective user interaction in immersive virtual environments. The use of 2D interaction techniques in 3D environments has received increased attention recently. We introduce two new concepts to the previous techniques: the use of 3D widget representations; and the imposition of simulated surface constraints. The studies were identical in terms of treatments, but differed in the tasks performed by subjects. In both studies, we compared the use of two-dimensional (2D) versus three-dimensional (3D) interface widget representations, as well as the effect of imposing simulated surface constraints on precise manipulation tasks. The first study entailed a drag-and-drop task, while the second study looked at a slider-bar task. We empirically show that using 3D widget representations can have mixed results on user performance. Furthermore, we show that simulated surface constraints can improve user performance on typical interaction tasks in the absence of a physical manipulation surface. Finally, based on these results, we make some recommendations to aid interface designers in constructing effective interfaces for virtual environments.", "keywords": ["Mice", "Virtual environment", "Computational modeling", "Computer simulation", "Surface treatment", "Fingers", "Computer science", "Feedback", "Two dimensional displays", "Virtual reality", "virtual reality", "human factors", "graphical user interfaces", "digital simulation", "interactive devices", "3D widget representation", "simulated surface constraints", "user interaction", "immersive virtual environments", "2D interaction techniques", "3D environments", "3D widget representations", "interface widget representations", "precise manipulation tasks", "drag-and-drop task", "slider-bar task", "user performance", "interaction tasks", "physical manipulation surface", "interface designers"], "published_in": "Proceedings IEEE Virtual Reality 2001", "publication_date": "2001", "citations": 18, "isbn": ["0-7695-0948-7"], "doi": "10.1109/VR.2001.913780", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=913780", "paper_url": "https://ieeexplore.ieee.org/document/913780"}, {"title": "3D Arrow: A virtual pointer for immersive sculpting", "authors": ["J\u00e9r\u00f4me Grosjean", "J\u00e9r\u00f4me Simonin", "Eric Galin", "St\u00e9phane M\u00e9rillou"], "abstract": "Interaction inside virtual reality applications begins usually with simple selection tasks that can be achieved with moderate degrees of performances. More complex operations like modeling or sculpting inside an immersive environment raises however increased needs for precise 3D selections and visual feedback. In a modeling context any 3D location must be targetable, even ones that are outside of initial reach or not graphically represented. Common interaction techniques in virtual reality do not fullfill these goals without switching between different tools. We present an extensive and precise selection technique called the 3D arrow, used for moving and sculpting inside an immersive terrain modeling application. Composed of a graphic mouse-like pointer constrained inside a workspace defined by the region of space at arm's reach, the 3D arrow allow the users to seamlessly reach and target any position of space within the same interaction paradigm and to rotate the view around the pointer. The graphic appearance of the pointer is carefully designed to help users estimate its global and local position in the terrain scenery.", "keywords": ["Three dimensional displays", "Solid modeling", "Virtual reality", "Navigation", "Rocks", "Shape", "Visualization", "solid modelling", "virtual reality", "3D arrow", "virtual pointer", "immersive sculpting", "visual feedback", "3D modeling context", "virtual reality interaction", "terrain modeling application", "graphic mouse-like pointer", "Virtual reality", "Selection technique", "Navigation technique"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 1, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759472", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759472", "paper_url": "https://ieeexplore.ieee.org/document/5759472"}, {"title": "Pedagogical Agent Responsive to Eye Tracking in Educational VR", "authors": ["Adil Khokhar", "Andrew Yoshimura", "Christoph W. Borst"], "abstract": "We present an architecture to make a VR pedagogical agent responsive to shifts in user attention monitored by eye tracking. The behavior-based AI includes low-level sensor elements, sensor combiners that compute attention metrics for higher-level sensors called generalized hotspots, an annotation system for arranging scene elements and responses, and its response selection system. We show that the techniques can control the playback of teacher avatar clips that point out and explain objects in a VR oil rig for training.", "keywords": ["Gaze tracking", "History", "Computer architecture", "Artificial intelligence", "Monitoring", "Conferences", "Virtual reality", "avatars", "computer aided instruction", "object detection", "object tracking", "virtual reality", "pedagogical agent responsive", "eye tracking", "educational VR", "VR pedagogical agent", "user attention", "behavior-based AI", "low-level sensor elements", "annotation system", "scene elements", "response selection system", "VR oil rig", "attention metrics", "Human-centered computing-Visualization"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797896", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797896", "paper_url": "https://ieeexplore.ieee.org/document/8797896"}, {"title": "Object-Capability Security in Virtual Environments", "authors": ["Martin Scheffler", "Jan P. Springer", "Bernd Froehlich"], "abstract": "Access control is an important aspect of shared virtual environments. Resource access may not only depend on prior authorization, but also on context of usage such as distance or position in the scene graph hierarchy. In virtual worlds that allow user-created content, participants must be able to define and exchange access rights to control the usage of their creations. Using object capabilities, fine-grained access control can be exerted on the object level. We describe our experiences in the application of the object-capability model for access control to object-manipulation tasks common to collaborative virtual environments. We also report on a prototype implementation of an object-capability safe virtual environment that allows anonymous, dynamic exchange of access rights between users, scene elements, and autonomous actors.", "keywords": ["Virtual environment", "Access control", "Permission", "Layout", "Virtual prototyping", "Vehicle dynamics", "Computer security", "Information security", "Object oriented programming", "Computer graphics", "object-oriented programming", "user interfaces", "virtual reality", "object-capability security", "virtual environments", "access control", "scene graph hierarchy", "object-manipulation tasks", "collaborative virtual environments", "object-capability safe virtual environment", "Object Capabilities", "Security", "Virtual Environments", "D.1.5 [Programming Techniques]: Object-Oriented Programming", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u00bfVirtual Reality", "K.6.5 [Computing Milieux]: Management of Computing and Information Systems\u00bfSecurity and Protection"], "published_in": "2008 IEEE Virtual Reality Conference", "publication_date": "March 2008", "citations": 3, "isbn": ["978-1-4244-1971-5", "978-1-4244-1972-2"], "doi": "10.1109/VR.2008.4480750", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4480750", "paper_url": "https://ieeexplore.ieee.org/document/4480750"}, {"title": "Virtual technical trainer: learning how to use milling machines with multi-sensory feedback in virtual reality", "authors": ["F. Crison", "A. Lecuyer", "D.M. D'Huart", "J.-M. Burkhardt", "G. Michel", "J.-L. Dautin"], "abstract": "This paper describes the use and characteristics of a virtual reality system called the virtual technical trainer (VTT). This system is dedicated to vocational training courses on using and programming numerically-controlled milling machines. It aims at replacing the conventional mechanical milling machines which are currently used in such vocational training courses. VTT proposes an interactive manipulation of a cutter of a virtual milling machine, with visual, audio and haptic (force) feedback. VTT uses a haptic device which was specifically designed for the purpose of our pedagogical application. When the virtual cutter mills a piece of material, a plastic deformation algorithm is used and the material is progressively carved. Trainees can feel the cutting effort thanks to a force feedback which varies as a function of different simulation parameters (rotation speed of the tool, type of material to carve, etc). Realistic audio feedback (which was recorded in real situations) and additional visual assistance may be added, in order to increase the perception and understanding of the milling task. A preliminary evaluation of VTT showed that this simulator could be used by vocational trainers successfully. It could help them to teach the basic principles of machining at the first stages of vocational training courses on numerically-controlled milling machines.", "keywords": ["Machine learning", "Metalworking machines", "Virtual reality", "Vocational training", "Haptic interfaces", "Force feedback", "Programming profession", "Milling machines", "Plastics", "Machining", "computer based training", "educational courses", "milling", "milling machines", "cutting", "cutting tools", "virtual reality", "haptic interfaces", "force feedback", "teaching", "user centred design", "plastic deformation", "virtual technical trainer", "learning", "multisensory feedback", "virtual reality", "vocational training courses", "numerically-controlled milling machines", "cutter manipulation", "virtual milling machine", "visual feedback", "audio feedback", "haptic feedback", "force feedback", "haptic device", "pedagogical application", "plastic deformation algorithm"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 12, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492766", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492766", "paper_url": "https://ieeexplore.ieee.org/document/1492766"}, {"title": "Jumping Further: Forward Jumps in a Gravity-reduced Immersive Virtual Environment", "authors": ["Hyeong Yeop Kang", "Geonsun Lee", "Dae Seok Kang", "Ohung Kwon", "Jun Yeup Cho", "Ho-Jung Choi", "Jung Hyun Han"], "abstract": "In a cable-driven suspension system developed to simulate the reduced gravity of lunar or Martian surfaces, we propose to manipu-late/reduce the physical cues of forward jumps so as to overcome the limited workspace problem. The physical cues should be manipulated in a way that the discrepancy from the visual cues provided through the HMD is not noticeable by users. We identified the extent to which forward jumps can be manipulated naturally. We combined it with visual gains, which can scale visual cues without being noticed by users. The test results obtained in a prototype application show that we can use both trajectory manipulation and visual gains to overcome the spatial limit. We also investigated the user experiences when making significantly high and far jumps. The results will be helpful in designing astronaut-training systems and various VR entertainment content.", "keywords": ["Trajectory", "Visualization", "Moon", "Wires", "Acceleration", "Gravity", "Tracking", "astronomy computing", "cables (mechanical)", "computer based training", "helmet mounted displays", "suspensions (mechanical components)", "user experience", "virtual reality", "forward jumps", "gravity-reduced immersive virtual environment", "cable-driven suspension system", "physical cues", "visual cues", "visual gains", "trajectory manipulation", "high jumps", "far jumps", "HMD", "user experiences", "astronaut-training systems", "VR entertainment content", "Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Virtual reality", "Human-centered computing\u2014Human computer interaction (HCI)\u2014HCI design and evaluation methods\u2014User studies"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798251", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798251", "paper_url": "https://ieeexplore.ieee.org/document/8798251"}, {"title": "Can you stand on virtual grounds? A study on postural affordances in virtual reality", "authors": ["Tony Regia-Corte", "Maud Marchal", "Anatole L\u00e9cuyer"], "abstract": "The concept of affordance, introduced by the psychologist James Gibson, can be defined as the functional utility of an object, a surface or an event. The purpose of this article was to evaluate the perception of affordances in virtual environments (VE). In order to test this perception, we considered the affordances for standing on a virtual slanted surface. The participants were asked to judge whether a virtual slanted surface supported upright stance. The perception was investigated by manipulating the texture of the slanted surface (Wooden texture vs. Ice texture). Results showed an effect of the texture: the perceptual boundary (or critical angle) with the Ice texture was significantly lower than with the Wooden texture. These results reveal that perception of affordances for standing on a slanted surface in virtual reality is possible and comparable to previous studies conducted in real environments.", "keywords": ["Virtual reality", "Psychology", "Testing", "Ice", "Surface texture", "Legged locomotion", "Apertures", "Velocity measurement", "Virtual environment", "Multimedia systems", "virtual reality", "virtual grounds", "postural affordances", "virtual reality", "functional utility", "virtual environments", "virtual slanted surface", "perceptual boundary", "critical angle", "Ice texture", "Wooden texture", "real environments", "H.5.1 Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented and virtual realities", "H.1.2 [Information Systems]: User/Machine Systems-human factors", "human information processing"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 2, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444789", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444789", "paper_url": "https://ieeexplore.ieee.org/document/5444789"}, {"title": "Leveraging change blindness for redirection in virtual environments", "authors": ["Evan A. Suma", "Seth Clark", "David Krum", "Samantha Finkelstein", "Mark Bolas", "Zachary Warte"], "abstract": "We present change blindness redirection, a novel technique for allowing the user to walk through an immersive virtual environment that is considerably larger than the available physical workspace. In contrast to previous redirection techniques, this approach, based on a dynamic environment model, does not introduce any visual-vestibular conflicts from manipulating the mapping between physical and virtual motions, nor does it require breaking presence to stop and explicitly reorient the user. We conducted two user studies to evaluate the effectiveness of the change blindness illusion when exploring a virtual environment that was an order of magnitude larger than the physical walking space. Despite the dynamically changing environment, participants were able to draw coherent sketch maps of the environment structure, and pointing task results indicated that they were able to maintain their spatial orientation within the virtual world. Only one out of 77 participants across both both studies definitively noticed that a scene change had occurred, suggesting that change blindness redirection provides a remarkably compelling illusion. Secondary findings revealed that a wide field-of-view increases pointing accuracy and that experienced gamers reported greater sense of presence than those with little or no experience with 3D video games.", "keywords": ["Virtual environment", "Legged locomotion", "Three dimensional displays", "Blindness", "Games", "Visualization", "Monitoring", "virtual reality", "virtual environments", "change blindness redirection", "dynamic environment model", "virtual environments", "redirection", "change blindness"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 67, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759455", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759455", "paper_url": "https://ieeexplore.ieee.org/document/5759455"}, {"title": "Mixed reality in virtual world teleconferencing", "authors": ["Tuomas Kantonen", "Charles Woodward", "Neil Katz"], "abstract": "In this paper we present a Mixed Reality (MR) teleconferencing application based on Second Life (SL) and the OpenSim virtual world. Augmented Reality (AR) techniques are used for displaying virtual avatars of remote meeting participants in real physical spaces, while Augmented Virtuality (AV), in form of video based gesture detection, enables capturing of human expressions to control avatars and to manipulate virtual objects in virtual worlds. The use of Second Life for creating a shared augmented space to represent different physical locations allows us to incorporate the application into existing infrastructure. The application is implemented using open source Second Life viewer, ARToolKit and OpenCV libraries.", "keywords": ["Virtual reality", "Teleconferencing", "Second Life", "Avatars", "Augmented virtuality", "Augmented reality", "Video sharing", "Object detection", "Humans", "Libraries", "augmented reality", "avatars", "teleconferencing", "mixed reality", "virtual world teleconferencing", "second life", "OpenSim virtual world", "augmented reality", "augmented virtuality", "virtual avatars", "video based gesture detection", "mixed reality", "virtual worlds", "Second Life", "teleconferencing", "immersive virtual environments", "collaborative augmented reality"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 19, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444792", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444792", "paper_url": "https://ieeexplore.ieee.org/document/5444792"}, {"title": "Systematic design of interactive illustration techniques for user guidance in virtual environments", "authors": ["V. Paelke"], "abstract": "A usability-centred design approach is critically important for content development of virtual environments in real-world applications. We provide a framework that allows one to review existing design techniques and tools against the special requirements of virtual environments, so that appropriate methods can be selected. Focusing on the specific usability requirement of interaction guidance in presentation and instructional environments and its implementation through interactive illustration techniques, we demonstrate the use of the framework. Based on the specific requirements, we derive a suitable design process for interactive illustration techniques and identify appropriate techniques from multimedia and GUI design. The use of the design process and techniques is then illustrated with an example.", "keywords": ["Electrical capacitance tomography", "Virtual environment", "Animation", "Design methodology", "Research and development", "Interactive systems", "Tail", "Rendering (computer graphics)", "Layout", "Cameras", "virtual reality", "user centred design", "computer aided instruction", "multimedia computing", "graphical user interfaces", "systematic design", "interactive illustration techniques", "user guidance", "virtual environment content development", "usability-centred design", "design techniques review", "interaction guidance", "presentation environments", "instructional environments", "multimedia design", "GUI design"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 2, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840500", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840500", "paper_url": "https://ieeexplore.ieee.org/document/840500"}, {"title": "[DC] Dimensionality of Augmented Reality Spatial Interfaces", "authors": ["Alla Vovk"], "abstract": "The next wave of computing is visible on the horizon, heralding the widespread adoption of augmented reality spatial information manipulation on wearable and environment integrated form factors such as head-worn displays. This application for the doctoral consortium at IEEE VR 2019 describes the research conducted and planned in the context of a PhD thesis, which aims at determining a multidimensional model of user performance in spatial user interaction in augmented reality for training.", "keywords": ["Training", "Augmented reality", "Task analysis", "Usability", "Ergonomics", "User interfaces", "augmented reality", "helmet mounted displays", "augmented reality spatial interfaces", "augmented reality spatial information manipulation", "wearable environment integrated form factors", "head-worn displays", "IEEE VR", "spatial user interaction", "multidimensional model", "Augmented reality", "spatial user interface", "trainings", "[Human-centered computing]: Human computer interaction", "Interaction paradigms", "Mixed/Augmented reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798171", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798171", "paper_url": "https://ieeexplore.ieee.org/document/8798171"}, {"title": "The Globefish: A 3D Motion Controller", "authors": ["Alexander Kulik", "Jan Hochstrate", "Andre Kunert", "Bernd Froehlich"], "abstract": "The Globefish is a novel desktop input device for three-dimensional object manipulation and viewpoint navigation. It consists of an elastically suspended 3D trackball, which provides a natural mapping for position-controlled 3D rotations. Rate-controlled 3D translations are performed by pushing the trackball into the appropriate direction. The device is manipulated by the fingertips allowing for precise interaction with virtual objects. The Globefish was designed for 3D graphics applications such as computer aided design (CAD), digital content creation (DCC) and 3D games.", "keywords": ["Motion control", "Virtual reality", "Fingers", "Prototypes", "Mice", "Cameras", "Control systems", "Navigation", "Computer graphics", "Application software", "CAD", "computer games", "computer graphics", "control engineering computing", "motion control", "position control", "Globefish", "3D motion controller", "three-dimensional object manipulation", "viewpoint navigation", "elastically suspended 3D trackball", "position-controlled 3D rotations", "rate-controlled 3D translations", "3D graphics applications", "computer aided design", "digital content creation", "3D games"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 0, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811064", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811064", "paper_url": "https://ieeexplore.ieee.org/document/4811064"}, {"title": "Effects of 3D Rotational Jitter and Selection Methods on 3D Pointing Tasks", "authors": ["Anil Ufuk Batmaz", "Wolfgang Stuerzlinger"], "abstract": "3D pointing is an integral part of Virtual Reality interaction. Typical pointing devices rely on 3D trackers and are thus subject to fluctuations in the reported pose, i.e., jitter. In this work, we explored how different levels of rotational jitter affect pointing performance and if different selection methods can mitigate the effects of jitter. Towards this, we designed a Fitts' Law experiment with three selection methods. In the first method, subjects used a single controller to position and select the object. In the second method, subjects used the controller in their dominant hand to point at objects and the trigger button of a second controller, held in their non-dominant hand, to select objects. Finally, subjects used the controller in their dominant hand to point the objects and pressed the space bar on a keyboard to select the object in the third condition. During the pointing task we added five different levels of jitter: no jitter, \u00b10.5\u00b0, \u00b11\u00b0, and \u00b12\u00b0 uniform noise, as well as White Gaussian noise with 1\u00b0 standard deviation. Results showed that the Gaussian noise and \u00b12\u00b0 of jitters significantly reduced the throughput of the participants. Moreover, subjects made fewer errors when they performed the experiment with two controllers. Our results inform the design of 3D user interfaces, input devices and interaction techniques.", "keywords": ["Jitter", "Three-dimensional displays", "Task analysis", "Performance evaluation", "Keyboards", "Gaussian noise", "Standards", "Gaussian noise", "human computer interaction", "jitter", "mouse controllers (computers)", "virtual reality", "rotational jitter", "pointing performance", "dominant hand", "nondominant hand", "3D user interfaces", "3D pointing tasks", "pointing devices", "selection methods", "virtual reality interaction", "trigger button", "uniform noise", "white Gaussian noise", "standard deviation", "Human-centered computing", "Virtual Reality", "Keyboards", "Pointing devices"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798038", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798038", "paper_url": "https://ieeexplore.ieee.org/document/8798038"}, {"title": "Visualization Techniques for Precise Alignment in VR: A Comparative Study", "authors": ["Alejandro Martin-Gomez", "Ulrich Eck", "Nassir Navab"], "abstract": "Many studies explored the effectiveness of augmented, virtual, and mixed reality for object placement tasks. Two main approaches for assisting users during object alignment exist: static visualization techniques and interactive guides. This paper presents a comparative evaluation of four static visualization techniques used to render virtual objects when precise alignment in 6 degrees of freedom (DoF) is required. The selection of these techniques is based on the amount of occlusion caused by the visual guides during the alignment task. To the best of our knowledge, no previous work exists that evaluates which visualization technique is most suitable to support users while precisely aligning objects in virtual environments. We designed a virtual reality scenario considering two conditions -with and without time constraints- in which users aligned pairs of objects. To evaluate the users performance, quantitative and qualitative scores were collected. Our results suggest that visualization techniques with low levels of occlusion can improve alignment performance and increase user acceptance.", "keywords": ["Visualization", "Task analysis", "Solid modeling", "Solids", "Virtual reality", "Three-dimensional displays", "Rendering (computer graphics)", "augmented reality", "data visualisation", "human factors", "rendering (computer graphics)", "virtual reality", "visualization technique", "augmented reality", "mixed reality", "object placement tasks", "static visualization techniques", "virtual objects", "visual guides", "alignment task", "virtual environments", "virtual reality scenario", "virtual reality", "object alignment", "user acceptance", "quantitative score", "qualitative score", "Human-centered computing\u2014Human computer interaction (HCI)\u2014HCI design and evaluation methods\u2014User studies", "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods", "Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798135", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798135", "paper_url": "https://ieeexplore.ieee.org/document/8798135"}, {"title": "Exploring realtime visualisation of large abstract data spaces with QSPACE", "authors": ["S. Pettifer", "J. Cook", "J. Mariani", "J. Trevor"], "abstract": "Virtual environments present opportunities for novel interaction with and visualisation of abstract data. However, the inherent difficulties of rendering 3D environments, as well as bottlenecks in traditional virtual reality systems, make real-time manipulation of realistic amounts of data difficult. We present QSPACE, a data visualisation tool with which we have begun to explore techniques and architectures for making real-time interaction with largest amounts of data possible.", "keywords": ["Data visualization", "Virtual reality", "Books", "Virtual environment", "Rendering (computer graphics)", "Real time systems", "Hardware", "Clouds", "Computer science", "Computer interfaces", "real-time systems", "data visualisation", "virtual reality", "real-time data visualisation", "large abstract data spaces", "QSPACE data visualisation tool", "virtual environments", "3D environment rendering", "virtual reality systems", "real-time data manipulation", "real-time interaction"], "published_in": "Proceedings IEEE Virtual Reality 2001", "publication_date": "2001", "citations": 0, "isbn": ["0-7695-0948-7"], "doi": "10.1109/VR.2001.913803", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=913803", "paper_url": "https://ieeexplore.ieee.org/document/913803"}, {"title": "Tactile Feedback at the Finger Tips for Improved Direct Interaction in Immersive Environments", "authors": ["Robert Scheibe", "Mathias Moehring", "Bernd Froehlich"], "abstract": "We present a new tactile feedback system for finger-based interactions in immersive virtual reality applications that consists of shape memory alloy wires wrapped around tracked linger thimbles. The wires touch the inside of the finger tips and provide an impression when they are shortened. We use this system to communicate linger contacts with virtual objects in an application for usability and reachability studies of car interiors. Our experiments and an initial pilot study revealed that this type of feedback helps users to perform direct manipulation tasks with more reliability", "keywords": ["Feedback", "Fingers", "Shape memory alloys", "Wires", "Virtual reality", "Displays", "Temperature", "Crystalline materials", "Usability", "Computer graphics", "feedback", "haptic interfaces", "virtual reality", "direct interaction", "immersive environments", "tactile feedback system", "finger-based interactions", "immersive virtual reality", "shape memory alloy wires", "tracked linger thimbles", "virtual objects", "reachability study", "Direct Interaction", "Tactile Feedback", "Shape Memory Alloys", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics", "Realism - Virtual Reality B.4.2 [Input/Output and Data Communication]: Input/Output Devices - Channels and Controllers"], "published_in": "2007 IEEE Virtual Reality Conference", "publication_date": "March 2007", "citations": 5, "isbn": ["1-4244-0905-5", "1-4244-0906-3"], "doi": "10.1109/VR.2007.352508", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4161050", "paper_url": "https://ieeexplore.ieee.org/document/4161050"}, {"title": "Exploiting change blindness to expand walkable space in a virtual environment", "authors": ["Evan A. Suma", "Seth Clark", "Samantha L. Finkelstein", "Zachary Wartell"], "abstract": "We present a technique for exploiting change blindness to allow the user to walk through an immersive virtual environment that is much larger than the available physical workspace. This approach relies on subtle manipulations to the geometry of a dynamic environment model to redirect the user's walking path without becoming noticeable. We describe a virtual environment which was implemented both as a proof-of-concept and a test case for future evaluation. Anecdotal evidence from our informal tests suggest a compelling illusion, though a formal study against existing methods is required to evaluate the usefulness of this technique.", "keywords": ["Blindness", "Virtual environment", "Legged locomotion", "Computer graphics", "Navigation", "Switches", "Layout", "Geometry", "Solid modeling", "Testing", "virtual reality", "change blindness", "walkable space", "immersive virtual environment", "geometry", "dynamic environment model", "proof-of-concept", "virtual environments", "real walking", "locomotion"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 16, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444752", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444752", "paper_url": "https://ieeexplore.ieee.org/document/5444752"}, {"title": "Is this bridge safe? Evaluation of audiovisual cues for a walk on a small bridge over a canyon", "authors": ["Erik Sikstr\u00f6m", "Niels Christian Nilsson", "Amalia De G\u00f6tzen", "Stefania Serafin"], "abstract": "This paper presents two within-subjects studies (n=23) exploring how different combinations of visual and auditory feedback influence perceived realism, virtual self-perception and the experience of safety during walks on a virtual platform suspended over a canyon. In the first study, the frequency factor of the footstep sounds was altered and the visual appearance was changed between a newly built wooden bridge and an old bridge with a weaker structure and broken planks. In the second study, the sounds of creaking wood were added to the footstep sounds in half of the trails and compared against footsteps without creaking sounds. Moreover, the frequency factor of the frequency controls for footsteps was also manipulated between trails, but the visual appearance of the bridge was limited to the model of the old broken bridge.", "keywords": ["Bridges", "Visualization", "Legged locomotion", "Virtual environments", "Electronic mail", "Avatars", "Frequency control", "virtual reality", "audiovisual cues", "canyon", "visual feedback", "auditory feedback", "perceived realism", "virtual self-perception", "virtual platform", "frequency factor", "footstep sounds", "creaking wood", "broken bridge", "H.1.2 [Information Systems]: User/Machine Systems \u2014 Human factors", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Virtual Reality"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 0, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504765", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504765", "paper_url": "https://ieeexplore.ieee.org/document/7504765"}, {"title": "cMotion: A New Game Design to Teach Emotion Recognition and Programming Logic to Children using Virtual Humans", "authors": ["Samantha L. Finkelstein", "Andrea Nickel", "Lane Harrison", "Evan A. Suma", "Tiffany Barnes"], "abstract": "This paper presents the design of the final stage of a new game currently in development, entitled cMotion, which will use virtual humans to teach emotion recognition and programming concepts to children. Having multiple facets, cMotion is designed to teach the intended users how to recognize facial expressions and manipulate an interactive virtual character using a visual drag-and-drop programming interface. By creating a game which contextualizes emotions, we hope to foster learning of both emotions in a cultural context and computer programming concepts in children. The game will be completed in three stages which will each be tested separately: a playable introduction which focuses on social skills and emotion recognition, an interactive interface which focuses on computer programming, and a full game which combines the first two stages into one activity.", "keywords": ["Logic programming", "Emotion recognition", "Logic design", "Humans", "Autism", "Computer aided instruction", "Programming profession", "Computer science education", "Educational programs", "Face recognition", "computer aided instruction", "computer games", "computer science education", "emotion recognition", "face recognition", "programming", "teaching", "virtual reality", "cMotion game design", "emotion recognition teaching", "computer programming logic", "virtual human", "facial expression recognition", "interactive virtual character", "visual drag-and-drop programming interface", "K.3.1 [Computers and Education]: Computer Uses in Education\u00bfComputer-assisted instruction (CAI)", "K.3.2 [Computers and Education]: Computer and Information Science Education\u00bfComputer science education", "virtual humans", "serious games", "emotion recognition"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 16, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811039", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811039", "paper_url": "https://ieeexplore.ieee.org/document/4811039"}, {"title": "Exploring the perception of co-location errors during tool interaction in visuo-haptic augmented reality", "authors": ["Ulrich Eck", "Liem Hoang", "Christian Sandor", "Goshiro Yamamoto", "Takafumi Taketom", "Hirokazu Kato", "Hamid Laga"], "abstract": "Co-located haptic feedback in mixed and augmented reality environments can improve realism and user performance, but it also requires careful system design and calibration. In this poster, we determine the thresholds for perceiving co-location errors through two psychophysics experiments in a typical fine-motor manipulation task. In these experiments we simulate the two fundamental ways of implementing VHAR systems: first, attaching a real tool; second, augmenting a virtual tool. We determined the just-noticeable co-location errors for position and orientation in both experiments and found that users are significantly more sensitive to co-location errors with virtual tools. Our overall findings are useful for designing visuo-haptic augmented reality workspaces and calibration procedures.", "keywords": ["Haptic interfaces", "Augmented reality", "Visualization", "Calibration", "Performance evaluation", "Rendering (computer graphics)", "Australia", "augmented reality", "feedback", "haptic interfaces", "human computer interaction", "colocation error perception", "tool interaction", "colocated haptic feedback", "mixed reality environment", "realism", "system design", "system calibration", "psychophysics experiment", "fine-motor manipulation task", "VHAR system", "virtual tool", "position error", "orientation error", "visuo-haptic augmented reality workspace", "H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems \u2014 Artificial, augmented and virtual realities"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 1, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504708", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504708", "paper_url": "https://ieeexplore.ieee.org/document/7504708"}, {"title": "Using virtual environments to evaluate assumptions of the human visual system", "authors": ["Eric Palmer", "Aaron Michaux", "Zygmunt Pizlo"], "abstract": "Virtual reality applications provide an opportunity to test human vision in well-controlled scenarios that would be difficult or impossible to generate in real physical spaces. This paper presents a study intended to evaluate the importance of possible assumptions made by the human visual system. Using a CAVE simulation, participants viewed and counted virtual furniture objects in a variety of experimental manipulations. The assumption of uprightness against inversion, or the `gravity constraint,' was identified as a significant assumption of the visual system (p <; 0.001). Monocular vs. binocular vision was also demonstrated as an important factor in this study (p = 0.01), while color vs. grayscale did not have a significant impact on task performance (p = 0.16). By including the binocular cue, and the assumption about the direction of gravity, the scene reconstruction produced by our computer vision model is reliable. The model can detect and count symmetrical objects in a 3D real scene and then recover their 3D shapes.", "keywords": ["Three-dimensional displays", "Computational modeling", "Solid modeling", "Visual systems", "Gravity", "Image color analysis", "Gray-scale", "computer vision", "virtual reality", "human visual system", "virtual reality applications", "CAVE simulation", "virtual furniture objects", "gravity direction", "scene reconstruction", "computer vision model", "3D real scene", "3D shapes", "Psychology", "perception", "computer vision"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 0, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504751", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504751", "paper_url": "https://ieeexplore.ieee.org/document/7504751"}, {"title": "fARFEEL: Providing Haptic Sensation of Touched Objects Using Visuo-Haptic Feedback", "authors": ["Naruki Tanabe", "Yushi Sato", "Kohei Morita", "Michiya Inagaki", "Yuichi Fujino", "Parinya Punpongsanon", "Haruka Matsukura", "Daisuke Iwai", "Kosuke Sato"], "abstract": "We present fARFEEL, a remote communication system that provides visuo-haptic feedback allows a local user to feel touching distant objects. The system allows the local and remote users to communicate by using the projected virtual hand (VH) for the agency of his/her own hands. The necessary haptic information is provided to the non-manipulating hand of the local user that does not bother the manipulation of the projected VH. We also introduce the possible visual stimulus that could potentially provide the sense of the body ownership over the projected VH.", "keywords": ["Haptic interfaces", "Cameras", "Visualization", "Three-dimensional displays", "Shape", "Conferences", "Strain", "feedback", "haptic interfaces", "touch (physiological)", "virtual reality", "touched objects", "visuo-haptic feedback", "fARFEEL", "remote communication system", "distant objects", "local users", "remote users", "projected virtual hand", "necessary haptic information", "projected VH", "haptic sensation", "body ownership", "Virtual hand illusion", "Spatial augmented reality", "Visuo-haptic", "Haptic feedback", "Extended body interface"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798195", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798195", "paper_url": "https://ieeexplore.ieee.org/document/8798195"}, {"title": "Navigation in REVERIE's virtual environments", "authors": ["Fiona M. Rivera", "Fons Fuijk", "Ebroul Izquierdo"], "abstract": "This work presents a novel navigation system for social collaborative virtual environments populated with multiple characters. The navigation system ensures collision free movement of avatars and agents. It supports direct user manipulation, automated path planning, positioning to get seated, and follow-me behaviour for groups. In follow-me mode, the socially aware system manages the mise en place of individuals within a group. A use case centred around on an educational virtual trip to the European Parliament created for the REVERIE FP7 project, also serves as an example to bring forward aspects of such navigational requirements.", "keywords": ["Navigation", "Avatars", "Electronic countermeasures", "Path planning", "Europe", "Conferences", "avatars", "path planning", "social sciences computing", "REVERIE", "social collaborative virtual environments", "collision free movement", "avatars", "direct user manipulation", "automated path planning", "educational virtual trip", "European Parliament", "Virtual reality", "3D navigation", "Avatars", "3D world"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 0, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223401", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223401", "paper_url": "https://ieeexplore.ieee.org/document/7223401"}, {"title": "Effect of Environment Size on Curvature Redirected Walking Thresholds", "authors": ["Anh Nguyen", "Yannick Rothacher", "Andreas Kunz", "Peter Brugger", "Bigna Lenggenhager"], "abstract": "Redirected walking (RDW) refers to a number of techniques that enable users to explore a virtual environment larger than the real physical space. These techniques are based on the introduction of a mismatch in rotation, translation and curvature between the virtual and real trajectories, quantified as rotational, translational and curvature gains. When these gains are applied within certain thresholds, the manipulation is unnoticeable and immersion is maintained. Existing studies on RDW thresholds reported a wide range of threshold values. These differences could be attributed to many factors such as individual differences, walking speed, or environment settings. In this paper, we propose a study to investigate one of the environment settings that could potentially influence curvature RDW thresholds: the environment size. The detailed description of the study is also provided, where the adaptive, 2-alternative forced choice method is used to identify the detection thresholds.", "keywords": ["Legged locomotion", "Visualization", "Optical sensors", "Integrated optics", "Virtual environments", "Adaptive optics", "gait analysis", "virtual reality", "environment size", "virtual environment", "virtual trajectories", "real trajectories", "threshold values", "curvature RDW thresholds", "curvature redirected walking thresholds", "walking speed", "2-alternative forced choice method", "detection thresholds identification", "adaptive method"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446225", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446225", "paper_url": "https://ieeexplore.ieee.org/document/8446225"}, {"title": "Please Don't Puke: Early Detection of Severe Motion Sickness in VR", "authors": ["Courtney Hutton", "Shelby Ziccardi", "Julio Medina", "Evan Suma Rosenbarg"], "abstract": "Motion sickness is a potentially debilitating side effect experienced by certain users of virtual reality systems. Unexpected results from a user study on redirected walking suggest that there is a need to quickly identify participants who have an extremely low tolerance for virtual motion manipulations and remove them from the experience. In this poster, we investigate the use of a previously introduced \u201cfast motion sickness\u201d measure to identify potential outliers with heightened levels of sensitivity. This work demonstrates a promising experimental methodology and suggests possible shared characteristics among users in this group.", "keywords": ["Frequency modulation", "Calibration", "Electronic mail", "Virtual environments", "Tutorials", "Headphones", "human factors", "virtual reality", "severe motion sickness", "VR", "potentially debilitating side effect", "virtual reality systems", "user study", "redirected walking", "virtual motion manipulations", "potential outliers", "fast motion sickness measure", "Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality", "Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446382", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446382", "paper_url": "https://ieeexplore.ieee.org/document/8446382"}, {"title": "Object Size Perception in Immersive Virtual Reality: Avatar Realism Affects the Way We Perceive", "authors": ["Nami Ogawa", "Takuji Narumi", "Michitaka Hirose"], "abstract": "How does the representation of an embodied avatar influence the way in which a human perceives the scale of a virtual environment? It has been shown that the scale of the external environment is perceived relative to the size of one's body. However, the influence of avatar realism on the perceived scale has not been investigated, despite the fact that it is common to embody avatars of various representations, from iconic to realistic. This study examined how avatar realism would affect perceived graspable object sizes as the size of the avatar hand changes. In the experiment, we manipulated the realism (high, medium, and low) and size (veridical and enlarged) of the avatar hand, and measured the perceived size of a cube. The results showed that the size of the cube was perceived to be smaller when the avatar hand was enlarged for all degrees of realism of the hand. However, the enlargement of the avatar hand had a greater influence on the perceived cube size for the highly realistic avatar than for the medium-level and low-level realism conditions. This study shed new light on the importance of the avatar representation in a three-dimensional user interface field, in how it can affect the manner in which we perceive the scale of a virtual environment.", "keywords": ["Avatars", "Estimation", "Hysteresis", "Skin", "Virtual environments", "avatars", "user interfaces", "virtual reality", "visual perception", "virtual environment", "avatar realism", "low-level realism conditions", "avatar representation", "object size perception", "immersive virtual reality", "realistic avatar", "three-dimensional user interface", "Human-centered computing-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 2, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446318", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446318", "paper_url": "https://ieeexplore.ieee.org/document/8446318"}, {"title": "Hands-Free Interaction for Augmented Reality in Vascular Interventions", "authors": ["Alon Grinshpoon", "Shirin Sadri", "Gabrielle J. Loeb", "Carmine Elvezio", "Steven K. Feiner"], "abstract": "Vascular interventions are minimally invasive surgical procedures in which a physician navigates a catheter through a patient's vasculature to a desired destination in the patient's body. Since perception of relevant patient anatomy is limited in procedures of this sort, virtual reality and augmented reality systems have been developed to assist in 3D navigation. These systems often require user interaction, yet both of the physician's hands may already be busy performing the procedure. To address this need, we demonstrate hands-free interaction techniques that use voice and head tracking to allow the physician to interact with 3D virtual content on a head-worn display while making both hands available intraoperatively. Our approach supports rotation and scaling of 3D anatomical models that appear to reside in the surrounding environment through small head rotations using first-order control, and rigid body transformation of those models using zero-order control. This allows the physician to easily manipulate a model while it stays close to the center of their field of view.", "keywords": ["Head", "Three-dimensional displays", "Solid modeling", "Surgery", "Cameras", "Augmented reality", "augmented reality", "biomedical equipment", "catheters", "computer graphics", "medical computing", "medical robotics", "surgery", "augmented reality", "vascular interventions", "minimally invasive surgical procedures", "physician", "catheter", "relevant patient anatomy", "virtual reality", "reality systems", "user interaction", "hands-free interaction techniques", "head tracking", "3D virtual content", "head-worn display", "3D anatomical models", "head rotations", "rigid body transformation", "Hands-free interaction", "augmented reality", "vascular interventions", "head tracking", "head-worn display"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 2, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446259", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446259", "paper_url": "https://ieeexplore.ieee.org/document/8446259"}, {"title": "Development of Wearable Motion Capture System Using Fiber Bragg Grating Sensors for Measuring Arm Motion", "authors": ["Minsu Jang", "Jun Sik Kim", "Kyumin Kang", "Soong Ho Um", "Sungwook Yang", "Jinseok Kim"], "abstract": "Motion capture systems are gaining much attention in various fields, including entertainment, medical and sports fields. Although many types of motion capture sensor have been emerging, they have limitations and disadvantages such as occlusion, drift and interference by electromagnetic fields. Here, we introduce the novel wearable motion capture system using fiber Bragg gratings (FBGs) sensors. Since the human joints have different degrees of freedom (DOF), we developed three types of sensors to reconstruct the human body motion from the strains induced on the FBGs. First, a shape sensor using three fibers provides the position and orientation of joints in three dimensional space. Second, we introduce the angle sensor which is capable of measuring bending angle with high curvature using single fiber. Lastly, to detect the twisting of joints, a sensor with fiber attached on a soft material spirally is used. With the optical fiber based motion capture sensors, we reconstruct the motion of arm in realtime. In detail, the joints of the arm include the sternoclavicular, acromioclavicular, shoulder and elbow. By arranging the three types of sensors on the joints in accordance with the DOF, the accuracy of the reconstructed motion is evaluated, resulting in an average error below 2.42\u00b0. Finally, to prove the feasibility of applying in virtual reality, we successfully manipulate the virtual avatar in real-time.", "keywords": ["angular measurement", "bending", "Bragg gratings", "fibre optic sensors", "mechanical variables measurement", "motion measurement", "shape measurement", "fiber Bragg gratings sensors", "shape sensor", "angle sensor", "electromagnetic field interference", "FBG sensors", "wearable optical fiber based motion capture sensor system", "arm motion measurement", "occlusion", "degrees of freedom", "DOF", "human body motion reconstruction", "bending angle measurement", "sternoclavicular", "acromioclavicular", "virtual reality", "virtual avatar", "Fiber Bragg gratings", "Motion capture", "Virtual reality", "FBG [Fiber Bragg grating]: DOF [Degree of freedom]"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798045", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798045", "paper_url": "https://ieeexplore.ieee.org/document/8798045"}, {"title": "Haptic Interface Based on Optical Fiber Force Myography Sensor", "authors": ["Eric Fujiwara", "Yu Tzu Wu", "Matheus K. Gomes", "Willian H. A. Silva", "Carlos K. Suzuki"], "abstract": "A haptic grasp interface based on the force myography technique is reported. The hand movements and forces during the object manipulation are assessed by an optical fiber sensor attached to the forearm, so the virtual contact is computed, and the reaction forces are delivered to the subject by graphical and vibrotactile feedbacks. The system was successfully tested for different objects, providing a non-invasive and realistic approach for applications in virtual-reality environments.", "keywords": ["Force", "Haptic interfaces", "Optical sensors", "Biomedical optical imaging", "Visualization", "Transducers", "Robot sensing systems", "fibre optic sensors", "haptic interfaces", "virtual reality", "optical fiber sensor", "virtual contact", "reaction forces", "graphical feedbacks", "vibrotactile feedbacks", "haptic interface", "optical fiber force myography sensor", "haptic grasp interface", "force myography technique", "hand movements", "object manipulation", "Human-centered computing", "Interaction devices", "Haptic devices"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797788", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797788", "paper_url": "https://ieeexplore.ieee.org/document/8797788"}, {"title": "Interaction with virtual object using deformable hand", "authors": ["Koichi Hirota", "Kazuyoshi Tagawa"], "abstract": "This study investigated the implementation of a hand model and contact simulation method for the purpose of improving the reality of object manipulation in a virtual environment. The study focused both on the hand tracking method that takes advantage of nails and also contact simulation using a deformable hand model. The manipulation of an object using a hand, is known to make more frequent use of the fingertips and palm. The proposed method seeks hand form that minimizes the position and orientation errors on those areas. Deformation of the soft tissue of the hand is considered to have an effect on both visual reality and the physical state of contact. In our implementation, the deformation was simulated by FEM and the friction of contact was introduced by the penalty method. In addition, a model that is based on metaballs (or blobs) was employed to represent the smooth surface of the object and to eliminate the problem that derives from polygon modeling. Through experimental implementation, it was proved that object manipulation such as pinching and grasping are possible and that the update rate of simulation can be approximately 50 Hz.", "keywords": ["Computational modeling", "Deformable models", "Solid modeling", "Skin", "Thumb", "Sensors", "finite element analysis", "human computer interaction", "user interfaces", "virtual reality", "human-computer interaction", "virtual object", "deformable hand model", "contact simulation", "object manipulation reality", "hand tracking method", "FEM simulation", "Deformable Hand Model", "Manipulation", "Hand-Tracking"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 7, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504687", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504687", "paper_url": "https://ieeexplore.ieee.org/document/7504687"}, {"title": "Design and Assessment of a Collaborative 3D Interaction Technique for Handheld Augmented Reality", "authors": ["Jer\u00f4nimo G Grandi", "Henrique G Debarba", "Iago Bemdt", "Luciana Nedel", "Anderson Maciel"], "abstract": "We present the design of a handheld-based interface for collaborative manipulations of 3D objects in mobile augmented reality. Our approach combines touch gestures and device movements for fast and precise control of 7-DOF transformations. Moreover, the interface creates a shared medium where several users can interact through their point-of-view and simultaneously manipulate 3D virtual augmentations. We evaluated our collaborative solution in two parts. First, we assessed our interface in single user mode, comparing the user task performance in three conditions: touch gestures, device movements and hybrid. Then, we conducted a study with 30 participants to understand and classify the strategies that arise while working in pairs, when partners are free to make their task organization. Furthermore, we investigated the effectiveness of simultaneous manipulations compared with the individual approach.", "keywords": ["Three-dimensional displays", "Collaboration", "Task analysis", "Augmented reality", "Performance evaluation", "Cameras", "Handheld computers", "augmented reality", "computer graphics", "gesture recognition", "groupware", "human computer interaction", "mobile computing", "device movements", "3D virtual augmentations", "user task performance", "touch gestures", "hybrid", "collaborative 3D interaction technique", "handheld augmented reality", "handheld-based interface", "mobile augmented reality", "3D objects collaborative manipulations", "single user mode interface", "Human-centered computing-Human computer interaction (HCI)-Interaction techniques", "Human-centered computing-Human computer interaction CHCI)-Interaction paradigms-Mixed/augmented reality Human-centered computing-Collaborative and social computing"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "Mar. 2018", "citations": 5, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446295", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446295", "paper_url": "https://ieeexplore.ieee.org/document/8446295"}, {"title": "A taxonomy for deploying redirection techniques in immersive virtual environments", "authors": ["Evan A. Suma", "Gerd Bruder", "Frank Steinicke", "David M. Krum", "Mark Bolas"], "abstract": "Natural walking can provide a compelling experience in immersive virtual environments, but it remains an implementation challenge due to the physical space constraints imposed on the size of the virtual world. The use of redirection techniques is a promising approach that relaxes the space requirements of natural walking by manipulating the user's route in the virtual environment, causing the real world path to remain within the boundaries of the physical workspace. In this paper, we present and apply a novel taxonomy that separates redirection techniques according to their geometric flexibility versus the likelihood that they will be noticed by users. Additionally, we conducted a user study of three reorientation techniques, which confirmed that participants were less likely to experience a break in presence when reoriented using the techniques classified as subtle in our taxonomy. Our results also suggest that reorientation with change blindness illusions may give the impression of exploring a more expansive environment than continuous rotation techniques, but at the cost of negatively impacting spatial knowledge acquisition.", "keywords": ["Virtual environments", "Legged locomotion", "Taxonomy", "Thyristors", "Optical character recognition software", "Blindness", "computational geometry", "knowledge acquisition", "virtual reality", "immersive virtual environments", "redirection techniques", "natural walking", "geometric flexibility", "reorientation techniques", "change blindness illusions", "continuous rotation techniques", "spatial knowledge acquisition", "Virtual environments", "redirection", "taxonomy"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 56, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180877", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180877", "paper_url": "https://ieeexplore.ieee.org/document/6180877"}, {"title": "Influence of tactile feedback and presence on egocentric distance perception in virtual environments", "authors": ["Farahnaz Ahmed", "Joseph D. Cohen", "Katherine S. Binder", "Claude L. Fennema"], "abstract": "A number of studies have reported that distance judgments are underestimated in virtual environments (VE) when compared to those made in the real world. Studies have also reported that providing users with visual feedback in the VE improves their distance perception and made them feel more immersed in the virtual world. In this study, we investigated the effect of tactile feedback and visual manipulation of the VE on egocentric distance perception. In contrast to previous studies which have focused on task-specific and error-corrective feedback (for example, providing knowledge about the errors in distance estimations), we demonstrate that exploratory feedback is sufficient for reducing errors in distance estimation. In Experiment 1, the effects of different types of feedback (visual, tactile and visual plus tactile) on distance judgments were studied. Tactile feedback was given to participants as they explored and touched objects in a VE. Results showed that distance judgments improved in the VE regardless of the type of sensory feedback provided. In Experiment 2, we presented a real world environment to the participants and then situated them in a VE that was either a replica or an altered representation of the real world environment. Results showed that participants made significant underestimation in their distance judgments when the VE was not a replica of the physical space. We further found that providing both visual and tactile feedback did not reduce distance compression in such a situation. These results are discussed in the light of the nature of feedback provided and how assumptions about the VE may affect distance perception in virtual environments.", "keywords": ["Feedback", "Virtual environment", "Psychology", "Educational institutions", "Computer graphics", "Space technology", "Extraterrestrial measurements", "Computer science", "Estimation error", "Virtual reality", "feedback", "haptic interfaces", "virtual reality", "tactile feedback", "egocentric distance perception", "virtual environments", "visual feedback", "task-specific feedback", "error-corrective feedback", "Immersive virtual reality", "Egocentric distance perception"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 1, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444791", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444791", "paper_url": "https://ieeexplore.ieee.org/document/5444791"}, {"title": "Unique shared-aperture display with head or target tracking", "authors": ["W. Chinthammit", "E.J. Seibel", "T.A. Furness"], "abstract": "The extreme environment of a military cockpit requires a novel display technology, introduced as the virtual retinal display (VRD). A head-worn VRD generates an image by optical scanning light directly to the viewer's eye. This novel display allows the direct coupling of the display to an infrared optical head tracking system, resulting in an interactive VRD. With only minor adjustments, the interactive VRD can be used in many non-military AR and VR line-of-sight target tracking applications, such as image plane manipulations. Since the unique tracking technology shares the same aperture or scanned optical beam with the visual display, the tracking produces high accuracy and computational efficiency without the motion artifacts from video frame rate tracking. The apparatus and performance of static and dynamic 3D target tracking in 2D projection overlay are demonstrated.", "keywords": ["Displays", "Target tracking", "Head", "Ultraviolet sources", "Retina", "Image generation", "Optical coupling", "Virtual reality", "Apertures", "Optical beams", "virtual reality", "user interfaces", "computer displays", "target tracking", "aircraft displays", "military aircraft", "aerospace computing", "shared-aperture display", "military cockpit", "display technology", "virtual retinal display", "head-worn display", "optical scanning", "infrared optical head tracking system", "virtual reality", "line-of-sight target tracking", "image plane manipulations", "video frame rate tracking", "performance", "3D target tracking", "2D projection overlay", "head tracking"], "published_in": "Proceedings IEEE Virtual Reality 2002", "publication_date": "2002", "citations": 0, "isbn": ["0-7695-1492-8"], "doi": "10.1109/VR.2002.996527", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=996527", "paper_url": "https://ieeexplore.ieee.org/document/996527"}, {"title": "Human movement performance in relation to path constraint - the law of steering in locomotion", "authors": ["Shumin Zhai", "R. Woltjer"], "abstract": "We examine the law of steering - a quantitative model of human movement time in relation to path width and length previously established in hand drawing movement - in a VR locomotion paradigm. Participants drove a simulated vehicle in a virtual environment on paths whose shape and width were manipulated Results showed that the law of steering also applies to locomotion. Participants' mean trial completion times linearly correlated (r/sup 2/ between 0.985 and 0.999) with an index of difficulty quantified as path distance to width ratio for the straight and circular paths used in this experiment. Their average mean and maximum speed was linearly proportional to path width. Such human performance regularity provides a quantitative tool for 3D human machine interface design and evaluation.", "keywords": ["Virtual reality", "User interfaces", "Shape", "Human computer interaction", "Vehicles", "Virtual environment", "Electrical engineering", "Keyboards", "Navigation", "Trajectory", "virtual reality", "user interfaces", "human factors", "human movement performance", "steering", "locomotion", "quantitative model", "hand drawing movement", "virtual reality", "simulated vehicle", "experiment", "speed", "3D human machine interface design"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 4, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191133", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191133", "paper_url": "https://ieeexplore.ieee.org/document/1191133"}, {"title": "A Simulation-based VR System for Interactive Hairstyling", "authors": ["K. Ward", "N. Galoppo", "M.C. Lin"], "abstract": "We have developed a physically-based VR system that enables users to interactively style dynamic virtual hair by using multiresolution simulation techniques and graphics hardware rendering acceleration for simulating and rendering hair in real time. With a 3D haptic interface, users can directly manipulate and position hair strands, as well as employ real-world styling applications (cutting, blow-drying, etc.) to create hairstyles more intuitively than previous techniques.", "keywords": ["Virtual reality", "Hair", "Rendering (computer graphics)", "Haptic interfaces", "Graphics", "Hardware", "Acceleration", "Animation", "Real time systems", "Computational modeling"], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 5, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.17", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667654", "paper_url": "https://ieeexplore.ieee.org/document/1667654"}, {"title": "wavEMS: Improving Signal Variation Freedom of Electrical Muscle Stimulation", "authors": ["Michinari Kono", "Jun Rekimoto"], "abstract": "There has been a long history in electrical muscle stimulation (EMS), which has been used for medical and interaction purposes. Human-computer interaction (HCI) researchers are now working on various applications, including virtual reality (VR), notification, and learning. For the electric signals applied to the human body, various types of waveforms have been considered and tested. In typical applications, pulses with short duration are applied, however, many perspectives are required to be considered. In addition to the duration and polarity of the pulse/waves, the wave shapes can also be an essential factor to consider. A problem of conventional EMS toolkits and systems are that they have a limitation to the variety of signals that it can produce. For example, some may be limited to monophonic pulses. Furthermore, they are usually limited to rectangular pulses and a limited range of frequencies, and other waveforms cannot be produced. These kinds of limitations make us challenging to consider variations of EMS signals in HCI research and applications. The purpose of \u201cwavEMS\u201d is to encourage testing of a variety of waveforms for EMS, which can be manipulated through audio output. We believe that this can help improve HCI applications, and to open up new application areas.", "keywords": ["Energy management", "Human computer interaction", "Medical services", "Muscles", "Safety", "History", "electromyography", "human computer interaction", "medical signal processing", "virtual reality", "EMS signals", "wavEMS", "HCI applications", "electrical muscle stimulation", "medical interaction purposes", "virtual reality", "electric signals", "human body", "wave shapes", "monophonic pulses", "rectangular pulses", "human computer interaction researchers", "signal variation freedom", "EMS toolkits", "VR", "waves polarity", "Human-centered computing", "Human computer interaction (HCI)", "Interaction devices", "Haptic devices", "General and reference", "Document types", "General literature"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798102", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798102", "paper_url": "https://ieeexplore.ieee.org/document/8798102"}, {"title": "A Multimodal Interface for Artifact's Exploration", "authors": ["Pablo Figueroa", "Pierre Boulanger", "Juan Borda", "Eduardo Londono", "Diego Restrepo", "Flavio Prieto"], "abstract": "We present an integrated interface that takes advantage of several VR technologies for the exploration of small artifacts in a Museum. This interface allows visitors to observe pieces in 3D from several viewpoints, touch them, feel their weight, and hear their sound at touch. From one hand, it will allow visitors to observe artifacts closer, in order to know more about selected pieces and to enhance their overall experience in a Museum. From the other hand, it leverages existing technologies in order to provide a multimodal interface, which can be easily replaced for others depending on costs or functionality. We describe some of the early results and experiences we provide in this setup.", "keywords": ["Haptic interfaces", "Cost function", "Computer interfaces", "Navigation", "Data visualization", "Cleaning", "Switches", "Virtual reality", "Computer graphics", "Prototypes", "user interfaces", "virtual reality", "multimodal interface", "artifact exploration", "Museum", "VR technologies", "I.3.7 [Computing Methodologies]: Computer Graphics\u00bfThree dimensional graphics and realism", "H.5.2 [Information Interfaces and Presentation]: User Interfaces\u00bfHaptic I/O"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 2, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811054", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811054", "paper_url": "https://ieeexplore.ieee.org/document/4811054"}, {"title": "mirracle: An augmented reality magic mirror system for anatomy education", "authors": ["Tobias Blum", "Valerie Kleeberger", "Christoph Bichlmeier", "Nassir Navab"], "abstract": "We present an augmented reality magic mirror for teaching anatomy. The system uses a depth camera to track the pose of a user standing in front of a large display. A volume visualization of a CT dataset is augmented onto the user, creating the illusion that the user can look into his body. Using gestures, different slices from the CT and a photographic dataset can be selected for visualization. In addition, the system can show 3D models of organs, text information and images about anatomy. For interaction with this data we present a new interaction metaphor that makes use of the depth camera. The visibility of hands and body is modified based on the distance to a virtual interaction plane. This helps the user to understand the spatial relations between his body and the virtual interaction plane.", "keywords": ["Visualization", "Glass", "Computed tomography", "Mirrors", "Cameras", "Augmented reality", "Education", "augmented reality", "biomedical education", "cameras", "computer aided instruction", "computerised tomography", "data visualisation", "mirracle", "anatomy education", "augmented reality magic mirror system", "depth camera", "pose tracking", "large display", "volume visualization", "CT dataset", "CT slices", "photographic dataset", "3D models", "organs", "text information", "interaction metaphor", "H.5.1 [Information Interfaces and Presentation]: Artificial, augmented, and virtual realities"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 75, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180909", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180909", "paper_url": "https://ieeexplore.ieee.org/document/6180909"}, {"title": "The effect of multi-sensory cues on performance and experience during walking in immersive virtual environments", "authors": ["Mi Feng", "Arindam Dey", "Robert W. Lindeman"], "abstract": "To examine the effects of multi-sensory cues during non-fatiguing walking in immersive virtual environments, we selected sensory cues including movement wind, directional wind, footstep vibration, and footstep sounds, and investigated their influence and interaction with each other. We developed a virtual reality system with non-fatiguing walking interaction and low-latency, multi-sensory feedback, and used it to conduct two successive experiments measuring user experience and performance through a triangle-completion task. We noticed some positive effects due to the addition of footstep vibration on task performance, and saw significant improvement in reported user experience due to the added wind and vibration cues.", "keywords": ["Legged locomotion", "Virtual environments", "Australia", "Navigation", "Conferences", "Vibrations", "user interfaces", "virtual reality", "multisensory cues", "immersive virtual environments", "movement wind", "directional wind", "footstep vibration", "footstep sounds", "virtual reality system", "nonfatiguing walking interaction", "multisensory feedback", "user experience", "triangle-completion task", "vibration cues", "Immersive Virtual Environments", "Multi-sensory Cues"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 2, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504709", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504709", "paper_url": "https://ieeexplore.ieee.org/document/7504709"}, {"title": "Realistic occlusion effects in mirror-based co-located augmented reality systems", "authors": ["J.D. Mulder"], "abstract": "This paper describes the incorporation of realistic occlusion effects into mirror-based, stereoscopic co-location augmented reality display systems. By adding a light-blocking device in the form of an LCD panel underneath the semi-transparent mirror, the view on the physical world can be selectively blocked out such that virtual objects can fully occlude physical objects. Furthermore, by removing areas of the virtual objects rendered on the reflected display, physical objects seen through the semi-transparent mirror and the transmissive LCD panel appear to occlude these virtual objects. We describe the governing principles of the approach, and present an efficient algorithm for the generation of the occlusion masks with the use of vision-based scene reconstruction. Finally, a first prototype implementation of the system is presented.", "keywords": ["Augmented reality", "Virtual reality", "Mirrors", "Computer displays", "Liquid crystal displays", "Layout", "Three dimensional displays", "Prototypes", "Computer graphics", "Costs", "augmented reality", "hidden feature removal", "rendering (computer graphics)", "computer displays", "three-dimensional displays", "liquid crystal displays", "mirrors", "realistic images", "image reconstruction", "realistic occlusion effects", "mirror-based colocated augmented reality systems", "stereoscopic display systems", "light-blocking device", "semitransparent mirror", "virtual objects", "rendering", "transmissive LCD panel", "occlusion masks", "vision-based scene reconstruction", "object occlusion"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 3, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492775", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492775", "paper_url": "https://ieeexplore.ieee.org/document/1492775"}, {"title": "Elastic-Arm: Human-scale passive haptic feedback for augmenting interaction and perception in virtual environments", "authors": ["Merwan Achibet", "Adrien Girard", "Anthony Talvas", "Maud Marchal", "Anatole L\u00e9cuyer"], "abstract": "Haptic feedback is known to improve 3D interaction in virtual environments but current haptic interfaces remain complex and tailored to desktop interaction. In this paper, we introduce the \u201cElastic-Arm\u201d, a novel approach for incorporating haptic feedback in immersive virtual environments in a simple and cost-effective way. The Elastic-Arm is based on a body-mounted elastic armature that links the user's hand to her shoulder. As a result, a progressive resistance force is perceived when extending the arm. This haptic feedback can be incorporated with various 3D interaction techniques and we illustrate the possibilities offered by our system through several use cases based on well-known examples such as the Bubble technique, Redirected Touching and pseudo-haptics. These illustrative use cases provide users with haptic feedback during selection and navigation tasks but they also enhance their perception of the virtual environment. Taken together, these examples suggest that the Elastic-Arm can be transposed in numerous applications and with various 3D interaction metaphors in which a mobile hap-tic feedback can be beneficial. It could also pave the way for the design of new interaction techniques based on \u201chuman-scale\u201d egocentric haptic feedback.", "keywords": ["Haptic interfaces", "Virtual environments", "Three-dimensional displays", "Navigation", "Visualization", "Force", "Prototypes", "feedback", "haptic interfaces", "navigation", "virtual reality", "human-scale passive haptic feedback", "perception", "haptic interfaces", "desktop interaction", "immersive virtual environments", "body-mounted elastic armature", "3D interaction techniques", "navigation tasks", "human-scale egocentric haptic feedback", "H.5.2 [Information Interfaces and Presentation]: User Interfaces \u2014 Interaction styles", "Haptic I/O"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 17, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223325", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223325", "paper_url": "https://ieeexplore.ieee.org/document/7223325"}, {"title": "A six DOF haptic interface for medical virtual reality applications: design, control and human factors", "authors": ["A. Benali", "P. Richard", "P. Bidaud"], "abstract": "We present a haptic interface dedicated to endoscopy training, where the endoscope is allowed to react to contact forces along six degrees of freedom. The model takes into account end point position and appropriate contact forces. The illusion of physical movement inside the environment is obtained by computing control inputs based on the impedance environment. The user can manipulate and react with the simulated virtual environment.", "keywords": ["Haptic interfaces", "Virtual reality", "Force control", "Force feedback", "Human factors", "Force sensors", "Graphics", "Surface impedance", "Endoscopes", "Physics computing", "virtual reality", "computer based training", "medical computing", "haptic interfaces", "force feedback", "six DOF haptic interface", "medical virtual reality", "human factors", "control", "design", "endoscopy training", "contact force", "end point position", "appropriate contact force", "impedance environment", "simulated virtual environment", "LRP Haptic Impedance System", "desk-top force feedback system"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 2, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840512", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840512", "paper_url": "https://ieeexplore.ieee.org/document/840512"}, {"title": "Interaction, navigation, and visualization props in complex virtual environments using image based rendering techniques", "authors": ["S.L. Stoev", "I. Peter", "W. Strasser"], "abstract": "Present an image-based technique for accelerated rendering of world-in-miniature (WIM)-based and multiple-viewpoint-based props. The WIM technique offers an intuitive and useful tool for navigation and manipulation within virtual environments. The multiple-viewpoint technique is often applied for enhancing visualization and navigation. Unfortunately, both approaches require multiple rendering of the viewed data. This can significantly deteriorate the frame rate and negatively influence the interaction. The proposed approach circumvents this drawback, while still providing the features of these tools.", "keywords": ["Navigation", "Visualization", "Rendering (computer graphics)", "Layout", "Virtual environment", "Geometry", "Portals", "Pixel", "Displays", "Acceleration", "virtual reality", "rendering (computer graphics)", "data visualisation", "utility programs", "interaction props", "navigation props", "visualization props", "complex virtual environments", "image-based rendering techniques", "world-in-miniature-based props", "multiple-viewpoint-based props", "frame rate"], "published_in": "Proceedings IEEE Virtual Reality 2001", "publication_date": "2001", "citations": 0, "isbn": ["0-7695-0948-7"], "doi": "10.1109/VR.2001.913802", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=913802", "paper_url": "https://ieeexplore.ieee.org/document/913802"}, {"title": "AMMP-EXTN: Managing User Privacy and Cooperation Demand in a Collaborative Molecule Modeling Virtual System", "authors": ["Wenjun Ma", "Ying Zhu", "Robert W. Harrison", "G. Scott Owen"], "abstract": "We present a novel design for managing competing user interests and privacy protection cooperation in a collaborative virtual environment for molecular modeling. Based on the user feedback, this design includes four levels of access control for collaborative sessions and provides dynamic action priority specification for manipulations on shared molecular models. Furthermore, we implement a messaging system that includes a text chatting tool and system broadcasting functions. Our design supports flexible user group formation and interaction. We have implemented our design on a set of Microsoft Windows PCs connected to a Linux molecule simulation server. A GUI interface developed in Python is also illustrated", "keywords": ["Privacy", "Collaboration", "Environmental management", "Protection", "Virtual environment", "Feedback", "Access control", "Manipulator dynamics", "Broadcasting", "Personal communication networks", "authorisation", "biology computing", "data privacy", "feedback", "graphical user interfaces", "groupware", "Linux", "molecular biophysics", "virtual reality", "user privacy", "cooperation demand", "collaborative molecule modeling", "collaborative virtual environment", "user feedback", "access control", "Linux molecule simulation server", "GUI interface", "Collaborative virtual reality", "Graphical environments", "Access control", "Computer conferencing"], "published_in": "2007 IEEE Virtual Reality Conference", "publication_date": "March 2007", "citations": 2, "isbn": ["1-4244-0905-5", "1-4244-0906-3"], "doi": "10.1109/VR.2007.352512", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4161054", "paper_url": "https://ieeexplore.ieee.org/document/4161054"}, {"title": "Automatic color realism enhancement for virtual reality", "authors": ["Hyunjung Shim", "Seungkyu Lee"], "abstract": "Photorealism has been one of essential goals for virtual reality. The state-of-the-art techniques employ various rendering algorithms to simulate physically accurate light transport for generating the photorealistic appearance of scene. However, they still require a labor-intensive tone mapping and color tunes by an experienced artist. In this paper, we propose an automatic photorealism enhancement algorithm by manipulating the color distribution of graphics to match with that of real photographs. Based on the hypothesis that photorealism is highly correlated with the frequency of color characteristics appearing in real photographs, we find principal color components. Then, we transfer the statistical characteristics of photographs onto graphics so to enhance their photorealism. Experiments and a user study have confirmed the effectiveness of proposed method.", "keywords": ["Image color analysis", "Support vector machines", "Minimization", "Virtual reality", "Vectors", "Computers", "rendering (computer graphics)", "statistical analysis", "virtual reality", "automatic color realism enhancement", "virtual reality", "rendering algorithms", "physically accurate light transport", "photorealistic scene appearance", "tone mapping", "color tunes", "graphics color distribution", "principal color components", "statistical photographs characteristics", "automatic photorealism enhancement algorithm"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 1, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180881", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180881", "paper_url": "https://ieeexplore.ieee.org/document/6180881"}, {"title": "The effect of an occluder on near field depth matching in optical see-through augmented reality", "authors": ["Chunya Hua", "Kenneth Moser", "J. Edward Swan"], "abstract": "We have conducted an experiment to study the effect of an occluding surface on the accuracy of near field depth matching in augmented reality (AR). Our experiment was based on replicating a similar experiment conducted by Edwards et al. [2]. We used an AR haploscope [1], which allows us to independently manipulate accommodative demand and vergence angle of the visible image. Fifteen observers matched the perceived depth of an AR-presented virtual object with a physical pointer. Overall, observers overestimated depth by 5 mm or less in the presence of the occluder, while in the absence of an occluder they overestimated depth by 5 to 10 mm. The data from Edwards et al. [2] is normalized, and when we performed the same normalization procedure on our own data, our results do not agree with Edwards et al. [2]. We suspect that eye vergence explains these results.", "keywords": ["Augmented reality", "Optical imaging", "Educational institutions", "Observers", "Hardware", "Light emitting diodes", "Abstracts", "augmented reality", "computer graphic equipment", "normalization", "eye vergence", "AR-presented virtual object", "visible image vergence angle", "AR haploscope", "occluding surface effect", "optical see-through augmented reality", "near field depth matching", "occluder effect"], "published_in": "2014 IEEE Virtual Reality (VR)", "publication_date": "March 2014", "citations": 1, "isbn": ["978-1-4799-2871-2"], "doi": "10.1109/VR.2014.6802095", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6802095", "paper_url": "https://ieeexplore.ieee.org/document/6802095"}, {"title": "The effect of an occluder on near field depth matching in optical see-through augmented reality", "authors": ["Chunya Hua", "J. Edward Swan"], "abstract": "We have conducted an experiment to study the effect of an occluding surface on the accuracy of near field depth matching in augmented reality (AR). Our experiment was based on replicating a similar experiment conducted by Edwards et al. [2]. We used an AR haploscope, which allows us to independently manipulate accommodative demand and vergence angle. Sixteen observers matched the perceived depth of an AR-presented virtual object with a physical pointer. Overall, observers overestimated depth by 6 mm or less with or without the presence of the occluder. The data from Edwards et al. [2] is normalized, and when we performed the same normalization procedure on our own data, our results do not agree with Edwards et al. [2]. We suspect that eye vergence explains these results.", "keywords": ["Observers", "Augmented reality", "Surgery", "Context", "Head", "Biomedical optical imaging", "Educational institutions", "augmented reality", "pattern matching", "occluder", "near field depth matching", "optical see-through augmented reality", "occluding surface", "AR haploscope", "accommodative demand", "vergence angle", "AR-presented virtual object", "perceived depth", "normalization procedure", "Depth perception", "augmented reality"], "published_in": "2014 IEEE Virtual Reality (VR)", "publication_date": "March 2014", "citations": 0, "isbn": ["978-1-4799-2871-2"], "doi": "10.1109/VR.2014.6802061", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6802061", "paper_url": "https://ieeexplore.ieee.org/document/6802061"}, {"title": "Route Previews: Enhancing the Route Selection", "authors": ["P. Sadeghian", "M. Kantardzic", "O. Lozitskiy", "Y. Lozitskiy"], "abstract": "In large-scale virtual environments (VEs), there are often several different routes to a target destination. In this paper, we introduce a route preview tool to help users of VEs with the route selection process. Our experiments showed that the proposed route preview tool provides the necessary support to help users select a desirable route.", "keywords": ["Virtual reality", "USA Councils", "Cities and towns", "Virtual Environments", "Route Selection", "Virtual Environments", "Route Selection"], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 0, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.117", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667662", "paper_url": "https://ieeexplore.ieee.org/document/1667662"}, {"title": "Simulation of Augmented Reality Systems in Purely Virtual Environments", "authors": ["Eric Ragan", "Curtis Wilkes", "Doug A. Bowman", "Tobias Hollerer"], "abstract": "We propose the use of virtual environments to simulate augmented reality (AR) systems for the purposes of experimentation and usability evaluation. This method allows complete control in the AR environment, providing many advantages over testing with true AR systems. We also discuss some of the limitations to the simulation approach. We have demonstrated the use of such a simulation in a proof of concept experiment controlling the levels of registration error in the AR scenario. In this experiment, we used the simulation method to investigate the effects of registration error on task performance for a generic task involving precise motor control for AR object manipulation. Isolating jitter and latency errors, we provide empirical evidence of the relationship between accurate registration and task performance.", "keywords": ["Augmented reality", "Virtual environment", "Virtual reality", "System testing", "Error correction", "Displays", "Computational modeling", "Computer simulation", "Computer science", "Usability", "augmented reality", "digital simulation", "augmented reality system", "virtual environment", "usability evaluation", "system simulation", "registration error", "task performance", "motor control", "object manipulation", "Augmented reality", "AR simulation", "registration error"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 24, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811058", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811058", "paper_url": "https://ieeexplore.ieee.org/document/4811058"}, {"title": "A scalable, multi-user VRML server", "authors": ["T. Rischbeck", "P. Watson"], "abstract": "VRML97 allows the description of dynamic worlds that can change with both the passage of time, and user interaction. Unfortunately, the current VRML usage model prevents its full potential from being realized. Initially, the whole world must be loaded into the user's desktop browser, and so large worlds can take a very long time to download and render, while a world cannot be shared among multiple users. This paper describes the design and implementation of a client-server architecture that was built to overcome these problems. The major novelty is the decoupling of VRML world execution from world rendering. Parallelism and information filtering are exploited to produce a highly scalable system that can support huge, highly active worlds, accessed simultaneously by large numbers of users. A cluster-based parallel server is responsible for maintaining the dynamic world state, and most of the world dynamics are evaluated on the server side. The server streams VRML to the client, using view frustum culling and dynamic LOD selection to reduce clients' network bandwidth, storage and rendering requirements. Clients with limited resources (e.g. wireless-connected PDAs) can therefore participate in highly complex virtual worlds. While the implementation of the design focuses on VRML worlds, the design ideas could be exploited in other types of VR system, e.g. X3D.", "keywords": ["Read only memory", "Vehicle dynamics", "Virtual reality", "Internet", "Cities and towns", "Traffic control", "Filtering", "Bandwidth", "Personal digital assistants", "Navigation", "groupware", "virtual reality languages", "user interfaces", "client-server systems", "Internet", "multi-user VRML server", "dynamic worlds", "user interaction", "VRML usage model", "desktop browser", "client-server architecture", "world rendering", "information filtering", "Internet", "scalable system", "cluster-based parallel server", "view frustum culling", "dynamic LOD selection", "network bandwidth", "Virtual Reality Modeling Language"], "published_in": "Proceedings IEEE Virtual Reality 2002", "publication_date": "2002", "citations": 1, "isbn": ["0-7695-1492-8"], "doi": "10.1109/VR.2002.996523", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=996523", "paper_url": "https://ieeexplore.ieee.org/document/996523"}, {"title": "Acoustic redirected walking with auditory cues by means of wave field synthesis", "authors": ["Malte Nogalski", "Wolfgang Fohl"], "abstract": "We present an experiment to identify detection thresholds for acoustic redirected walking by means of wave field synthesis. The most natural way to navigate an avatar through an immersive virtual environment (IVE) is by copying the tracked physical movements of a user. Redirected walking offers an approach to tackle the discrepancy between the potentially infinite IVE and the generally limited available physical space or tracking area, by applying manipulations, such as rotations or translations, to the IVE in form of gains to the users movements. 39 blindfolded test subjects performed a total of 2777 constant stimulus trials with various amounts of rotation and curvature gains. The test subjects were divided into four groups with different knowledge of the experiment, and one group performed two-alternative-forced-choice tasks, while the others could give feedback freely. The detection thresholds were greatly dependent on the groups i.e., the knowledge of the experiment. The 25% detection threshold was reached by the most relevant test group at gains that up-scaled rotations by 5%, down-scaled them by 37.5%, and bend a straight path into a circle with a radius of 5.71 meters. Almost no signs of simulator sickness could be observed.", "keywords": ["Legged locomotion", "Acoustics", "Tracking", "Virtual environments", "Navigation", "Software", "avatars", "acoustic redirected walking", "auditory cues", "wave field synthesis", "avatar", "immersive virtual environment", "IVE", "curvature gains", "simulator sickness", "H.1.2 [Models and Principles]: User/Machine Systems \u2014 Software psychology, Human factors", "J.7 [Computers in other systems]: Realtime \u2014", "H.5.1 [Information and Interfaces and Presentation]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 4, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504745", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504745", "paper_url": "https://ieeexplore.ieee.org/document/7504745"}, {"title": "Towards a disambiguation canvas", "authors": ["Henrique G. Debarba", "Jer\u00f4nimo Grandi", "Anderson Maciel", "Luciana Nedel", "Ronan Boulic"], "abstract": "We present Disambiguation canvas, a technique for selection by progressive refinement using a mobile device and consisting of two steps. During the first, the user defines a subset of objects through the orientation sensors of the device and a volume-casting pointing technique. The subsequent step consists of the disambiguation of the desired target among the previously-defined subset of objects, and is accomplished using the mobile device touchscreen. By relying on the touchscreen for the last step the user can disambiguate among hundreds of objects at once, previous progressive refinement techniques do not scale as well as ours. Disambiguation canvas is mainly developed for easy, accurate and fast selection of small objects, or objects inside cluttered virtual environments. User tests show that our technique performs faster than ray-casting for targets with \u2248 0.53\u00b0 of angular size, and is also much more accurate for all the tested target sizes.", "keywords": ["Mobile handsets", "Layout", "Electronic mail", "Performance evaluation", "Casting", "User interfaces", "Accuracy", "mobile computing", "ray tracing", "touch sensitive screens", "virtual reality", "disambiguation canvas", "mobile device", "orientation sensors", "volume-casting pointing technique", "mobile device touchscreen", "progressive refinement techniques", "cluttered virtual environments", "ray-casting", "H.5.2 [User Interfaces]: Input devices and strategies (e.g mouse, touchscreen)"], "published_in": "2013 IEEE Virtual Reality (VR)", "publication_date": "March 2013", "citations": 0, "isbn": ["978-1-4673-4796-9", "978-1-4673-4795-2"], "doi": "10.1109/VR.2013.6549388", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6549388", "paper_url": "https://ieeexplore.ieee.org/document/6549388"}, {"title": "mpCubee: Towards a mobile perspective cubic display using mobile phones", "authors": ["Jens Grubert", "Matthias Kranz"], "abstract": "While we witness significant changes in display technologies, to date, the majority of display form factors remain flat. The research community has investigated other geometric display configuration given the rise to cubic displays that create the illusion of a 3D virtual scene within the cube. We present a self-contained mobile perspective cubic display (mpCubee) assembled from multiple smartphones. We achieve perspective correct projection of 3D content through head-tracking using built-in cameras in smartphones. Furthermore, our prototype allows to spatially manipulate 3D objects on individual axes due to the orthogonal configuration of touch displays.", "keywords": ["Three-dimensional displays", "Mobile communication", "Smart phones", "Head", "Rendering (computer graphics)", "Cameras", "cameras", "computer displays", "mobile computing", "mobile handsets", "smart phones", "mpCubee", "mobile phones", "geometric display configuration", "3D virtual scene", "self-contained mobile perspective cubic display", "smartphones", "3D content perspective correct projection", "head-tracking", "built-in cameras", "3D object manipulation", "orthogonal configuration", "touch displays", "H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems \u2014 Artificial, augmented, and virtual realities"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 1, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892378", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892378", "paper_url": "https://ieeexplore.ieee.org/document/7892378"}, {"title": "Coretet: A Dynamic Virtual Musical Instrument for the Twenty-First Century", "authors": ["Rob Hamilton"], "abstract": "Coretet is a virtual reality musical instrument that explores the translation of performance gesture and mechanic from traditional bowed string instruments into an inherently non-physical implementation. Built using the Unreal Engine 4 and Pure Data, Coretet offers musicians both a flexible and articulate musical instrument to playas well as a networked performance environment capable of supporting and presenting a traditional four-member string quartet. Building on traditional stringed instrument performance practices, Coretet was designed as a futuristic `21\nst\n Century' implementation of the core gestural and interaction modalities that generate musical sound in the violin, viola and cello. Coretet exists as a client-server software system designed to be controlled using an Oculus Rift head-mounted display (HMD) and the Oculus Touch hand-tracking controllers. The instrument and performance environment are built using the Unreal Engine 4. Gesture and audio output is generated using interaction data from the engine streamed to a Pure Data (PD) [2] server via Open Sound Control (OSC) [3]. Within PD, gestural control data from Coretet is processed and used to control a variety of audio generation and manipulation processes including the [bowed~] string physical model from the Synthesis Toolkit (STK) [1].", "keywords": ["Music", "Instruments", "Engines", "Virtual reality", "Conferences", "Manipulator dynamics", "Solid modeling", "client-server systems", "gesture recognition", "helmet mounted displays", "music", "musical acoustics", "musical instruments", "virtual reality", "Coretet", "dynamic virtual musical instrument", "virtual reality musical instrument", "performance gesture", "nonphysical implementation", "Unreal Engine 4", "flexible instrument", "articulate musical instrument", "networked performance environment", "four-member string quartet", "core gestural", "interaction modalities", "musical sound", "Oculus Touch hand-tracking controllers", "gestural control data", "string physical model", "Pure Data server", "Virtual Instruments for Music Expression (VIME)"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797680", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797680", "paper_url": "https://ieeexplore.ieee.org/document/8797680"}, {"title": "Coupling Ontologies with Graphics Content for Knowledge Driven Visualization", "authors": ["E. Kalogerakis", "S. Christodoulakis", "N. Moumoutzis"], "abstract": "A great challenge in information visualization today is to provide models and software that effectively integrate the graphics content of scenes with domain-specific knowledge so that the users can effectively query, interpret, personalize and manipulate the visualized information [1]. Moreover, it is important that the intelligent visualization applications are interoperable in the semantic web environment and thus, require that the models and software supporting them integrate state-of-the-art international standards for knowledge representation, graphics and multimedia. In this paper, we present a model, a methodology and a software framework for the semantic web (Intelligent 3D Visualization Platform - I3DVP) for the development of interoperable intelligent visualization applications that support the coupling of graphics and virtual reality scenes with domain knowledge of different domains. The graphics content and the semantics of the scenes are married into a consistent and cohesive ontological model while at the same time knowledge- based techniques for the querying, manipulation, and semantic personalization of the scenes are introduced. We also provide methods for knowledge driven information visualization and visualization- aided decision making based on inference by reasoning.", "keywords": ["Ontologies", "Graphics", "Visualization", "Layout", "Application software", "Semantic Web", "Software standards", "Knowledge representation", "Virtual reality", "Decision making", "ontologies", "web graphics", "semantic driven visualization", "intelligent virtual environments", "domain knowledge", "ontologies", "web graphics", "semantic driven visualization", "intelligent virtual environments", "domain knowledge"], "published_in": "IEEE Virtual Reality Conference (VR 2006)", "publication_date": "2006", "citations": 32, "isbn": ["1-4244-0224-7"], "doi": "10.1109/VR.2006.41", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1667625", "paper_url": "https://ieeexplore.ieee.org/document/1667625"}, {"title": "Spatialized Haptic Rendering: Providing Impact Position Information in 6DOF Haptic Simulations Using Vibrations", "authors": ["Jean Sreng", "Anatole Lecuyer", "Claude Andriot", "Bruno Arnaldi"], "abstract": "In this paper we introduce a \"spatialized haptic rendering\" technique to enhance 6DOF haptic manipulation of virtual objects with impact position information using vibrations. This rendering technique uses our perceptive ability to determine the contact position by using the vibrations generated by the impact. In particular, the different vibrations generated by a beam are used to convey the impact position information. We present two experiments conducted to tune and evaluate our spatialized haptic rendering technique. The first experiment investigates the vibration parameters (amplitudes/frequencies) needed to enable an efficient discrimination of the force patterns used for spatialized haptic rendering. The second experiment is an evaluation of spatialized haptic rendering during 6DOF manipulation. Taken together, the results suggest that spatialized haptic rendering can be used to improve the haptic perception of impact position in complex 6DOF interactions.", "keywords": ["Haptic interfaces", "Vibrations", "Rendering (computer graphics)", "Virtual reality", "Frequency", "Force feedback", "Rough surfaces", "Surface roughness", "Probes", "Multimedia systems", "haptic interfaces", "rendering (computer graphics)", "virtual reality", "spatialized haptic rendering", "6DOF haptic simulations", "vibrations", "virtual objects", "force patterns", "Haptic rendering", "force-feedback", "vibration", "spatialization", "6DOF", "contact", "impact", "open-loop", "H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems\u00bfArtificial, Augmented, and Virtual Realities", "H.5.2 [Information Interfaces and Presentation]: User Interfaces\u00bfHaptics I/O"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 3, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4810990", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4810990", "paper_url": "https://ieeexplore.ieee.org/document/4810990"}, {"title": "Perception of Redirected Pointing Precision in Immersive Virtual Reality", "authors": ["Henrique G Debarba", "Jad-Nicolas Khoury", "Sami Perrin", "Bruno Herbelin", "Ronan Boulic"], "abstract": "We investigate the self-attribution of distorted pointing movements in immersive virtual reality. Participants had to complete a multidirectional pointing task in which the visual feedback of the tapping finger could be deviated in order to increase or decrease the motor size of a target relative to its visual appearance. This manipulation effectively makes the task easier or harder than the visual feedback suggests. Participants were asked whether the seen movement was equivalent to the movement they performed, and whether they have been successful in the task. We show that participants are often unaware of the movement manipulation, even when it requires higher pointing precision than suggested by the visual feedback. Moreover, subjects tend to self-attribute movements that have been modified to make the task easier more often than movements that have not been distorted. We discuss the implications and applications of our results.", "keywords": ["Task analysis", "Distortion", "Visualization", "Haptic interfaces", "Indexes", "Electronic mail", "Virtual reality", "human computer interaction", "user interfaces", "virtual reality", "immersive virtual reality", "multidirectional pointing task", "visual feedback", "tapping finger", "motor size", "visual appearance", "movement manipulation", "higher pointing precision", "self-attribute movements", "redirected pointing precision", "distorted pointing movements", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 1, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8448285", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8448285", "paper_url": "https://ieeexplore.ieee.org/document/8448285"}, {"title": "Physically-based manipulation on the Responsive Workbench", "authors": ["B. Frohlich", "H. Tramberend", "A. Beers", "M. Agrawala", "D. Baraff"], "abstract": "Describes how a physical simulation can be integrated with our Responsive Workbench system to support complex assembly tasks involving multiple hands and users. Our system uses the CORIOLIS physical simulation package extended to meet the real-time requirements for our highly interactive virtual environment. We develop a new set of interface tools that exploit the natural properties of physical simulation (i.e. the superposition of forces). Our tools are based on sets of springs connecting the user's hand to a virtual object. Visualizing these springs provides \"visual force feedback\" of the applied forces and facilitates the prediction of the objects' behavior. Our force-based interaction concept allows multiple hands and users to manipulate a single object without the need for locking the object.", "keywords": ["Virtual environment", "Object detection", "Springs", "Computational modeling", "Displays", "Information technology", "Assembly systems", "Packaging", "Joining processes", "Visualization", "force feedback", "virtual reality", "real-time systems", "assembling", "digital simulation", "software packages", "industrial manipulators", "engineering graphics", "data gloves", "production engineering computing", "physically-based manipulation", "Responsive Workbench", "CORIOLIS physical simulation package", "complex assembly tasks", "multiple hands", "multiple users", "real-time requirements", "interactive virtual environment", "user interface tools", "force superposition", "springs", "virtual object behaviour prediction", "visual force feedback", "force-based interaction"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 18, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840357", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840357", "paper_url": "https://ieeexplore.ieee.org/document/840357"}, {"title": "Immersive well-path editing: investigating the added value of immersion", "authors": ["K. Gruchalla"], "abstract": "The benefits of immersive visualization are primarily anecdotal; there have been few controlled user studies that have attempted to quantify the added value of immersion for problems requiring the manipulation of virtual objects. This research quantifies the added value of immersion for a real-world industrial problem: oil well-path planning. An experiment was designed to compare human performance between an immersive virtual environment (IVE) and a desktop workstation. This work presents the results of sixteen participants who planned the paths of four oil wells. Each participant planned two well-paths on a desktop workstation with a stereoscopic display and two well-paths in a CAVE/spl trade/-like IVE. Fifteen of the participants completed well-path editing tasks faster in the IVE than in the desktop environment. The increased speed was complimented by a statistically significant increase in correct solutions in the IVE. The results suggest that an IVE allows for faster and more accurate problem solving in a complex three-dimensional domain.", "keywords": ["Workstations", "Displays", "Navigation", "Virtual environment", "Buildings", "Visualization", "Petroleum", "Graphics", "Humans", "Layout", "virtual reality", "industries", "data visualisation", "human factors", "immersive well-path editing", "immersive visualization", "virtual object manipulation", "industrial problem", "oil well-path planning", "immersive virtual environment", "desktop workstation", "stereoscopic display", "CAVE/spl trade/"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 26, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310069", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310069", "paper_url": "https://ieeexplore.ieee.org/document/1310069"}, {"title": "Realistic virtual grasping", "authors": ["C.W. Borst", "A.P. Indugula"], "abstract": "We present a physically-based approach to grasping and manipulation of virtual objects that produces visually realistic results, addresses the problem of visual interpenetration of hand and object models, and performs force rendering for force-feedback gloves in a single framework. Our approach couples tracked hand configuration to a simulation-controlled articulated hand model using a system of linear and torsional spring-dampers. We discuss an implementation of our approach that uses a widely-available simulation tool for collision detection and response. We illustrate the resulting behavior of the virtual hand model and of grasped objects, and we show that the simulation rate is sufficient for control of current force-feedback glove designs. We also present a prototype of a system we are developing to support natural whole-hand interactions in a desktop-sized workspace.", "keywords": ["Grasping", "Force control", "Physics computing", "Computer graphics", "Virtual environment", "Haptic interfaces", "Force feedback", "Computational modeling", "Prototypes", "Chromium", "virtual reality", "data gloves", "human computer interaction", "force feedback", "solid modelling", "rendering (computer graphics)", "realistic virtual grasping", "virtual object manipulation", "visual interpenetration", "object model", "force rendering", "force-feedback gloves", "hand configuration tracking", "simulation-controlled articulated hand model", "linear spring-dampers", "torsional spring-dampers", "collision detection", "virtual hand model", "physically-based interaction", "data glove", "desktop virtual reality", "3D interaction"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 19, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492758", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492758", "paper_url": "https://ieeexplore.ieee.org/document/1492758"}, {"title": "Enabling functional validation of virtual cars through Natural Interaction metaphors", "authors": ["Mathias Moehring", "Bernd Froehlich"], "abstract": "Natural Interaction in virtual environments is a key requirement for the virtual validation of functional aspects in automotive product development processes. Natural Interaction is the metaphor people encounter in reality: the direct manipulation of objects by their hands. To enable this kind of Natural Interaction, we propose a pseudo-physical metaphor that is both plausible enough to provide realistic interaction and robust enough to meet the needs of industrial applications. Our analysis of the most common types of objects in typical automotive scenarios guided the development of a set of refined grasping heuristics to support robust finger-based interaction of multiple hands and users. The objects' behavior in reaction to the users' finger motions is based on pseudo-physical simulations, which also take various types of constrained objects into account. In dealing with real-world scenarios, we had to introduce the concept of Normal Proxies, which extend objects with appropriate normals for improved grasp detection and grasp stability. An expert review revealed that our interaction metaphors allow for an intuitive and reliable assessment of several functionalities of objects found in a car interior.", "keywords": ["Virtual reality", "Grasping", "Robustness", "Automotive engineering", "Robust stability", "Fingers", "Object detection", "Virtual environment", "Hardware", "Electrical equipment industry", "automotive engineering", "haptic interfaces", "human computer interaction", "product development", "virtual reality", "functional validation", "virtual cars", "natural interaction metaphors", "virtual environments", "automotive product development", "metaphor people encounter", "realistic interaction", "industrial applications", "automotive scenarios", "grasping heuristics", "robust finger-based interaction", "user finger motions", "pseudo-physical simulations", "real-world scenarios", "normal proxies", "grasp detection", "grasp stability", "pseudophysical metaphor", "Direct Interaction", "Immersive Applications"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 13, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444819", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444819", "paper_url": "https://ieeexplore.ieee.org/document/5444819"}, {"title": "Streaming 3D shape deformations in collaborative virtual environment", "authors": ["Ziying Tang", "Guodong Rong", "Xiaohu Guo", "B. Prabhakaran"], "abstract": "Collaborative virtual environment has been limited on static or rigid 3D models, due to the difficulties of real-time streaming of large amounts of data that is required to describe motions of 3D deformable models. Streaming shape deformations of complex 3D models arising from a remote user's manipulations is a challenging task. In this paper, we present a framework based on spectral transformation that encodes surface deformations in a frequency format to successfully meet the challenge, and demonstrate its use in a distributed virtual environment. Our research contributions through this framework include: i) we reduce the data size to be streamed for surface deformations since we stream only the transformed spectral coefficients and not the deformed model; ii) we propose a mapping method to allow models with multi-resolutions to have the same deformations simultaneously; iii) our streaming strategy can tolerate loss without the need for special handling of packet loss. Our system guarantees real-time transmission of shape deformations and ensures the smooth motions of 3D models. Moreover, we achieve very effective performance over real Internet conditions as well as a local LAN. Experimental results show that we get low distortion and small delays even when surface deformations of large and complicated 3D models are streamed over lossy networks.", "keywords": ["Shape", "Collaboration", "Virtual environment", "Deformable models", "High performance computing", "Computer networks", "Multiresolution analysis", "Simultaneous localization and mapping", "Spatial resolution", "Internet", "deformation", "groupware", "Internet", "local area networks", "virtual reality", "3D shape deformations streaming", "collaborative virtual environment", "real time streaming", "3D deformable models", "spectral transformation", "surface deformations", "distributed virtual environment", "mapping method", "streaming strategy", "Internet", "local LAN", "lossy networks", "Collaborative Interaction", "Distributed Virtual Reality", "3D Shape Deformation"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 5, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444793", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444793", "paper_url": "https://ieeexplore.ieee.org/document/5444793"}, {"title": "Virtual Heliodon: Spatially Augmented Reality for Architectural Daylighting Design", "authors": ["Yu Sheng", "Theodore C. Yapo", "Christopher Young", "Barbara Cutler"], "abstract": "We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the inter-reflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural illumination within a proposed design by controlling the time of day, season, and climate. Furthermore, participants may interactively redesign the geometry and materials of the space by manipulating physical design elements and see the updated lighting simulation.", "keywords": ["Augmented reality", "Daylighting", "Lighting", "Rendering (computer graphics)", "Buildings", "Cameras", "Layout", "Computer graphics", "Hybrid power systems", "Animation", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u00bfRadiosity, Virtual Reality", "H.5.1 [Information Interfaces and Presentation (HCI)]: Multimedia Information Systems\u00bfArtificial, augmented, and virtual realities"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 7, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4811000", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4811000", "paper_url": "https://ieeexplore.ieee.org/document/4811000"}, {"title": "The Impact of Avatar Tracking Errors on User Experience in VR", "authors": ["Nicholas Toothman", "Michael Neff"], "abstract": "There is evidence that adding motion-tracked avatars to virtual environments increases users' sense of presence. High quality motion capture systems are cost sensitive for the average user and low cost resource-constrained systems introduce various forms of error to the tracking. Much research has looked at the impact of particular kinds of error, primarily latency, on factors such as body ownership, but it is still not known what level of tracking error is permissible in these systems to afford compelling social interaction. This paper presents a series of experiments employing a sizable subject pool (n=96) that study the impact of motion tracking errors on user experience for activities including social interaction and virtual object manipulation. Diverse forms of error that arise in tracking are examined, including latency, popping (jumps in position), stuttering (positions held in time) and constant noise. The focus is on error on a person's own avatar, but some conditions also include error on an interlocutor, which appears underexplored. The picture that emerges is complex. Certain forms of error impact performance, a person's sense of embodiment' enjoyment and perceived usability, while others do not. Notably, evidence was not found that tracking errors impact social presence, even when those errors are severe.", "keywords": ["Task analysis", "Tracking", "Avatars", "Delays", "Jitter", "Headphones", "Atmospheric measurements", "avatars", "user experience", "body ownership", "tracking error", "user experience", "virtual object manipulation", "social presence", "motion-tracked avatars", "virtual environments", "low cost resource-constrained systems", "social interaction", "motion capture systems"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798108", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798108", "paper_url": "https://ieeexplore.ieee.org/document/8798108"}, {"title": "Ad-hoc Study on Soldiers Calibration Procedure in Virtual Reality", "authors": ["Jean-Daniel Taupiac", "Nancy Rodriguez", "Olivier Strauss", "Martin Rabier"], "abstract": "French Army infantrymen's are equipped today with a combat system called FELIN, which includes an infrared sighting device: the IR sight. One of the first manipulations learned by the soldier is the IR sight calibration. Currently, calibration training is a two-step process. The first step consists of practicing on a 2D WIMP software until making no mistakes. Then, the soldiers can apply his knowledge in the real situation on the shooting range. In this paper, we present an ad-hoc study of a learning method including a prototype in Virtual Reality for training on the FELIN IR sight calibration procedure. It has been experimented on real infantrymen learners in an infantry school. Results showed an attractive added value of Virtual Reality in this specific use case. It improved the learners' intrinsic motivation to repeat the training task as well as the learning efficiency. It also helped the training team to identify specific mistake types not detected by the traditional learning software.", "keywords": ["Training", "Virtual reality", "Software", "Calibration", "Two dimensional displays", "Prototypes", "Task analysis", "calibration", "computer aided instruction", "human factors", "military computing", "military equipment", "virtual reality", "weapons", "training task", "learning efficiency", "training team", "virtual reality", "soldier calibration procedure", "2D WIMP software", "infantrymen learners", "FELIN IR sight calibration procedure", "learning method", "shooting range", "two-step process", "calibration training", "infrared sighting device", "combat system", "French Army infantrymen", "ad-hoc study"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 2, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797854", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797854", "paper_url": "https://ieeexplore.ieee.org/document/8797854"}, {"title": "Evaluating the Influence of Haptic Force-Feedback on 3D Selection Tasks using Natural Egocentric Gestures", "authors": ["Vijay M Pawar", "Anthony Steed"], "abstract": "Immersive virtual environments (IVEs) allow participants to interact with their 3D surroundings using natural hand gestures. Previous work shows that the addition of haptic feedback cues improves performance on certain 3D tasks. However, we believe this is not true for all situations. Depending on the difficulty of the task, we suggest that we should expect differences in the ballistic movement of our hands when presented with different types of haptic force-feedback conditions. We investigated how hard, soft and no haptic force-feedback responses, experienced when in contact with the surface of an object, affected user performance on a task involving selection of multiple targets. To do this, we implemented a natural egocentric selection interaction technique by integrating a two-handed large-scale force-feedback device in to a CAVE\nTM\n-like IVE system. With this, we performed a user study where we show that participants perform selection tasks best when interacting with targets that exert soft haptic force-feedback cues. For targets that have hard and no force-feedback properties, we highlight certain associated hand movement that participants make under these conditions, that we hypothesise reduce their performance.", "keywords": ["Haptic interfaces", "force feedback", "gesture recognition", "haptic interfaces", "virtual reality", "3D selection task", "natural egocentric gestures", "immersive virtual environment", "hand gestures", "ballistic movement", "natural egocentric selection interaction", "haptic force feedback cues", "hand movement", "Haptics", "3D selection", "task performance", "two-handed interaction", "force-feedback", "H.5.2 [Information Interfaces and Presentation]: User Interfaces\u00bfHaptics", "I.3.6 [Computer Graphics]: Methodology and Techniques\u00bfInteraction techniques"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 4, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4810992", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4810992", "paper_url": "https://ieeexplore.ieee.org/document/4810992"}, {"title": "3DTouch: A wearable 3D input device for 3D applications", "authors": ["Anh Nguyen", "Amy Banic"], "abstract": "3D applications appear in every corner of life in the current technology era. There is a need for an ubiquitous 3D input device that works with many different platforms, from head-mounted displays (HMDs) to mobile touch devices, 3DTVs, and even the Cave Automatic Virtual Environments. We present 3DTouch, a novel wearable 3D input device worn on the fingertip for 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D input device that is self-contained, mobile, and universally works across various 3D platforms. This paper presents a low-cost solution to designing and implementing such a device. Our approach relies on a relative positioning technique using an optical laser sensor and a 9-DOF inertial measurement unit. The device employs touch input for the benefits of passive haptic feedback, and movement stability. On the other hand, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices. We propose a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. We envision that modular solutions like 3DTouch opens up a whole new design space for interaction techniques to further develop on. With 3DTouch, we attempt to bring 3D applications a step closer to users.", "keywords": ["Three-dimensional displays", "Mice", "Tracking", "Accuracy", "Performance evaluation", "Optical sensors", "Shape", "feedback", "haptic interfaces", "helmet mounted displays", "mobile computing", "virtual reality", "wearable computers", "3DTouch", "wearable 3D input device", "3D applications", "ubiquitous 3D input device", "head-mounted displays", "HMD", "mobile touch devices", "3DTV", "cave automatic virtual environments", "3D manipulation tasks", "optical laser sensor", "9-DOF inertial measurement unit", "passive haptic feedback", "movement stability", "H.5.2 [Information interfaces and presentation]: User Interfaces \u2014 Graphical user interfaces \u2014 Input devices and strategies"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 1, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223324", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223324", "paper_url": "https://ieeexplore.ieee.org/document/7223324"}, {"title": "Extending locales: awareness management in MASSIVE-3", "authors": ["J. Purbrick", "C. Greenhalgh"], "abstract": "In the MASSIVE-3 system we have adopted the locale approach to organising large virtual environments, and extended it, integrating the notion of awareness, adding support for alternative representations of locales, integrating functional and organisation data management and introducing a flexible framework for defining dynamic locale selection policies.", "keywords": ["Virtual environment", "Cost function", "Computer science", "Information technology", "Technology management", "Environmental management", "Spline", "Tiles", "Bandwidth", "Topology", "virtual reality", "awareness management", "MASSIVE-3", "locale approach", "large virtual environments", "alternative representations", "organisation data management", "flexible framework", "dynamic locale selection policies", "interest management", "virtual environment systems", "undivided virtual worlds"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 10, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840515", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840515", "paper_url": "https://ieeexplore.ieee.org/document/840515"}, {"title": "Design and evaluation of visual feedback for virtual grasp", "authors": ["Mores Prachyabrued", "Christoph W. Borst"], "abstract": "We tuned and evaluated visual feedback techniques for virtual grasps. To date, development of such feedback has been largely ad-hoc, with minimal work that can guide technique selection. We considered several techniques including both standard and novel aspects. In terms of impact on real hand behavior, the best techniques all directly reveal penetrating hand configuration in some way. Subjectively, color changes are most liked.", "keywords": ["Visualization", "Thumb", "Color", "Standards", "Grasping", "Market research", "feedback", "virtual reality", "visual feedback", "virtual grasp", "ad-hoc", "Virtual grasping", "visual feedback"], "published_in": "2014 IEEE Virtual Reality (VR)", "publication_date": "March 2014", "citations": 2, "isbn": ["978-1-4799-2871-2"], "doi": "10.1109/VR.2014.6802075", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6802075", "paper_url": "https://ieeexplore.ieee.org/document/6802075"}, {"title": "Evaluating haptic feedback in virtual environments using ISO 9241\u20139", "authors": ["Robert J. Teather", "Daniel Natapov", "Michael Jenkin"], "abstract": "The ISO 9241 Part 9 standard pointing task is used to evaluate passive haptic feedback in target selection in a virtual environment (VE). Participants performed a tapping task using a tracked stylus in a CAVE both with, and without passive haptic feedback provided by a plastic panel co-located with the targets. Pointing throughput (but not speed nor accuracy alone) was significantly higher with haptic feedback than without it, confirming previous results using an alternative experimental paradigm.", "keywords": ["Haptic interfaces", "Feedback", "Virtual environment", "ISO standards", "Virtual reality", "Throughput", "Target tracking", "Plastics", "Computer science", "Multimedia systems", "feedback", "haptic interfaces", "ISO standards", "virtual reality", "virtual environment", "tracked stylus", "CAVE", "pointing throughput", "ISO 9241-9", "passive haptic feedback", "target selection", "tapping task", "pointing in 3D", "haptic feedback", "Fitts' law"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 6, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444753", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444753", "paper_url": "https://ieeexplore.ieee.org/document/5444753"}, {"title": "A unified linear algorithm for a novel view synthesis and camera pose estimation in mixed reality", "authors": ["T. Kobayashi", "G. Inoue", "Y. Ohta", "Long Quan"], "abstract": "We propose a linear algorithm that is useful for realizing geometric registration between the view of a real scene and that of a virtual object in an image-based rendering framework. In a unified framework, the novel view synthesis of a virtual object based on three views' matching constraints and the recovery of the camera pose that is necessary for the base image selection can be performed. The feasibility of the algorithm is demonstrated by using ground-truth synthesized data and real scene data.", "keywords": ["Cameras", "Virtual reality", "Rendering (computer graphics)", "Layout", "Position measurement", "Educational institutions", "Systems engineering and theory", "Augmented reality", "Humans", "Image sensors", "augmented reality", "image registration", "rendering (computer graphics)", "unified linear algorithm", "novel view synthesis", "camera pose estimation", "mixed reality", "geometric registration", "image-based rendering", "virtual objects", "matching constraints", "base image selection", "ground-truth synthesised data", "scene data"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 0, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840507", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840507", "paper_url": "https://ieeexplore.ieee.org/document/840507"}, {"title": "A taxonomy and comparison of haptic actions for disassembly tasks", "authors": ["A. Bloomfield", "Yu Deng", "J. Wampler", "P. Rondot", "D. Harth", "M. Mcmanus", "N. Badler"], "abstract": "The usefulness of modern day haptics equipment for virtual simulations of actual maintenance actions is examined. In an effort to categorize which areas haptic simulations may be useful, we have developed a taxonomy for haptic actions. This classification has two major dimensions: the general type of action performed and the type of force or torque required. Building upon this taxonomy, we selected three representative tasks from the taxonomy to evaluate in a virtual reality simulation. We conducted a series of human subject experiments to compare user performance and preference on a disassembly task with and without haptic feedback using CyberGlove, Phantom, and SpaceMouse interfaces. Analysis of the simulation runs shows Phantom users learned to accomplish the simulated actions significantly more quickly than did users of the CyberGlove or the SpaceMouse. Moreover, a lack of differences in the post-experiment questionnaire suggests that haptics research should include a measure of actual performance speed or accuracy rather than relying solely on subjective reports of a device's ease of use.", "keywords": ["Taxonomy", "Haptic interfaces", "Data gloves", "Imaging phantoms", "Analytical models", "Torque", "Virtual reality", "Humans", "Feedback", "Velocity measurement", "haptic interfaces", "virtual reality", "digital simulation", "assembling", "haptic actions", "disassembly tasks", "haptic simulations", "virtual reality simulation", "human subject experiments", "user performance", "disassembly task", "haptic feedback", "CyberGlove", "Phantom", "SpaceMouse interfaces", "simulation runs", "simulated actions", "haptics research"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 17, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191143", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191143", "paper_url": "https://ieeexplore.ieee.org/document/1191143"}, {"title": "Scalable metadata in- and output for multi-platform data annotation applications", "authors": ["Sebastian Pick", "Sascha Gebhardt", "Bernd Hentschel", "Torsten W. Kuhlen"], "abstract": "Metadata in- and output are important steps within the data annotation process. However, selecting techniques that effectively facilitate these steps is non-trivial, especially for applications that have to run on multiple virtual reality platforms. Not all techniques are applicable to or available on every system, requiring to adapt workflows on a per-system basis. Here, we describe a metadata handling system based on Android's Intent system that automatically adapts workflows and thereby makes manual adaption needless.", "keywords": ["Semantics", "Androids", "Humanoid robots", "Virtual reality", "Production facilities", "Manuals", "Android (operating system)", "data handling", "meta data", "virtual reality", "scalable metadata", "multiplatform data annotation applications", "multiple virtual reality platforms", "metadata handling system", "Android", "intent system", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism \u2014 Virtual reality", "H.5.2 [Information Interfaces and Presentation]: User Interfaces \u2014 Graphical user interfaces"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 0, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223395", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223395", "paper_url": "https://ieeexplore.ieee.org/document/7223395"}, {"title": "Live streaming system for omnidirectional video", "authors": ["Daisuke Ochi", "Yutaka Kunita", "Akio Kameda", "Akira Kojima", "Shinnosuke Iwaki"], "abstract": "NTT Media Intelligence Laboratories and DWANGO Co., Ltd. have jointly developed a virtual reality system that enables users to have an immersive experience visiting a remote site. This system makes it possible for users to watch video content wherever they want to watch it by using interactive streaming technology that selectively streams the user's watching section at a high bitrate in a limited network bandwidth. Applying this technology to omnidirectional video allows users to experience feelings of presence through the use of an intuitive head mount display. The system has also been released on a commercial platform and successfully streamed a real-time event. A demonstration is planned in which the details of the system and the streaming service results obtained with it will be presented.", "keywords": ["Streaming media", "Media", "Servers", "Bit rate", "Head", "Real-time systems", "Watches", "interactive systems", "video streaming", "virtual reality", "live streaming system", "omnidirectional video", "NTT media intelligence laboratories", "DWANGO Co Ltd", "virtual reality system", "video content watching", "interactive streaming technology", "user watching section", "omnidirectional video technology", "intuitive head mount display", "streaming service", "Omnidirectional video", "head mount display", "interactive streaming"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 11, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223439", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223439", "paper_url": "https://ieeexplore.ieee.org/document/7223439"}, {"title": "Using Pico Projectors with Spatial Contextual Awareness to Create Augmented Knowledge Spaces for Interdisciplinary Engineering Teams", "authors": ["Isabel Leber", "Matthias Merk", "Gabriela Tullius", "Peter Hertkorn"], "abstract": "Engineers of the research project \u201cDigital Product Life-Cycle\u201d are using a graph-based design language to model all aspects of the product they are working on. This abstract model is the base for all further investigations, developments and implementations. In particular at early stages of development, collaborative decision making is very important. We propose a semantic augmented knowledge space by means of mixed reality technology, to support engineering teams. Therefore we present an interaction prototype consisting of a pico projector and a camera. In our usage scenario engineers are augmenting different artefacts in a virtual working environment. The concept of our prototype contains both an interaction and a technical concept. To realise implicit and natural interactions, we conducted two prototype tests: (1) A test with a low-fidelity prototype and (2) a test by using the method Wizard of Oz. As a result, we present a prototype with interaction selection using augmentation spotlighting and an interaction zoom as a semantic zoom.", "keywords": ["Prototypes", "Knowledge engineering", "Collaboration", "Semantics", "Cameras", "Testing", "Creativity", "augmented reality", "decision making", "engineering education", "graph theory", "groupware", "interactive systems", "user interfaces", "virtual working environment", "natural interactions", "interaction selection", "augmentation spotlighting", "interaction zoom", "semantic zoom", "spatial contextual awareness", "interdisciplinary engineering teams", "Digital Product Life-Cycle", "graph-based design language", "abstract model", "collaborative decision making", "semantic augmented knowledge space", "mixed reality technology", "Pico Projectors", "augmented knowledge spaces", "Human-centered computing\u2015Ubiquitous computing\u2015", "Human-centered computing\u2015Mixed/augmented reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446184", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446184", "paper_url": "https://ieeexplore.ieee.org/document/8446184"}, {"title": "Comparing steering-based travel techniques for search tasks in a CAVE", "authors": ["Anette Von Kapri", "Tobias Rick", "Steven Feiner"], "abstract": "We present a novel bimanual body-directed travel technique, PenguFly (PF), and compare it with two standard travel-by-pointing techniques by conducting a between-subject experiment in a CAVE. In PF, the positions of the user's head and hands are projected onto the ground, and travel direction and speed are computed based on direction and magnitude of the vector from the midpoint of the projected hand positions to the projected head position. The two baseline conditions both use a single hand to control the direction, with speed controlled discretely by button pushes with the same hand in one case, and continuously by the distance between the hands in the other case. Users were asked to travel through a simple virtual world and collect virtual coins within a set time. We found no significant differences between travel conditions for reported presence or usability, but a significant increase in nausea with PF. Total travel distance was significantly higher for the baseline condition with discrete speed selection, whereas travel accuracy in terms of coin-to-distance ratio was higher with PF.", "keywords": ["Particle measurements", "Accuracy", "Atmospheric measurements", "Virtual reality", "Testing", "Usability", "Legged locomotion", "search problems", "user interfaces", "velocity control", "virtual reality", "steering based travel technique", "search task", "CAVE", "bimanual body directed travel technique", "PenguFly", "projected head position", "projected hand position", "speed control", "button pushes", "virtual world", "virtual coins", "baseline condition", "discrete speed selection", "travel accuracy", "coin-to-distance ratio"], "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 9, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759443", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759443", "paper_url": "https://ieeexplore.ieee.org/document/5759443"}, {"title": "Wearable tactile device using mechanical and electrical stimulation for fingertip interaction with virtual world", "authors": ["Vibol Yem", "Hiroyuki Kajimoto"], "abstract": "We developed \u201cFinger Glove for Augmented Reality\u201d (FinGAR), which combines electrical and mechanical stimulation to selectively stimulate skin sensory mechanoreceptors and provide tactile feedback of virtual objects. A DC motor provides high-frequency vibration and shear deformation to the whole finger, and an array of electrodes provide pressure and low-frequency vibration with high spatial resolution. FinGAR devices are attached to the thumb, index finger and middle finger. It is lightweight, simple in mechanism, easy to wear, and does not disturb the natural movements of the hand. All of these attributes are necessary for a general-purpose virtual reality system. User study was conducted to evaluate its ability to reproduce sensations of four tactile dimensions: macro roughness, friction, fine roughness and hardness. Result indicated that skin deformation and cathodic stimulation affect macro roughness and hardness, whereas high-frequency vibration and anodic stimulation affect friction and fine roughness.", "keywords": ["Vibrations", "Thumb", "Skin", "Electrodes", "DC motors", "Electrical stimulation", "augmented reality", "data gloves", "DC motors", "haptic interfaces", "shear deformation", "vibrations", "wearable tactile device", "electrical stimulation", "mechanical stimulation", "virtual world", "fingertip interaction", "finger glove for augmented reality", "skin sensory mechanoreceptors", "tactile feedback", "virtual objects", "DC motor", "high-frequency vibration", "shear deformation", "electrode array", "low-frequency vibration", "high spatial resolution", "FinGAR devices", "general-purpose virtual reality system", "tactile dimensions", "macro roughness", "friction", "fine roughness", "hardness", "anodic stimulation", "FinGAR", "mechanical stimulation", "electrical stimulation", "virtual touch"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 20, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892236", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892236", "paper_url": "https://ieeexplore.ieee.org/document/7892236"}, {"title": "Immersive Job Taste: a Concept of Demonstrating Workplaces with Virtual Reality", "authors": ["Mikhail Fominykh", "Ekaterina Prasolova-F\u00f8rland"], "abstract": "This paper presents a new concept of `Immersive Job Taste' - interactive virtual reality demonstration of a workplace that aims to give a feeling of going through an average workday of a professional with elements of basic training. The main target audiences of Job Taste simulations are young job seekers who can be aided in selecting a career path at school or a welfare center, choosing the first or a new occupation, often after a period of being unemployed. The design methodology behind the Immersive Job Taste concept includes presentation of a workplace, typical tasks, feedback on performance, and advice on applying for jobs in the specific industry. We developed several scenarios and applied different virtual and augmented reality concepts to build prototypes for different types of devices. The prototypes were evaluated by several groups of primary users and experts. The results indicate a generally very positive attitude towards the concept. In this paper, we discuss the potential impact of applying the concept and directions for future work.", "keywords": ["Task analysis", "Employment", "Interviews", "Fish", "Industries", "Training", "Engineering profession", "augmented reality", "employee welfare", "user interfaces", "augmented reality concepts", "workplace", "main target audiences", "young job seekers", "immersive job taste concept", "job taste simulations", "interactive virtual reality demonstration", "Virtual Reality", "Career guidance", "unemployment"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 1, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798356", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798356", "paper_url": "https://ieeexplore.ieee.org/document/8798356"}, {"title": "Investigating the Third Dimension for Authentication in Immersive Virtual Reality and in the Real World", "authors": ["Ceenu George", "Mohamed Khamis", "Daniel Buschek", "Heinrich Hussmann"], "abstract": "Immersive Virtual Reality (IVR) is a growing 3D environment, where social and commercial applications will require user authentication. Similarly, smart homes in the real world (RW), offer an opportunity to authenticate in the third dimension. For both environments, there is a gap in understanding which elements of the third dimension can be leveraged to improve usability and security of authentication. In particular, investigating transferability of findings between these environments would help towards understanding how rapid prototyping of authentication concepts can be achieved in this context. We identify key elements from prior research that are promising for authentication in the third dimension. Based on these, we propose a concept in which users' authenticate by selecting a series of 3D objects in a room using a pointer. We created a virtual 3D replica of a real world room, which we leverage to evaluate and compare the factors that impact the usability and security of authentication in IVR and RW. In particular, we investigate the influence of randomized user and object positions, in a series of user studies (N=48). We also evaluate shoulder surfing by real world bystanders for IVR (N=75). Our results show that 3D passwords within our concept are resistant against shoulder surfing attacks. Interactions are faster in RW compared to IVR, yet workload is comparable.", "keywords": ["Authentication", "Three-dimensional displays", "Password", "Virtual reality", "Resists", "Usability", "authorisation", "home computing", "user interfaces", "virtual reality", "immersive virtual reality", "IVR", "social applications", "commercial applications", "user authentication", "RW", "security", "virtual 3D replica", "real world", "smart homes", "rapid prototyping", "Human-centered computing\u2014User studies", "Human-centered computing\u2014Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 2, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797862", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797862", "paper_url": "https://ieeexplore.ieee.org/document/8797862"}, {"title": "Single-pass 3D lens rendering and spatiotemporal \u201cTime Warp\u201d example", "authors": ["Jan-Phillip Tiesel", "Christoph W. Borst", "Kaushik Das", "Emad Habib"], "abstract": "This paper extends 3D lens techniques. Interactive 3D lenses, often called volumetric lenses, provide users with alternative views of datasets within spatially bounded regions of interest (focus) while maintaining the surrounding overview (context). In contrast to previous multi-pass rendering work, we discuss the strengths, limitations, and performance cost of a single-pass technique. For a substantial range of effects, it supports several interactive composable lenses at interactive frame rates without performance loss during increasing lens intersections or manipulations. Other cases, for which this performance cannot be achieved, are also discussed. Finally, we illustrate possible applications of our lens system, especially new Time Warp lenses for exploring time-varying datasets in interactive VR.", "keywords": ["Lenses", "Spatiotemporal phenomena", "Rendering (computer graphics)", "Virtual reality", "Data visualization", "Time varying systems", "Computer graphics", "Costs", "Performance loss", "Application software", "interactive devices", "lenses", "rendering (computer graphics)", "solid modelling", "virtual reality", "single-pass 3D lens rendering", "spatiotemporal time warp example", "volumetric lenses", "multipass rendering", "interactive virtual reality", "Magic Lens", "volumetric lens"], "published_in": "2010 IEEE Virtual Reality Conference (VR)", "publication_date": "March 2010", "citations": 1, "isbn": ["978-1-4244-6238-4", "978-1-4244-6237-7", "978-1-4244-6236-0"], "doi": "10.1109/VR.2010.5444782", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5444782", "paper_url": "https://ieeexplore.ieee.org/document/5444782"}, {"title": "Repeat after me: Using mixed reality humans to influence best communication practices", "authors": ["Andrew Cordar", "Adam Wendling", "Casey White", "Samsun Lampotang", "Benjamin Lok"], "abstract": "In the past few years, advances have been made on how mixed reality humans (MRHs) can be used for interpersonal communication skills training for medical teams; however, little research has looked at how MRHs can influence communication skills during training. One way to influence communication skills is to leverage MRHs as models of communication behavior. We created a mixed reality medical team training exercise designed to impact communication behaviors that are critical for patient safety. We recruited anesthesia residents to go through an operating room training exercise with MRHs to assess and influence residents' closed loop communication behaviors during medication administration. We manipulated the behavior of the MRHs to determine if the MRHs could influence the residents' closed loop communication behavior. Our results showed that residents' closed loop communications behaviors were influenced by MRHs. Additionally, we found there was a statistically significant difference between groups based on which MRH behavior residents observed. Because the MRHs significantly impacted how residents communicated in simulation, this work expands the boundaries for how VR can be used and demonstrates that MRHs could be used as tools to address complex communication dynamics in a team setting.", "keywords": ["Surgery", "Training", "Virtual reality", "Solid modeling", "Safety", "Information exchange", "biomedical education", "closed loop systems", "computer based training", "medical computing", "virtual reality", "mixed reality humans", "MRH", "interpersonal communication skill training", "mixed reality medical team training exercise", "patient safety", "anesthesia residents", "operating room training exercise", "resident closed loop communication behaviors", "medication administration", "complex communication dynamics", "mixed reality", "virtual humans", "social influence", "training"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 4, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892242", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892242", "paper_url": "https://ieeexplore.ieee.org/document/7892242"}, {"title": "EEG Can Be Used to Measure Embodiment When Controlling a Walking Self-Avatar", "authors": ["Bilal Alchalabi", "Jocelyn Faubert", "David R. Labbe"], "abstract": "It has recently been shown that inducing the ownership illusion and then manipulating the movements of one's self-avatar can lead to compensatory motor control strategies in gait rehabilitation. In order to maximize this effect, there is a need for a method that measures, and monitors embodiment levels of participants immersed in VR to induce and maintain a strong ownership illusion. The objective of this study was to propose a novel approach to measuring embodiment by presenting visual feedback that conflicts with motor control to embodied subjects. Twenty healthy participants were recruited. During experimentations, participants wore an EEG cap and motion capture markers, with an avatar displayed in a HMD from a first-person perspective. They were cued to either perform, watch or imagine a single step forward or to initiate walking on the treadmill. For some of the trials, the avatar took a step with the contralateral limb or stopped walking before the participant stopped (modified feedback). Results show that subjective levels of embodiment correlate strongly with the difference in \u03bc - ERS power over the motor and pre-motor cortex between the modified and non-modified feedback trials.", "keywords": ["Avatars", "Legged locomotion", "Electroencephalography", "Atmospheric measurements", "Particle measurements", "Foot", "avatars", "bioelectric potentials", "electroencephalography", "feedback", "gait analysis", "medical signal processing", "neurophysiology", "patient rehabilitation", "embodiment level monitoring", "contralateral limb", "micro-ERS power", "pre-motor cortex", "subjective levels", "motion capture markers", "healthy participants", "visual feedback", "gait rehabilitation", "compensatory motor control strategies", "walking self-avatar", "Virtual reality", "rhythm EEG", "event-related-potentials", "gait rehabilitation", "mirror neuron system"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798263", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798263", "paper_url": "https://ieeexplore.ieee.org/document/8798263"}, {"title": "Testbed evaluation of navigation and text display techniques in an information-rich virtual environment", "authors": ["Jian Chen", "P.S. Pyla", "D.A. Bowman"], "abstract": "The fundamental question for an information-rich virtual environment is how to access and display abstract information. We investigated two existing navigation techniques: hand-centered object manipulation extending ray-casting (HOMER) and go-go navigation, and two text layout techniques: within-the-world display (WWD) and heads-up display (HUD). Four search tasks were performed to measure participants' performance in a densely packed environment. HUD enabled significantly better performance than WWD and the go-go technique enabled better performance than the HOMER technique for most of the tasks. We found that using HOMER navigation combined with the WWD technique was significantly worse than other combinations for difficult naive search tasks. Users also preferred the combination of go-go and HUD for all tasks.", "keywords": ["Testing", "Navigation", "Virtual environment", "Two dimensional displays", "Computer displays", "Animation", "Virtual reality", "Augmented reality", "Computer science", "Performance evaluation", "head-up displays", "computer displays", "virtual reality", "navigation", "text display", "virtual environment", "information access", "information display", "abstract information", "navigation techniques", "hand-centered object manipulation extending ray-casting", "HOMER navigation", "go-go navigation", "within-the-world display", "heads-up display"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 4, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310072", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310072", "paper_url": "https://ieeexplore.ieee.org/document/1310072"}, {"title": "A comparative study of user performance in a map-based virtual environment", "authors": ["J.E. Swan", "J.L. Gabbard", "D. Hix", "R.S. Schulman", "Keun Pyo Kim"], "abstract": "We present a comparative study of user performance with tasks involving navigation, visual search, and geometric manipulation, in a map-based battlefield visualization virtual environment (VE). Specifically, our experiment compared user performance of the same task across four different VE platforms: desktop, cave, workbench, and wall. Independent variables were platform type, stereopsis (stereo, mono), movement control mode (rate, position), and frame of reference (egocentric, exocentric). Overall results showed that users performed tasks fastest using the desktop and slowest using the workbench. Other results are detailed in the article. Notable is that we designed our task in an application context, with tasking much closer to how users would actually use a real-world battlefield visualization system. This is very uncommon for comparative studies, which are usually designed with abstract tasks to minimize variance. This is, we believe, one of the first and most complex studies to comparatively examine, in an application context, this many key variables affecting VE user interface design.", "keywords": ["Virtual environment", "User interfaces", "Navigation", "Visualization", "Usability", "Performance evaluation", "Design engineering", "Statistics", "MONOS devices", "User centered design", "military computing", "data visualisation", "virtual reality", "user centred design", "user performance", "map-based virtual environment", "visual search", "geometric manipulation", "map-based battlefield visualization virtual environment", "VE platforms", "stereopsis", "movement control mode", "frame of reference", "application context", "real-world battlefield visualization system", "abstract tasks", "VE user interface design", "user-centered design", "user interaction", "user assessment", "usability engineering", "usability evaluation", "expert heuristic evaluation", "formative evaluation", "summative evaluation"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 13, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191149", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191149", "paper_url": "https://ieeexplore.ieee.org/document/1191149"}, {"title": "An Image-Warping Architecture for VR: Low Latency versus Image Quality", "authors": ["Ferdi Smit", "Robert Van Liere", "Stephan Beck", "Bernd Froehlich"], "abstract": "Designing low end-to-end latency system architectures for virtual reality is still an open and challenging problem. We describe the design, implementation and evaluation of a client-server depth-image warping architecture that updates and displays the scene graph at the refresh rate of the display. Our approach works for scenes consisting of dynamic and interactive objects. The end-to-end latency is minimized as well as smooth object motion generated. However, this comes at the expense of image quality inherent to warping techniques. We evaluate the architecture and its design trade-offs by comparing latency and image quality to a conventional rendering system. Our experience with the system confirms that the approach facilitates common interaction tasks such as navigation and object manipulation.", "keywords": ["Virtual reality", "Delay", "Image quality", "Layout", "Displays", "File servers", "Rendering (computer graphics)", "Computer graphics", "Computer architecture", "Navigation", "I.3.3 [Computer Graphics]: Picture/Image Generation\u00bfDisplay Algorithms", "I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism\u00bfVirtual Reality"], "published_in": "2009 IEEE Virtual Reality Conference", "publication_date": "March 2009", "citations": 14, "isbn": ["978-1-4244-3943-0", "978-1-4244-3812-9"], "doi": "10.1109/VR.2009.4810995", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4810995", "paper_url": "https://ieeexplore.ieee.org/document/4810995"}, {"title": "Prism aftereffects for throwing with a self-avatar in an immersive virtual environment", "authors": ["Bobby Bodenheimer", "Sarah Creem-Regehr", "Jeanine Stefanucci", "Elena Shemetova", "William B. Thompson"], "abstract": "The use of first-person self-avatars in immersive virtual environments (VEs) has grown over recent years. It is unknown, however, how visual feedback from a self-avatar influences a user's online actions and subsequent calibration of actions within an immersive VE. The current paper uses a prism throwing adaptation paradigm to test the role of a self-avatar arm or full body on action calibration in a VE. Participants' throwing accuracy to a target on the ground was measured first in a normal viewing environment, then with the visual field rotated clockwise about their vertical axis by 17\u00b0 (prism simulation), and then again in the normal viewing environment with the prism distortion removed. Participants experienced either no-avatar, a first-person avatar arm and hand, or a first-person full body avatar during the entire experimental session, in a between-subjects manipulation. Results showed similar throwing error and adaptation during the prism exposure for all conditions, but a reduced aftereffect (displacement with respect to the target in the opposite direction of the prism-exposure) when the avatar arm or full body was present. The results are discussed in the context of how an avatar can provide a visual frame of reference to aid in action calibration.", "keywords": ["Avatars", "Visualization", "Calibration", "Legged locomotion", "Tracking", "Virtual environments", "Head", "avatars", "optical prisms", "immersive virtual environment", "first-person self-avatars", "visual feedback", "user online actions", "immersive VE", "self-avatar arm", "self-avatar full-body", "action calibration", "clockwise rotated visual field", "vertical axis", "normal viewing environment", "prism distortion", "first-person full-body avatar", "between-subject manipulation", "prism-exposure", "visual frame-of-reference", "Virtual reality", "prism adaptation", "self-avatar"], "published_in": "2017 IEEE Virtual Reality (VR)", "publication_date": "2017", "citations": 4, "isbn": ["978-1-5090-6647-6", "978-1-5090-6648-3"], "doi": "10.1109/VR.2017.7892241", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7892241", "paper_url": "https://ieeexplore.ieee.org/document/7892241"}, {"title": "Fluid Sketching\u2015Immersive Sketching Based on Fluid Flow", "authors": ["Sevinc Eroglu", "Sascha Gebhardt", "Patric Schmitz", "Dominik Rausch", "Torsten Wolfgang Kuhlen"], "abstract": "Fluid artwork refers to works of art based on the aesthetics of fluid motion, such as smoke photography, ink injection into water, and paper marbling. Inspired by such types of art, we created Fluid Sketching as a novel medium for creating 3D fluid artwork in immersive virtual environments. It allows artists to draw 3D fluid-like sketches and manipulate them via six degrees of freedom input devices. Different brush stroke settings are available, varying the characteristics of the fluid. Because of fluids' nature, the diffusion of the drawn fluid sketch is animated, and artists have control over altering the fluid properties and stopping the diffusion process whenever they are satisfied with the current result. Furthermore, they can shape the drawn sketch by directly interacting with it, either with their hand or by blowing into the fluid. We rely on particle advection via curl-noise as a fast procedural method for animating the fluid flow.", "keywords": ["Three-dimensional displays", "Computational modeling", "Fluids", "Art", "Real-time systems", "Mathematical model", "Ink", "art", "computer animation", "photography", "virtual reality", "Fluid Sketching-immersive Sketching", "fluid flow", "fluid artwork", "fluid motion", "immersive virtual environments", "fluids", "drawn fluid sketch", "fluid properties", "brush stroke settings", "Computing methodologies\u2015Computer graphics\u2015Graphics systems and interfaces\u2015Virtual reality", "Human-centered computing\u2015Human computer interaction (HCI)\u2015Interaction devices\u2015Sound-based input / output", "Human-centered computing\u2015Human computer interaction (HCI)\u2015HCI design and evaluation methods\u2015User studies"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 1, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8446595", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8446595", "paper_url": "https://ieeexplore.ieee.org/document/8446595"}, {"title": "Scale - Unexplored Opportunities for Immersive Technologies in Place-based Learning", "authors": ["Jiayan Zhao", "Alexander Klippel"], "abstract": "Immersive technologies have the potential to overcome physical limitations and virtually deliver field site experiences, for example, into the classroom. Yet, little is known about the features of immersive technologies that contribute to successful place-based learning. Immersive technologies afford embodied experiences by mimicking natural embodied interactions through a user's egocentric perspective. Additionally, they allow for beyond reality experiences integrating contextual information that cannot be provided at actual field sites. The current study singles out one aspect of place-based learning: Scale. In an empirical evaluation, scale was manipulated as part of two immersive virtual field trip (iVFT) experiences in order to disentangle its effect on place-based learning. Students either attended an actual field trip (AFT) or experienced one of two iVFTs using a head-mounted display. The iVFTs either mimicked the actual field trip or provided beyond reality experiences offering access to the field site from an elevated perspective using pseudo-aerial 360\u00b0 imagery. Results show that students with access to the elevated perspective had significantly better scores, for example, on their spatial situation model (SSM). Our findings provide first results on how an increased (geographic) scale, which is accessible through an elevated perspective, boosts the development of SSMs. The reported study is part of a larger immersive education effort. Inspired by the positive results, we discuss our plan for a more rigorous assessment of scale effects on both self- and objectively assessed performance measures of spatial learning.", "keywords": ["computer aided instruction", "human computer interaction", "virtual reality", "reality experiences", "elevated perspective", "larger immersive education effort", "spatial learning", "immersive technologies", "field site experiences", "immersive virtual field trip experiences", "iVFT experience", "spatial situation model", "Immersive learning", "virtual field trips", "scale", "place-based education", "Applied computing\u2014Education\u2014Interactive learning environments", "Human-centered computing\u2014Visualization\u2014Visualization design and evaluation methods"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 4, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797867", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797867", "paper_url": "https://ieeexplore.ieee.org/document/8797867"}, {"title": "LoD management on animating face models", "authors": ["Hyewon Seo", "N.M. Thalmann"], "abstract": "Presents our work on a level-of-detail (LoD) technique for human-like face models in virtual environments. Conventional LoD techniques have been adapted to allow facial animation on simplified geometric models. This includes the optimization of both geometric and animation parameters. Simplified models are generated in a region-based manner, considering the mobility of each region. The animation process is decomposed into two sub-processes, and each step is optimized. In the MPA (minimum perceptible action) level optimization, a hierarchical structure is devised for the multi-level animation model. The deformation level is simplified by reducing the number of control points. At run-time, the animation level is selected in combination with viewpoint information at the geometric level.", "keywords": ["Facial animation", "Humans", "Virtual environment", "Geometry", "Layout", "Runtime", "Face", "Graphics", "Rendering (computer graphics)", "Biological system modeling", "computer animation", "optimisation", "computational geometry", "deformation", "virtual reality", "real-time systems", "level-of-detail management", "human-like face model animation", "virtual environments", "geometric models", "geometric parameter optimization", "animation parameter optimization", "region-based model generation", "region mobility", "animation sub-processes", "MPA-level optimization", "minimum perceptible action", "hierarchical structure", "multi-level animation model", "deformation level", "control points", "run-time animation level selection", "geometric-level viewpoint information", "virtual humans", "real-time facial animation"], "published_in": "Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)", "publication_date": "2000", "citations": 4, "isbn": ["0-7695-0478-7"], "doi": "10.1109/VR.2000.840494", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=840494", "paper_url": "https://ieeexplore.ieee.org/document/840494"}, {"title": "A simplification architecture for exploring navigation tradeoffs in mobile VR", "authors": ["C.D. Correa", "I. Marsic"], "abstract": "Interactive applications on mobile devices often reduce data fidelity to adapt to resource constraints and variable user preferences. In virtual reality applications, the problem of reducing scene graph fidelity can be stated as a combinatorial optimization problem, where a part of the scene graph with maximum fidelity is chosen such that the resources it requires are below a given threshold and the hierarchical relationships are maintained. The problem can be formulated as a variation of the tree knapsack problem, which is known to be NP-hard. For this reason, solutions to this problem result in a tradeoff that affects user navigation. On one hand, exact solutions provide the highest fidelity but may take long time to compute. On the other hand, greedy solutions are fast but lack high fidelity. We present a simplification architecture that allows the exploration of such navigation tradeoffs. This is achieved by a formulating the problem in a generic way and developing software components that allow the dynamic selection of algorithms and constraints. The experimental results show that the architecture is flexible and supports dynamic reconfiguration.", "keywords": ["Navigation", "Virtual reality", "Layout", "Mobile computing", "Computer architecture", "Application software", "Tree graphs", "Network servers", "Bandwidth", "Heuristic algorithms", "virtual reality", "mobile computing", "computational complexity", "optimisation", "trees (mathematics)", "knapsack problems", "architecture", "mobile virtual reality", "interactive applications", "mobile devices", "resource constraints", "variable user preferences", "scene graph fidelity", "combinatorial optimization", "tree knapsack problem", "NP-hard problem", "user navigation", "dynamic reconfiguration"], "published_in": "IEEE Virtual Reality 2004", "publication_date": "2004", "citations": 1, "isbn": ["0-7803-8415-6"], "doi": "10.1109/VR.2004.1310066", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1310066", "paper_url": "https://ieeexplore.ieee.org/document/1310066"}, {"title": "Exploring individual differences in raybased selection: strategies and traits", "authors": ["C.A. Wingrave", "R. Tintner", "B.N. Walker", "D.A. Bowman", "L.F. Hodges"], "abstract": "User-centered design is often performed without regard to individual user differences in aptitude and experience. The methodology of this study is an anthropological and observational approach observing users performing a selection task using common virtual environment raybased techniques and analyzes the interaction through psychology aptitude tests, questionnaires and observation. The results of this study show the approach yields useful information about users even in a simple task. The study indicates correlations between performance and aptitude test and user behavior performed to overcome difficulties in the task.", "keywords": ["Testing", "Virtual environment", "Psychology", "Performance evaluation", "Feedback", "Measurement standards", "Virtual reality", "User centered design", "Performance analysis", "Chromium", "psychology", "user interfaces", "virtual reality", "user centred design", "human computer interaction", "user-centered design", "user difference", "user aptitude", "user experience", "virtual environment raybased techniques", "user interaction", "psychology", "aptitude tests", "questionnaires", "user behavior", "user strategies"], "published_in": "IEEE Proceedings. VR 2005. Virtual Reality, 2005.", "publication_date": "2005", "citations": 10, "isbn": ["0-7803-8929-8"], "doi": "10.1109/VR.2005.1492770", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1492770", "paper_url": "https://ieeexplore.ieee.org/document/1492770"}, {"title": "Crowd simulation using Discrete Choice Model", "authors": ["Wenxi Liu", "Rynson Lau", "Dinesh Manocha"], "abstract": "We present a new algorithm to simulate a variety of crowd behaviors using the Discrete Choice Model (DCM). DCM has been widely studied in econometrics to examine and predict customers' or households' choices. Our DCM formulation can simulate virtual agents' goal selection and we highlight our algorithm by simulating heterogeneous crowd behaviors: evacuation, shopping, and rioting scenarios.", "keywords": ["Computational modeling", "Solid modeling", "Navigation", "Negative feedback", "Heuristic algorithms", "Decision making", "Collision avoidance", "digital simulation", "econometrics", "software agents", "crowd behavior simulation", "discrete choice model", "econometrics", "customer choices", "household choice", "DCM formulation", "virtual agent goal selection simulation", "evacuation scenarios", "shopping scenarios", "rioting scenarios", "H.5.1 [Information Interfaces and Presentation]", "Multimedia Information Systems \u2014 Animations"], "published_in": "2012 IEEE Virtual Reality Workshops (VRW)", "publication_date": "March 2012", "citations": 1, "isbn": ["978-1-4673-1246-2", "978-1-4673-1247-9"], "doi": "10.1109/VR.2012.6180866", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6180866", "paper_url": "https://ieeexplore.ieee.org/document/6180866"}, {"title": "Using a virtual environment to study the impact of sending traffic alerts to texting pedestrians", "authors": ["Pooya Rahimian", "Elizabeth E. O'Neal", "Junghum Paul Yon", "Luke Franzen", "Yuanyuan Jiang", "Jodie M. Plumert", "Joseph K. Kearney"], "abstract": "This paper presents an experiment conducted in a large-screen immersive virtual environment to evaluate how texting pedestrians respond to permissive traffic alerts delivered via their cell phone. We developed a cell phone app that delivered information to texting pedestrians about when traffic conditions permit safe crossing. We compared gap selection and movement timing in three groups of pedestrians: texting, texting with alerts, and no texting (control). Participants in the control and alert groups chose larger gaps and were more discriminating in their gap choices than participants in the texting group. Both the control and alert groups had more time to spare than the texting group when they exited the roadway even though the alert group timed their entry relative to the lead car less tightly than the control and texting groups. By choosing larger gaps, participants in the alert group were able to compensate for their poorer timing of entry, resulting in a margin of safety that did not differ from those who were not texting. However, they also relied heavily on the alert system and paid less attention to the roadway. The discussion focuses on the potential of assistive technologies based on Vehicle-to-Pedestrian (V2P) communications technology for mitigating pedestrian-motor vehicle crashes.", "keywords": ["Roads", "Vehicles", "Cellular phones", "Mobile handsets", "Virtual environments", "Safety", "Electronic mail", "automobiles", "graphical user interfaces", "mobile computing", "pedestrians", "road safety", "smart phones", "virtual reality", "pedestrian-motor vehicle crash mitigation", "V2P communications technology", "vehicle-to-pedestrian communication technology", "margin-of-safety", "alert groups", "control groups", "no-texting pedestrian", "texting-with-alert pedestrian", "movement timing", "gap selection", "safe crossing", "traffic conditions", "cell phone application", "permissive traffic alerts", "large-screen immersive virtual environment", "texting pedestrians", "Pedestrian simulation", "Texting", "Mobile device use", "Pedestrian safety", "Connected vehicles technology", "Vehicle-to-pedestrian (V2P) communication"], "published_in": "2016 IEEE Virtual Reality (VR)", "publication_date": "March 2016", "citations": 7, "isbn": ["978-1-5090-0836-0"], "doi": "10.1109/VR.2016.7504697", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7504697", "paper_url": "https://ieeexplore.ieee.org/document/7504697"}, {"title": "Exploring the effects of image persistence in low frame rate virtual environments", "authors": ["David J. Zielinski", "Hrishikesh M. Rao", "Mark A. Sommer", "Regis Kopper"], "abstract": "In virtual reality applications, there is an aim to provide real time graphics which run at high refresh rates. However, there are many situations in which this is not possible due to simulation or rendering issues. When running at low frame rates, several aspects of the user experience are affected. For example, each frame is displayed for an extended period of time, causing a high persistence image artifact. The effect of this artifact is that movement may lose continuity, and the image jumps from one frame to another. In this paper, we discuss our initial exploration of the effects of high persistence frames caused by low refresh rates and compare it to high frame rates and to a technique we developed to mitigate the effects of low frame rates. In this technique, the low frame rate simulation images are displayed with low persistence by blanking out the display during the extra time such image would be displayed. In order to isolate the visual effects, we constructed a simulator for low and high persistence displays that does not affect input latency. A controlled user study comparing the three conditions for the tasks of 3D selection and navigation was conducted. Results indicate that the low persistence display technique may not negatively impact user experience or performance as compared to the high persistence case. Directions for future work on the use of low persistence displays for low frame rate situations are discussed.", "keywords": ["Navigation", "Visualization", "Interpolation", "Virtual environments", "Training", "Glass", "Complexity theory", "image processing", "real-time systems", "user interfaces", "virtual reality", "image persistence", "low frame rate virtual environments", "virtual reality applications", "real time graphics", "rendering", "user experience", "low frame rate simulation images", "3D selection", "navigation", "virtual environments", "low-persistence", "simulator sickness", "performance", "presence"], "published_in": "2015 IEEE Virtual Reality (VR)", "publication_date": "March 2015", "citations": 7, "isbn": ["978-1-4799-1727-3"], "doi": "10.1109/VR.2015.7223319", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223319", "paper_url": "https://ieeexplore.ieee.org/document/7223319"}, {"title": "Automatic Furniture Arrangement Using Greedy Cost Minimization", "authors": ["Peter K\u00e1n", "Hannes Kaufmann"], "abstract": "In this paper, we present a novel method for fast generation of furniture arrangements in interior scenes. Our method exploits the benefits of optimization-based approaches for global aesthetic rules and the advantages of procedural approaches for local arrangement of small objects. We generate the furniture arrangements for a given room in two steps: We first optimize the selection and arrangement of furniture objects in a room with respect to aesthetic and functional rules. The infinite trans-dimensional space of furniture layouts is rapidly explored by greedy cost minimization. In the second step, the procedural methods are locally applied in a stochastic fashion to generate important scene details. We demonstrate that our method achieves comparable results to a recent method for automatic interior design in terms of user preferences and that local procedural design enhances the result of optimization-based interior design. Additionally, our method is one order of magnitude faster than the compared method. Finally, the execution times of up to one second show that our method is suitable for generating large-scale indoor virtual environments during runtime.", "keywords": ["Layout", "Cost function", "Minimization", "Three-dimensional displays", "Space exploration", "Stochastic processes", "CAD", "furniture", "greedy algorithms", "optimisation", "stochastic processes", "virtual reality", "automatic interior design", "local procedural design", "optimization-based interior design", "automatic furniture arrangement", "greedy cost minimization", "interior scenes", "optimization-based approaches", "global aesthetic rules", "furniture objects", "functional rules", "furniture arrangements generation", "furniture layouts infinite trans-dimensional space", "indoor virtual environments", "Computing methodologies-Graphics systems and interfaces-Virtual reality"], "published_in": "2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2018", "citations": 0, "isbn": ["978-1-5386-3365-6", "978-1-5386-3366-3"], "doi": "10.1109/VR.2018.8448291", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8448291", "paper_url": "https://ieeexplore.ieee.org/document/8448291"}, {"title": "Dense 3D Scene Reconstruction from Multiple Spherical Images for 3-DoF+ VR Applications", "authors": ["Thiago L. T. Da Silveira", "Claudio R. Jung"], "abstract": "We propose a novel method for estimating the 3D geometry of indoor scenes based on multiple spherical images. Our technique produces a dense depth map registered to a reference view so that depth-image-based-rendering (DIBR) techniques can be explored for providing three-degrees-of-freedom plus immersive experiences to virtual reality users. The core of our method is to explore large displacement optical flow algorithms to obtain point correspondences, and use cross-checking and geometric constraints to detect and remove bad matches. We show that selecting a subset of the best dense matches leads to better pose estimates than traditional approaches based on sparse feature matching, and explore a weighting scheme to obtain the depth maps. Finally, we adapt a fast image-guided filter to the spherical domain for enforcing local spatial consistency, improving the 3D estimates. Experimental results indicate that our method quantitatively outperforms competitive approaches on computer-generated images and synthetic data under noisy correspondences and camera poses. Also, we show that the estimated depth maps obtained from only a few real spherical captures of the scene are capable of producing coherent synthesized binocular stereoscopic views by using traditional DIBR methods.", "keywords": ["Three-dimensional displays", "Cameras", "Optical imaging", "Feature extraction", "Image reconstruction", "Optical distortion", "Geometry", "cameras", "image filtering", "image matching", "image reconstruction", "image registration", "image sequences", "pose estimation", "rendering (computer graphics)", "stereo image processing", "virtual reality", "sparse feature matching", "fast image-guided filter", "spherical domain", "computer-generated images", "multiple spherical images", "3-DoF+ VR applications", "indoor scenes", "dense depth map", "depth-image-based-rendering techniques", "three-degrees-of-freedom", "virtual reality users", "displacement optical flow algorithms", "DIBR methods", "dense 3D scene reconstruction", "local spatial consistency", "synthesized binocular stereoscopic views", "Computing methodologies\u2014Computer vision problems\u2014Reconstruction", "Computing methodologies\u2014Computer graphics\u2014Image manipulation\u2014Image-based rendering", "Computing methodologies\u2014Computer graphics\u2014Graphics systems and interfaces\u2014Virtual reality"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798281", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798281", "paper_url": "https://ieeexplore.ieee.org/document/8798281"}, {"title": "Message from the Program Chairs", "authors": [], "abstract": "The IEEE Virtual Reality (VR) 2013 full papers program, contained herein, includes 21 papers that present research, applications, and systems in the field of virtual reality. They were selected from 97 full paper submissions by an international program committee of 64 members, supported by 225 external expert reviewers, leading to an acceptance rate for IEEE Virtual Reality 2013 of 21.6%. All papers appearing in this issue have undergone a two-round review process. In the first round review, at least four expert reviewers reviewed the work. The paper chairs selected the primary and secondary reviewers from the international program committee, and the primary reviewer then recruited at least two external experts. After completion of all reviews, the primary reviewer led an online discussion phase, which resulted in an initial recommendation for acceptance or rejection and a set of modifications that were deemed necessary. Based on this recommendation, the program committee, at the two-day online meeting, selected an initial set of papers for preliminary acceptance. The authors of these papers were given the opportunity to refine and resubmit their work. In the second round review, IPC members checked whether the changes made were sufficient to warrant final acceptance. Based on their input, paper chairs made the final decisions for papers appearing in the TVCG issue. The IEEE VR scientific program also includes 13 short papers published in a separate report.", "keywords": "", "published_in": "2013 IEEE Virtual Reality (VR)", "publication_date": "March 2013", "citations": 0, "isbn": ["978-1-4673-4796-9", "978-1-4673-4795-2"], "doi": "10.1109/VR.2013.6549334", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6549334", "paper_url": "https://ieeexplore.ieee.org/document/6549334"}, {"title": "Proceedings IEEE Virtual Reality 2003", "authors": [], "abstract": "The following topics are dealt with: clusters and system design for virtual reality; large display systems and augmented reality; applications; VR in medicine; human performance in VR; multi-user virtual environments and collaboration; tracking and user interfaces; haptic devices and object manipulation.", "keywords": ["virtual reality", "computer displays", "computer applications", "medical computing", "user interfaces", "human factors", "groupware", "interactive devices", "clusters", "system design", "virtual reality", "large display systems", "augmented reality", "computer applications", "medicine", "human performance", "multi-user virtual environments", "collaboration", "tracking", "user interfaces", "haptic devices", "object manipulation"], "published_in": "IEEE Virtual Reality, 2003. Proceedings.", "publication_date": "2003", "citations": 0, "isbn": ["0-7695-1882-6"], "doi": "10.1109/VR.2003.1191113", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1191113", "paper_url": "https://ieeexplore.ieee.org/document/1191113"}, {"title": "VitaZ: Gamified Mixed Reality Multisensorial lnteractions", "authors": ["Dimitrios Bitzas", "Sokratis Zouras", "Agapi Chrysanthakopoulou", "Dimitrios Laskos", "Konstantinos Kalatzis", "Michail Pavlou", "Ioanna Balasi", "Konstantinos Moustakas"], "abstract": "This paper presents multiple Mixed Reality 3D interaction, manipulation and simulation techniques in the context of the 2019 3DUI contest of the IEEE VR conference. The proposed schemes provide smart, seamless transition from the real to the virtual world and demonstrate passive haptics, mid-air haptics, object manipulation and abstract entities (time) manipulation. All techniques are integrated in the context of a mixed reality escape-room or treasure-hunt game, where information from both the real and the virtual world is necessary to solve the puzzle. The paper concludes with a discussion on the extensibility and translational application of the approaches in practical problem solving.", "keywords": ["Human-centered computing", "Human computer interaction (HCI)", "Interaction paradigms", "Mixed / augmented reality", "Interaction devices", "Haptic devices"], "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8798133", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8798133", "paper_url": "https://ieeexplore.ieee.org/document/8798133"}, {"title": "Cover image credits", "authors": [], "abstract": "Front, Top-Right: Parameter Estimation Variance of the Single Point Active Alignment Method in Optical See-Through Head Mounted Display Calibration, Magnus Axholt, Martin A. Skoglund, Stephen D. O'Connell, Matthew D. Cooper, Stephen R. Ellis, Anders Ynnerman; Page 27. Front, Top-Left: A Virtual Reality System for the Simulation and Manipulation of Wireless Communication Networks, Tobias Rick, Anette von Kapri, Torsten Kuhlen; Page 111. Front, Bottom: A Soft Hand Model for Physically-based Manipulation of Virtual Objects, Jan Jacobs, Bernd Froehlich; Page 11. Back, Top-Left: Pseudo-Gustatory Display System Based on Cross-Modal Integration of Vision, Olfaction and Gustation, Takuji Narumi, Takashi Kajinami, Shinya Nishizaka, Tomohiro Tanikawa, Michitaka Hirose; Page 127. Back, Top-Right: Continual Surface-Based Multi-Projector Blending for Moving Objects, Peter Lincoln, Greg Welch, Henry Fuchs; Page 115. Back, Top-Middle: \"Tap, Squeeze and Stir\" the Virtual World: Touching the Different States of Matter Through 6DoF Haptic Interaction, Gabriel Cirio, Maud Marchal, Aur\u00e9lien Le Gentil, Anatole L\u00e9cuyer; Page 123. Back, Middle: Comparing Steering-Based Travel Techniques for Search Tasks in a CAVE, Anette von Kapri, Tobias Rick, Steven Feiner; Page 91.", "keywords": "", "published_in": "2011 IEEE Virtual Reality Conference", "publication_date": "March 2011", "citations": 0, "isbn": ["978-1-4577-0038-5", "978-1-4577-0039-2", "978-1-4577-0037-8"], "doi": "10.1109/VR.2011.5759506", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5759506", "paper_url": "https://ieeexplore.ieee.org/document/5759506"}, {"title": "Proceedings IEEE Virtual Reality 2002", "authors": [], "abstract": "The following topics are dealt with: virtual reality; networked virtual environments; distributed and parallel techniques; augmented reality; authoring, multimedia, and templates; user interfaces; perception in virtual environments; system design and software; tracking, segmentation, and manipulation; and applications.", "keywords": ["virtual reality", "virtual reality", "networked virtual environments", "augmented reality", "authoring", "multimedia", "user interfaces", "system design"], "published_in": "Proceedings IEEE Virtual Reality 2002", "publication_date": "2002", "citations": 0, "isbn": ["0-7695-1492-8"], "doi": "10.1109/VR.2002.996498", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=996498", "paper_url": "https://ieeexplore.ieee.org/document/996498"}, {"title": "Pyramid Escape: Design of Novel Passive Haptics Interactions for an Immersive and Modular Scenario", "authors": ["Hugo Brument", "Rebecca Fribourg", "Gerard Gallagher", "Thomas Howard", "Flavien L\u00e9cuyer", "Tiffany Luong", "Victor Mercado", "Etienne Peillard", "Xavier De Tinguy", "Maud Marchal"], "abstract": "In this paper, we present the design of ten different 3D user interactions using passive haptics and embedded in an escape game scenario in which users have to escape from a pyramid in a limited time. Our solution is innovative by its modularity, allowing interactions with virtual objects using tangible props manipulated either directly using the hands and feet or indirectly through a single prop held in the hand, in order to perform several interactions with the virtual environment (VE). We also propose a navigation technique based on the \u201cimpossible spaces\u201d design, allowing users to naturally walk through several overlapping rooms of the VE. All together, our different interaction techniques allow the users to solve several enigmas built into a challenging scenario inside a pyramid.", "keywords": "", "published_in": "2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)", "publication_date": "March 2019", "citations": 0, "isbn": ["978-1-7281-1377-7", "978-1-7281-1378-4"], "doi": "10.1109/VR.2019.8797848", "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8797848", "paper_url": "https://ieeexplore.ieee.org/document/8797848"}], "total_number_of_results": 229}